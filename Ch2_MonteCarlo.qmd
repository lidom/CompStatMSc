# Monte Carlo Method

### Literature {-}

In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:

* [Monte Carlo Statistical Methods](http://www.springer.com/us/book/9780387212395), Ch. 3, @RobertCasella1999 
* [Introducing Monte Carlo Methods with R](https://link.springer.com/book/10.1007/978-1-4419-1576-4), Ch. 3, @RobertCasella2010

Monte Carlo methods take advantage of the availability of:

  i. computer generated random variables 
  ii. the law of large numbers
  iii. the central limit theorem


**Terminology:** 

* **Monte Carlo Method**: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.
* **Monte Carlo Integration**: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a **univariate and multivariate  integral**. (Integrals are everywhere in statistics!)
* **Stochastic Simulation** (or **Monte Carlo Simulation**): The application of the Monte Carlo method.  


\


## Classical Monte Carlo Integration


The generic problem here is the evaluation of integrals. For instance, 
$$
\mathbb{E}_{f_{X}}\left(h(X)\right)=\mathbb{E}\left(h(X)\right)=\int_\mathcal{X}h(x)\,f_X(x)\,dx,
$${#eq-MeanIntegral}
where $\mathcal{X}$ denotes the domain of the random variable $X\in\mathcal{X}\subseteq\mathbb{R}^d,$ and where $h$ is some transformation function, e.g., 
$$
h(x)=x^2,\;\;h(x)=\ln(x),\;\;h(x)=x,\;\;\text{etc.}
$$ 

::: {.callout-tip}
Computing means means computing integrals. To stress that one computes the integral with respect to the distribution characterized by the density function $f_X,$ one can write 
$$
\mathbb{E}_{f_{X}}\left(h(X)\right)
$$
instead of 
$$
\mathbb{E}\left(h(X)\right).
$$
We will use this notation more often below. 
:::

Often, analytical solutions for integrals such as in @eq-MeanIntegral are not readily available and one needs to use some numerical approaches/computational. Given our previous developments, it is kind of natural to propose using a realization 
$$
x_1,\dots,x_n
$$ 
from a (pseudo) random sample 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X
$$ 
to approximate the integral in @eq-MeanIntegral using the empirical mean
$$
\mathbb{E}\left(h(X)\right)\approx\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(x_i).
$$
By the [Strong Law of Large Numbers (SLLN)](http://www.statlect.com/asylln1.htm) we know that the empirical mean $\bar{h}_n$ converges almost surely (a.s.), and thus also "in probabiliuty" to the desired limit $\mathbb{E}\left(h(X)\right)$ as the sample size $n$ becomes large, i.e., as $n\to\infty$. Prerequisites for the SLLN: 

1. $h(X)$ has finite first moment, i.e., $\mathbb{E}\left(h(X)\right)<\infty$ and 
2. $\bar{h}_n$ is constructed from a random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X.$

As we can use the computer to produce realizations from the i.i.d. sample $X_1,\dots,X_n$, we can in principle choose an arbitrary **large sample** size $n$ such that $\bar{h}_n$ can, in principle, be **arbitrarily close** to the desired limit $\mathbb{E}\left(h(X)\right)$. 


**Though,** ... 

* ... which sample size $n$ is large enough? 
* or "equivalently", how fast converges $\bar{h}_n$ to the desired limit $\mathbb{E}\left(h(X)\right)$?


### Speed of Convergence {-}

OK, we know now that $\bar{h}_n$ reaches its limit (here in the "almost surely" sense, but likewise in the "in probability" case) as $n\to\infty$ under some rather loose conditions on the random sample $X_1,\dots,X_n$. 

If we are willing to additionally assume that $h(X)$ has finite *second* moments, i.e.
$$
\mathbb{E}(h(X)^2)<\infty,
$$ 
then we can additionally say something about **how fast** 
$$
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(X_i)\to_{p} \mathbb{E}(h(X)).
$$ 

The **speed of convergence** of the stochastic sequence 
$$
\{\bar{h}_n\}_{n=1,2,\dots}= \bar{h}_1,\;\bar{h}_2,\;\;\bar{h}_3,\;\dots 
$$ 
to its limit $\mathbb{E}(h(X))$ can be quantified by the rate at which the standard error 
$$
\operatorname{SE}\left(\bar{h}_n\right)=\sqrt{\mathbb{V}\left(\bar{h}_n\right)}
$$
converges to zero as $n\to\infty$. 

::: {.callout-tip}
We think of $\{\bar{h}_n\}_{n=1,2,\dots}$ as the sequence of **random variables**  
$$
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h({\color{red}{X_{i}}})
$$ 
with ${\color{red}{X_1}},\dots,{\color{red}{X_n}}\overset{\text{i.i.d.}}{\sim}f_X.$
:::


::: {.callout-tip}
Note that assuming finite second moments $\mathbb{E}(h(X)^2)<\infty$ is equivalent to assuming finite variance $\mathbb{V}\left(h(X)\right)<\infty,$ since
$$
\mathbb{V}\left(h(X)\right) = \mathbb{E}(h(X)^2) - \left(\mathbb{E}(h(X))\right)^2,
$$
and since if higher moments, like $\mathbb{E}(h(X)^2),$ are finite, also the lower moments, like $\mathbb{E}(h(X)),$ are finite. 
:::

The standard error of $\bar{h}_n$ is just the square root of the variance of $\bar{h}_n.$ The variance of $\bar{h}_n$ is given by 
$$
\begin{align*}
\mathbb{V}\left(\bar{h}_n\right)
&=\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\right) \\[2ex]
&=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n h(X_i)\right) \\[2ex]
&=\frac{n}{n^2}\mathbb{V}\left(h(X_1)\right)\quad \text{(since i.i.d.)} \\[2ex]
&=\frac{1}{n}  \mathbb{V}\left(h(X_1)\right)
\end{align*}
$$
The square root of $\mathbb{V}\left(h(X_1)\right)$ equals some finite, positive constant $0<\mathtt{const}<\infty$,  
$$
\mathtt{const}=\sqrt{\mathbb{V}\left(h(X_1)\right)}
$$ 
such that
$$
\sqrt{\mathbb{V}\left(\bar{h}_n\right)}=n^{-1/2}\mathtt{const}%\propto n^{-1/2}.
$$
I.e., the speed of convergence (or rate) of the stochastic sequence $\{\bar{h}_n\}$ is proportional to the deterministic sequence $\{n^{-1/2}\}.$ 



::: {.callout-note}

Even if we would not know the value of $\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)},$ we know now that the improvement from $n=10$ to $n=100$ will be *much* higher than from $n=110$ to $n=200$. In practice, a typical choice is $n=10,000;$ for moderate standard errors this choice will guarantee a very good approximation.
:::


### Limit Distribution {-}

Besides the speed of convergence of $\{\bar{h}_n\}_{n=1,2,\dots}$ for $n\to\infty,$ we can also say something about the distribution of the random variable $\bar{h}_n$ for large sample sizes $n.$


We can estimate the variance of the estimator $\mathbb{V}\left(\bar{h}_n\right)$ by its empirical version
$$
v_n^2=\frac{1}{n}\sum_{i=1}^n\left(h(x_i)-\bar{h}_n\right)^2,
$$
where by the SLLN, which also implies convergence in probability, 
$$
v_n^2\to_{p}\mathbb{V}\left(h(X)\right),\quad n\to\infty.
$$
<!--
I.e., after some rewriting, we have that: 
$$
\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{v_n}}\to_{\text{a.s.}}
\sqrt{m}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right).
$$
-->
Then, 
by the 
[Continuous Mapping Theorem (CMT)](https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#cmt), the 
[Central Limit Theorem (CLT)](http://www.statlect.com/central_limit_theorem.htm), and 
[Slutsky's theorem](https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#slutsky), we have that 
$$
\sqrt{n}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{v_n}\right)\to_d \mathcal{N}(0,1),\quad n\to\infty. 
$$
<!-- Note that the the above sequence $\{\sqrt{m}\}$ **just hinders** the convergence of the sequence $\bar{h}_n - \mathbb{E}\left(h(X)\right)\to_{a.s.}0$ such that the quotient converges to a "stable" distribution.  -->


The above result can now be used for the construction of (asymptotically valid) **convergence tests** and **confidence intervals** with respect to $\bar{h}_n$, since for large $n$ 
$$
\bar{h}_n\,\overset{d}{\approx}\mathcal{N}\left(\mathbb{E}\left(h(X)\right),\frac{\mathbb{V}\left(h(X)\right)}{n}\right).
$$

Since we can use the computer to generate realizations of the i.i.d. sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with $X\sim f_X,$ we can easily approximate the mean 
$$
\mathbb{E}\left(h(X)\right)\approx \bar{h}_n
$$ 
and the variance 
$$
\mathbb{V}\left(h(X)\right)\approx v_n^2
$$ 
with arbitrary accuracy as $n\to\infty$ (justification: strong/weak law of large numbers).




::: {#exm-MCInt1}

## A first Monte Carlo Integration

Let's say we want to compute the integral 
$$
\int_0^1h(x)dx
$$
with 
$$
h(x)=\left(\cos(50\,x)+\sin(20\,x)\right)^2
$$ 
over $x\in[0,1].$ Although this integral could also be computed analytically, it is a good first test case. The following code computes the analytic result that $\int_0^1h(x)dx = 0.9652009.$

```{r}
## install.packages("mosaicCalc")
suppressPackageStartupMessages(library("mosaicCalc"))

## Symbolic (= analytic) integration 
F <- antiD( (cos(50*x)+sin(20*x))^2 ~ x)

F(1) - F(0)
```

@fig-MCInt1_1 shows the graph of the function $h$.

```{r}
#| echo: true
#| eval: true
#| label: fig-MCInt1_1
#| fig-cap: Function $h$ of @exm-MCInt1. 

h_fun <- function(x){
  result <- (cos(50*x)+sin(20*x))^2
  return(result)
}

xx  <- seq(from=0, to=1, len=500)
plot(x = xx, 
     y = h_fun(xx), 
     type="l", 
     main="Function h", 
     xlab="x", ylab="h(x)")
```

To approximate the integral 
$$
\int_0^1 h(x)dx
$$ 
using Monte Carlo integration, we can use that
$$
\begin{align*}
\int_0^1 h(x)dx 
&=\int_0^11\cdot h(x)dx \\[2ex]
&=\int_0^1f_{\mathcal{U}\text{[0,1]}}(x)\cdot h(x)dx \\[2ex]
&= \mathbb{E}_{f_{\mathcal{U}\text{[0,1]}}}(h(X)),
\end{align*}
$$ 
where $f_{\mathcal{U}\text{[0,1]}}$ denotes the density function of the standard uniform distribution $\mathcal{U}\text{[0,1]}.$ 

Thus, to compute $\int_0^1 h(x)dx$ we generate a realization $(u_1,\dots,u_n)$ from the random sample $U_1,\dots,U_n\sim[0,1]$ and approximate 
$$
\int_0^1 h(x)dx\approx \bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(u_i).
$$ 

In order to assess how good this approximation is, we need to consider the stochastic properties of the random variable 
$$
\bar{h}_n = \frac{1}{n}\sum_{i=1}^n h(U_i).
$$ 
This is done using the above (review of) results on the limit distribution of the sample mean $\bar{h}_n$ which allows us to construct an approximate $95\%$ confidence interval, since for large $n$ 
$$
\begin{align*}
\operatorname{CI}^{95\%}_n
=&\left[\bar{h}_n - z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}, \bar{h}_n + z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}\right]\\[2ex]
\approx&
\left[\bar{h}_n - z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}, \bar{h}_n + z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}\right],
\end{align*}
$$
where $z_{1-\alpha/2}\approx 1.96$ denotes the $(1-\alpha/2)$-quantile of $\mathcal{N}(0,1),$ $v_n^2=n^{-1}\sum_{i=1}^n(h(u_i)-\bar{h}_n)^2,$ and where
$$
P\left(\int_0^1 h(x)dx  \in \operatorname{CI}^{95\%}_n \right) \to 0.95,\quad n \to\infty,
$$
by the CLT. 

@fig-MCInt1_2 shows *one* realization of the stochastic sequence 
$$
\bar{h}_1,\dots,\bar{h}_n
$$ 
with $n=10000$, where the realized value of $\bar{h}_n$ is $0.966$. This compares favorably with the with the exact value of $\int_0^1h(x)dx = 0.9652009.$


```{r}
#| echo: true
#| eval: true
#| label: fig-MCInt1_2
#| fig-cap: One realization of the stochastic sequence $\bar{h}_1,\dots,\bar{h}_n$ with $n=10000$, where the realized value of $\bar{h}_{n=10000}$ is $0.966$. The blue band shows the point-wise (i.e. for each given sample size $n$) confidence intervals $\operatorname{CI}^{95\%}_n.$
library("scales")
# h(x):
h_fun <- function(x){
  result <- (cos(50*x)+sin(20*x))^2
  return(result)
}

# sample size
n <- 10000

# Generate sample of uniforms
set.seed(321)
u_vec <- runif(n=n)

# Approximation of the integral 
h_bar_n <- cumsum(h_fun(u_vec))/c(1:n)

# True value:
true.value <- 0.9652009

# 95% CI
# Standard error of the estimator using the "algebraic 
# formula" for the variance (german: 'verschiebungssatz')
st.error_n <-  sqrt((cumsum(h_fun(u_vec)^2)/(1:n) - 
                     cumsum(h_fun(u_vec))^2/(1:n)^2))

CI_u       <-  h_bar_n + 1.96 * st.error_n / sqrt(1:n)
CI_l       <-  h_bar_n - 1.96 * st.error_n / sqrt(1:n)

plot(x = c(1:n), y = h_bar_n, type="n", 
     ylim=c(0.7,1.2), 
     xlab = "n", 
     ylab = "")
polygon(x = c(1:n, rev(1:n)), 
        y = c(CI_u, rev(CI_l)), 
        col    = alpha("blue", 0.5), 
        border = alpha("blue", 0.5))
lines(x = c(1:n), 
      y = h_bar_n, type="l")
lines(x = c(1:n), 
      y = rep(true.value, n), type="l", col="red")
##
legend("topright", 
       legend = c(expression(bar(h)[n]), 
                  "True Value", "95% CI"), 
       lty   = c(1,1,0), pch=c(22,22,22), pt.cex=c(0,0,2),
       pt.bg = c("black", "red", alpha("blue", 0.5)), 
       col   =   c("black", "red", alpha("blue", 0.5)))
```

:::

::: {.callout-note}

The approach of @exm-MCInt1 can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency/accuracy through numerical methods (e.g., [Riemann Sum](https://en.wikipedia.org/wiki/Riemann_sum), [Trapezoidal Rule](https://en.wikipedia.org/wiki/Trapezoidal_rule), [Simpson's Rule](https://en.wikipedia.org/wiki/Simpson%27s_rule), etc.) in dimensions 1 or 2; see the following code example: 
```{r}
numericalIntegration <- integrate(f     = h_fun, 
                                  lower = 0, 
                                  upper = 1)
numericalIntegration
```

However, the Monte Carlo integration approach is particularly useful for approximating integrals over higher dimensional sets $\mathcal{X}\subseteq\mathbb{R}^d.$ 
::: 

::: {#exm-ApproxNormTables}

## Approximation of Normal Distribution Tables

A possible way to construct normal distribution tables is to use MC simulations. 

Generate a realization $(x_1,\dots,x_n)$ from an i.i.d. standard normal random sample $(X_1,\dots,X_n)$, e.g., using the Box-Muller algorithm. 

Then, the approximation of the standard normal cdf
$$
\Phi(t)=\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy
$$
by the Monte Carlo method is thus
$$
\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n 1_{(x_i\leq t)}.
$$
The corresponding random variable  
$$
\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n1_{(X_i\leq t)}
$$ 
has (exact) variance 
$$
\mathbb{V}(\hat{\Phi}_n(t))=\frac{\Phi(t)(1-\Phi(t))}{n},
$$
since the single random variables $1_{(X_i\leq t)}$ are independent Bernoulli distributed random variables with success probability $p=\Phi(t),$ and thus $\hat{\Phi}_n(t)$ is a binomial distributed random variables with parameters $n$ and $p=\Phi(t).$ 

For values of $t$ around $t=0$, the variance is thus approximately $1/4n,$
$$
\mathbb{V}(\hat{\Phi}_n(t))\approx \frac{\Phi(0)(1-\Phi(0))}{n}=\frac{0.5^2}{n},\quad t\approx 0.
$$  

To achieve a precision of **four decimals** by means of a $99.9\%$ confidence interval, the approximation requires on average $n\approx 10^8$ simulations. 

The table below gives the evolution of this approximation for several values of $t$ and shows a very accurate evaluation for $n=10^8$. 




$n$  |t=0.0|t=0.67 |t=0.84 |t=1.28 |t=1.65 |t=2.32 |t=2.58 |t=3.09 |t=3.72 
-----|-------|-------|-------|-------|-------|-------|-------|-------|---------
10^2 |0.485  |0.74   |0.77   |0.9    |0.945  |0.985  |0.995  |1      |1      
10^3 |0.4925 |0.7455 |0.801  |0.902  |0.9425 |0.9885 |0.9955 |0.9985 |1      
10^4 |0.4962 |0.7425 |0.7941 |0.9    |0.9498 |0.9896 |0.995  |0.999  |0.9999 
10^5 |0.4995 |0.7489 |0.7993 |0.9003 |0.9498 |0.9898 |0.995  |0.9989 |0.9999 
10^6 |0.5001 |0.7497 |0.8    |0.9002 |0.9502 |0.99   |0.995  |0.999  |0.9999 
10^7 |0.5002 |0.7499 |0.8    |0.9001 |0.9501 |0.99   |0.995  |0.999  |0.9999 
10^8 |0.5    |0.75   |0.8    |0.9    |0.95   |0.99   |0.995  |0.999  |0.9999 



<!-- $$
\begin{array}{cccccccccc}
\hline
n   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\
\hline
10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\
10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\
10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\
10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\
10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\
10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\
10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\
\end{array}
$$ -->




::: {.callout-note}
* To achieve a precision of **two decimals** by means of a $99.9\%$  confidence interval, already $n=10^4$ leads to satisfactory results. 
* Note that **greater accuracy is achieved in the tails** and that more efficient simulation methods could be used (e.g., Importance Sampling). 
:::

:::


## Importance Sampling

Importance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it's refereed to as a **variance reduction** technique. This variance reduction is achieved by weighting functions, so-called **importance functions**. 

As in the case of Monte Carlo integration the focus lies on evaluating the integral
$$
\mathbb{E}_{f_X}(h(X))=\int_\mathcal{X}h(x)f(x)\,dx.
$$

However, it turns out that the above approach, i.e., sampling from $f$ is often suboptimal. 

Observe that the value of the above integral can be represented by infinitely many alternative choices of the triplet 
$$
(\mathcal{X}, h, f_X).
$$ 
Therefore, the search for an optimal estimator should encompass all these possible representations.

Let's illustrate this with a simple example.

::: {#exm-CauchTailProb}

# Cauchy Tail Probability

This example is from @Ripley_2009.

Suppose that the quantity of interest is the probability, say $p$, that a [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution) $\mathcal{C}(0,1)$-distributed random variable is larger than $2$, i.e.
$$
p=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx.
$${#eq-CauchIntegral}
This is a nice example. One the one hand, it allows us to showcase possibilities to improve efficiency of Monte Carlo integration. On the other hand, we know already the result of @eq-CauchIntegral; namely, $p=0.1476$
<!-- $$
\begin{align*}
p
&=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx\\[2ex]
&=\frac{1}{\pi}\int_{2}^{+\infty}\frac{1}{1+x^2}\,dx\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left[\frac{1}{\tan(x)}\right]_2^b\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left(\frac{1}{\tan(b)}-\frac{1}{\tan(2)}\right)\\[2ex]
\end{align*}
$$ -->
```{r}
round(1 - pcauchy(2), 4)
```


**1. Approach: The Naive Approach** 

If $p$ is approximated through the empirical mean
$$
\hat{p}_{1}=\frac{1}{n}\sum_{i=1}^n 1_{(X_i>2)}
$$
of a random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{C}(0,1),$ then $\hat{p}$ is a binomial distributed random variable with parameters $n$ and $p.$ The variance is thus 
$$
\begin{align*}
\mathbb{V}(\hat{p}_{1})
&=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(X_i>2)}\right)\\[2ex]
&=\frac{n}{n^2}\mathbb{V}\left(1_{(X_1>2)}\right)\quad\text{(i.i.d.)}\\[2ex]
&=\frac{p(1-p)}{n},
\end{align*}
$$
which is equal to $`r round(0.1476 * (1-0.1476),4)`/n,$ since we know that $p=0.1476$. 




**2. Approach: Accounting for Symmetry**

In this approach, we use the "adjusting Screws" $\mathcal{X}$ and $h.$

We can achieve a **more efficient estimator** (i.e., an estimator with lower variance for a given same sample size $n$) if we take into account the symmetric nature of $\mathcal{C}(0,1).$ Obviously, our target integral can be equivalently written as
$$
p=\frac{1}{2}\left(\int_{-\infty}^{-2}\frac{1}{\pi(1+x^2)}\,dx + \int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx \right).
$$
This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean:
$$
\begin{align*}
\hat{p}_{2}
&=\frac{1}{2}\left(\frac{1}{n}\sum_{i=1}^n1_{(X_i < -2)}+ \frac{1}{n}\sum_{i=1}^n 1_{(X_i > 2)}\right)\\[2ex]
&=\frac{1}{2n}\sum_{i=1}^n 1_{(|X_i|>2)}.
\end{align*}
$$
The variance of this new estimator,
$$
\begin{align*}
\mathbb{V}(\hat{p}_{2})
&=\frac{1}{4n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(|X_i|>2)}\right)\\[2ex]
&=\frac{2p(1-2p)}{4n},
\end{align*}
$$
which is equal to $`r round( (2*0.1476 * (1- 2*0.1476) )/4, 4)`/n,$ since we know that $p=0.1476$. This is clearly lower than in the naive approach, where we had $`r round(0.1476 * (1-0.1476),4)`/n.$




**3. Approach:**

In this approach, we use all the "adjusting screws" $\mathcal{X}$, $h$, and $f.$

The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, $[2,+\infty)$, which are in some sense irrelevant for the approximation of $p$. This motivates the following reformulation of $p$: 

By symmetry of $f$:
$$
\begin{align*}
\frac{1}{2} & =\int_{0}^2\frac{1}{\pi(1+x^2)}dx + \underbrace{\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}dx}_{=p}\\[2ex]
\Leftrightarrow \; p& =\frac{1}{2}-\int_{0}^2\frac{1}{\pi(1+x^2)}dx.
\end{align*}
$$
Furthermore, we can re-arrange the last integral a bit such that 
$$
\begin{align*}
 &\int_{0}^2\;\left(\frac{1}{2}\cdot 2\right)\;\frac{1}{\pi(1+x^2)}\,dx \\[2ex]
=&\int_{0}^2\;\underbrace{\frac{1}{2}}_{f_{\mathcal{U}[0,2]}}\;\underbrace{\frac{2}{\pi(1+x^2)}}_{=h(x)}\,dx \\[2ex]
=&\mathbb{E}_{f_{\mathcal{U}[0,2]}}(h(U)),
\end{align*}
$$
where $U\sim\mathcal{U}[0,2].$

Therefore a new alternative method for evaluating $p$ is:
$$
\hat{p}_{3}=\frac{1}{2} - \frac{1}{m}\sum_{i=1}n h(U_i),\quad\text{where}\quad U_i\sim\mathrm{Unif}[0,2].
$$
Using integration by parts, it can be shown that $\mathbb{V}(\hat p_3)=0.0285/n$. (Compare this to the former results: $\mathbb{V}(\hat{p}_{2})=0.0525/n$ and $\mathbb{V}(\hat{p}_{1})=0.1275/n$.)

::: 


**A More General Point of View:**

The idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating 
$$
\theta=\mathbb{E}_f(h(X))=\int h(x)f(x)dx.
$$

Some outcomes of $X\sim f$ may be more important than others in determining $\theta$ and we wish to select such values more frequently. 

For instance, if $\theta$ denotes the probability of the occurrence of a very rare event, then the only way to estimate $\theta$ at all accurately may be to produce the rare events more frequently. 

To achieve this, we can simulate a model which gives pdf $g$ to $X$ instead of the correct pdf $f$, where both pdfs need to be known. This can be easily done, since
$$
\theta=\mathbb{E}_f(h(X))=\int h(x)\left(\frac{g(x)}{g(x)}\right)\;f(x)dx=
\int \underbrace{\left(h(x)\frac{f(x)}{g(x)}\right)}_{=\psi(x)}\;g(x)dx=
\int \psi(x)\;g(x)dx=
\mathbb{E}_g(\psi(X)).
$$

This leads to the following unbiased estimator for $\theta$ based on sampling from $g$:
$$
\hat{\theta}_g=\frac{1}{n}\sum_{i=1}^n\psi(X_i)\quad\text{with}\quad X_i\sim g,
$$
which is a weighted mean of the $h(X_i)$ with weights inversely proportional to the "**selection factor**" $\frac{g(X_i)}{f(X_i)}$.
<!-- 
For appropriately chosen pdfs f and g: 
The *selection factor* refers to "how more likely is it to select a 'rare event'?".
The inverse weight re-scales these 'too often' chosen rare events. 
-->


For the variance of the estimator $\hat{\theta}_g$ we have
$$
\mathbb{V}(\hat{\theta}_g)=\frac{1}{n}\mathbb{V}(\psi(X_i))=
\frac{1}{n}\int\left(\psi(x)-\theta\right)^2g(x)dx=
\frac{1}{n}\int\left(\frac{h(x)\,f(x)}{g(x)}-\theta\right)^2g(x)dx,
$$
which, depending on the choice of $g(.)$, can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean.
<!-- 
$$
\mathbb{V}(\hat{\theta}_{\text{naive}})=
\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n 1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=
\frac{1}{n}\mathbb{V}\left(1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=\frac{\theta(1-\theta)}{n}.
$$ 
-->

\
\


*Minimum Variance Theorem*

:    The **importance function** $g(.)$ which minimizes the variance $\mathbb{V}(\psi(X_i))$, and therefore the variance $\mathbb{V}(\hat{\theta}_g)$,      is given by 
     $$
     g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
     $$ 


**Proof:** Done in the lecture. 

\


Though, this result is rather formal (in the sense of "impractical"), since, e.g., if $h(x)>0$ then $g^\ast$ requires us to know $\int h(z)f(z)dz$, which is just the integral of interest! 

**Remarks:**

The above minimum variance result is still useful:

*  It tells us that a good choice of $g(x)$ shall mimic the shape of $|h(x)|f(x)$, since the optimal $g^\ast(x)\propto |h(x)|f(x)$.
*  Furthermore, $g(x)$ should be chosen such that it has a thicker tail than $f(x)$, since the variance $\mathbb{V}(\hat{\theta}_g)$ crucially depends on the quotient $f(x)/g(x)$ which would "explode" for $g(x)\approx 0$.


\


Let's apply our new insights to the above example on the Cauchy tail probability $p$. 

**Example: Cauchy Tail Probability (cont.)**


Above we had: 

1. $f(x)=\frac{1}{\pi(1+x^2)}$, the pdf of $\mathrm{C}(0,1)$ and
2. $h(x)=1_{(x>2)}$, i.e., here $|h(x)|=h(x)$.

Therefore
$$
p=\mathbb{E}_f(h(X))=\int h(x)f(x)dx=\int_{2}^{\infty}f(x)dx=\int_{2}^{\infty}\underbrace{\frac{f(x)}{g(x)}}_{=\psi(x)}\;g(x)dx=\mathbb{E}_g(\psi(X)),
$$
where the $h$ function is absorbed by the formulation of the definite integral.

A possibly good (and simple) choice of $g$ is, e.g., $g(x)=2/(x^2)$, since this function:

* "closely" matches $h(x)f(x)$ and 
* $g$ has thicker tails than $f$. 


```{r, fig.width=7, fig.height=4, fig.align='center', echo=FALSE}
# g(x):
g_fun <- function(x){2/(x^2)}
# 
xx    <- seq(from= 0, to=15, length.out = 500)
# plot
plot(x = xx, dcauchy(xx, location = 0, scale = 1), 
     type="l", xlab="", ylab="", ylim=c(0,0.55), axes = FALSE)
axis(side = 1, at=c(0,2,5,10,15)); axis(side = 2)
# add graph of g(x)
lines(x = xx[xx>2], g_fun(xx[xx>2]), col="red")
# region of interest selected by h(x): x>2
abline(v=2, lty=2)
# legend
legend("topright", legend = c("pdf-Cauchy(0,1)", expression(g(x)==2/(x^2))), lty=c(1,1),
     col = c("black", "red"))
```

\

**Caution:** It is not straight forward to directly sample from $g$, therefore we need some further steps:

\

The choice of $g$ leads to 
$$
p=\mathbb{E}_g(\psi(X))=
\int_{2}^{+\infty}\left(\frac{x^2}{2\,\pi(1+x^2)}\right)\,\frac{2}{x^2}\,dx=
\int_{2}^{+\infty}\left(\frac{1}{\pi(1+x^{-2})}\right)\,x^{-2}\,dx.
$$

\


Now we can apply some additional (rather case-specific) re-arrangements:

Integration by substitution (substituting $u=x^{-1}$) yields:
$$
p=\int_{0}^{1/2}\frac{1}{\pi(1+u^2)}du.
$$
Again, we can re-arrange the last integral a bit such that
$$
p=\int_{0}^{1/2}\underbrace{2}_{f_{\mathrm{Unif}[0,1/2]}}\;\underbrace{\frac{1}{2\,\pi(1+u^2)}}_{=h(u)}\,du=\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
Therefore, we have a final fourth version of the estimator of $p$:
$$
\hat{p}_4=\sum_{i=1}n h(U_i),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2] 
$$
and $h(u)=1/(2\pi(1+u^2))$. 

The variance of $\hat{p}_4$ is $(\mathbb{E}(h(U)^2)-\mathbb{E}(h(U))^2)/n$ and an integration by parts shows that $\mathbb{V}(\hat{p}_4)=0.95\cdot 10^{-4}/n$. Compare this to the former results: $\mathbb{V}(\hat p_3)=0.0285/n$, $\mathbb{V}(\hat{p}_{2})=0.0525/n$ and $\mathbb{V}(\hat{p}_{1})=0.1275/n$. The variance of $\hat{p}_4$ is by a factor of $10^{-3}$ lower than the variance of the original $\hat{p}_1$.


<!--
**Version 2:** It can be shown that (see [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))
$$
p=\int_0^{1/2}\frac{y^{-2}}{\pi(1+y^{-2})}dy,
$$
where this integral can also be seen as the expectation of 
$$
\frac{1}{4}h(U)=\frac{1}{2\pi(1+U^2)},\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
-->




<!--
## The 'Real Need' of Monte Carlo Simulation 

**Monte Carlo Simulation applied to Hypothesis Tests**

* **Problem:** Very often, statistical test procedures (or estimators) rely on asymptotic arguments. Asymptotic arguments ease the live of a statistician. In practice, however, we never have something like a diverging sample size of $n\to\infty$, but need to deal with a finite sample size $n$. All we can hope for is that the asymptotic results (e.g., on the level of significance of a test statistic and its power) are good approximations to the finite $n$ case. 

* **Solution:** Monte Carlo simulation the classical tool to investigate the finite $n$ performance of statistical test procedures (or estimators). It helps to answer questions like: How good are the asymptotic results given finite sample size scenarios of $n=100$, $n=500$, etc.

For instance, the **likelihood ratio (LR)** test statistic  
$$
-2\,\log\left[\ell(\hat\theta|x)/\ell(\hat\theta_0|x)\right]=-2\,\left\{\log\ell(\hat\theta|x)-\log\ell(\hat\theta_0|x)\right\}\to_d\chi^2_r
$$
is distributed as $\chi^2_r$ generally only in the limiting case as $n\to\infty$; and under some regularity constraints on the likelihood function. In the formula above $\ell(\theta|x)$ denotes the likelihood function, $\hat\theta\in\mathbb{R}^k$ is the estimated (via maximum likelihood) parameter vector from the unconstrained model and $\hat\theta_0\in\mathbb{R}^k$ is the estimated parameter vector from the constrained model with $r\leq k$ restrictions.

\

**Example: Contingency Tables**

The following table gives the results of a study comparing radiation therapy with surgery in treating cancer of the larynx. 


$$
\begin{array}{c|cc|c}
                   &  \text{Cancer}  &  \text{Cancer not}&           \\
                   &  \text{Controlled}  &  \text{Controlled}&           \\
\hline                   
\text{Surgery}     &  y_{11}=21      &   y_{12}=2        &  n_{1.}=23\\
\text{Radiation}   &  y_{21}=15      &   y_{22}=3        &  n_{2.}=18\\
\hline
                   &  n_{.1}=36      &   n_{.2}=5        &  n=41
\end{array}
$$

\

Let's condition on a fix number of total observations $n$. Then the random vector $(Y_{11},Y_{12},Y_{21},Y_{22})^\top$ comes from a [multinomial distribiton](https://en.wikipedia.org/wiki/Multinomial_distribution) with 4 cells and cell probabilities 
$$
p=(p_{11}, p_{12}, p_{21}, p_{22}),\quad\text{with}\quad\sum_{ij}p_{ij}=1,
$$
that is, 
$$
(Y_{11},Y_{12},Y_{21},Y_{22})^\top\sim\mathcal{M}_4(n, p).
$$
With $y_{ij}$ denoting the number of realizations in cell $ij$, the likelihood function can be written as 
$$
\ell(p|y)\propto\prod_{ij}p_{ij}^{y_{ij}},
$$
where the 4-dimensional parameter space can be displayed as following:
$$
\begin{array}{cc|c}
p_{11} & p_{12} & p_{1.}\\
p_{21} & p_{22} & p_{2.}\\
\hline
p_{.1} & p_{.2} & 1\\
\end{array}
$$



**Null Hypothesis:** The null hypothesis to be tested is one of independence, which is to say that the surgery treatment has no bearing on the control of cancer. Translated into a parameter statement this means
$$\text{H}_0: p_{11}=p_{1.}\,p_{.1}\quad\text{against}\quad\text{H}_1: p_{11}\neq p_{1.}\,p_{.1}.$$


The likelihood ratio statistic for testing this hypothesis is 
$$
\lambda=\frac{\max_{p\text{ s.t. }p_{11}=p_{1.}p_{.1}}\ell(p|y)}{\max_{p}\ell(p|y)}.
$$
It is "straightforward" to show that the denominator maximum is attained at:
$$
\hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all}\quad ij. 
$$
and the numerator maximum at
$$
\hat{p}_{11}=\hat{p}_{1.}\hat{p}_{.1}\quad\text{with}\quad \hat{p}_{1.}=\frac{n_{1.}}{n}\quad\text{and}\quad\hat{p}_{2.}=\frac{n_{2.}}{n},
$$
$$
\text{and} \quad \hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all other}\quad ij. 
$$

As mentioned above, under H$_0$, $-2\log \lambda$ is asymptotically distributed as $\chi^2_1$. However, with only $n=42$ observations, the asymptotics do not necessarily apply. One alternative is to use devise a **Monte Carlo experiment** to simulate the null distribution of $-2\log \lambda$ or equivalently of $\lambda$ in order to obtain a cutoff point for a hypothesis test. A more sophisticated approach is that of [Mehta at el. (2000)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10473906).  

**Description of the Procedure:**

* Let's denote the finite $n$ null distribution of $\lambda$ by $f_{0,n}(.)$. As we are interested in an $\alpha$ level test, we need to specify $\alpha$ (e.g., $\alpha=0.05$) and to solve the following integral for the $1-\alpha$ quantile $\lambda_\alpha$:
$$
\int_0^{\lambda_\alpha}f_{0,n}(u)du=1-\alpha.
$$

* The standard Monte Carlo approach to this problem is to generate random variables $\lambda_k\sim f_{0,n}$, $k=1,\dots,m$, then order the sample 
$$
\lambda_{(1)}\leq \lambda_{(2)}\leq\dots\leq \lambda_{(m)}
$$
and finally calculate the empirical $1-\alpha$ quantile $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$, where $\lfloor x \rfloor$ is the next lower integer to $x$, e.g., $\lfloor 2.9 \rfloor=2$.

* Similarly to the above integration example which builds on the SLLN, the central idea here is to use the so-to-say SLLN for quantiles:
$$
\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}\to_{a.s.} \lambda_{\alpha}\quad\text{as}\quad m \to\infty;
$$
see, e.g., the classical book "Approximation Theorems of Mathematical Statistics" of R. Serfling Ch. 2.3.1. As a computer is doing this job for us, we can in principle choose an arbitrary large $n$ such that the above approximation of $\lambda_{\alpha}$ by $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$ can be arbitrarily good. 

To run the 

```{r}
set.seed(123)
#
p_init <- runif(4)
#
p_11   <- p_init[1]
p_12   <- p_init[2]
p_21   <- p_init[3]
p_22   <- p_init[4]
#
p_1.   <- p_11 + p_12
p_.1   <- p_11 + p_21
#
p_11   <- p_1. * p_.1

# probabilities under H0:
p_0 <- c(p_11, p_21, p_12, p_22)


Y_vec <- rmultinom(n=1, size=4, prob=p_0)

```
-->

<!-- 
**Example: LR Test**

Let's assume we want to test the following regression model 
$$
Y_t=\beta_0 + \beta_1 t +\varepsilon_{t}
$$
against the constrained model without a time trend:
$$
Y_t=\beta_0 + \varepsilon_{t},
$$
where $\varepsilon_{t}\sim N(0,\sigma_\varepsilon^2)$ with $0<\varepsilon^2<\infty$.


```{r}
LL_fun <- function(beta0, beta1, sigma) {
      # residuals
      res_vec   <-  y - x * beta1 - beta0
      # log-transformed probabilities of observing the vector of residuals res_vec: 
      Log_probs <- dnorm(x = R, mean = 0, sd = sigma, log = TRUE)
      # 
      result <- - sum(Log_probs)
      return(result)
}
```
-->


## References {-}