# Monte Carlo Integration

### Literature {-}

In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:

* [Monte Carlo Statistical Methods](http://www.springer.com/us/book/9780387212395), Ch. 3, @RobertCasella1999 
* [Introducing Monte Carlo Methods with R](https://link.springer.com/book/10.1007/978-1-4419-1576-4), Ch. 3, @RobertCasella2010
* [Numerical Methods in Economics](https://mitpress.mit.edu/9780262547741/numerical-methods-in-economics/), Ch. 8.2 Monte Carlo Integration, @Kenneth_Judd_Book_1998

Monte Carlo methods take advantage of the availability of:

  i. computer generated random variables 
  ii. the law of large numbers
  iii. the central limit theorem


**Terminology:** 

* **Monte Carlo Method**: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.
* **Monte Carlo Integration**: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a **univariate and multivariate  integral**. (Integrals are everywhere in statistics!)
* **Stochastic Simulation** (or **Monte Carlo Simulation**): The application of the Monte Carlo method.  


This chapter is about **Monte Carlo Integration** which is a stochastic alternative to deterministic numerical integration methods such as numerical quadrature. 

@fig-MCIntPublishedExample shows a screenshot of a published example (see @Bourreau2021market), where the authors use Monte Carlo simulation to solve a hard-to-compute integral. 


![Market share function in @Bourreau2021market, which involves a hard-to-compute integral. Here Monte Carlo integration is used to solve the integral.](images/MCIntPublishedExample2.png){#fig-MCIntPublishedExample}

## Classical Monte Carlo Integration


The generic problem here is the evaluation of integrals. For instance, 
$$
\mathbb{E}_{f_{X}}\left(h(X)\right)=\mathbb{E}\left(h(X)\right)=\int_\mathcal{X}h(x)\,f_X(x)\,dx,
$${#eq-MeanIntegral}
where $\mathcal{X}$ denotes the domain of the random variable $X\in\mathcal{X}\subseteq\mathbb{R}^d,$ and where $h$ is some transformation function, e.g., 
$$
h(x)=x^2,\;\;h(x)=\ln(x),\;\;h(x)=x,\;\;\text{etc.}
$$ 

::: {.callout-tip}
Computing means means computing integrals. To stress that one computes the integral with respect to the distribution characterized by the density function $f_X,$ one can write 
$$
\mathbb{E}_{f_{X}}\left(h(X)\right)
$$
instead of 
$$
\mathbb{E}\left(h(X)\right).
$$
We will use this notation more often below. 
:::

Often, analytical solutions for integrals such as in @eq-MeanIntegral are not readily available and one needs to use some numerical approaches/computational. Given our previous developments, it is kind of natural to propose using a realization 
$$
x_1,\dots,x_n
$$ 
from a (pseudo) random sample 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X
$$ 
to approximate the integral in @eq-MeanIntegral using the empirical mean
$$
\mathbb{E}\left(h(X)\right)\approx\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(x_i).
$$
By the [Strong Law of Large Numbers (SLLN)](http://www.statlect.com/asylln1.htm) we know that the empirical mean $\bar{h}_n$ converges almost surely (a.s.), and thus also "in probabiliuty" to the desired limit $\mathbb{E}\left(h(X)\right)$ as the sample size $n$ becomes large, i.e., as $n\to\infty$. Prerequisites for the SLLN: 

1. $h(X)$ has finite first moment, i.e., $\mathbb{E}\left(h(X)\right)<\infty$ and 
2. $\bar{h}_n$ is constructed from a random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X.$

As we can use the computer to produce realizations from the i.i.d. sample $X_1,\dots,X_n$, we can in principle choose an arbitrary **large sample** size $n$ such that $\bar{h}_n$ can, in principle, be **arbitrarily close** to the desired limit $\mathbb{E}\left(h(X)\right)$. 


**Though,** ... 

* ... which sample size $n$ is large enough? 
* or "equivalently", how fast converges $\bar{h}_n$ to the desired limit $\mathbb{E}\left(h(X)\right)$?


### Speed of Convergence {-}

OK, we know now that $\bar{h}_n$ reaches its limit (here in the "almost surely" sense, but likewise in the "in probability" case) as $n\to\infty$ under some rather loose conditions on the random sample $X_1,\dots,X_n$. 

If we are willing to additionally assume that $h(X)$ has finite *second* moments, i.e.
$$
\mathbb{E}(h(X)^2)<\infty,
$$ 
then we can additionally say something about **how fast** 
$$
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(X_i)\to_{p} \mathbb{E}(h(X)).
$$ 

The **speed of convergence** of the stochastic sequence 
$$
\{\bar{h}_n\}_{n=1,2,\dots}= \bar{h}_1,\;\bar{h}_2,\;\;\bar{h}_3,\;\dots 
$$ 
to its limit $\mathbb{E}(h(X))$ can be quantified by the rate at which the standard error 
$$
\operatorname{SE}\left(\bar{h}_n\right)=\sqrt{\mathbb{V}\left(\bar{h}_n\right)}
$$
converges to zero as $n\to\infty$. 

::: {.callout-tip}
We think of $\{\bar{h}_n\}_{n=1,2,\dots}$ as the sequence of **random variables**  
$$
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h({\color{red}{X_{i}}})
$$ 
with ${\color{red}{X_1}},\dots,{\color{red}{X_n}}\overset{\text{i.i.d.}}{\sim}f_X.$
:::


::: {.callout-tip}
Note that assuming finite second moments $\mathbb{E}(h(X)^2)<\infty$ is equivalent to assuming finite variance $\mathbb{V}\left(h(X)\right)<\infty,$ since
$$
\mathbb{V}\left(h(X)\right) = \mathbb{E}(h(X)^2) - \left(\mathbb{E}(h(X))\right)^2,
$$
and since if higher moments, like $\mathbb{E}(h(X)^2),$ are finite, also the lower moments, like $\mathbb{E}(h(X)),$ are finite. 
:::

The standard error of $\bar{h}_n$ is just the square root of the variance of $\bar{h}_n.$ The variance of $\bar{h}_n$ is given by 
$$
\begin{align*}
\mathbb{V}\left(\bar{h}_n\right)
&=\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\right) \\[2ex]
&=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n h(X_i)\right) \\[2ex]
&=\frac{n}{n^2}\mathbb{V}\left(h(X_1)\right)\quad \text{(since i.i.d.)} \\[2ex]
&=\frac{1}{n}  \mathbb{V}\left(h(X_1)\right)
\end{align*}
$$
The square root of $\mathbb{V}\left(h(X_1)\right)$ equals some finite, positive constant $0<\mathtt{const}<\infty$,  
$$
\mathtt{const}=\sqrt{\mathbb{V}\left(h(X_1)\right)}
$$ 
such that
$$
\sqrt{\mathbb{V}\left(\bar{h}_n\right)}=n^{-1/2}\mathtt{const}%\propto n^{-1/2}.
$$
I.e., the speed of convergence (or rate) of the stochastic sequence $\{\bar{h}_n\}$ is proportional to the deterministic sequence $\{n^{-1/2}\}.$ 



::: {.callout-note}

Even if we would not know the value of $\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)},$ we know now that the improvement from $n=10$ to $n=100$ will be *much* higher than from $n=110$ to $n=200$. In practice, a typical choice is $n=10,000;$ for moderate standard errors this choice will guarantee a very good approximation.
:::


### Limit Distribution {-}

Besides the speed of convergence of $\{\bar{h}_n\}_{n=1,2,\dots}$ for $n\to\infty,$ we can also say something about the distribution of the random variable $\bar{h}_n$ for large sample sizes $n.$


We can estimate the variance of the estimator $\mathbb{V}\left(\bar{h}_n\right)$ by its empirical version
$$
v_n^2=\frac{1}{n}\sum_{i=1}^n\left(h(x_i)-\bar{h}_n\right)^2,
$$
where by the SLLN, which also implies convergence in probability, 
$$
v_n^2\to_{p}\mathbb{V}\left(h(X)\right),\quad n\to\infty.
$$
<!--
I.e., after some rewriting, we have that: 
$$
\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{v_n}}\to_{\text{a.s.}}
\sqrt{m}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right).
$$
-->
Then, 
by the 
[Continuous Mapping Theorem (CMT)](https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#cmt), the 
[Central Limit Theorem (CLT)](http://www.statlect.com/central_limit_theorem.htm), and 
[Slutsky's theorem](https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#slutsky), we have that 
$$
\sqrt{n}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{v_n}\right)\to_d \mathcal{N}(0,1),\quad n\to\infty. 
$$
<!-- Note that the the above sequence $\{\sqrt{m}\}$ **just hinders** the convergence of the sequence $\bar{h}_n - \mathbb{E}\left(h(X)\right)\to_{a.s.}0$ such that the quotient converges to a "stable" distribution.  -->


The above result can now be used for the construction of (asymptotically valid) **convergence tests** and **confidence intervals** with respect to $\bar{h}_n$, since for large $n$ 
$$
\bar{h}_n\,\overset{d}{\approx}\mathcal{N}\left(\mathbb{E}\left(h(X)\right),\frac{\mathbb{V}\left(h(X)\right)}{n}\right).
$$

Since we can use the computer to generate realizations of the i.i.d. sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with $X\sim f_X,$ we can easily approximate the mean 
$$
\mathbb{E}\left(h(X)\right)\approx \bar{h}_n
$$ 
and the variance 
$$
\mathbb{V}\left(h(X)\right)\approx v_n^2
$$ 
with arbitrary accuracy as $n\to\infty$ (justification: strong/weak law of large numbers).




::: {#exm-MCInt1}

## A first Monte Carlo Integration

Let's say we want to compute the integral 
$$
\int_0^1h(x)dx
$$
with 
$$
h(x)=\left(\cos(50\,x)+\sin(20\,x)\right)^2
$$ 
over $x\in[0,1].$ Although this integral could also be computed analytically, it is a good first test case. The following code computes the analytic result that $\int_0^1h(x)dx = 0.9652009.$

```{r}
## install.packages("mosaicCalc")
suppressPackageStartupMessages(library("mosaicCalc"))

## Symbolic (= analytic) integration 
F <- antiD( (cos(50*x)+sin(20*x))^2 ~ x)

F(1) - F(0)
```

@fig-MCInt1_1 shows the graph of the function $h$.

```{r}
#| echo: true
#| eval: true
#| label: fig-MCInt1_1
#| fig-cap: Function $h$ of @exm-MCInt1. 

h_fun <- function(x){
  result <- (cos(50*x)+sin(20*x))^2
  return(result)
}

xx  <- seq(from=0, to=1, len=500)
plot(x = xx, 
     y = h_fun(xx), 
     type="l", 
     main="Function h", 
     xlab="x", ylab="h(x)")
```

To approximate the integral 
$$
\int_0^1 h(x)dx
$$ 
using Monte Carlo integration, we can use that
$$
\begin{align*}
\int_0^1 h(x)dx 
&=\int_0^11\cdot h(x)dx \\[2ex]
&=\int_0^1f_{\mathcal{U}\text{[0,1]}}(x)\cdot h(x)dx \\[2ex]
&= \mathbb{E}_{f_{\mathcal{U}\text{[0,1]}}}(h(X)),
\end{align*}
$$ 
where $f_{\mathcal{U}\text{[0,1]}}$ denotes the density function of the standard uniform distribution $\mathcal{U}\text{[0,1]}.$ 

Thus, to compute $\int_0^1 h(x)dx$ we generate a realization $(u_1,\dots,u_n)$ from the random sample $U_1,\dots,U_n\sim \mathcal{U}[0,1]$ and approximate 
$$
\int_0^1 h(x)dx\approx \bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(u_i).
$$ 

In order to assess how good this approximation is, we need to consider the stochastic properties of the random variable 
$$
\bar{h}_n = \frac{1}{n}\sum_{i=1}^n h(U_i).
$$ 
This is done using the above (review of) results on the limit distribution of the sample mean $\bar{h}_n$ which allows us to construct an approximate $95\%$ confidence interval, since for large $n$ 
$$
\begin{align*}
\operatorname{CI}^{95\%}_n
=&\left[\bar{h}_n - z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}, \bar{h}_n + z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}\right]\\[2ex]
\approx&
\left[\bar{h}_n - z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}, \bar{h}_n + z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}\right],
\end{align*}
$$
where $z_{1-\alpha/2}\approx 1.96$ denotes the $(1-\alpha/2)$-quantile of $\mathcal{N}(0,1),$ $v_n^2=n^{-1}\sum_{i=1}^n(h(u_i)-\bar{h}_n)^2,$ and where
$$
P\left(\int_0^1 h(x)dx  \in \operatorname{CI}^{95\%}_n \right) \to 0.95,\quad n \to\infty,
$$
by the CLT. 

@fig-MCInt1_2 shows *one* realization of the stochastic sequence 
$$
\bar{h}_1,\dots,\bar{h}_n
$$ 
with $n=10000$, where the realized value of $\bar{h}_n$ is $0.966$. This compares favorably with the with the exact value of $\int_0^1h(x)dx = 0.9652009.$


```{r}
#| echo: true
#| eval: true
#| label: fig-MCInt1_2
#| fig-cap: One realization of the stochastic sequence $\bar{h}_1,\dots,\bar{h}_n$ with $n=10000$, where the realized value of $\bar{h}_{n=10000}$ is $0.966$. The blue band shows the point-wise (i.e. for each given sample size $n$) confidence intervals $\operatorname{CI}^{95\%}_n.$
library("scales")
# h(x):
h_fun <- function(x){
  result <- (cos(50*x)+sin(20*x))^2
  return(result)
}

# sample size
n <- 10000

# Generate sample of uniforms
set.seed(321)
u_vec <- runif(n=n)

# Approximation of the integral 
h_bar_n <- cumsum(h_fun(u_vec))/c(1:n)

# True value:
true.value <- 0.9652009

# 95% CI
# Standard error of the estimator using the "algebraic 
# formula" for the variance (german: 'verschiebungssatz')
st.error_n <-  sqrt((cumsum(h_fun(u_vec)^2)/(1:n) - 
                     cumsum(h_fun(u_vec))^2/(1:n)^2))

CI_u       <-  h_bar_n + 1.96 * st.error_n / sqrt(1:n)
CI_l       <-  h_bar_n - 1.96 * st.error_n / sqrt(1:n)

plot(x = c(1:n), y = h_bar_n, type="n", 
     ylim=c(0.7,1.2), 
     xlab = "n", 
     ylab = "")
polygon(x = c(1:n, rev(1:n)), 
        y = c(CI_u, rev(CI_l)), 
        col    = alpha("blue", 0.5), 
        border = alpha("blue", 0.5))
lines(x = c(1:n), 
      y = h_bar_n, type="l")
lines(x = c(1:n), 
      y = rep(true.value, n), type="l", col="red")
##
legend("topright", 
       legend = c(expression(bar(h)[n]), 
                  "True Value", "95% CI"), 
       lty   = c(1,1,0), pch=c(22,22,22), pt.cex=c(0,0,2),
       pt.bg = c("black", "red", alpha("blue", 0.5)), 
       col   =   c("black", "red", alpha("blue", 0.5)))
```

:::

::: {.callout-note}

The approach of @exm-MCInt1 can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency/accuracy through numerical methods (e.g., [Riemann Sum](https://en.wikipedia.org/wiki/Riemann_sum), [Trapezoidal Rule](https://en.wikipedia.org/wiki/Trapezoidal_rule), [Simpson's Rule](https://en.wikipedia.org/wiki/Simpson%27s_rule), etc.) in dimensions 1 or 2; see the following code example: 
```{r}
numericalIntegration <- integrate(f     = h_fun, 
                                  lower = 0, 
                                  upper = 1)
numericalIntegration
```

However, the Monte Carlo integration approach is particularly useful for approximating integrals over higher dimensional sets $\mathcal{X}\subseteq\mathbb{R}^d.$ 
::: 

::: {#exm-ApproxNormTables}

## Approximation of Normal Distribution Tables 

A possible way to construct normal distribution tables, i.e., table for the values of the distribution function $0\leq \Phi(x)\leq 1,$ is to use Monte Carlo integration. Besides this specific usecase, this example showcases the treatment of integrals that can be approximated using propoartions 
$$
\hat{p} =\frac{\text{Number of successes}}{\text{Sample size}},
$$
where we choose the sample size $n$ to achieve a certain level of precision for our integral approximation. 

Observe that the distribution function $\Phi(x)$ can be written as the mean of a binary (taking values 0 and 1) random variable,
$$
\begin{align*}
\Phi(x)
&=\int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy\\[2ex]
&=P\left(X \leq x\right)\\[2ex]
&=\mathbb{E}\left(1_{(X \leq x)}\right),
\end{align*}
$$
where $X\sim\mathcal{N}(0,1),$ and where $1_{(\texttt{TRUE})}=1$ and $1_{(\texttt{FALSE})}=0.$

Monte Carlo integration, allows us to approximate the integral $\Phi(x)$ using the empirical mean 
$$
\hat{p}_n(x)=\frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
$$
with 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,1),
$$
and 
$$
\hat{p}_n(x)\in[0,1]
$$
for all $x\in\mathbb{R},$ and all "sample sizes" (chosen by us) $n=1,2,\dots$


#### Distributional Properties of Proportions {-}

Now, to assess the accuracy of our Monte Carlo integration for approximating $\Phi_n(x),$ we need to derive the distributional properties of our estimator $\hat{p}_n(x).$

For this we consider, firstly, the transformed random variable 
$$
n \hat{p}_n(x)=\sum_{i=1}^n1_{(X_i\leq x)},
$$ 
which is just the sum of independent Bernoulli distributed random variables with success probability $p=\Phi(x),$ 
$$
1_{(X_1\leq x)},\dots,1_{(X_n\leq x)}\overset{\text{i.i.d.}}{\sim} \mathcal{Bern}\left(p=\Phi(x)\right)
$$
Thus, the transformed random variable 
$$
n\hat{p}_n(x)=\sum_{i=1}^n1_{(X_i\leq x)}
$$ 
is **binomial distributed** with parameters $n$ and $p=\Phi(x),$
$$
n \hat{p}_n(x)\sim \mathcal{Binom}\left(n, p=\Phi(x)\right).
$$ 
Thus, we know the *exact* variance which is 
$$
\begin{align*}
\mathbb{V}\left(n\hat{p}_n(x)\right)&=\Phi(x)(1-\Phi(x)).
%\Leftrightarrow \mathbb{V}\left(\hat{p}_n(t)\right) &=\frac{\Phi(x)(1-\Phi(x))}{n}
\end{align*}
$$
The standard error for our estimator $\hat{p}_n(x)$ thus is equal to 
$$
\begin{align*}
\operatorname{SE}\left(\hat{p}_n(x)\right) 
&=\sqrt{\frac{\Phi(x)(1-\Phi(x))}{n}}\\[2ex]
&=\texttt{const}\cdot \frac{1}{\sqrt{n}}
\end{align*}
$$
That is, the Monte Carlo integration algorithm has the typical parametric convergence rate of 
$$
\frac{1}{\sqrt{n}}.
$$




<!-- For values of $x$ around $x=0$, the variance is thus approximately $1/4n,$
$$
\mathbb{V}(\hat{\Phi}_n(t))\approx \frac{\Phi(0)(1-\Phi(0))}{n}=\frac{0.5^2}{n},\quad t\approx 0.
$$   -->

@tbl-CIProp gives the evolution of the Monte Carlo integration results 
$$
\hat{p}_n(x)\approx \Phi(x)
$$
for several values of $x$ and $n.$ Very accurate approximations are achieved for $n=10^8.$ 


$n$    |$x=0.0$  |$x=0.84$ |$x=3.72$ 
-------|---------|---------|---------
$10^2$ |$0.4850$ |$0.7700$ |$1.0000$      
$10^3$ |$0.4925$ |$0.8010$ |$1.0000$      
$10^4$ |$0.4962$ |$0.7941$ |$0.9999$ 
$10^5$ |$0.4995$ |$0.7993$ |$0.9999$ 
$10^6$ |$0.5001$ |$0.8000$ |$0.9999$ 
$10^7$ |${\color{red}0.5002}$ |$0.8000$ |$0.9999$ 
$10^8$ |$0.5000$ |$0.8000$ |$0.9999$ 

: Monte Carlo integration results $\hat{p}_n(x)\approx \Phi(x)$ for different values of $x$ and sample sizes $n.$  {#tbl-CIProp}


::: {.callout-note}
Note that **greater accuracy is achieved in the tails**. 
:::

#### Confidence Intervals for Proportions {-}

As an additional tool for showing the accuracy of our Monte Carlo integration, we can report confidence intervals. 

For large $n,$ we have by the CLT that 
$$
\begin{align*}
\frac{\hat{p}_n(x) -  \Phi(x)}{\operatorname{SE}\left(\hat{p}_n(x)\right)} \overset{d}{\approx}\mathcal{N}(0,1).
\end{align*}
$${#eq-AsymDistrProp}
Thus, for large $n,$ we can provide an approximate confidence interval 
$$
\begin{align*}
\operatorname{CI}^{95\%}_n\left(\Phi(x)\right)
=&\left[
  \hat{p}_n(x) \pm z_{1-\alpha/2}\operatorname{SE}\left(\hat{p}_n(x)\right)
  \right].
\end{align*}
$${#eq-CIProp1}
This confidence interval is an approximate one, since the quantile $z_{1-\alpha/2}$ uses the approximate asymptotic $\mathcal{N}(0,1)$ distribution in @eq-AsymDistrProp. To make the confidence interval in @eq-CIProp1 usable in practice, we need to plug-in an estimate for the unknown $\operatorname{SE}\left(\hat{p}_n(x)\right),$ 
$$
\widehat{\operatorname{SE}}\left(\hat{p}_n(x)\right) = \sqrt{\frac{\hat{p}_n(x)(1-\hat{p}_n(x))}{n}}.
$$

However, it turns out that going the indirect way via the central limit theorem to derive a confidence interval is not optimal here. A more efficient **confidence interval for proportions** is, for instance, the [Clopper-Pearson confidence interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Clopper%E2%80%93Pearson_interval) (@Clopper_Pearson_1934). 

The following code snippet computes the 99% Clopper-Pearson confidence interval for the <span style="color:red">red marked</span> Monte Carlo integration result, $\hat{p}_n(0)=0.5002,$ in @tbl-CIProp. 

```{r}
## install.packages("PropCIs")
library("PropCIs")

## Clopper-Pearson confidence interval

n <- 10e7       # "sample" size (chosen by use)
x <- 0.5002 * n # observed value of n * \hat{p}_n(x) 

CI <- exactci(x          = x, 
              n          = n, 
              conf.level = 0.99)

## Lower and upper CI-border              
c(CI$conf.int[1], CI$conf.int[2]) 
```



<!-- $n$    |$x=0.0$  |$x=0.84$ |$x=1.65$ |$x=2.58$ |$x=3.72$ 
-------|---------|---------|---------|---------|---------
$10^2$ |$0.485$  |$0.77$   |$0.945$  |$0.995$  |$1$      
$10^3$ |$0.4925$ |$0.801$  |$0.9425$ |$0.9955$ |$1$      
$10^4$ |$0.4962$ |$0.7941$ |$0.9498$ |$0.995$  |$0.9999$ 
$10^5$ |$0.4995$ |$0.7993$ |$0.9498$ |$0.995$  |$0.9999$ 
$10^6$ |$0.5001$ |$0.8$    |$0.9502$ |$0.995$  |$0.9999$ 
$10^7$ |$0.5002$ |$0.8$    |$0.9501$ |$0.995$  |$0.9999$ 
$10^8$ |$0.5$    |$0.8$    |$0.95$   |$0.995$  |$0.9999$  -->


<!--                  X                   X                   X                   X
$n$    |$x=0.0$  |$x=0.67$ |$x=0.84$ |$x=1.28$ |$x=1.65$ |$x=2.32$ |$x=2.58$ |$x=3.09$ |$x=3.72$ 
-----  |-------  |-------  |-------  |-------  |-------  |-------  |-------  |-------  |---------
$10^2$ |$0.485$  |$0.74$   |0.77     |0.9      |0.945    |0.985    |0.995    |1        |1      
$10^3$ |$0.4925$ |$0.7455$ |0.801    |0.902    |0.9425   |0.9885   |0.9955   |0.9985   |1      
$10^4$ |$0.4962$ |$0.7425$ |0.7941   |0.9      |0.9498   |0.9896   |0.995    |0.999    |0.9999 
$10^5$ |$0.4995$ |$0.7489$ |0.7993   |0.9003   |0.9498   |0.9898   |0.995    |0.9989   |0.9999 
$10^6$ |$0.5001$ |$0.7497$ |0.8      |0.9002   |0.9502   |0.99     |0.995    |0.999    |0.9999 
$10^7$ |$0.5002$ |$0.7499$ |0.8      |0.9001   |0.9501   |0.99     |0.995    |0.999    |0.9999 
$10^8$ |$0.5$    |$0.75$   |0.8      |0.9      |0.95     |0.99     |0.995    |0.999    |0.9999  
-->



<!-- $$
\begin{array}{cccccccccc}
\hline
n   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\
\hline
10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\
10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\
10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\
10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\
10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\
10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\
10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\
\end{array}
$$ -->


<!-- * To achieve a precision of **four decimals** by means of a $99.9\%$ confidence interval, the approximation requires on average $n\approx 10^8$ simulations. 

* To achieve a precision of **two decimals** by means of a $99.9\%$  confidence interval, already $n=10^4$ leads to satisfactory results.  -->

<!-- * Note that **greater accuracy is achieved in the tails** 

* More efficient simulation methods could be used (e.g., Importance Sampling).  -->


That is, we actually need (at least) a sample size of $n^7$ to achieve a precision of **three decimal places** by means of a 99% Clopper-Pearson confidence interval.

:::



## Importance Sampling

As demonstrated in @exm-ApproxNormTables, the accuracy of the Monte Carlo integration method as a tool for approximating integral values depends on the variance of the estimate that approximates the integral value. 

"Importance sampling" aims to reduce the variance of the Monte Carlo integral estimator. Therefore, importance sampling is also refereed to as a **variance reduction** technique. This variance reduction is achieved by weighting functions, so-called **importance functions**. 

As in the case of Monte Carlo integration the focus lies on evaluating the integral
$$
\mathbb{E}_{f_X}(h(X))=\int_\mathcal{X}h(x)f(x)\,dx.
$$

However, it turns out that the above approach, i.e., sampling from $f$ is often suboptimal. 

Observe that the value of the above integral can be represented by infinitely many alternative choices of the triplet 
$$
(\mathcal{X}, h, f_X).
$$ 
Therefore, the search for an optimal estimator should encompass all these possible representations.

Let's illustrate this with a simple example.

::: {#exm-CauchTailProb}

# Cauchy Tail Probability

This example is from @Ripley_2009.

Suppose that the quantity of interest is the probability, say $p$, that a [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution) $\mathcal{C}(0,1)$-distributed random variable is larger than $2$, i.e.
$$
p=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx.
$${#eq-CauchIntegral}
This is a nice example. One the one hand, it allows us to showcase possibilities to improve efficiency of Monte Carlo integration. On the other hand, we know already the result of @eq-CauchIntegral; namely, $p=0.1476$
<!-- $$
\begin{align*}
p
&=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx\\[2ex]
&=\frac{1}{\pi}\int_{2}^{+\infty}\frac{1}{1+x^2}\,dx\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left[\frac{1}{\tan(x)}\right]_2^b\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left(\frac{1}{\tan(b)}-\frac{1}{\tan(2)}\right)\\[2ex]
\end{align*}
$$ -->
```{r}
round(1 - pcauchy(2), 4)
```


**1. Approach: The Naive Approach** 

If $p$ is approximated through the empirical mean
$$
\hat{p}_{1}=\frac{1}{n}\sum_{i=1}^n 1_{(X_i>2)}
$$
of a random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{C}(0,1),$ then $\hat{p}$ is a binomial distributed random variable with parameters $n$ and $p.$ The variance is thus 
$$
\begin{align*}
\mathbb{V}(\hat{p}_{1})
&=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(X_i>2)}\right)\\[2ex]
&=\frac{n}{n^2}\mathbb{V}\left(1_{(X_1>2)}\right)\quad\text{(i.i.d.)}\\[2ex]
&=\frac{p(1-p)}{n},
\end{align*}
$$
which is equal to $`r round(0.1476 * (1-0.1476),3)`/n,$ since we know that $p=0.1476$. 



**2. Approach: Accounting for Symmetry**

In this approach, we use the "adjusting Screws" $\mathcal{X}$ and $h.$

We can achieve a **more efficient estimator** (i.e., an estimator with lower variance for a given same sample size $n$) if we take into account the symmetric nature of $\mathcal{C}(0,1).$ Obviously, our target integral can be equivalently written as
$$
p=\frac{1}{2}\left(\int_{-\infty}^{-2}\frac{1}{\pi(1+x^2)}\,dx + \int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx \right).
$$
This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean:
$$
\begin{align*}
\hat{p}_{2}
&=\frac{1}{2}\left(\frac{1}{n}\sum_{i=1}^n1_{(X_i < -2)}+ \frac{1}{n}\sum_{i=1}^n 1_{(X_i > 2)}\right)\\[2ex]
&=\frac{1}{2n}\sum_{i=1}^n 1_{(|X_i|>2)}.
\end{align*}
$$
The variance of this new estimator,
$$
\begin{align*}
\mathbb{V}(\hat{p}_{2})
&=\frac{1}{4n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(|X_i|>2)}\right)\\[2ex]
&=\frac{2p(1-2p)}{4n},
\end{align*}
$$
which is equal to $`r round( (2*0.1476 * (1- 2*0.1476) )/4, 4)`/n,$ since we know that $p=0.1476$. This is clearly lower than in the naive approach, where we had $`r round(0.1476 * (1-0.1476),3)`/n.$




**3. Approach:**

In this approach, we use all the "adjusting screws" $\mathcal{X}$, $h$, and $f.$

The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, $[2,+\infty)$, which are in some sense irrelevant for the approximation of $p$. This motivates the following reformulation of $p$: 

By symmetry of $f$:
$$
\begin{align*}
\frac{1}{2} & =\int_{0}^2\frac{1}{\pi(1+x^2)}dx + \underbrace{\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}dx}_{=p}\\[2ex]
\Leftrightarrow \; p& =\frac{1}{2}-\int_{0}^2\frac{1}{\pi(1+x^2)}dx.
\end{align*}
$$
Furthermore, we can re-arrange the last integral a bit such that 
$$
\begin{align*}
 &\int_{0}^2\;\left(\frac{1}{2}\cdot 2\right)\;\frac{1}{\pi(1+x^2)}\,dx \\[2ex]
=&\int_{0}^2\;\underbrace{\frac{1}{2}}_{f_{\mathcal{U}[0,2]}}\;\underbrace{\frac{2}{\pi(1+x^2)}}_{=h(x)}\,dx \\[2ex]
=&\mathbb{E}_{f_{\mathcal{U}[0,2]}}(h(U)),
\end{align*}
$$
where $U\sim\mathcal{U}[0,2].$

Therefore a new alternative method for evaluating $p$ is:
$$
\hat{p}_{3}=\frac{1}{2} - \frac{1}{m}\sum_{i=1}n h(U_i),
$$
where $U_1,\dots,U_n\overset{\text{i.i.d.}}{\sim}\mathcal{U}[0,2].$

Using integration by parts and that $p=0.1476$, it can be shown that 
$$
\mathbb{V}(\hat p_3)=0.029/n,
$$ 
which is lower than both previous approaches, where we had that $\mathbb{V}(\hat{p}_{2})=0.052/n$ and $\mathbb{V}(\hat{p}_{1})=0.126/n$.

::: 


### A More General Point of View 

The idea of **importance sampling** is related to weighted and stratified sampling ideas, when estimating 
$$
\theta=\mathbb{E}_{f}(h(X))=\int h(x)f(x)dx,
$$
as already illustrated in @exm-CauchTailProb.

Some outcomes of $X\sim f$ may be more important than others in determining $\theta$ and we wish to select such values more frequently. 

For instance, if $\theta$ denotes the probability of the occurrence of a very rare event, then the only way to estimate $\theta$ at all accurately may be to produce the rare events more frequently. 

To achieve this, we can simulate a model which gives a density  $g$ to $X$ instead of the correct density $f,$ where both density functions need to be known. This can be easily done, since
$$
\begin{align*}
\theta
&=\mathbb{E}_{f}(h(X))\\[2ex]
&=\int h(x)\left(\frac{g(x)}{g(x)}\right)\;f(x)dx\\[2ex]
&=\int \underbrace{\left(h(x)\frac{f(x)}{g(x)}\right)}_{=:\psi(x)}\;g(x)dx\\[2ex]
&=\int \psi(x)\;g(x)dx\\[2ex]
&=\mathbb{E}_g(\psi(X)).
\end{align*}
$$

This leads to the following unbiased estimator for $\theta$ based on sampling from $g$:
$$
\begin{align*}
\hat{\theta}_g
&=\frac{1}{n}\sum_{i=1}^n\psi(X_i) \\[2ex]
&=\frac{1}{n}\sum_{i=1}^n\underbrace{\left(\frac{f(X_i)}{g(X_i)}\right)}_{=:W_i} h(X_i)
\end{align*}
$${#eq-ImportSample}
with 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} g.
$$

Note that @eq-ImportSample can be seen as a weighted mean of the transformed random variables $h(X_i)$ with weights 
$$
W_i = \frac{f(X_i)}{g(X_i)}
$$
that are inversely proportional to the so-called **selection factor** 
$$
\frac{g(X_i)}{f(X_i)}.
$$
<!-- 
For appropriately chosen pdfs f and g: 
The *selection factor* refers to "how more likely is it to select a 'rare event'?".
The inverse weight re-scales these 'too often' chosen rare events. 
-->


For the variance of the estimator $\hat{\theta}_g$ we have
$$
\begin{align*}
\mathbb{V}_g(\hat{\theta}_g)
&=\mathbb{V}_g\left(\frac{1}{n}\sum_{i=1}^n\psi(X_i)\right) \\[2ex]
&=\frac{1}{n}\mathbb{V}_g(\psi(X_i))\quad\text{(i.i.d)}\\[2ex]
&=\frac{1}{n}\int\left(\psi(x)-\theta\right)^2g(x)\;dx\\[2ex]
&=\frac{1}{n}\int\left(\frac{h(x)\,f(x)}{g(x)}-\theta\right)^2g(x)\;dx,
\end{align*}
$$
which, depending on the choice of $g$, can be *much* smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary (unweighted) empirical mean.
<!-- 
$$
\mathbb{V}(\hat{\theta}_{\text{naive}})=
\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n 1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=
\frac{1}{n}\mathbb{V}\left(1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=\frac{\theta(1-\theta)}{n}.
$$ 
-->

::: {.callout-note icon=false} 

## 
::: {#thm-MinVarTheorem}

## Minimum Variance Theorem

The **importance function** $g$ which minimizes the variance $\mathbb{V}_g(\psi(X_i))$, and therefore also the variance $\mathbb{V}_g(\hat{\theta}_g),$  is given by 
$$
g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
$$ 
:::
::: 

**Proof:** Done in the lecture. 


Note that @thm-MinVarTheorem may appear as rather impractical, since, for instance, if $h(x)>0$ then $g^\ast$ requires us to know $\theta=\int h(z)f(z)dz,$ which, however, is just the integral of interest! 


The above minimum variance result is still useful:

*  It tells us that a good choice of $g(x)$ shall mimic the shape of $|h(x)|f(x)$, since the optimal $g^\ast(x)\propto |h(x)|f(x)$.
*  Furthermore, $g(x)$ should be chosen such that it has a thicker tail than $f(x)$, since the variance $\mathbb{V}(\hat{\theta}_g)$ crucially depends on the quotient $f(x)/g(x)$ which would "explode" for $g(x)\approx 0$.




Let's apply our new insights to the above @exm-CauchTailProb on the Cauchy tail probability $p$. 

**Example: Cauchy Tail Probability (cont.)**


Above we had: 

1. $f(x)=\frac{1}{\pi(1+x^2)}$, the pdf of $\mathrm{C}(0,1)$ and
2. $h(x)=1_{(x>2)}$, i.e., here $|h(x)|=h(x)$.

Therefore
$$
p=\mathbb{E}_f(h(X))=\int h(x)f(x)dx=\int_{2}^{\infty}f(x)dx=\int_{2}^{\infty}\underbrace{\frac{f(x)}{g(x)}}_{=\psi(x)}\;g(x)dx=\mathbb{E}_g(\psi(X)),
$$
where the $h$ function is absorbed by the formulation of the definite integral.

A possibly good (and simple) choice of $g$ is, e.g., $g(x)=2/(x^2)$, since this function:

* "closely" matches $h(x)f(x)$ and 
* $g$ has thicker tails than $f$. 


```{r, fig.width=7, fig.height=4, fig.align='center', echo=FALSE}
# g(x):
g_fun <- function(x){2/(x^2)}
# 
xx    <- seq(from= 0, to=15, length.out = 500)
# plot
plot(x = xx, dcauchy(xx, location = 0, scale = 1), 
     type="l", xlab="", ylab="", ylim=c(0,0.55), axes = FALSE)
axis(side = 1, at=c(0,2,5,10,15)); axis(side = 2)
# add graph of g(x)
lines(x = xx[xx>2], g_fun(xx[xx>2]), col="red")
# region of interest selected by h(x): x>2
abline(v=2, lty=2)
# legend
legend("topright", legend = c("pdf-Cauchy(0,1)", expression(g(x)==2/(x^2))), lty=c(1,1),
     col = c("black", "red"))
```

\

**Caution:** It is not straight forward to directly sample from $g$, therefore we need some further steps:

\

The choice of $g$ leads to 
$$
p=\mathbb{E}_g(\psi(X))=
\int_{2}^{+\infty}\left(\frac{x^2}{2\,\pi(1+x^2)}\right)\,\frac{2}{x^2}\,dx=
\int_{2}^{+\infty}\left(\frac{1}{\pi(1+x^{-2})}\right)\,x^{-2}\,dx.
$$

\


Now we can apply some additional (rather case-specific) re-arrangements:

Integration by substitution (substituting $u=x^{-1}$) yields:
$$
p=\int_{0}^{1/2}\frac{1}{\pi(1+u^2)}du.
$$
Again, we can re-arrange the last integral a bit such that
$$
p=\int_{0}^{1/2}\underbrace{2}_{f_{\mathrm{Unif}[0,1/2]}}\;\underbrace{\frac{1}{2\,\pi(1+u^2)}}_{=h(u)}\,du=\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
Therefore, we have a final fourth version of the estimator of $p$:
$$
\hat{p}_4=\sum_{i=1}n h(U_i),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2] 
$$
and $h(u)=1/(2\pi(1+u^2))$. 

The variance of $\hat{p}_4$ is $(\mathbb{E}(h(U)^2)-\mathbb{E}(h(U))^2)/n$ and an integration by parts shows that $\mathbb{V}(\hat{p}_4)=0.95\cdot 10^{-4}/n$. Compare this to the former results: $\mathbb{V}(\hat p_3)=0.0285/n$, $\mathbb{V}(\hat{p}_{2})=0.0525/n$ and $\mathbb{V}(\hat{p}_{1})=0.1275/n$. The variance of $\hat{p}_4$ is by a factor of $10^{-3}$ lower than the variance of the original $\hat{p}_1$.


<!--
**Version 2:** It can be shown that (see [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))
$$
p=\int_0^{1/2}\frac{y^{-2}}{\pi(1+y^{-2})}dy,
$$
where this integral can also be seen as the expectation of 
$$
\frac{1}{4}h(U)=\frac{1}{2\pi(1+U^2)},\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
-->




<!--
## The 'Real Need' of Monte Carlo Simulation 

**Monte Carlo Simulation applied to Hypothesis Tests**

* **Problem:** Very often, statistical test procedures (or estimators) rely on asymptotic arguments. Asymptotic arguments ease the live of a statistician. In practice, however, we never have something like a diverging sample size of $n\to\infty$, but need to deal with a finite sample size $n$. All we can hope for is that the asymptotic results (e.g., on the level of significance of a test statistic and its power) are good approximations to the finite $n$ case. 

* **Solution:** Monte Carlo simulation the classical tool to investigate the finite $n$ performance of statistical test procedures (or estimators). It helps to answer questions like: How good are the asymptotic results given finite sample size scenarios of $n=100$, $n=500$, etc.

For instance, the **likelihood ratio (LR)** test statistic  
$$
-2\,\log\left[\ell(\hat\theta|x)/\ell(\hat\theta_0|x)\right]=-2\,\left\{\log\ell(\hat\theta|x)-\log\ell(\hat\theta_0|x)\right\}\to_d\chi^2_r
$$
is distributed as $\chi^2_r$ generally only in the limiting case as $n\to\infty$; and under some regularity constraints on the likelihood function. In the formula above $\ell(\theta|x)$ denotes the likelihood function, $\hat\theta\in\mathbb{R}^k$ is the estimated (via maximum likelihood) parameter vector from the unconstrained model and $\hat\theta_0\in\mathbb{R}^k$ is the estimated parameter vector from the constrained model with $r\leq k$ restrictions.

\

**Example: Contingency Tables**

The following table gives the results of a study comparing radiation therapy with surgery in treating cancer of the larynx. 


$$
\begin{array}{c|cc|c}
                   &  \text{Cancer}  &  \text{Cancer not}&           \\
                   &  \text{Controlled}  &  \text{Controlled}&           \\
\hline                   
\text{Surgery}     &  y_{11}=21      &   y_{12}=2        &  n_{1.}=23\\
\text{Radiation}   &  y_{21}=15      &   y_{22}=3        &  n_{2.}=18\\
\hline
                   &  n_{.1}=36      &   n_{.2}=5        &  n=41
\end{array}
$$

\

Let's condition on a fix number of total observations $n$. Then the random vector $(Y_{11},Y_{12},Y_{21},Y_{22})^\top$ comes from a [multinomial distribiton](https://en.wikipedia.org/wiki/Multinomial_distribution) with 4 cells and cell probabilities 
$$
p=(p_{11}, p_{12}, p_{21}, p_{22}),\quad\text{with}\quad\sum_{ij}p_{ij}=1,
$$
that is, 
$$
(Y_{11},Y_{12},Y_{21},Y_{22})^\top\sim\mathcal{M}_4(n, p).
$$
With $y_{ij}$ denoting the number of realizations in cell $ij$, the likelihood function can be written as 
$$
\ell(p|y)\propto\prod_{ij}p_{ij}^{y_{ij}},
$$
where the 4-dimensional parameter space can be displayed as following:
$$
\begin{array}{cc|c}
p_{11} & p_{12} & p_{1.}\\
p_{21} & p_{22} & p_{2.}\\
\hline
p_{.1} & p_{.2} & 1\\
\end{array}
$$



**Null Hypothesis:** The null hypothesis to be tested is one of independence, which is to say that the surgery treatment has no bearing on the control of cancer. Translated into a parameter statement this means
$$\text{H}_0: p_{11}=p_{1.}\,p_{.1}\quad\text{against}\quad\text{H}_1: p_{11}\neq p_{1.}\,p_{.1}.$$


The likelihood ratio statistic for testing this hypothesis is 
$$
\lambda=\frac{\max_{p\text{ s.t. }p_{11}=p_{1.}p_{.1}}\ell(p|y)}{\max_{p}\ell(p|y)}.
$$
It is "straightforward" to show that the denominator maximum is attained at:
$$
\hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all}\quad ij. 
$$
and the numerator maximum at
$$
\hat{p}_{11}=\hat{p}_{1.}\hat{p}_{.1}\quad\text{with}\quad \hat{p}_{1.}=\frac{n_{1.}}{n}\quad\text{and}\quad\hat{p}_{2.}=\frac{n_{2.}}{n},
$$
$$
\text{and} \quad \hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all other}\quad ij. 
$$

As mentioned above, under H$_0$, $-2\log \lambda$ is asymptotically distributed as $\chi^2_1$. However, with only $n=42$ observations, the asymptotics do not necessarily apply. One alternative is to use devise a **Monte Carlo experiment** to simulate the null distribution of $-2\log \lambda$ or equivalently of $\lambda$ in order to obtain a cutoff point for a hypothesis test. A more sophisticated approach is that of [Mehta at el. (2000)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10473906).  

**Description of the Procedure:**

* Let's denote the finite $n$ null distribution of $\lambda$ by $f_{0,n}(.)$. As we are interested in an $\alpha$ level test, we need to specify $\alpha$ (e.g., $\alpha=0.05$) and to solve the following integral for the $1-\alpha$ quantile $\lambda_\alpha$:
$$
\int_0^{\lambda_\alpha}f_{0,n}(u)du=1-\alpha.
$$

* The standard Monte Carlo approach to this problem is to generate random variables $\lambda_k\sim f_{0,n}$, $k=1,\dots,m$, then order the sample 
$$
\lambda_{(1)}\leq \lambda_{(2)}\leq\dots\leq \lambda_{(m)}
$$
and finally calculate the empirical $1-\alpha$ quantile $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$, where $\lfloor x \rfloor$ is the next lower integer to $x$, e.g., $\lfloor 2.9 \rfloor=2$.

* Similarly to the above integration example which builds on the SLLN, the central idea here is to use the so-to-say SLLN for quantiles:
$$
\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}\to_{a.s.} \lambda_{\alpha}\quad\text{as}\quad m \to\infty;
$$
see, e.g., the classical book "Approximation Theorems of Mathematical Statistics" of R. Serfling Ch. 2.3.1. As a computer is doing this job for us, we can in principle choose an arbitrary large $n$ such that the above approximation of $\lambda_{\alpha}$ by $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$ can be arbitrarily good. 

To run the 

```{r}
set.seed(123)
#
p_init <- runif(4)
#
p_11   <- p_init[1]
p_12   <- p_init[2]
p_21   <- p_init[3]
p_22   <- p_init[4]
#
p_1.   <- p_11 + p_12
p_.1   <- p_11 + p_21
#
p_11   <- p_1. * p_.1

# probabilities under H0:
p_0 <- c(p_11, p_21, p_12, p_22)


Y_vec <- rmultinom(n=1, size=4, prob=p_0)

```
-->

<!-- 
**Example: LR Test**

Let's assume we want to test the following regression model 
$$
Y_t=\beta_0 + \beta_1 t +\varepsilon_{t}
$$
against the constrained model without a time trend:
$$
Y_t=\beta_0 + \varepsilon_{t},
$$
where $\varepsilon_{t}\sim N(0,\sigma_\varepsilon^2)$ with $0<\varepsilon^2<\infty$.


```{r}
LL_fun <- function(beta0, beta1, sigma) {
      # residuals
      res_vec   <-  y - x * beta1 - beta0
      # log-transformed probabilities of observing the vector of residuals res_vec: 
      Log_probs <- dnorm(x = R, mean = 0, sd = sigma, log = TRUE)
      # 
      result <- - sum(Log_probs)
      return(result)
}
```
-->



## Exercises of Chapters 1 & 2 {-}

1. Consider @exm-AREfficiency ("Normals from Double Exponentials"). Let $f$ be the density of the standard normal distribution $\mathcal{N}(0,1),$
$$
\begin{align*}
f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right),
\end{align*}
$$
and $g$ the density of the Laplace (double exponential) distribution
$$
\begin{align*}
g(x|b) = \frac{1}{2b}\exp\left(-\frac{|x|}{2}\right),\quad b>0.
\end{align*}
$$

    (a) Show that 
    $$
    \begin{align*}
    \frac{f(x)}{g(x|b)}\leq \sqrt{\frac{2}{\pi}} \; b\;\exp\left(\frac{1}{2b^2}\right).
    \end{align*}
    $$
    (b) Show that the minimum of the bound (in $b>0$) is attained at $b=1.$

1. Consider the Accept-Reject Algorithm for a target density function $f_X$ and an instrumental density function $g$ with $f_X(x) \leq Mg(x)$ for all $x\in\mathbb{R}$ and $g(x)>0$ for all $x\in\operatorname{supp}(f_X).$ 
  
   (a) What is the probability of accepting $Y$ from a simulation 
   $$
   (Y,U)\sim\mathcal{U}\left(\left\{(y,u)|y\in \operatorname{supp}(f_X)\;\text{and}\;0\leq u\leq Mg(x)\right\}\right)?
   $$ 
   (b) Show that $M\geq 1.$

2. Consider the following faster version of the Box-Muller algorithm:<br> 
  **1. Step:** Generate 
  $$
  U_1, U_2\overset{\text{i.i.d.}}{\sim}\mathcal{U}[0,1] 
  $$
  until $S = U_1^2+ U_2^2 \leq 1.$<br>
  **2. Step:** Define 
  $$
  Z = \sqrt{-2\log(S)/S}
  $$
  and take 
  $$
  X_1 = Z U_1\quad\text{and}\quad X_2 = Z U_2
  $$

   (a) Show that $(U_1,U_2)$ is uniformly distributed over the unit sphere. 
   (b) Show that $X_1$ and $X_2$ are independent. 



<!-- MC Stat Methods. Problem 2.10. -->

<!-- 1. A possible algorithm for generating standard normal random variables from $n$ standard uniformly distributed random variables $U_1,\dots,U_n\sim\mathcal{U}[0,1],$ would be to use the Central Limit Theorem (CLT), i.e.
$$
Y_n=\frac{\sqrt{n}\left(\bar{U}_n - \mu\right)}{\sigma}\to_d \mathcal{N}(0,1).
$$
as $n\to\infty,$ where $\mathbb{E}(U_1)=\mu$ and $\mathbb{V}(U_1)=\sigma^2.$

Asses this "algorithm" critically by answering the following questions: 
 (a) What is the maximal range of values of the generated random variables $Y_n$?  
 (b) Compare the moments of $Y_n$ with those of $\mathcal{N}(0,1)$ using the moment generating function 
$$
 M_X(t) = \mathbb{E}\left(\exp\left(t X\right)\right)
$$ 
 Hint: For the case of $U\sim\mathcal{U}[0,1],$ the moment generating function is 
$$
 M_U(t) = \mathbb{E}\left(\exp\left(t U\right)\right)=\frac{\exp(t) - 1}{t}.
$$ 

::: {.callout-tip}
The distribution of a random variable $X$ can be characterized (completely) using any of the following functions (provided they exist):

* Density function $f_X$

* Distribution function $F_X(x)=\int_{-\infty}^x f_X(u)\,du$

* Moment generating function $M_X(t)=\mathbb{E}\left(\exp\left(t X\right)\right),$ where $t\in\mathbb{R}.$ The $j$th derivative of $M_X$ evaluated at $t=0$ gives the $j$th moment  
  $$
  \left.M_X^{(j)}(t)\right|_{t=0} = \mathbb{E}(X^j),\quad j=1,2,\dots 
  $$
:::
-->



<!-- {{< include Ch2_Solutions.qmd >}} -->



## References {-}