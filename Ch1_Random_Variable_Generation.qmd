# Random Variable Generation

### Literature {-}

In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:

* [Monte Carlo Statistical Methods](http://www.springer.com/us/book/9780387212395), Ch. 2, @RobertCasella1999 
* [Introducing Monte Carlo Methods with R](https://link.springer.com/book/10.1007/978-1-4419-1576-4), Ch. 2, @RobertCasella2010

<!-- * Further:
  * [Non-Uniform Random Variate Generation](http://luc.devroye.org/rnbookindex.html), Devroye, L. 
  * [Nonparametric Density Estimation: The L1 View](http://luc.devroye.org/L1bookBW.pdf), Devroye, L., Ch. 8
  * [Monte Carlo and Quasi-Monte Carlo Sampling](http://www.springer.com/us/book/9780387781648), Lemieux, C., Ch. 2 and 3 -->



### Introduction & Motivation {-}

In many complex statistical models parameter estimates can only be calculated numerically and their (statistical) behavior can only be investigated through a **simulation** on the computer: 

**Stylized Algorithm:**

* **Step 1** Generate artificial data from a statistical model and compute the estimate(s) of interest. 
* **Step 2** Repeat this 10000 (or more) times.
* **Step 3** From these 10000 "pseudo" realizations of the estimator we can directly compute, e.g., the bias or the mean squared error (MSE) of the estimator---without the need of complicated statistical theory.


```{r}
## Step 1
generate_mean_estimates <- function(n = 100){
  ## generate artificial data
  artificial_data <- rnorm(n)   
  ## compute the estimate
  estimate        <- mean(artificial_data) 
  return(estimate)
}

## Step 2 
set.seed(223)
simulated_estimates <- replicate(1000, 
                                 generate_mean_estimates(n = 100))

## Step 3
mean(simulated_estimates)
sd(simulated_estimates)

hist(simulated_estimates, 
     main = "Histogram of Simulated Mean Estimates", 
     xlab = "")
```


In order to conduct Steps 1 and 2 we need artificial realizations (here we used `rnorm()`) of a random variable---so-called **Pseudo Random Numbers**. The generation of such pseudo random numbers is the topic of the following chapters. 


## Uniform Simulation


**General procedure:**

* Usually, a random integer with values **uniformly** in $[0,m]$ with a large integer $m$ is generated. 
* To achieve a random number in $[0, 1]$, we divide this number by $m$. 
* From this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.


There are many different **Random Number Generators (RNGs)**, we consider the most simple class of RNGs:

::: {#def-LinConGen}

## Linear Congruential Generators 

Here the $i$th random integer $u_i$ is generated by
$$
u_i=(a u_{i-1}+c) \,\operatorname{mod}\, m,
$$
where the starting value $u_0$ is a chosen and fixed value called **seed**.

Furthermore:
    
* $m$, with $0<m$, is called the **modulus**
* $a$, with $0<a<m$, is called the **multiplier**
* $c$, with $0\leq c<m$, is called the **increment**
:::

::: {.callout-tip}

## The modulo operator: $\operatorname{mod}$

"$b\,\operatorname{mod}\,c$" denotes the remainder of the division of $b$ by $c$.

For instance 
$$
\begin{align*}
4\,&\operatorname{mod}\,2 = 0\\
5\,&\operatorname{mod}\,2 = 1\\
1\,&\operatorname{mod}\,2 = 1\\
\end{align*}
$$


```{r, eval=FALSE}
# Modulo computation using the modulo operator '%%'
5 %% 4
9 %% 4
4 %% 5

# own modulo-function:
my_mod <- function(x,m){
  t1 <- floor(x/m)
  return(x-t1*m)
}
```

:::


Some Facts:

* The above recursion generates a completely **nonrandom** sequence, therefore it is often called a ***pseudo* random** sequence. 
* Under appropriate choices of $u_0$ , $a$ and $m$ the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on $[0, m]$.
* The cycle length of linear congruential generators will never exceed modulus $m$, but can maximized with the three following conditions (see [Knuth (2002)](https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) for a proof):
    * The increment $c$ is [relatively prime](https://en.wikipedia.org/wiki/Coprime_integers) to $m$,
    * $a - 1$ is a multiple of every prime dividing $m$,
    * $a - 1$ is a multiple of $4$ when $m$ is a multiple of $4$.
    


<!--
Further reading:
Ripley (1987) provides a review of number generators. 
Devroye (1986) is very comprehensive book on non-uniform random variate generation.
-->



**Bad choice of parameters** for the linear congruential random number generator:
```{r}
m <- 64    # modulus
a <- 33    # multiplier
c <- 12    # increment
s <- 57    # seed
n <- 1000  # length of run (including seed)

r_vec    <- numeric(n) # initialize vector
r_vec[1] <- s # set seed

## Recursive generation 
for (i in 1:(n-1)){
 r_vec[i+1] <- (a * r_vec[i] + c) %% m
}

# scale result from [0,m] to [0,1]:
my_bad_runif_vec <- r_vec/m

# BUT! Very short cycle-length (here: period=16)
r_vec[ 1:16]
r_vec[17:32]
```

\

::: {#exm-GoodVsBadRNG}

## Good vs. Bad RNGs 

Average heads ratios 
$$
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i
$$ 
for $n=1,2,\dots$ simulated independent tosses of a fair coin $C_i$ with  
$$
C_{i}=\left\{\begin{array}{ll}
1&\text{if Head}\\
0&\text{if Tail}
\end{array}\right.
$$ 
and
$$
P(C_i=0)=P(C_i=1)=0.5.
$$ 
By the strong (or weak) law of large numbers this average should converge **stochastically** (i.e., almost surely or in probability) to $0.5$ as $n$ becomes large $(n\to\infty).$

```{r, fig.width=7, fig.height=4, fig.align='center'}
#| label: fig-goodvsbad
#| fig-cap: "Two sample paths showing the pseudo random convergence of $\\bar{C}_n$ to the limit 0.5---one based on a good RNG and the other based on a bad RNG."

# using the above bad RNG:
bar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)

# using R's high-quality RNG:
set.seed(223)
bar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)

# plotting the results:
plot(bar_x_bad, type="l", ylim=c(0.46,0.54), 
     xlab="", ylab="", main="Good vs. Bad RNG")
lines(bar_x_good, col="darkblue")
```
::: 

<!--
```{r, eval=FALSE}
## See also:
install.packages("randtoolbox")
library("randtoolbox")
?congruRand()
```
-->

::: {.callout-note}
IBM's [RANDU](https://en.wikipedia.org/wiki/RANDU) is a famous example of an miss-specified linear congruential RNG.
:::



## Generation of Discrete Random Variables

<!-- https://rpubs.com/maheshraje982/617094 -->

Assume the discrete random variable $X$ of interest takes on the values 
$$
X\in \{x_1, \dots , x_k \}
$$ 
with
$$
p_i = \mathbb{P}(X = x_i ), \quad i = 1,\dots , k,
$$
and 
$$
\sum_{i=1}^kp_i = 1.
$$

Assume that you can generate pseudo-random realizations $u\in[0,1]$ from a uniformly distributed random variable $U\sim\mathcal{U}[0, 1]$ using an RNG.


**General principle:** 

1. Subdivide $[0, 1]$ into $k$ intervals with
$$
I_i = (a_{i-1}, a_i],
$$
where 
$$
a_i = \sum_{j=1}^ip_j\quad\text{and}\quad a_0 = 0.
$$
2. Define the new discrete realizations 
$$x=\left\{
\begin{array}{cc}
          x_1&\quad\text{if}\quad u\in I_1\\
          \vdots& \vdots\\
          x_k&\quad\text{if}\quad u\in I_k
          \end{array}\right.
$$



::: {#lem-GenDiscrRV}

Let $u$ be a realization from $\mathcal{U}[0, 1]$ and if $u\in I_i$, set $x = x_i$. Then $x$ is a realizaton from the discrete distribution of $X$.
      
::: 

**Proof:** Done in the lecture.


<!-- **Proof:** 

For any $i = 1, \dots, k$ we have that
$$
\begin{align*}
\mathbb{P}(X = x_i) 
& = \mathbb{P}(U \in I_i)  \\
& = F_\mathcal{U}(a_i) - F_\mathcal{U}(a_{i-1})\\
& = a_i - a_{i-1}\\
& = \sum_{j=1}^ip_j - \sum_{j=1}^{i-1}p_j  = p_i,
\end{align*}
$$
which shows the statement of @lem-GenDiscrRV.  -->


::: {#exm-Bernoulli}

## Bernoulli Distribution

Generate random numbers from 
$$
X\sim\mathrm{Bernoulli}(p),
$$
where $p$ is the probability of success, i.e., 
$$
\mathbb{P}(X=1)=p\quad\text{and}\quad\mathbb{P}(X=0)=1-p.
$$

**Algorithm:** If $U\sim\mathcal{U}[0,1]$ and $p$ is specified, define
$$
X=\left\{
  \begin{matrix}
  1 & \text{if }U\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then $X\sim\mathrm{Bernoulli}(p)$.


```{r}
set.seed(321)
# Generate one random number from Bernoulli(p) with p=0.5
p  <- 0.5
U  <- runif(1)

if(U<=p) X=1 else X=0

X
```
:::




::: {#exm-Binom}

## Binomial Distribution

Generate random numbers from 
$$
X\sim\mathrm{Binomial}(n,p),
$$
where $n$ is the number of trials and $p$ the probability of success such that 
$$
\mathbb{P}(X=i)=\binom{n}{i}p^i(1-p)^{n-1}
$$
for $i=1,\dots,n.$

::: {.callout-tip}
If $X_1,\dots,X_n\overset{i.i.d}{\sim}\mathrm{Bernoulli}(p),$ then 
$$
X=\sum_{i=1}^nX_i \sim\mathrm{Binomial}(n,p).
$$
::: 

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $n$ and $p$ are specified, define
$$
X_i=\left\{
  \begin{matrix}
  1 & \text{if }U_i\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then 
$$
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Binomial}(n,p).
$$

```{r}
set.seed(321)

# Generate one random number from B(n=10, p=0.5). 
n <- 10
X <- numeric(n)
p <- 0.5

for(i in 1:n){
  U <- runif(1)
  if(U<=p) X[i]=1 else X[i]=0
}
Y <- sum(X)
Y 
```
:::



::: {#exm-Poisson}

## Poisson Distribution

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=\min\left\{n=0,1,2\dots,\text{ such that }\prod_{i=1}^{n+1} U_i \leq e^{-\lambda}
\right\}. 
$$ 
Then 
$$
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Poisson}(\lambda).
$$

```{r}
set.seed(321)

# Generate one random number from Poisson(lambda) 
lambda <- 2

## Initializations
U <- 1
n <- 0

while(U > exp(-lambda)){
  U <- U * runif(1)
  n <- n + 1
}
n <- n-1
n
```
:::



## Generation of Continuous Random Variables

### The Inverse Method

A rather general method to generate continuous random variables is the **Inverse Method**.


::: {#thm-InverseMethod}

## Inverse Method

Let $U\sim\mathcal{U}[0,1],$ and let $F_X$ be an **invertible** distribution function. The transformed random variable   
$$
X=F_X^{-1}(U)
$$
has then the distribution function $F_X,$ 
$$
P(X\leq x) = F_X(x).
$$
:::

::: {.callout-important}
@thm-InverseMethod can only be used to generate random variables $X$ with **invertible** distribution functions $F_X.$
:::
      
**Proof:**

The distribution function of the transformed random variable 
$$
X=F^{-1}(U)
$$ 
can be derived as 
$$
\begin{align*}
\mathbb{P}(X\leq x) 
&= \mathbb{P}(F_X^{-1}(U)\leq x) \\
&= \mathbb{P}(U\leq F_X(x)) \\
&= F_U(F_X(x)) \\
& = F_X(x),
\end{align*}
$$
which shows the result of @thm-InverseMethod. The last (and important) equality follows since the distribution function of $U\sim\mathcal{U}[0,1]$ is 
$$
\mathbb{P}(U\leq u) = F_U(u) = u, \quad 0\leq u \leq 1
$$
since the distribution function $F_U$ of $U\sim\mathcal{U}[0,1]$ is
$$
F_U(u) = \left\{
  \begin{array}{ll}
  0 & \text{for } u < 0\\
  u & \text{for } 0 \leq  u \leq 1\\
  1 & \text{for } 1 < u.\\
  \end{array}
\right.  
$${#eq-FUnif}

<!-- https://statproofbook.github.io/P/cdf-itm.html#:~:text=Proof%3A%20Inverse%20transformation%20method%20using%20cumulative%20distribution%20function&text=has%20a%20probability%20distribution%20characterized,)%20F%20X%20(%20x%20)%20.&text=U%E2%88%BCU(0%2C1,U%E2%89%A4u)%3Du. -->


<!-- http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf -->


::: {#exm-Exponential}

## Exponential Distribution

Since 
$$
F(x)= 1 - \exp(-\lambda x),
$$ 
we have 
$$
F^{-1}(u) = - \frac{\ln(1-u)}{\lambda}.
$$

::: {.callout-tip}
Note that $1-U$ has the same distribution as $U$, if $U\sim U[0,1]$. Therefore also $-\frac{\ln(u)}{\lambda}$ leads to a value from $\mathrm{Exp}(\lambda).$
:::



**Algorithm:** If $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=-\frac{\ln(U)}{\lambda}.
$$
Then 
$$
X\sim \mathrm{Exp}(\lambda).
$$
:::


::: {.callout-note}

The inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods. 

The inverse method often cannot be applied, because the inverse of many important distribution functions cannot be derived explicitly:

* The Gaussian distribution function $\Phi$ and therefore also its inverse $\Phi^{-1}$ is not available in explicit terms. 
* For discontinuous random variables we need efficient algorithms for computing the *generalized* inverse of their distribution function $F.$
:::


### Transformation Methods



**Idea:** Construct algorithms from theoretical links between distributions. 

**Pro:** These methods can be advantageous if a distribution $f$ is linked (in a relatively simple way) to another distribution that is easy to simulate. 

**Con:** Generally, these methods are rather case-specific, and difficult to generalize. 



::: {#exm-BuildOnExp}

## Building on Exponential RVs

In @exm-Exponential, we learned to generate an exponential random variable $X\sim\operatorname{Exp}(\lambda)$ starting from a uniform random variable $U\sim\mathcal{U}[0,1].$ In the following we generate random variables starting from exponential random variables $X\sim\mathrm{Exp}(1):$

If the $X_1, X_2,\dots$ are i.i.d. as $X\sim\mathrm{Exp}(1),$ then

$$Y\sim \chi^2_{2\nu}\quad\text{if}       \quad Y= 2     \sum_{i=1}^\nu X_i,\quad\nu=1,2,\dots $$
$$Y\sim \Gamma(\alpha,\beta)\quad\text{if}\quad Y= \beta \sum_{i=1}^\alpha X_i,\quad \alpha=1,2,\dots $$
$$Y\sim \mathrm{Beta}(a,b)\quad\text{if}  \quad Y= \frac{\sum_{i=1}^a X_i}{\sum_{j=1}^{a+b} X_j},\quad a,b=1,2,\dots $$
:::


::: {.callout-note}

There are better algorithms to generate Gamma and Beta random variables.

We cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter $\alpha$.

* This implies that we cannot generate a $\chi^2_{1}$-distributed random variable, because the $\chi^2_{1}$ distribution is identical to the $\Gamma(\alpha, 2)$ distribution with $\alpha=\frac{1}{2}.$ 

* This then also implies that we cannot generate a $\mathcal{N}(0,1)$-distrbuted random variable, since $X^2\sim \chi^2_{1}$ for $X\sim\mathcal{N}(0,1)$.  
:::        


The well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of @BoxMuller1958:


::: {#thm-NormalVariableGeneration}

## Normal Variable Generation (Box and Muller, 1958)

If $U_1$ and $U_2$ are i.i.d. as $U\sim\mathcal{U}[0,1]$, then 
$$
X_1 =\sqrt{-2 \ln(U_1)}\, \cos(2\pi U_2)
$$
and 
$$
X_2=\sqrt{-2\ln(U_1)}\,\sin(2\pi U_2)
$$
are both i.i.d. as $X\sim\mathcal{N}(0,1).$
::: 

**Proof:** 

Define the random variables 
$$
R = \sqrt{-2 \ln(U_1)}\quad\text{and}\quad Q = 2\pi U_2,
$$
where
$$
R\in[0,\infty)\quad\text{and}\quad Q\in[0,2\pi].
$$

::: {.callout-tip}

**Idea of the proof:**

1. Derive the bivariate density of $(R, Q).$
2. Determine the functional connection $g$ between $(R, Q)$ and $(X_1, X_2)$ and note that $g$ is invertible.
3. Use the **transformation formula** for densities to derive the bivariate density of $(X_1,X_2)$ using $g^{-1}$ and the bivariate density of $(R, Q).$ 
4. The result follows, if the bivariate density of $(X_1,X_2)$ equals the product of two standard normal densities.  


**Transformation formula (bivariate case):**

Assume that the bivariate random variable $\left(\begin{matrix}R\\ Q\end{matrix}\right)$ 
has a bivariate density $f_{RQ}(r, q)$ and that there is a one-to-one mapping $g$ between the bivariate random variables $\left(\begin{matrix}R\\ Q\end{matrix}\right)$ and $\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)$ such that 
$$
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)=
g(R, Q)=
\left(\begin{matrix}g_1(R, Q)\\ g_2 (R, Q)\end{matrix}\right),
$$
where $g:\mathbb{R}^2\to\mathbb{R}^2$ is a one-to-one differentiable transformation with inverse $g^{-1}$. 

Then, the bivariate density of $\left(\begin{matrix}X_1\\X_2\end{matrix}\right)$ 
is given by 
$$
f_{X_1X_2}(x_1,x_2)=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\,\left|\det\left(J_{g^{-1}}(x_1,x_2)\right)\right|,
$${#eq-TransformFormula}
where $\det\left(J_{g^{-1}}(x_1,x_2)\right)$ denotes the determinant of the Jacobian matrix of $g^{-1}$ evaluated at $(x_1,x_2),$
$$
J_{g^{-1}}(x_1,x_2)=\left(\begin{matrix}
\frac{\partial g_1^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_1^{-1}}{\partial x_2}(x_1,x_2)\\
\frac{\partial g_2^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_2^{-1}}{\partial x_2}(x_1,x_2)\\
\end{matrix}\right).
$$
Note that the Jacobian of $g^{-1}$ equals the inverse of the Jacobian of $g,$
$$
J_{g^{-1}}(x_1,x_2) = \left(J_{g}(r,q)\right)^{-1},
$$
with points $(x_1,x_2)$ and $(r,q)$ such that 
$$
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right)=
g(r, q)=
\left(\begin{matrix}g_1(r, q)\\ g_2 (r, q)\end{matrix}\right),
$$
:::

We can derive the distribution function of $R$ as following
$$
\begin{align*}
F_R(r)=\mathbb{P}\left(R\leq r\right) 
& = \mathbb{P}\left(\sqrt{-2 \ln(U_1)}\leq r\right) \\
& = \mathbb{P}\left(\ln(U_1)\geq -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) < -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) \leq -\frac{r^2}{2}\right) \quad \text{(continous)}\\
& = 1 - \mathbb{P}\left(U_1 \leq \exp\left(-\frac{r^2}{2}\right)\right) \\
& = 1 - F_U\left(\exp\left(-\frac{r^2}{2}\right)\right)\\
& = 1 - \exp\left(-\frac{r^2}{2}\right),
\end{align*}
$$
where the last step follows from applying the distribution $F_U$ of $U\sim\mathcal{U}[0,1];$ see @eq-FUnif. 

For the density function $f_R$ of $R$ we get
$$
f_R(r)=F_R'(r)=\left\{
  \begin{array}{ll}
  \exp\left(\frac{r^2}{2}\right)\cdot r&\text{for }r \in[0,\infty)\\
  0&\text{for }r < 0.\\
  \end{array}\right.
$$
Next, define the random variable 
$$
Q = 2\pi U_2.
$$
Since $U_2\sim\mathcal{U}[0,1],$ 
$$
Q\sim\mathcal{U}[0,2\pi].
$$
with density function 
$$
f_Q(q)=\left\{
  \begin{array}{ll}
  \frac{1}{2\pi}&\text{for } q\in [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
$$
Since $U_1$ and $U_2$ are independent, $R=\sqrt{-2 \ln(U_1)}$ and $Q = 2\pi U_2$ must also be independent, such that
$$
\begin{align*}
f_{RQ}(r,q) 
& = f_R(r)\cdot f_Q(q) \\
& = \left\{
  \begin{array}{ll}
  \exp\left(\frac{r^2}{2}\right) r\cdot \frac{1}{2\pi} & \text{for } (r,q) \in [0,\infty)\times [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
\end{align*}
$$  
Now, as we know the bivariate density of $(R,Q)$ we can use the functional connection 
$$
\begin{align*}
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
& = g(R,Q) \\
& = \left(\begin{matrix}
       g_1(R,Q)\\
       g_2(R,Q)\end{matrix}\right)
 = \left(\begin{matrix}
  R\cos(Q)\\
  R\sin(Q)
  \end{matrix}\right)
% & = \left(\begin{matrix}
%   \sqrt{-2\ln(U_1)}\cos\left(2\pi U_2\right)\\
%   \sqrt{-2\ln(U_1)}\sin\left(2\pi U_2\right)\\
%   \end{matrix}\right)\\
\end{align*}
$$
with 
$$
R = \sqrt{-\ln(U_1)}\in [0,\infty)
$$ 
and 
$$
Q=2\pi U_2\in[0, 2\pi].
$$
<!-- $$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
&=  g(r,q)\\
& = \left(\begin{matrix}g_1(r,q)\\g_2(r,q)\end{matrix}\right)\\
& = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$
for all $r\in [0,\infty)$ and all $q\in[0, 2\pi].$ -->

::: {.callout-tip}
Note that, $g$ is just the one-to-one transformation that maps points $(r,q)$ of the polar coordinate system (**radius** $r\in [0,\infty)$ and **angle** $q\in[0, 2\pi]$) to points $(x_1,x_2)$ of the Cartesian coordinate system:
$$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
& = g(r,q)\\ 
& = \left(\begin{matrix}g_1(r,q) \\ g_2(r,q)\end{matrix}\right) 
 = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$

Play around with this mapping here: 
[https://mathinsight.org/applet/polar_coordinates_map_rectangle](https://mathinsight.org/applet/polar_coordinates_map_rectangle)

The inverse mapping $g^{-1}$ maps points $(x_1,x_2)$ from the Cartesian coordinate system to polar-points $(r,q)$ in the polar coordinate system
$$
\begin{align*}
\left(\begin{matrix}r\\ q\end{matrix}\right) 
& = g^{-1}(x_1,x_2)\\
& = \left(\begin{matrix}g_1^{-1}(x_1,x_2) \\ g_2^{-1}(x_1,x_2)\end{matrix}\right) 
 = \left(\begin{matrix}\sqrt{x_1^2 + x_2^2}\\ \operatorname{atan2}(x_1,x_2)
\end{matrix}\right), 
\end{align*}
$$
<!-- where the Jacobian determinant of $g^{-1}$ is given by (derivation is a little cumbersome)
$$
\left|J_{g^{-1}}(r,q)\right| = r.
$$ -->
:::

$$
\begin{align*}
J_{g^{-1}}(x_1,x_2)
&=\left(J_{g}(r,q)\right)^{-1}\\
&=\left(\begin{matrix}
\frac{\partial g_1}{\partial r}(r,q) & \frac{\partial g_1}{\partial q}(r,q)\\
\frac{\partial g_2}{\partial r}(r,q) & \frac{\partial g_2}{\partial q}(r,q)\\
\end{matrix}\right)^{-1}\\
&=\left(\begin{matrix}
\cos(q) & -r\sin(q)\\
\sin(q) & \phantom{-}r\cos(q)\\
\end{matrix}\right)^{-1}\\
&=
\frac{1}{r\cos^2(q) + r\sin^2(q)}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)\\
&=
\frac{1}{r}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)
\end{align*},
$$
where the last step follows from Pythagorean's identity $\cos^2(q) + \sin^2(q)=1.$
So 
$$
\begin{align*}
\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|
&=\left|\operatorname{det}\left(\left(J_{g}(r,q)\right)^{-1}\right)\right|\\
&=
\left|\operatorname{det}\left(
  \begin{matrix}
            \cos(q) &            \sin(q)\\
-\frac{1}{r}\sin(q) & \frac{1}{r}\cos(q)\\
\end{matrix}\right)
\right|\\
&=
\left|\frac{1}{r}\cos^2(q) + \frac{1}{r}\sin^2(q)\right| = \frac{1}{r},
\end{align*}
$$
again using Pythagorean's identity $\cos^2(x_2) + \sin^2(x_2)=1$ and using that $r\in[0,\infty).$


Thus, by the transformation formula for bivariate densities (@eq-TransformFormula), we have 
$$
\begin{align*}
f_{X_1X_2}(x_1,x_2)
&=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|\\
&=f_{RQ}\Big(\underbrace{\sqrt{x_1^2+x_2^2}}_{=r},\underbrace{\operatorname{atan2}(x_1,x_2)}_{=q}\Big)\frac{1}{r}\\
&=\exp\left(\frac{x_1^2+x_2^2}{2}\right) \sqrt{x_1^2+x_2^2} \cdot \frac{1}{2\pi} \frac{1}{r}\\
&=\exp\left(\frac{x_1^2+x_2^2}{2}\right)  \frac{1}{2\pi},
\end{align*}
$$
where the last step uses that $r=\sqrt{x_1^2+x_2^2}.$ 

This shows the result of @thm-NormalVariableGeneration, since 
$$
f_{X_1X_2}(x_1,x_2) = \frac{1}{2\pi}\exp\left(\frac{x_1^2+x_2^2}{2}\right) 
$$
is [known to be](https://online.stat.psu.edu/stat505/lesson/4/4.2) the bivariate standard normal density for two uncorrelated (thus independent) standard normal random variables with marginal distributions $X_1\sim\mathcal{N}(0,1)$ and $X_2\sim\mathcal{N}(0,1).$ 

**Implementation** of the Box-Muller algorithm:
```{r,fig.width=10, fig.height=5, out.width='\\textwidth', fig.align='center'}
# Implementation:
BM_Algo <- function(){
  # Generate U_1, U_2 iid U[0,1]
  U <- runif(2)
  # Transformation
  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])
  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])
  # Return result
  return(c(X1, X2))
}

# Generate n standard normal random variables:
set.seed(321)

n     <- 1000
X_vec <- numeric(n)

for(i in seq(1, n, by=2)){
  X_vec[c(i, i+1)] <- BM_Algo()
}

# Descriptive Plots
par(mfrow=c(1,2))
hist(X_vec, freq = FALSE, xlim=c(-4,4))
curve(dnorm, add = TRUE, col="blue", lwd=1.3)
qqnorm(X_vec)

# Testing for Normality using the Shapiro-Wilk Test 
# H0: Normality
shapiro.test(X_vec)
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

\


### Accept-Reject Methods


For many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the distribution function $F$ is somehow unusable. For instance, surprisingly often there is no explicit form of $F$ available or its inverse does not exists.

 
Accept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density $f$ of interest---actually, $f$ needs to be known only up to a multiplicative constant. No deep analytic study of $f$ is necessary.

::: {#thm-FundamentalThmSimulation}

## Fundamental Theorem of Simulation

Let $X\in\mathbb{R}^d$ be a random variable with density function $f_X.$ Then simulating $X$ is equivalent to simulating from a  
$$
(X,U)\sim\mathcal{U}(\mathcal{A}),
$$
where $\mathcal{U}(\mathcal{A})$ denotes the uniform distribution over the area
$$
\mathcal{A}=\left\{(x,u)\text{ such that } x\in\mathbb{R}^d, 0<u<f_X(x)\right\}.
$$
:::

@fig-FundThmSim visualizes the statement of @thm-FundamentalThmSimulation.


```{r}
#| echo: false
#| eval: true
#| label: fig-FundThmSim
#| fig-cap: To simulate a random variable $X,$ one can simulate $(X,U)$ uniformely over $\mathcal{A}$ and take those $X$'s as simulation results (see tick-marks at $x$-axis). 

set.seed(123)
# Accept-Reject Algorithm:
Y <- runif(500, min = -4.5, max = 4.5) 
U <- runif(500, min =    0, max = 0.4) 
# A-R Step:
accept <- U <  dnorm(Y) - 0.025
X      <- Y[accept]
U      <- U[accept]
par(mfrow=c(1,1))
# curve(dnorm, col="blue", lwd=1.3, xlim=c(-4.5,4.5), ylab="", xlab="x")
# legend("topright", legen=expression(f[X](x)), bty="n")
curve(dnorm, col="blue", lwd=1.3, 
xlim=c(-4.5,4.5), ylim=c(0,0.45), ylab="u", xlab="x")
legend("topright", 
lty=c(1, NA), pch=c(NA, 19), col=c("blue", "orange"), 
legend=c(expression(f[X](x)), "Points (X,U) uniformly over A"), bty="n")
points(y=U, x=X, col="orange", pch=19)
axis(side=1, at = X, 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkorange", 0.5))
par(mfrow=c(1,1))
```


It turns out that sampling $(X, U)$ uniformly over the set $\mathcal{A}$ is often challenging. However, one can consider some superset $\mathcal{S},$
$$
\mathcal{A}\subseteq \mathcal{S},
$$ 
such that simulating a random variable uniformly distributed over $\mathcal{S}$ is easy. 

A uniform distribution on $\mathcal{A}$ can then be obtained by drawing from a uniform distribution on $\mathcal{S},$ and rejecting samples that are in $\mathcal{S},$ but not in $\mathcal{A}.$



#### The case of densities with compact support 

The general principle of the accept-reject method is easily explained using a *bounded* density function $f$ with *compact support*. 

::: {.callout-tip}

* **Bounded** means that there exists a constant $m$ with $0<m<\infty$ such that 
$$
\sup_xf(x)\leq m
$$ 
Note that only degenerated density functions are not bounded.
* An interval $[a,b]$ is called **compact** if it is closed and the boundaries are finite. For instance, the Gaussian density $\phi$ has *not* a compact support, but $\mathrm{supp}(\phi)=(-\infty,\infty)$.
  

Example:
$$
f(x)=\frac{3}{4}\left(1-\left(x-1\right)^2\right)\,1_{(|x-1|\leq 1)},
$$
where the (compact) support of $f$ is $[a,b]=[-1,1]$ and its range is $[0,m]=[0,3/4]$, i.e., $f$ is bounded from above by $3/4$. \
<!-- (Yes, it's a stupid example as we do not necessarily need the Accept-Reject Method here.) -->
:::

To simulate 
$$
X\sim f_X
$$
with a bounded and compactly supported density function $f_X,$ simulate the random pair 
$$
(Y,U)\sim\mathcal{U}([a,b]\times[0,m])
$$ 
by simulating   
<!-- $$Y\sim \mathrm{Unif}[a,b]\quad\text{and}\quad U\sim \mathrm{Unif}[0,m],$$ -->
$$
Y\sim\mathcal{U}[a,b]\quad\text{and}\quad U|Y=y \sim \mathcal{U}[0,m]. 
$$
<!-- but with **accepting** the pair $(Y,U)$ only if $U\leq f_X(Y)$ and **rejecting** all others.  -->
Then **accept** a simulated $Y$ as a simulation for $X$, i.e. 
$$
X=Y, 
$$
**only if** 
$$
U\leq f_X(Y),
$$ 
and **reject** all other $Y$'s. 


::: {.callout-note}
Simulating 
$$
U|Y=y \sim \mathcal{U}[0,m]
$$ 
is particularly simple, since the distribution of $U|Y=y$ is here equal to the distribution of the unconditional random variable 
$$
U|Y=y \sim U\sim \mathcal{U}[0,m]
$$
for any possible realization $Y=y.$
:::


**The Accept-Reject Algorithm (for compact densities):**

```{r}
#| echo: true
#| eval: false
# Accept-Reject Algorithm:
Y <- runif(n, min = a, max = b) 
U <- runif(n, min = 0, max = m) 
# A-R Step:
accept <- U <= f(Y)
X      <- Y[accept]
```


The following derivation shows that the simulated random variable $X$ has the correct distribution $F_X(x)=\int_a^xf_X(x)dx.$
$$
\begin{align*}
\mathbb{P}(X\leq x)
&=\mathbb{P}(Y\leq x|U\leq f_X(Y))\\[2ex]
&= \frac{\mathbb{P}(Y\leq x, U\leq f_X(Y))}{\mathbb{P}(U\leq f_X(Y))}\\[2ex]
&= \frac{\mathbb{P}(a\leq Y\leq x, \; 0\leq U\leq f_X(Y))}{\mathbb{P}(0\leq U\leq f_X(Y))}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,c\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,c\;du\,dy},
\end{align*}
$$
where the constant $c$ is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation 
$$
\int_a^{b} \int_0^{m}\,c\,du\,dy = 1,
$$
but which is irrelevant here since 
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{c\;\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{c\;\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}.\\
\end{align*}
$$
Now, using that $\int_{0}^{f_X(y)}1du=\big[x\big]^{f_X(y)}_0=f_X(y)$ yields
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{\int_a^x f_X(y)\,dy}{\int_a^b f_X(y)\,dy}\\[2ex]
& =\int_a^x f_X(y)dy = F_X(x).
\end{align*}
$$


In the following you see a graphical illustration of this procedure:

```{r,echo=FALSE, out.width='\\textwidth', fig.align='center', fig.width=10, fig.height=5}
# target pdf
target_pdf <- function(x){
  pdf <- (3/4) * (1-(x)^2)
  pdf[(x)^2 > 1] <- 0
  ##
  return(pdf)
}

# Accept-Reject Algo:
Y <- runif(n=1000, min = -1, max = 1) 
U <- runif(n=1000, min =  0, max = 3/4) 
# A-R Step:
accept <- U <= target_pdf(Y)
X    <- Y[accept]

# #######
# plots #
# #######
library("scales")
xx <- seq(-1.2, 1.2, len=500)

par(mfrow=c(1,3))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main=expression(f[X](x)), 
     xlab = "X", ylab = "Density", ylim=c(0,.85))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,.85))
rect(xleft = -1, ybottom = 0, xright = 1, ytop = c((3/4)),      col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U, x=Y, pch=19, cex=.4)
legend("topleft", legend = c("Accept","Reject"), pch=22, pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), 
     col =   c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)))
legend("topright", legend = c("(Y,U) ~ Unif([-1,1]x[0,3/4])"), pch=19, pt.cex=.4)
# 3. plot 
hist(x = X, main="Histogram of \n X=Y|(U<= f(Y))", 
     freq = FALSE, ylim=c(0,.85), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

The good thing is that we only need to evaluate the density function $f_X,$ nothing more.  




#### The case of densities with non-compact support {-}

The larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any "larger set", enclosing the pdf $f$, as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of $f$ is unbounded. 

Let the larger set denote by
$$
\mathscr{L}=\{(y,u):\, 0<u<m(y)\},
$$
where: 

* simulation of a uniform on $\mathscr{L}$ is **feasible** and 
\

* $m(x)\geq f(x)$ for all $x$.

\


From the **feasibility-requirement** it follows that $m(.)$ is necessarily integrable, i.e., that
$$\int_{\mathcal{X}}m(x)dx=M,$$
where $M$ exists and is finite (and positive), since otherwise, $\mathscr{L}$ would not have finite mass and a uniform distribution would not exists on $\mathscr{L}$.

\


Integrability of $m(.)$ is crucial here, since it allows us to relate $m(.)$ with a corresponding (auxiliary) pdf $g(.)$ as following: 
$$m(x)=M\,g(x),\quad\text{where}\quad\int_{\mathcal{X}}m(x)\,dx=\int_{\mathcal{X}}M\,g(x)\,dx=M.$$

Terminology:

* The pdf $g(.)$ is called the **instrumental density**. (Choose $g(.)$ as a pdf from which it is easy to simulate!)
* The pdf $f(.)$ is called the **target density**.

\


In order to simulate the pair $(Y,U)\sim\mathrm{Unif}(\mathscr{L})$ we can now simulate
$$Y\sim g\quad\text{and}\quad U|Y={\color{red}y}\sim\mathrm{Unif}[0,M\,g({\color{red}y})],$$
but **accept** the pair $(Y,U)$ only if $U\leq f(Y)$ and to **reject** all others.

This results in the correct distribution of the accepted value of $Y$, call it $X$, because
$$
\mathbb{P}(X\in A)=\mathbb{P}(Y\in A|U\leq f(Y))
=\frac{\int_{\color{red}A}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}{\int_\mathcal{X}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}
=\frac{\int_A f(y)\,dy}{\int_\mathcal{X} f(y)\,dy}
=\int_A f(y)dy,
$$
for every set $A$, 
<!-- measurable set $A$, i.e., is a member of the corresponding sigma algebra. -->
where we again used that $f(y)=\int_{0}^{f(y)}du$. 


Note that the above derivation implies that we only need to know the pdf $f(.)$ **up to an unkown multiplicative constant** $c>0$. I.e., it is enough to know $f(x)=c\,\tilde{f}_{\textrm{true}}(x)$, often written as $f(x)\propto \tilde{f}_{\textrm{true}}(x)$, since the unknown constant $c$ cancels out in the above quotient anyways. This is not so much of importance for us, but useful in **Bayesian Statistics**.

\

All this leads to a more general version of the Fundamental Theorem of Simulation:

*Fundamental Theorem of Simulation (General Version):*

:   Let $X\sim f$ and let $g(.)$ be a pdf s.t. $f(x)\leq M\,g(x)$ for some $M$ with $1\leq M<\infty$ and all $x$. 
    Then to simulate $X\sim f$ it is sufficient to generate
    $$Y\sim g\quad\text{and}\quad U|Y=y\sim\mathrm{Unif}[0,M\,g(y)]$$
    if one **accepts** the pair $(Y,U)$ only if $U\leq f(Y)$ and **rejects** all others.

\


The Accept-Reject Algorithm (General Version):

```
# Accept-Reject Algorithm:
Y   <- generate n random numbers from g(.)

# Specify function m():
m <- function(y){YOUR CODE}

U   <- numeric(n)
for(i in 1:n){
  U[i] <- runif(n=1, min = 0, max = m(Y[i])) 
}

# A-R Step:
accept <- U <= f(Y)
X      <- Y[accept]
```

\


**Example**

Let the target "density" be 
$$f(x)\propto \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)$$
with upper bound (or, rather, dominating density) the standard normal density
$$g(x)=\exp(-x^2/2)/\sqrt{2\pi},$$
which is obviously straightforward to generate. 

In this example we can set $m(x)=M\,g(x)$ with $M=1$, since we can simply scale the target "density" $f$ such that $f(x)\leq g(x)$ for all $x$. Specifically, we set $f(x)=0.075 \cdot \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)$. 

In the following you see the graphical illustration of this example:

```{r,echo=FALSE, out.width='\\textwidth', fig.align='center', fig.width=10, fig.height=5}
# Target pdf
target_pdf <- function(x, c=.075){
  pdf <- c * (exp(-x^2 / 2) * (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1))
  return(pdf)
}

# Upper bound
m_fun <- function(x){
  m <- exp(-x^2 / 2)/sqrt(2*pi)
  return(m)  
}

# Accept-Reject Algo:
set.seed(32280)
Y   <- rnorm(n=1000) 
U   <- numeric(length(Y))
for(i in 1:length(Y)){
  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) 
}

# A-R Step:
accept <- U <= target_pdf(Y)
X      <- Y[accept]

# #######
# plots #
# #######
xx <- seq(-4, 4, len=500)

par(mfrow=c(1,3))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main="pdf f(x)", 
     xlab = "X", ylab = "Density", ylim=c(0,.40))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,0.40))
polygon(x=c(xx, rev(xx)), y=c(m_fun(x=xx), rep(0,length(xx))), col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U, x=Y, pch=19, cex=.4)
legend("topleft", legend = c("Accept","Reject"), pch=22, pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), 
     col =   c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)))
legend("topright", legend = c("(Y,U) ~ Unif(L)"), pch=19, pt.cex=.4)
# 3. plot
hist(x = X, main="Histogram of \n X=Y|(U<= f(Y))", 
     freq = FALSE, ylim=c(0,.40), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```



**Efficiency of the Accept-Reject algorithm:** 

Statements with respect to the efficiency of the Accept-Reject algorithm can be made if $f$ and $g$ are **normalized** such that they are **both pdfs**. Then:

* The constant $M$ is necessarily larger than $1$.
* The probability of acceptance is $1/M$. (See Exercises.)
* $M$ is interpreted as the efficiency of the Accept-Reject algorithm. (The closer $M$ is to $1$ the better.)
* $M$ is a function of how closely $g$ can imitate $f$. 

Note that, for such normalized $f$ and $g$ the inequality $f(x)\leq M\,g(x)$ with $1\leq M<\infty$ for all $x$ is equivalent to saying that the quotient $f/g$ is bounded, i.e., that 
$$
0\leq \frac{f(x)}{g(x)}\leq M <\infty\quad\text{for all}\quad x.
$$
That is, it is necessary for $g$ to have, e.g., thicker tails than $f$. This makes it, for instance, impossible to simulate a Cauchy distribution $f$ using a normal distribution $g$. The reverse, however, works quite well. 
<!--
Remember: Using the Cauchy as instrumental pdf $g$ does not harm the integrability/feasibility requirement! The fact that the Cauchy distribution has no mean, does not change the fact that the density of the Cauchy integrates to one!
-->


\

**Example: Normals from Double Exponentials**

Consider generating a $N(0,1)$ by the Accept-Reject algorithm using a double-exponential distribution $\mathcal{L}(\alpha)$, also called [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution), 
with density $g(x|b)=(1/(2b))\exp(-\,|x|/b)$. <!-- here: location-param mu=0, scale.param=b\geq 0, precision=alpha=b^{-1} -->
It is then straightforward to show that 
$$
\frac{f(x)}{g(x|b)}
%=\frac{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right)}{\frac{1}{2b}\exp\left(-\frac{|x|}{b}\right)}
%=\sqrt{\frac{2}{\pi}}\,b\,\exp\left(-\frac{1}{2}x^2+\frac{|x|}{b}\right)
\leq\sqrt{\frac{2}{\pi}}\,b\,\exp\left(\frac{1}{2\,b^2}\right)
$$
and that the minimum of the bound (in $b$) is attained for $b=1$. 

This leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf:
$$
\frac{f(x)}{g(x|1)}
\leq M=\sqrt{\frac{2}{\pi}}\,\exp\left(\frac{1}{2}\right).
$$

The probability of acceptance is then $\sqrt{\pi/(2e)}=0.76$. I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average $1/0.76\approx 1.3$ uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1.

\


# Monte Carlo Method


Monte Carlo methods take advantage of the availability of:

  i. computer generated RVs, 
  ii. the Laws of Large Numbers, and 
  iii. the Central Limit Theorems.


**Notions:** 

* **Monte Carlo Method**: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.
* **Monte Carlo Integration**: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a **univariate and multivariate  integral**. (Integrals are everywhere in statistics!)
* **Stochastic Simulation** (or **Monte Carlo Simulation**): The application of the Monte Carlo method.  


\


## Classical Monte Carlo Integration


The generic problem here is the evaluation of integrals. (Be aware: Integrals are everywhere in statistics!). For instance, 
$$
\mathbb{E}_f\left(h(X)\right)=\mathbb{E}\left(h(X)\right)=\int_\mathcal{X}h(x)\,f(x)\,dx.
$$


**Convergence:**

Given our previous developments, it is natural to propose using a realization $x_1,\dots,x_m$ from a (pseudo random) i.i.d. sample $X_1,\dots,X_m$ with each $X_j$ distributed as $X\sim f$ to approximate the above integral by the empirical mean
$$
\bar{h}_m=\frac{1}{m}\sum_{j=1}^m h(x_j).
$$
By the [Strong Law of Large Numbers](http://www.statlect.com/asylln1.htm) we know that the empirical mean $\bar{h}_m$ converges almost surely (a.s.) to the desired limit $\mathbb{E}\left(h(X)\right)$ as $m\to\infty$. (The only prerequisits are that $f$ has finite first moments, i.e., $\mathbb{E}\left(h(X)\right)<\infty$, and that $\bar{h}_m$ is constructed from an i.i.d. sample $X_1,\dots,X_m$.)

As we can use the computer to produce realizations from the i.i.d. sample $X_1,\dots,X_m$, we can in principle choose an arbitrary **large sample** size $m$ such that $\bar{h}_m$ can (in principle) be **arbitrarily close** to the desired limit $\mathbb{E}\left(h(X)\right)$. 


**Though,** ... 

* ... which sample size $m$ is large enough? 
* Or "equivalently": How fast converges $\bar{h}_m$ to the desired limit $\mathbb{E}\left(h(X)\right)$?

\

**Speed of Convergence:**

OK, we know now that $\bar{h}_m$ reaches its limit (here in the "almost surely" sense) as $m\to\infty$ under some rather loose conditions on the random sample $X_1,\dots,X_m$. 

If we are willing to additionally assume that $f$ has finite second moments, i.e., $\mathbb{E}(h(X)^2)<\infty$, we can additionally say something about **how fast** $\bar{h}_m$ converges (a.s.) to $\mathbb{E}(h(X))$. 

The **speed of convergence** of the stochastic sequence $\{\bar{h}_m\}$ (i.e., now we think of $\bar{h}_m$ as the {\color{red}RV} $\bar{h}_m=\frac{1}{m}\sum_{j=1}^m h({\color{red}{X_{j}}})$) to its limit $\mathbb{E}(h(X))$ can be assessed by answering the question how fast the standard deviation (which is a function of $m$) of the stochastic sequence converges to zero as $m\to\infty$. 

* The variance of $\bar{h}_m$ is given by 
  $$
  \mathbb{V}_f\left(\bar{h}_m\right)=
  \mathbb{V}\left(\frac{1}{m}\sum_{j=1}^m h(X_j)\right)=
  \frac{1}{m}\mathbb{V}\left(h(X)\right)
  $$

* Note that assuming finite second moments $\mathbb{E}(h(X)^2)<\infty$ is equivalent to assuming finite variance $\mathbb{V}\left(h(X)\right)<\infty$. Consequently, we can set   $\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)}$ with $0<\mathtt{const}<\infty$ such that
  $$
  \sqrt{\mathbb{V}\left(\bar{h}_m\right)}=m^{-1/2}\mathtt{const}\propto m^{-1/2}.
  $$

I.e., the speed of convergence (or rate) of the stochastic sequence $\{\bar{h}_m\}$ is proportional to the deterministic sequence $\{m^{-1/2}\}$.   

\


**Remark:**
Even if we would not know the value of $\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)}$, we know now that the improvement from $m=10$ to $m=100$ will be *much* higher than from $m=110$ to $m=200$. In practice, a typical choice is $m=10000$; for moderate standard deviations this choice will guarantee a very good approximation.


\


**Limit Distribution:**

Of course, we can estimate the variance of the estimator $\mathbb{V}\left(\bar{h}_m\right)$ by its empirical version
$$
v_m=\frac{1}{m}\left(\frac{1}{m}\sum_{j=1}^m\left(h(x_j)-\bar{h}_m\right)^2\right),
$$
where again by the [Strong Law of Large Numbers (SLLN)](http://www.statlect.com/asylln1.htm)
$$
\left(\frac{1}{m}\sum_{j=1}^m\left(h(x_j)-\bar{h}_m\right)^2\right)\to_{\text{a.s.}}\mathbb{V}\left(h(X)\right).
$$
<!--
I.e., after some rewriting, we have that: 
$$
\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{v_m}}\to_{\text{a.s.}}
\sqrt{m}\left(\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right).
$$
-->
By the [Central Limit Theorem (CLT)](http://www.statlect.com/central_limit_theorem.htm) we have 
$$
\sqrt{m}\left(\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right)\to_d Z,
$$
where $Z\sim N(0,1)$. Note that the the above sequence $\{\sqrt{m}\}$ **just hinders** the convergence of the sequence $\bar{h}_m - \mathbb{E}\left(h(X)\right)\to_{a.s.}0$ such that the quotient converges to a "stable" distribution. 


The above result can now be used for the construction of (asymptotically valid) **convergence tests** and **confidence intervals** with respect to $\bar{h}_m$, since for large $m$ 
$$
\bar{h}_m\,\overset{d}{\approx} N\left(\mathbb{E}\left(h(X)\right),\frac{\mathbb{V}\left(h(X)\right)}{m}\right).
$$

And as we can use the computer to generate realizations of the i.i.d. sample $X_1,\dots,X_m$ from a generic $X\sim f$, we can easily approximate the mean $\mathbb{E}\left(h(X)\right)$ and the variance $\mathbb{V}\left(h(X)\right)$ with arbitrary accuracy as $m\to\infty$; by the SLLN (or the WLLN).


\


**Example: A first Monte Carlo Integration**

Let's say we want to integrate the function $h(x)=\left(\cos(50\,x)+\sin(20\,x)\right)^2$. Although this function could be integrated analytically it is a good first test case. The left plot below shows the graph of the function $h(.)$.

To approximate the integral 
$$
\int_\mathcal{X}h(x)dx\quad\text{with}\quad\mathcal{X}=[0,1]
$$ 
we can use that
$$
\int_\mathcal{X}h(x)dx=\int_\mathcal{[0,1]}1\cdot h(x)dx =\mathbb{E}_{f_\text{Unif[0,1]}}(h(X)).
$$ 

Thus, we generate a realization $(u_1,\dots,u_n)$ from the i.i.d. random sample $U_1,\dots,U_n\sim[0,1]$ and approximate 
$$
\int_\mathcal{X}h(x)dx\approx \bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(u_i).
$$ 

In order to assess how good this approximation is, we need to consider the stochastic propoerties of the RV 
$$
\frac{1}{n}\sum_{i=1}^n h(U_i).
$$ 
This is done using the above (review of) results on the limit distribution of the sample mean which allows us to construct an approximative $95\%$ confidence interval, since for large $n$ 
$$
\left[\bar{h}_n - 1.96\frac{\mathtt{std.error}_n}{\sqrt{n}}, \bar{h}_n + 1.96\frac{\mathtt{std.error}_n}{\sqrt{n}}\right]\approx
\left[\bar{h}_n - 1.96  \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}, \bar{h}_n + 1.96  \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}\right],
$$
where $\mathtt{std.error}_n^2=n^{-1}\sum_{i=1}^n(h(u_i)-\bar{h}_n)^2$. 

The right plot below shows one realization of the stochastic sequence $\{\bar{h}_1,\dots,\bar{h}_n\}$ with $n=10000$, where the realized value of $\bar{h}_n$ is $0.966$. This compares favorably with the with the exact value of $0.965$.



```{r,echo=FALSE, out.width='\\textwidth', fig.align='center', fig.width=10, fig.height=5}
# h(x):
h_fun <- function(x){
  result <- (cos(50*x)+sin(20*x))^2
  return(result)
}

# sample size
n <- 10000

# Generate sample of uniforms
set.seed(321)
u_vec <- runif(n=n)

# Approximation of the integral 
# More precisely: This is one realization of the stochastic 
# sequence \bar{h}_1,...,\bar{h}_n.
h_bar_n <- cumsum(h_fun(u_vec))/c(1:n)

# 'Final' value: 
# round(h_bar_n[n], digits = 3)
# True value:
true.value <- 0.965

# 95% CI
# Standard error of the estimator using the algebraic 
# formula for the variance (german: 'verschiebungssatz')
st.error_n <-  sqrt((cumsum(h_fun(u_vec)^2) - cumsum(h_fun(u_vec))^2/(1:n))/(1:n))
CI_u       <-  h_bar_n + 1.96 * st.error_n / sqrt(1:n)
CI_l       <-  h_bar_n - 1.96 * st.error_n / sqrt(1:n)

par(mfrow=c(1,2))
# plot 1:
xx  <- seq(from=0, to=1, len=500)
plot(x = xx, y = h_fun(xx), type="l", main="Function h(.)", xlab="x", ylab="h(x)")
# plot 2: 
plot(x = c(1:n), y = h_bar_n, type="n", ylim=c(0.7,1.2), 
     xlab = "n", ylab = "")
polygon(x = c(1:n, rev(1:n)), y = c(CI_u, rev(CI_l)), 
        col=alpha("blue", 0.5), border=alpha("blue", 0.5))
lines(x = c(1:n), y = h_bar_n, type="l")
lines(x = c(1:n), y = rep(true.value, n), type="l", col="red")
##
legend("topright", legend = c(expression(bar(h)[n]), "True Value", "95% CI"), lty=c(1,1,0), pch=c(22,22,22), pt.cex=c(0,0,2),
     pt.bg = c("black", "red", alpha("blue", 0.5)), 
     col =   c("black", "red", alpha("blue", 0.5)))
```
```{r, echo=FALSE}
par(mfrow=c(1,1))
```

\

**Remarks:**

* The approach followed in the above example can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency through numerical methods (e.g., [Riemann Sum](https://en.wikipedia.org/wiki/Riemann_sum), [Trapezoidal Rule](https://en.wikipedia.org/wiki/Trapezoidal_rule), [Simpson's Rule](https://en.wikipedia.org/wiki/Simpson%27s_rule), etc.) in dimensions 1 or 2. 
* The approach is particularly useful for approximating integrals over higher dimensional sets. 


\


**Example: Approximation of Normal Distribution Tables**

A possible way to construct normal distribution tables is to use MC simulations. 

Generate a realization $(x_1,\dots,x_n)$ from an i.i.d. standard normal random sample, e.g., using the Box-Muller algorithm. 

The approximation of the standard normal cdf
$$
\Phi(t)=\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy
$$
by the Monte Carlo method is thus
$$
\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n 1_{(x_i\leq t)}.
$$
The corresponding RV $\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n1_{(X_i\leq t)}$ has (exact) variance 
$$
\mathbb{V}(\hat{\Phi}_n(t))=\frac{\Phi(t)(1-\Phi(t))}{n},
$$
since the single RVs $1_{(X_i\leq t)}$ are independent Bernoulli with success probability $\Phi(t)$. 

For values of $t$ around $t=0$, the variance is thus approximately $1/4n$. 

To achieve a precision of **four decimals** by means of a $99.9\%$ confidence interval, the approximation requires on average $n\approx 10^8$ simulations. 

The table below gives the evolution of this approximation for several values of $t$ and shows a very accurate evaluation for $n=10^8$. 

\

$$
\begin{array}{cccccccccc}
\hline
n   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\
\hline
10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\
10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\
10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\
10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\
10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\
10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\
10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\
\end{array}
$$

\

**Remarks:**

* To achieve a precision of **two decimals** by means of a $99.9\%$  confidence interval, already $n=10^4$ leads to satisfactory results. 
* Note that **greater accuracy is achieved in the tails** and that more efficient simulation methods could be used (e.g., Importance Sampling). 

\


## Importance Sampling

Importance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it's refereed to as a **variance reduction** technique. This variance reduction is achieved by weighting functions, so-called **importance functions**. 

As in the case of Monte Carlo integration the focus lies on evaluating the integral
$$
\mathbb{E}_f(h(X))=\int_\mathcal{X}h(x)f(x)\,dx.
$$

* Though, it turns out that the above approach, i.e., sampling from $f$ is often suboptimal. 

* Observe that the value of the above integral can be represented by infinitely many alternative choices of the triplet $(\mathcal{X}, h, f)$. Therefore, the search for an optimal estimator should encompass all these possible representations.

Let's illustrate this with a simple example.


**Example: Cauchy Tail Probability (from [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))**

Suppose that the quantity of interest is the probability, say $p$, that a [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution) $\mathrm{C}(0,1)$ RV is larger than $2$, i.e.:
$$
p=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx.
$$

**1. Naive Approach:** 
If $p$ is approximated through the empirical mean
$$
\hat{p}_{1}=\frac{1}{m}\sum_{j=1}^m1_{(X_j>2)}
$$
of an i.i.d. sample $X_1,\dots,X_m\sim\mathrm{C}(0,1)$, then the variance of this estimator, a [binomial](https://en.wikipedia.org/wiki/Binomial_distribution) RV scaled by $1/m$, is 
$$
\mathbb{V}(\hat{p}_{1})=\frac{1}{m^2}\mathbb{V}\left(\sum_{j=1}^m1_{(X_j>2)}\right)=\frac{p(1-p)}{m},
$$
which is equal to $0.1275/m$, since (we already know that) $p=0.15$. 

\



**2. Accounting for Symmetry (i.e., using the 'Adjusting Screws' $\mathcal{X}$ and $h$):**
We can achieve a **more efficient estimator** (i.e., an estimator with lower variance for a given same sample size $n$) if we take into account the symmetric nature of $\mathrm{C}(0,1)$. Obviously, our target integral can be equivalently written as
$$
p=\frac{1}{2}\left(\int_{-\infty}^{-2}\frac{1}{\pi(1+x^2)}\,dx + \int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx \right).
$$
This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean:
$$
\hat{p}_{2}=
\frac{1}{2}\left(\frac{1}{m}\sum_{j=1}^m1_{(X_j<-2)}+ \frac{1}{m}\sum_{j=1}^m1_{(X_j>2)}\right)\;=\;
\frac{1}{2m}\sum_{j=1}^m1_{(|X_i|>2)}.
$$
The variance of this new estimator,
$$
\mathbb{V}(\hat{p}_{2})=\frac{1}{4m^2}\mathbb{V}\left(\sum_{j=1}^m1_{(|X_i|>2)}\right)=\frac{2p(1-2p)}{4m},
$$
is equal to $0.0525/m$, i.e., lower than in the naive approach.

\



**3. Using all 'Adjusting Screws' $\mathcal{X}$, $h$, and $f$:**
The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, $[2,+\infty)$, which are in some sense irrelevant for the approximation of $p$. This motivates the following reformulation of $p$: 

By symmetry of $f$:
$$
\frac{1}{2}=\int_{0}^2\frac{1}{\pi(1+x^2)}dx + \underbrace{\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}dx}_{=p}
$$
$$
\Leftrightarrow \; p=\frac{1}{2}-\int_{0}^2\frac{1}{\pi(1+x^2)}dx.
$$
Furthermore, we can re-arrange the last integral a bit such that 
$$
\int_{0}^2\;\left(\frac{1}{2}\cdot 2\right)\;\frac{1}{\pi(1+x^2)}\,dx = 
\int_{0}^2\;\underbrace{\frac{1}{2}}_{f_{\mathrm{Unif}[0,2]}}\;\underbrace{\frac{2}{\pi(1+x^2)}}_{=h(x)}\,dx = 
\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,2].
$$


Therefore a new alternative method for evaluating $p$ is:
$$
\hat{p}_{3}=\frac{1}{2} - \frac{1}{m}\sum_{j=1}^m h(U_j),\quad\text{where}\quad U_j\sim\mathrm{Unif}[0,2].
$$
Using integration by parts, it can be shown that $\mathbb{V}(\hat p_3)=0.0285/m$. (Compare this to the former results: $\mathbb{V}(\hat{p}_{2})=0.0525/m$ and $\mathbb{V}(\hat{p}_{1})=0.1275/m$.)

\


**A More General Point of View:**

The idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating 
$$
\theta=\mathbb{E}_f(h(X))=\int h(x)f(x)dx.
$$

Some outcomes of $X\sim f$ may be more important than others in determining $\theta$ and we wish to select such values more frequently. 

For instance, if $\theta$ denotes the probability of the occurrence of a very rare event, then the only way to estimate $\theta$ at all accurately may be to produce the rare events more frequently. 

To achieve this, we can simulate a model which gives pdf $g$ to $X$ instead of the correct pdf $f$, where both pdfs need to be known. This can be easily done, since
$$
\theta=\mathbb{E}_f(h(X))=\int h(x)\left(\frac{g(x)}{g(x)}\right)\;f(x)dx=
\int \underbrace{\left(h(x)\frac{f(x)}{g(x)}\right)}_{=\psi(x)}\;g(x)dx=
\int \psi(x)\;g(x)dx=
\mathbb{E}_g(\psi(X)).
$$

This leads to the following unbiased estimator for $\theta$ based on sampling from $g$:
$$
\hat{\theta}_g=\frac{1}{n}\sum_{i=1}^n\psi(X_i)\quad\text{with}\quad X_i\sim g,
$$
which is a weighted mean of the $h(X_i)$ with weights inversely proportional to the "**selection factor**" $\frac{g(X_i)}{f(X_i)}$.
<!-- 
For appropriately chosen pdfs f and g: 
The *selection factor* refers to "how more likely is it to select a 'rare event'?".
The inverse weight re-scales these 'too often' chosen rare events. 
-->


For the variance of the estimator $\hat{\theta}_g$ we have
$$
\mathbb{V}(\hat{\theta}_g)=\frac{1}{n}\mathbb{V}(\psi(X_i))=
\frac{1}{n}\int\left(\psi(x)-\theta\right)^2g(x)dx=
\frac{1}{n}\int\left(\frac{h(x)\,f(x)}{g(x)}-\theta\right)^2g(x)dx,
$$
which, depending on the choice of $g(.)$, can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean.
<!-- 
$$
\mathbb{V}(\hat{\theta}_{\text{naive}})=
\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n 1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=
\frac{1}{n}\mathbb{V}\left(1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=\frac{\theta(1-\theta)}{n}.
$$ 
-->

\
\


*Minimum Variance Theorem*

:    The **importance function** $g(.)$ which minimizes the variance $\mathbb{V}(\psi(X_i))$, and therefore the variance $\mathbb{V}(\hat{\theta}_g)$,      is given by 
     $$
     g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
     $$ 


**Proof:** Done in the lecture. 

\


Though, this result is rather formal (in the sense of "impractical"), since, e.g., if $h(x)>0$ then $g^\ast$ requires us to know $\int h(z)f(z)dz$, which is just the integral of interest! 

**Remarks:**

The above minimum variance result is still useful:

*  It tells us that a good choice of $g(x)$ shall mimic the shape of $|h(x)|f(x)$, since the optimal $g^\ast(x)\propto |h(x)|f(x)$.
*  Furthermore, $g(x)$ should be chosen such that it has a thicker tail than $f(x)$, since the variance $\mathbb{V}(\hat{\theta}_g)$ crucially depends on the quotient $f(x)/g(x)$ which would "explode" for $g(x)\approx 0$.


\


Let's apply our new insights to the above example on the Cauchy tail probability $p$. 

**Example: Cauchy Tail Probability (cont.)**


Above we had: 

1. $f(x)=\frac{1}{\pi(1+x^2)}$, the pdf of $\mathrm{C}(0,1)$ and
2. $h(x)=1_{(x>2)}$, i.e., here $|h(x)|=h(x)$.

Therefore
$$
p=\mathbb{E}_f(h(X))=\int h(x)f(x)dx=\int_{2}^{\infty}f(x)dx=\int_{2}^{\infty}\underbrace{\frac{f(x)}{g(x)}}_{=\psi(x)}\;g(x)dx=\mathbb{E}_g(\psi(X)),
$$
where the $h$ function is absorbed by the formulation of the definite integral.

A possibly good (and simple) choice of $g$ is, e.g., $g(x)=2/(x^2)$, since this function:

* "closely" matches $h(x)f(x)$ and 
* $g$ has thicker tails than $f$. 


```{r, fig.width=7, fig.height=4, fig.align='center', echo=FALSE}
# g(x):
g_fun <- function(x){2/(x^2)}
# 
xx    <- seq(from= 0, to=15, length.out = 500)
# plot
plot(x = xx, dcauchy(xx, location = 0, scale = 1), 
     type="l", xlab="", ylab="", ylim=c(0,0.55), axes = FALSE)
axis(side = 1, at=c(0,2,5,10,15)); axis(side = 2)
# add graph of g(x)
lines(x = xx[xx>2], g_fun(xx[xx>2]), col="red")
# region of interest selected by h(x): x>2
abline(v=2, lty=2)
# legend
legend("topright", legend = c("pdf-Cauchy(0,1)", expression(g(x)==2/(x^2))), lty=c(1,1),
     col = c("black", "red"))
```

\

**Caution:** It is not straight forward to directly sample from $g$, therefore we need some further steps:

\

The choice of $g$ leads to 
$$
p=\mathbb{E}_g(\psi(X))=
\int_{2}^{+\infty}\left(\frac{x^2}{2\,\pi(1+x^2)}\right)\,\frac{2}{x^2}\,dx=
\int_{2}^{+\infty}\left(\frac{1}{\pi(1+x^{-2})}\right)\,x^{-2}\,dx.
$$

\


Now we can apply some additional (rather case-specific) re-arrangements:

Integration by substitution (substituting $u=x^{-1}$) yields:
$$
p=\int_{0}^{1/2}\frac{1}{\pi(1+u^2)}du.
$$
Again, we can re-arrange the last integral a bit such that
$$
p=\int_{0}^{1/2}\underbrace{2}_{f_{\mathrm{Unif}[0,1/2]}}\;\underbrace{\frac{1}{2\,\pi(1+u^2)}}_{=h(u)}\,du=\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
Therefore, we have a final fourth version of the estimator of $p$:
$$
\hat{p}_4=\sum_{j=1}^m h(U_j),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2] 
$$
and $h(u)=1/(2\pi(1+u^2))$. 

The variance of $\hat{p}_4$ is $(\mathbb{E}(h(U)^2)-\mathbb{E}(h(U))^2)/m$ and an integration by parts shows that $\mathbb{V}(\hat{p}_4)=0.95\cdot 10^{-4}/m$. Compare this to the former results: $\mathbb{V}(\hat p_3)=0.0285/m$, $\mathbb{V}(\hat{p}_{2})=0.0525/m$ and $\mathbb{V}(\hat{p}_{1})=0.1275/m$. The variance of $\hat{p}_4$ is by a factor of $10^{-3}$ lower than the variance of the original $\hat{p}_1$.


<!--
**Version 2:** It can be shown that (see [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))
$$
p=\int_0^{1/2}\frac{y^{-2}}{\pi(1+y^{-2})}dy,
$$
where this integral can also be seen as the expectation of 
$$
\frac{1}{4}h(U)=\frac{1}{2\pi(1+U^2)},\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
-->




<!--
## The 'Real Need' of Monte Carlo Simulation 

**Monte Carlo Simulation applied to Hypothesis Tests**

* **Problem:** Very often, statistical test procedures (or estimators) rely on asymptotic arguments. Asymptotic arguments ease the live of a statistician. In practice, however, we never have something like a diverging sample size of $n\to\infty$, but need to deal with a finite sample size $n$. All we can hope for is that the asymptotic results (e.g., on the level of significance of a test statistic and its power) are good approximations to the finite $n$ case. 

* **Solution:** Monte Carlo simulation the classical tool to investigate the finite $n$ performance of statistical test procedures (or estimators). It helps to answer questions like: How good are the asymptotic results given finite sample size scenarios of $n=100$, $n=500$, etc.

For instance, the **likelihood ratio (LR)** test statistic  
$$
-2\,\log\left[\ell(\hat\theta|x)/\ell(\hat\theta_0|x)\right]=-2\,\left\{\log\ell(\hat\theta|x)-\log\ell(\hat\theta_0|x)\right\}\to_d\chi^2_r
$$
is distributed as $\chi^2_r$ generally only in the limiting case as $n\to\infty$; and under some regularity constraints on the likelihood function. In the formula above $\ell(\theta|x)$ denotes the likelihood function, $\hat\theta\in\mathbb{R}^k$ is the estimated (via maximum likelihood) parameter vector from the unconstrained model and $\hat\theta_0\in\mathbb{R}^k$ is the estimated parameter vector from the constrained model with $r\leq k$ restrictions.

\

**Example: Contingency Tables**

The following table gives the results of a study comparing radiation therapy with surgery in treating cancer of the larynx. 


$$
\begin{array}{c|cc|c}
                   &  \text{Cancer}  &  \text{Cancer not}&           \\
                   &  \text{Controlled}  &  \text{Controlled}&           \\
\hline                   
\text{Surgery}     &  y_{11}=21      &   y_{12}=2        &  n_{1.}=23\\
\text{Radiation}   &  y_{21}=15      &   y_{22}=3        &  n_{2.}=18\\
\hline
                   &  n_{.1}=36      &   n_{.2}=5        &  n=41
\end{array}
$$

\

Let's condition on a fix number of total observations $n$. Then the random vector $(Y_{11},Y_{12},Y_{21},Y_{22})^\top$ comes from a [multinomial distribiton](https://en.wikipedia.org/wiki/Multinomial_distribution) with 4 cells and cell probabilities 
$$
p=(p_{11}, p_{12}, p_{21}, p_{22}),\quad\text{with}\quad\sum_{ij}p_{ij}=1,
$$
that is, 
$$
(Y_{11},Y_{12},Y_{21},Y_{22})^\top\sim\mathcal{M}_4(n, p).
$$
With $y_{ij}$ denoting the number of realizations in cell $ij$, the likelihood function can be written as 
$$
\ell(p|y)\propto\prod_{ij}p_{ij}^{y_{ij}},
$$
where the 4-dimensional parameter space can be displayed as following:
$$
\begin{array}{cc|c}
p_{11} & p_{12} & p_{1.}\\
p_{21} & p_{22} & p_{2.}\\
\hline
p_{.1} & p_{.2} & 1\\
\end{array}
$$



**Null Hypothesis:** The null hypothesis to be tested is one of independence, which is to say that the surgery treatment has no bearing on the control of cancer. Translated into a parameter statement this means
$$\text{H}_0: p_{11}=p_{1.}\,p_{.1}\quad\text{against}\quad\text{H}_1: p_{11}\neq p_{1.}\,p_{.1}.$$


The likelihood ratio statistic for testing this hypothesis is 
$$
\lambda=\frac{\max_{p\text{ s.t. }p_{11}=p_{1.}p_{.1}}\ell(p|y)}{\max_{p}\ell(p|y)}.
$$
It is "straightforward" to show that the denominator maximum is attained at:
$$
\hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all}\quad ij. 
$$
and the numerator maximum at
$$
\hat{p}_{11}=\hat{p}_{1.}\hat{p}_{.1}\quad\text{with}\quad \hat{p}_{1.}=\frac{n_{1.}}{n}\quad\text{and}\quad\hat{p}_{2.}=\frac{n_{2.}}{n},
$$
$$
\text{and} \quad \hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all other}\quad ij. 
$$

As mentioned above, under H$_0$, $-2\log \lambda$ is asymptotically distributed as $\chi^2_1$. However, with only $n=42$ observations, the asymptotics do not necessarily apply. One alternative is to use devise a **Monte Carlo experiment** to simulate the null distribution of $-2\log \lambda$ or equivalently of $\lambda$ in order to obtain a cutoff point for a hypothesis test. A more sophisticated approach is that of [Mehta at el. (2000)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10473906).  

**Description of the Procedure:**

* Let's denote the finite $n$ null distribution of $\lambda$ by $f_{0,n}(.)$. As we are interested in an $\alpha$ level test, we need to specify $\alpha$ (e.g., $\alpha=0.05$) and to solve the following integral for the $1-\alpha$ quantile $\lambda_\alpha$:
$$
\int_0^{\lambda_\alpha}f_{0,n}(u)du=1-\alpha.
$$

* The standard Monte Carlo approach to this problem is to generate random variables $\lambda_k\sim f_{0,n}$, $k=1,\dots,m$, then order the sample 
$$
\lambda_{(1)}\leq \lambda_{(2)}\leq\dots\leq \lambda_{(m)}
$$
and finally calculate the empirical $1-\alpha$ quantile $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$, where $\lfloor x \rfloor$ is the next lower integer to $x$, e.g., $\lfloor 2.9 \rfloor=2$.

* Similarly to the above integration example which builds on the SLLN, the central idea here is to use the so-to-say SLLN for quantiles:
$$
\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}\to_{a.s.} \lambda_{\alpha}\quad\text{as}\quad m \to\infty;
$$
see, e.g., the classical book "Approximation Theorems of Mathematical Statistics" of R. Serfling Ch. 2.3.1. As a computer is doing this job for us, we can in principle choose an arbitrary large $m$ such that the above approximation of $\lambda_{\alpha}$ by $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$ can be arbitrarily good. 

To run the 

```{r}
set.seed(123)
#
p_init <- runif(4)
#
p_11   <- p_init[1]
p_12   <- p_init[2]
p_21   <- p_init[3]
p_22   <- p_init[4]
#
p_1.   <- p_11 + p_12
p_.1   <- p_11 + p_21
#
p_11   <- p_1. * p_.1

# probabilities under H0:
p_0 <- c(p_11, p_21, p_12, p_22)


Y_vec <- rmultinom(n=1, size=4, prob=p_0)

```
-->

<!-- 
**Example: LR Test**

Let's assume we want to test the following regression model 
$$
Y_t=\beta_0 + \beta_1 t +\varepsilon_{t}
$$
against the constrained model without a time trend:
$$
Y_t=\beta_0 + \varepsilon_{t},
$$
where $\varepsilon_{t}\sim N(0,\sigma_\varepsilon^2)$ with $0<\varepsilon^2<\infty$.


```{r}
LL_fun <- function(beta0, beta1, sigma) {
      # residuals
      res_vec   <-  y - x * beta1 - beta0
      # log-transformed probabilities of observing the vector of residuals res_vec: 
      Log_probs <- dnorm(x = R, mean = 0, sd = sigma, log = TRUE)
      # 
      result <- - sum(Log_probs)
      return(result)
}
```
-->


\
\
\
\
<!-- 
Alternative Header-Syntax:
  ## A level-two header
  ### A level-three header ###
-->


