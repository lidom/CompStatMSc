# Random Variable Generation

### Literature {-}

In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:

* [Monte Carlo Statistical Methods](http://www.springer.com/us/book/9780387212395), Ch. 2, @RobertCasella1999 
* [Introducing Monte Carlo Methods with R](https://link.springer.com/book/10.1007/978-1-4419-1576-4), Ch. 2, @RobertCasella2010

<!-- * Further:
  * [Non-Uniform Random Variate Generation](http://luc.devroye.org/rnbookindex.html), Devroye, L. 
  * [Nonparametric Density Estimation: The L1 View](http://luc.devroye.org/L1bookBW.pdf), Devroye, L., Ch. 8
  * [Monte Carlo and Quasi-Monte Carlo Sampling](http://www.springer.com/us/book/9780387781648), Lemieux, C., Ch. 2 and 3 -->



### Introduction & Motivation {-}

In many complex statistical models parameter estimates can only be calculated numerically and their (statistical) behavior can only be investigated through a **simulation** on the computer: 

**Stylized Algorithm:**

* **Step 1** Generate artificial data from a statistical model and compute the estimate(s) of interest. 
* **Step 2** Repeat this 10000 (or more) times.
* **Step 3** From these 10000 "pseudo" realizations of the estimator we can directly compute, e.g., the bias or the mean squared error (MSE) of the estimator---without the need of complicated statistical theory.


```{r}
## Step 1
generate_mean_estimates <- function(n = 100){
  ## generate artificial data
  artificial_data <- rnorm(n)   
  ## compute the estimate
  estimate        <- mean(artificial_data) 
  return(estimate)
}

## Step 2 
set.seed(223)
simulated_estimates <- replicate(1000, 
                                 generate_mean_estimates(n = 100))

## Step 3
mean(simulated_estimates)
sd(simulated_estimates)

hist(simulated_estimates, 
     main = "Histogram of Simulated Mean Estimates", 
     xlab = "")
```


In order to conduct Steps 1 and 2 we need artificial realizations (here we used `rnorm()`) of a random variable---so-called **Pseudo Random Numbers**. The generation of such pseudo random numbers is the topic of the following chapters. 


## Uniform Simulation


**General procedure:**

* Usually, a random integer with values **uniformly** in $[0,m]$ with a large integer $m$ is generated. 
* To achieve a random number in $[0, 1]$, we divide this number by $m$. 
* From this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.


There are many different **Random Number Generators (RNGs)**, we consider the most simple class of RNGs:

::: {.callout-note icon=false} 

## 
::: {#def-LinConGen}

## Linear Congruential Generators 

Here the $i$th random integer $u_i$ is generated by
$$
u_i=(a u_{i-1}+c) \,\operatorname{mod}\, m,
$$
where the starting value $u_0$ is a chosen and fixed value called **seed**.

Furthermore:
    
* $m$, with $0<m$, is called the **modulus**
* $a$, with $0<a<m$, is called the **multiplier**
* $c$, with $0\leq c<m$, is called the **increment**
:::
:::

::: {.callout-tip}

## The modulo operator: $\operatorname{mod}$

"$b\,\operatorname{mod}\,c$" denotes the remainder of the division of $b$ by $c$.

For instance 
$$
\begin{align*}
4\,&\operatorname{mod}\,2 = 0\\
5\,&\operatorname{mod}\,2 = 1\\
1\,&\operatorname{mod}\,2 = 1\\
\end{align*}
$$


```{r, eval=FALSE}
# Modulo computation using the modulo operator '%%'
5 %% 4
9 %% 4
4 %% 5

# own modulo-function:
my_mod <- function(x,m){
  t1 <- floor(x/m)
  return(x-t1*m)
}
```

:::


Some Facts:

* The above recursion generates a completely **nonrandom** sequence, therefore it is often called a ***pseudo* random** sequence. 
* Under appropriate choices of $u_0$ , $a$ and $m$ the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on $[0, m]$.
* The cycle length of linear congruential generators will never exceed modulus $m$, but can maximized with the three following conditions (see [Knuth (2002)](https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) for a proof):
    * The increment $c$ is [relatively prime](https://en.wikipedia.org/wiki/Coprime_integers) to $m$,
    * $a - 1$ is a multiple of every prime dividing $m$,
    * $a - 1$ is a multiple of $4$ when $m$ is a multiple of $4$.
    


<!--
Further reading:
Ripley (1987) provides a review of number generators. 
Devroye (1986) is very comprehensive book on non-uniform random variate generation.
-->



**Bad choice of parameters** for the linear congruential random number generator:
```{r}
m <- 64    # modulus
a <- 33    # multiplier
c <- 12    # increment
s <- 57    # seed
n <- 1000  # length of run (including seed)

r_vec    <- numeric(n) # initialize vector
r_vec[1] <- s # set seed

## Recursive generation 
for (i in 1:(n-1)){
 r_vec[i+1] <- (a * r_vec[i] + c) %% m
}

# scale result from [0,m] to [0,1]:
my_bad_runif_vec <- r_vec/m

# BUT! Very short cycle-length (here: period=16)
r_vec[ 1:16]
r_vec[17:32]
```



::: {#exm-GoodVsBadRNG}

## Good vs. Bad RNGs 

Average heads ratios 
$$
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i
$$ 
for $n=1,2,\dots$ simulated independent tosses of a fair coin $C_i$ with  
$$
C_{i}=\left\{\begin{array}{ll}
1&\text{if Head}\\
0&\text{if Tail}
\end{array}\right.
$$ 
and
$$
P(C_i=0)=P(C_i=1)=0.5.
$$ 
By the strong (or weak) law of large numbers the average should converge **stochastically** (i.e., almost surely or in probability) to $0.5$ as $n$ becomes large $(n\to\infty),$
$$
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i \to_p 0.5,\quad n \to\infty.
$$ 

```{r, fig.width=7, fig.height=4, fig.align='center'}
#| label: fig-goodvsbad
#| fig-cap: "Two sample paths showing the pseudo random convergence of $\\bar{C}_n$ to the limit 0.5---one based on a good RNG and the other based on a bad RNG."

# using the above bad RNG:
bar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)

# using R's high-quality RNG:
set.seed(223)
bar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)

# plotting the results:
plot(bar_x_bad, type="l", ylim=c(0.46,0.54), 
     xlab="", ylab="", main="Good vs. Bad RNG")
lines(bar_x_good, col="darkblue")
```
::: 

<!--
```{r, eval=FALSE}
## See also:
install.packages("randtoolbox")
library("randtoolbox")
?congruRand()
```
-->

::: {.callout-note}
IBM's [RANDU](https://en.wikipedia.org/wiki/RANDU) is a famous example of an miss-specified linear congruential RNG.
:::



## Generation of Discrete Random Variables

<!-- https://rpubs.com/maheshraje982/617094 -->

Assume the discrete random variable $X$ of interest takes on the values 
$$
X\in \{x_1, \dots , x_k \}
$${#eq-DiscRV}
with
$$
p_i = \mathbb{P}(X = x_i ), \quad i = 1,\dots , k,
$$
and 
$$
\sum_{i=1}^kp_i = 1.
$$

Assume that you can generate pseudo-random realizations $u\in[0,1]$ from a uniformly distributed random variable $U\sim\mathcal{U}[0, 1]$ using an RNG.


**General principle:** 

1. Subdivide $[0, 1]$ into $k$ intervals with
$$
I_i = (a_{i-1}, a_i],
$$
where 
$$
a_i = \sum_{j=1}^ip_j\quad\text{and}\quad a_0 = 0.
$$
2. Define the new discrete realizations 
$$x=\left\{
\begin{array}{cc}
          x_1&\quad\text{if}\quad u\in I_1\\
          \vdots& \vdots\\
          x_k&\quad\text{if}\quad u\in I_k
          \end{array}\right.
$${#eq-U2DiscrX}


::: {.callout-note icon=false} 

## 
::: {#lem-GenDiscrRV}

Let $u$ be a realization from $\mathcal{U}[0, 1]$ and if $u\in I_i$, set $x = x_i$ (see @eq-U2DiscrX). Then $x$ is a realizaton of the discrete random variable $X$ (see @eq-DiscRV).
      
::: 
:::
<!-- **Proof:** Done in the lecture. -->


**Proof of @lem-GenDiscrRV:** 

For any $i = 1, \dots, k$ we have that
$$
\begin{align*}
\mathbb{P}(X = x_i) 
& = \mathbb{P}(U \in I_i)  \\
& = F_\mathcal{U}(a_i) - F_\mathcal{U}(a_{i-1})\\
& = a_i - a_{i-1}\\
& = \sum_{j=1}^ip_j - \sum_{j=1}^{i-1}p_j  = p_i,
\end{align*}
$$
which shows the statement of @lem-GenDiscrRV. 


::: {#exm-Bernoulli}

## Bernoulli Distribution

Generate random numbers from 
$$
X\sim\mathrm{Bernoulli}(p),
$$
where $p$ is the probability of success, i.e., 
$$
\mathbb{P}(X=1)=p\quad\text{and}\quad\mathbb{P}(X=0)=1-p.
$$

**Algorithm:** If $U\sim\mathcal{U}[0,1]$ and $p$ is specified, define
$$
X=\left\{
  \begin{matrix}
  1 & \text{if }U\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then $X\sim\mathrm{Bernoulli}(p)$.


```{r}
set.seed(321)
# Generate one random number from Bernoulli(p) with p=0.5
p  <- 0.5
U  <- runif(1)

if(U<=p) X=1 else X=0

X
```
:::




::: {#exm-Binom}

## Binomial Distribution

Generate random numbers from 
$$
X\sim\mathrm{Binomial}(n,p),
$$
where $n$ is the number of trials and $p$ the probability of success such that 
$$
\mathbb{P}(X=i)=\binom{n}{i}p^i(1-p)^{n-i}
$$
for $i=1,\dots,n.$

::: {.callout-tip}
If $X_1,\dots,X_n\overset{i.i.d}{\sim}\mathrm{Bernoulli}(p),$ then 
$$
X=\sum_{i=1}^nX_i \sim\mathrm{Binomial}(n,p).
$$
::: 

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $n$ and $p$ are specified, define
$$
X_i=\left\{
  \begin{matrix}
  1 & \text{if }U_i\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then 
$$
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Binomial}(n,p).
$$

```{r}
set.seed(321)

# Generate one random number from B(n=10, p=0.5). 
n <- 10
X <- numeric(n)
p <- 0.5

for(i in 1:n){
  U <- runif(1)
  if(U<=p) X[i]=1 else X[i]=0
}
Y <- sum(X)
Y 
```
:::



::: {#exm-Poisson}

## Poisson Distribution

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=\min\left\{n=0,1,2\dots,\text{ such that }\prod_{i=1}^{n+1} U_i \leq e^{-\lambda}
\right\}. 
$$ 
Then 
$$
X\sim\mathrm{Poisson}(\lambda).
$$

```{r}
set.seed(321)

# Generate one random number from Poisson(lambda) 
lambda <- 2

## Initializations
U <- 1
n <- 0

while(U > exp(-lambda)){
  U <- U * runif(1)
  n <- n + 1
}
n <- n-1
n
```
:::



## Generation of Continuous Random Variables

### The Inverse Method

A rather general method to generate continuous random variables is the **Inverse Method**.

::: {.callout-note icon=false} 

## 
::: {#thm-InverseMethod}

## Inverse Method

Let $U\sim\mathcal{U}[0,1],$ and let $F_X$ be an **invertible** distribution function. The transformed random variable   
$$
X=F_X^{-1}(U)
$$
has then the distribution function $F_X,$ 
$$
P(X\leq x) = F_X(x).
$$
:::
:::

::: {.callout-important}
@thm-InverseMethod can only be used to generate random variables $X$ with **invertible** distribution functions $F_X.$
:::
      
**Proof of @thm-InverseMethod:**

The distribution function of the transformed random variable 
$$
X=F^{-1}(U)
$$ 
can be derived as 
$$
\begin{align*}
\mathbb{P}(X\leq x) 
&= \mathbb{P}(F_X^{-1}(U)\leq x) \\
&= \mathbb{P}(U\leq F_X(x)) \\
&= F_U(F_X(x)) \\
& = F_X(x),
\end{align*}
$$
which shows the result of @thm-InverseMethod. The last (and important) equality follows since the distribution function of $U\sim\mathcal{U}[0,1]$ is 
$$
\mathbb{P}(U\leq u) = F_U(u) = u, \quad 0\leq u \leq 1
$$
since the distribution function $F_U$ of $U\sim\mathcal{U}[0,1]$ is
$$
F_U(u) = \left\{
  \begin{array}{ll}
  0 & \text{for } u < 0\\
  u & \text{for } 0 \leq  u \leq 1\\
  1 & \text{for } 1 < u.\\
  \end{array}
\right.  
$${#eq-FUnif}

<!-- https://statproofbook.github.io/P/cdf-itm.html#:~:text=Proof%3A%20Inverse%20transformation%20method%20using%20cumulative%20distribution%20function&text=has%20a%20probability%20distribution%20characterized,)%20F%20X%20(%20x%20)%20.&text=U%E2%88%BCU(0%2C1,U%E2%89%A4u)%3Du. -->


<!-- http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf -->


::: {#exm-Exponential}

## Exponential Distribution

Since 
$$
F(x)= 1 - \exp(-\lambda x),
$$ 
we have 
$$
F^{-1}(u) = - \frac{\ln(1-u)}{\lambda}.
$$

::: {.callout-tip}
Note that $1-U$ has the same distribution as $U$, if $U\sim U[0,1]$. Therefore also $-\frac{\ln(u)}{\lambda}$ leads to a value from $\mathrm{Exp}(\lambda).$
:::



**Algorithm:** If $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=-\frac{\ln(U)}{\lambda}.
$$
Then 
$$
X\sim \mathrm{Exp}(\lambda).
$$
:::


::: {.callout-note}

The inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods. 

The inverse method often cannot be applied or is often inefficient, because the inverse of many important distribution functions cannot be derived in [closed form](https://en.wikipedia.org/wiki/Closed-form_expression):

* The Gaussian distribution function $\Phi$ and therefore also its inverse $\Phi^{-1}$ is not available in closed form. 
* For discontinuous random variables we need efficient algorithms for computing the *generalized* inverse of their distribution function $F.$
:::


### Transformation Methods



**Idea:** Construct algorithms from [theoretical links](http://www.math.wm.edu/~leemis/2008amstat.pdf) between distributions. 

**Pro:** These methods can be advantageous if a distribution $f$ is linked (in a relatively simple way) to another distribution that is easy to simulate. 

**Con:** Generally, these methods are rather case-specific, and difficult to generalize. 



::: {#exm-BuildOnExp}

## Building on Exponential RVs

In @exm-Exponential, we learned to generate an exponential random variable $X\sim\operatorname{Exp}(\lambda)$ starting from a uniform random variable $U\sim\mathcal{U}[0,1].$ In the following we generate random variables starting from exponential random variables $X\sim\mathrm{Exp}(1):$

If the $X_1, X_2,\dots$ are i.i.d. as $X\sim\mathrm{Exp}(1),$ then

$$Y\sim \chi^2_{2\nu}\quad\text{if}       \quad Y= 2     \sum_{i=1}^\nu X_i,\quad\nu=1,2,\dots $$
$$Y\sim \Gamma(\alpha,\beta)\quad\text{if}\quad Y= \beta \sum_{i=1}^\alpha X_i,\quad \alpha=1,2,\dots $$
$$Y\sim \mathrm{Beta}(a,b)\quad\text{if}  \quad Y= \frac{\sum_{i=1}^a X_i}{\sum_{j=1}^{a+b} X_j},\quad a,b=1,2,\dots $$
:::


::: {.callout-note}

There are better algorithms to generate Gamma and Beta random variables.

We cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter $\alpha$.

* This implies that we cannot generate a $\chi^2_{1}$-distributed random variable, because the $\chi^2_{1}$ distribution is identical to the $\Gamma(\alpha, 2)$ distribution with $\alpha=\frac{1}{2}.$ 

* This then also implies that we cannot generate a $\mathcal{N}(0,1)$-distrbuted random variable, since $X^2\sim \chi^2_{1}$ for $X\sim\mathcal{N}(0,1)$.  
:::        


The well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of @BoxMuller1958:

::: {.callout-note icon=false} 

## 

::: {#thm-NormalVariableGeneration}

## Normal Variable Generation (Box and Muller, 1958)

If $U_1$ and $U_2$ are i.i.d. as $U\sim\mathcal{U}[0,1]$, then 
$$
X_1 =\sqrt{-2 \ln(U_1)}\, \cos(2\pi U_2)
$$
and 
$$
X_2=\sqrt{-2\ln(U_1)}\,\sin(2\pi U_2)
$$
are both i.i.d. as $X\sim\mathcal{N}(0,1).$
::: 
:::

**Proof of @thm-NormalVariableGeneration:** 

Define the random variables 
$$
R = \sqrt{-2 \ln(U_1)}\quad\text{and}\quad Q = 2\pi U_2,
$$
where
$$
R\in(0,\infty)\quad\text{and}\quad Q\in[0,2\pi].
$$

::: {.callout-tip}

**Idea of the proof:**

1. Derive the bivariate density of $(R, Q).$
2. Determine the functional connection $g$ between $(R, Q)$ and $(X_1, X_2)$ and note that $g$ is invertible.
3. Use the **transformation formula** for densities to derive the bivariate density of $(X_1,X_2)$ using $g^{-1}$ and the bivariate density of $(R, Q).$ 
4. The result follows, if the bivariate density of $(X_1,X_2)$ equals the product of two standard normal densities.  


**Transformation formula (bivariate case):**

Assume that the bivariate random variable 
$$
\left(\begin{matrix}R\\ Q\end{matrix}\right)
$$ 
has a bivariate density $f_{RQ}(r, q)$ and that there is a mapping $g$ between the bivariate random variables 
$$
\left(\begin{matrix}R\\ Q\end{matrix}\right)\text{ and }
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
$$ 
such that 
$$
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)=
g(R, Q)=
\left(\begin{matrix}g_1(R, Q)\\ g_2 (R, Q)\end{matrix}\right),
$$
where $g:\mathbb{R}^2\to\mathbb{R}^2$ is a differentiable and invertible function with inverse $g^{-1}.$ 

Then, the bivariate density of $\left(\begin{matrix}X_1\\X_2\end{matrix}\right)$ 
is given by 
$$
f_{X_1X_2}(x_1,x_2)=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\,\left|\det\left(J_{g^{-1}}(x_1,x_2)\right)\right|,
$${#eq-TransformFormula}
where 
$$
\det\left(J_{g^{-1}}(x_1,x_2)\right)
$$ 
denotes the determinant of the Jacobian matrix of $g^{-1}$ evaluated at $(x_1,x_2),$
$$
J_{g^{-1}}(x_1,x_2)=\left(\begin{matrix}
\frac{\partial g_1^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_1^{-1}}{\partial x_2}(x_1,x_2)\\
\frac{\partial g_2^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_2^{-1}}{\partial x_2}(x_1,x_2)\\
\end{matrix}\right).
$$
Note that the Jacobian of $g^{-1}$ equals the inverse of the Jacobian of $g,$
$$
J_{g^{-1}}(x_1,x_2) = \left(J_{g}(r,q)\right)^{-1},
$$
with points $(x_1,x_2)$ and $(r,q)$ such that 
$$
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right)=
g(r, q)=
\left(\begin{matrix}g_1(r, q)\\ g_2 (r, q)\end{matrix}\right),
$$
:::

We can derive the distribution function of $R$ as following
$$
\begin{align*}
F_R(r)=\mathbb{P}\left(R\leq r\right) 
& = \mathbb{P}\left(\sqrt{-2 \ln(U_1)}\leq r\right) \\
& = \mathbb{P}\left(\ln(U_1)\geq -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) < -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) \leq -\frac{r^2}{2}\right) \quad \text{(continous)}\\
& = 1 - \mathbb{P}\left(U_1 \leq \exp\left(-\frac{r^2}{2}\right)\right) \\
& = 1 - F_U\left(\exp\left(-\frac{r^2}{2}\right)\right)\\
& = 1 - \exp\left(-\frac{r^2}{2}\right),
\end{align*}
$$
where the last step follows from applying the distribution $F_U$ of $U\sim\mathcal{U}[0,1];$ see @eq-FUnif. 

For the density function $f_R$ of $R$ we get
$$
f_R(r)=F_R'(r)=\left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right)\cdot r&\text{for }r \in(0,\infty)\\
  0&\text{otherwise}\\
  \end{array}\right.
$$
Next, we derive the density function of 
$$
Q = 2\pi U_2.
$$
Since $U_2\sim\mathcal{U}[0,1],$ 
$$
Q\sim\mathcal{U}[0,2\pi].
$$
with density function 
$$
f_Q(q)=\left\{
  \begin{array}{ll}
  \frac{1}{2\pi}&\text{for } q\in [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
$$
Since $U_1$ and $U_2$ are independent, $R=\sqrt{-2 \ln(U_1)}$ and $Q = 2\pi U_2$ must also be independent, such that
$$
\begin{align*}
f_{RQ}(r,q) 
& = f_R(r)\cdot f_Q(q) \\
& = \left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right) r\cdot \frac{1}{2\pi} & \text{for } (r,q) \in (0,\infty)\times [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
\end{align*}
$$  
Now, as we know the bivariate density of $(R,Q)$ we can use the functional connection 
$$
\begin{align*}
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
& = g(R,Q) \\
& = \left(\begin{matrix}
       g_1(R,Q)\\
       g_2(R,Q)\end{matrix}\right)
 = \left(\begin{matrix}
  R\cos(Q)\\
  R\sin(Q)
  \end{matrix}\right)
% & = \left(\begin{matrix}
%   \sqrt{-2\ln(U_1)}\cos\left(2\pi U_2\right)\\
%   \sqrt{-2\ln(U_1)}\sin\left(2\pi U_2\right)\\
%   \end{matrix}\right)\\
\end{align*}
$$
with 
$$
R = \sqrt{-\ln(U_1)}\in (0,\infty)
$$ 
and 
$$
Q=2\pi U_2\in[0, 2\pi].
$$
<!-- $$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
&=  g(r,q)\\
& = \left(\begin{matrix}g_1(r,q)\\g_2(r,q)\end{matrix}\right)\\
& = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$
for all $r\in [0,\infty)$ and all $q\in[0, 2\pi].$ -->

::: {.callout-tip}
Note that, $g$ is just the one-to-one transformation that maps points $(r,q)$ of the polar coordinate system 

* **radius** $r\in (0,\infty)$ and 
* **angle** $q\in[0, 2\pi]$ 

to points $(x_1,x_2)$ of the Cartesian coordinate system:
$$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
& = g(r,q)\\ 
& = \left(\begin{matrix}g_1(r,q) \\ g_2(r,q)\end{matrix}\right) 
 = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$

Play around with this mapping here: 
[https://mathinsight.org/applet/polar_coordinates_map_rectangle](https://mathinsight.org/applet/polar_coordinates_map_rectangle)

The inverse mapping $g^{-1}$ maps points $(x_1,x_2)$ from the Cartesian coordinate system to points $(r,q)$ in the polar coordinate system
$$
\begin{align*}
\left(\begin{matrix}r\\ q\end{matrix}\right) 
& = g^{-1}(x_1,x_2)\\
& = \left(\begin{matrix}g_1^{-1}(x_1,x_2) \\ g_2^{-1}(x_1,x_2)\end{matrix}\right) 
 = \left(\begin{matrix}\sqrt{x_1^2 + x_2^2}\\ \operatorname{atan2}(x_1,x_2)
\end{matrix}\right), 
\end{align*}
$$
where the function $\operatorname{atan2}$ is the [2-argument arctangent](https://en.wikipedia.org/wiki/Atan2).
<!-- where the Jacobian determinant of $g^{-1}$ is given by (derivation is a little cumbersome)
$$
\left|J_{g^{-1}}(r,q)\right| = r.
$$ -->
:::

$$
\begin{align*}
J_{g^{-1}}(x_1,x_2)
&=\left(J_{g}(r,q)\right)^{-1}\\
&=\left(\begin{matrix}
\frac{\partial g_1}{\partial r}(r,q) & \frac{\partial g_1}{\partial q}(r,q)\\
\frac{\partial g_2}{\partial r}(r,q) & \frac{\partial g_2}{\partial q}(r,q)\\
\end{matrix}\right)^{-1}\\
&=\left(\begin{matrix}
\cos(q) & -r\sin(q)\\
\sin(q) & \phantom{-}r\cos(q)\\
\end{matrix}\right)^{-1}\\
&=
\frac{1}{r\cos^2(q) + r\sin^2(q)}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)\\
&=
\frac{1}{r}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)
\end{align*},
$$
where the last step follows from [Pythagorean's identity](https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity) $\cos^2(q) + \sin^2(q)=1.$
So 
$$
\begin{align*}
\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|
&=\left|\operatorname{det}\left(\left(J_{g}(r,q)\right)^{-1}\right)\right|\\
&=
\left|\operatorname{det}\left(
  \begin{matrix}
            \cos(q) &            \sin(q)\\
-\frac{1}{r}\sin(q) & \frac{1}{r}\cos(q)\\
\end{matrix}\right)
\right|\\
&=
\left|\frac{1}{r}\cos^2(q) + \frac{1}{r}\sin^2(q)\right| = \frac{1}{r},
\end{align*}
$$
again using [Pythagorean's identity](https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity) $\cos^2(x_2) + \sin^2(x_2)=1$ and using that $r\in(0,\infty).$


Thus, by the transformation formula for bivariate densities (@eq-TransformFormula), we have 
$$
\begin{align*}
f_{X_1X_2}(x_1,x_2)
&=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|\\
&=f_{RQ}\Big(\underbrace{\sqrt{x_1^2+x_2^2}}_{=r},\underbrace{\operatorname{atan2}(x_1,x_2)}_{=q}\Big)\frac{1}{r}\\
&=\exp\left(-\frac{x_1^2+x_2^2}{2}\right) \sqrt{x_1^2+x_2^2} \cdot \frac{1}{2\pi} \frac{1}{r}\\
&=\exp\left(-\frac{x_1^2+x_2^2}{2}\right)  \frac{1}{2\pi},
\end{align*}
$$
where the last step uses that $r=\sqrt{x_1^2+x_2^2}.$ 

This shows the result of @thm-NormalVariableGeneration, since the bivariate density of $(X_1,X_2)$ equals the product of two marginal standard normal density functions,
$$
\begin{align*}
f_{X_1X_2}(x_1,x_2) 
& = \frac{1}{2\pi}\exp\left(-\frac{x_1^2+x_2^2}{2}\right) \\ 
& = \underbrace{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x_1^2}{2}\right)}_{=\phi(x_1)} \cdot 
    \underbrace{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x_2^2}{2}\right)}_{=\phi(x_2)}, 
\end{align*}
$$
where $\phi$ denotes the density function of the standard normal distribution $\mathcal{N}(0,1)$. 


**Implementation** of the Box-Muller algorithm:
```{r,fig.width=10, fig.height=5, out.width='\\textwidth', fig.align='center'}
# Implementation:
BM_Algo <- function(){
  # Generate U_1, U_2 iid U[0,1]
  U <- runif(2)
  # Transformation
  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])
  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])
  # Return result
  return(c(X1, X2))
}

# Generate n standard normal random variables:
set.seed(321)

n     <- 1000
X_vec <- vector(length = n, mode = "double")

for(i in seq(1, n, by=2)){
  X_vec[c(i, i+1)] <- BM_Algo()
}

# Descriptive Plots
par(mfrow=c(1,2))
hist(X_vec, freq = FALSE, xlim=c(-4,4))
curve(dnorm, add = TRUE, col="blue", lwd=1.3)
qqnorm(X_vec)

# Testing for Normality using the Shapiro-Wilk Test 
# H0: Normality
shapiro.test(X_vec)
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

That is, the Shapiro Wilk is not able to reject its null-hypothesis that the data comes from a normal distribution. However, this was only a single test result and the results may differ in other samples. So, let's check the test results across $B=5000$ samples.

```{r}
## Monte Carlo (MC) replications
B     <- 5000

## Sample size
n     <- 1000

## Significance level
alpha <- 0.05

## Container vectors
X_vec <- vector(length = n, mode = "double")
T_vec <- vector(length = B, mode = "logical")

## MC-Simulation
for(b in 1:B){ 

  for(i in seq(1, n, by=2)){
    X_vec[c(i, i+1)] <- BM_Algo()
  }

  ## Collect test decisions
  tmp      <- shapiro.test(X_vec)
  T_vec[b] <- tmp$p.value < alpha
}

round(mean(T_vec), 2)
```

It turns out that the false positive rate (empirical frequency of Type I errors) of the Shapiro Wilk test equals the chosen significance level $\alpha.$ That is, the test is not able to reject its null-hypothesis ("normal distribution") beyond the expected frequency of a Type I error.  



### Accept-Reject Methods


For many distributions $F$ it is difficult (or impossible) to apply the Inverse Method or some Transformation Method, since the distribution function $F$ is somehow "unsuitable". For instance, surprisingly often there is no explicit form of $F$ available or its inverse does not exists.

 
Accept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density $f$ of interest---actually, $f$ needs to be known only up to a multiplicative constant. No deep analytic study of $f$ is necessary.


::: {.callout-note icon=false} 

## 

::: {#thm-FundamentalThmSimulation}

## Fundamental Theorem of Simulation

Let $X\in\mathbb{R}^d$ be a random variable with density function $f_X,$
$$
X\sim f_X.
$$ 
Then simulating $X$ is equivalent to simulating $(X,U)\in\mathbb{R}^{d+1},$  
$$
(X,U)\sim\mathcal{U}(\mathcal{A}),
$$
where $\mathcal{U}(\mathcal{A})$ denotes the uniform distribution over the area
$$
\mathcal{A}=\left\{(x,u)\;|\; x\in\mathbb{R}^d \text{ and } 0<u<f_X(x)\right\}.
$$
:::
:::

@fig-FundThmSim visualizes the statement of @thm-FundamentalThmSimulation for the univariate $(d=1)$ case.
```{r}
#| echo: false
#| eval: true
#| label: fig-FundThmSim
#| fig-cap: To simulate a random variable $X,$ one can simulate $(X,U)$ uniformely over $\mathcal{A},$ and then take those $X$ realizations (tick-marks at $x$-axis) as simulation results. 

set.seed(123)
# Accept-Reject Algorithm:
Y <- runif(500, min = -4.5, max = 4.5) 
U <- runif(500, min =    0, max = 0.4) 
# A-R Step:
accept <- U <  dnorm(Y) - 0.025
X      <- Y[accept]
U      <- U[accept]
par(mfrow=c(1,1))
# curve(dnorm, col="blue", lwd=1.3, xlim=c(-4.5,4.5), ylab="", xlab="x")
# legend("topright", legen=expression(f[X](x)), bty="n")
curve(dnorm, col="blue", lwd=1.3, 
xlim=c(-4.5,4.5), ylim=c(0,0.45), ylab="u", xlab="x")
legend("topright", 
lty=c(1, NA), pch=c(NA, 19), col=c("blue", "orange"), 
legend=c(expression(f[X](x)), "Points (X,U) uniformly over A"), bty="n")
points(y=U, x=X, col="orange", pch=19)
axis(side=1, at = X, 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkorange", 0.5))
par(mfrow=c(1,1))
```


It turns out that sampling $(X, U)$ uniformly over the set $\mathcal{A}$ is often challenging. However, one can consider some superset $\mathcal{S},$
$$
\mathcal{A}\subseteq \mathcal{S},
$$ 
such that simulating a random variable uniformly distributed over $\mathcal{S}$ is easy. 

A uniform distribution on $\mathcal{A}$ can then be obtained by 

1. drawing from a uniform distribution on $\mathcal{S}$ and 
2. rejecting samples that are in $\mathcal{S},$ but not in $\mathcal{A}.$



#### The case of bounded densities with compact support {#sec-ARd1compact}

The general principle of the accept-reject method is easily explained using a *bounded* density function $f$ with *compact support*. 

::: {.callout-tip}

* **Bounded** (from above) means that there exists a constant $m$ with $0<m<\infty$ such that 
$$
\sup_xf(x)\leq m
$$ 

* An interval $[a,b]$ is called **compact** if it is closed and the boundaries are finite. For instance, the Gaussian density $\phi$ has *not* a compact support, but $\mathrm{supp}(\phi)=(-\infty,\infty)$.
  
* Examples:   

  * A **bounded** density function with **compact support**:
$$
f_c(x)=\frac{3}{4}\left(1-\left(x-c\right)^2\right)\,1_{(|x-c|\leq 1)},
$$
where the (compact) support of $f$ is $[a,b]=[c-1,c+1]$ and its range is $[0,m]=[0,3/4]$, i.e., $f_c$ is bounded from above by $3/4$ for all $c\in\mathbb{R}.$
<!-- (Yes, it's a stupid example as we do not necessarily need the Accept-Reject Method here.) -->
   * An **unbounded** density function with **compact support**:
  $$
  f(x)=\left\{
  \begin{array}{ll}
  \frac{x}{b^2}\left(1-\frac{x^2}{b^2}\right)^{-1/2}&\text{ for }x\in(0,b)\\
  0&\text{ otherwise}
  \end{array}
  \right.
  $$
  $f(x)$ is $0$ for $x=0,$ but diverges to positive infinity for $x\to b.$
:::

**The Accept-Reject Algorithm for bounded densities with compact support:**

* **Compact support:** Let 
   $$
   \operatorname{supp}(f)=[a_1,b_1]\times \dots\times[a_d,b_d],
   $$ 
   where $[a_j,b_j]\subseteq\mathbb{R}$ are compact intervals for each $j=1,\dots,d.$

* **Bounded (from above):** Let 
   $$
   \sup_{x\in[a_1,b_1]\times \dots\times[a_d,b_d]} f(x)\leq m,
   $$ 
   where $m$ is a constant with $0<m<\infty.$

To simulate 
$$
X\sim f_X
$$
simulate the random pair 
$$
(Y,U)\sim\mathcal{U}\big(\underbrace{[a_1,b_1]\times \dots\times[a_d,b_d] \times[0,m]}_{=\mathcal{S}}\big)
$$ 
by simulating independently 
$$
Y\sim\mathcal{U}([a_1,b_1]\times \dots\times[a_d,b_d])
$$
and 
$$
U \sim \mathcal{U}[0,m]. 
$$

**Accept-Reject Step:**

**Accept** a simulated $Y$ as a simulation for $X$, i.e. set 
$$
X=Y, 
$$
**only if** 
$$
U\leq f_X(Y),
$$ 
and **reject** all other $Y$'s. 


<!-- ::: {.callout-tip}
Simulating 
$$
U|Y=y \sim \mathcal{U}[0,m]
$$ 
is particularly simple, since the conditional random variable $U|Y$ does here not depend on $Y$ and thus has the same distribution as the **unconditional** random variable 
$$
U|Y \sim U,
$$
where $U\sim\mathcal{U}[0,m].$ 
::: -->

::: {#exm-ARAlgo1Example} 

Let the target density be 
$$
f_X(x)=\frac{3}{4} \left(1-x^2\right)\mathbf{1}_{(|x|\leq 1)},
$$
where 
$$
\operatorname{supp}(f_X)=[-1,1]
$$ 
and 
$$
\sup_{x\in[-1,1]}f_X(x)\leq\frac{3}{4}.
$$
Thus the Accept-Reject algorithm for simulating $X\sim f_X$ can be implemented by generating  
$$
(Y,X)\sim\mathcal{U}([-1,1]\times[0,3/4])
$$
and accepting $Y$ as realizations of $X$ only if $U\leq f_X(Y).$ 
:::

Look at the following code. It's a very simple algorithm, where we effectively only need to evaluate the density function $f_X,$ `target_pdf`:

```{r}
#| echo: true
#| eval: false
# Target pdf f_X
target_pdf <- function(x){
  pdf <- (3/4) * (1-(x)^2)
  pdf[(x)^2 > 1] <- 0
  ##
  return(pdf)
}

# Accept-Reject Algo:
Y <- runif(n=1000, min = -1, max = 1) 
U <- runif(n=1000, min =  0, max = 3/4) 
# A-R Step:
accept <- U <= target_pdf(Y)
X      <- Y[accept]
```

@fig-ARExample1 illustrates @exm-ARAlgo1Example.


```{r}
#| echo: false
#| eval: true
#| label: fig-ARExample1
#| fig-cap: Accept-Reject algorithm for univariate $(d=1)$ bounded densities with compact support.

# target pdf
target_pdf <- function(x){
  pdf <- (3/4) * (1-(x)^2)
  pdf[(x)^2 > 1] <- 0
  ##
  return(pdf)
}

# Accept-Reject Algo:
Y <- runif(n=1000, min = -1, max = 1) 
U <- runif(n=1000, min =  0, max = 3/4) 
# A-R Step:
accept <- U <= target_pdf(Y)
X    <- Y[accept]

# #######
# plots #
# #######
library("scales")
xx <- seq(-1.2, 1.2, len=500)

par(mfrow=c(2,2))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main=expression(f[X](x)), 
     xlab = "X", ylab = "Density", ylim=c(0,1.25))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,1.25))
rect(xleft = -1, ybottom = 0, xright = 1, ytop = c((3/4)),      col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U[1:200], x=Y[1:200], pch=19, cex=.4)
legend("topleft", 
legend = c("Accept"), 
pch=c(22), 
pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5)), 
     col =   c(alpha("darkgreen", 0.5)), bty="n")
     legend("topright", 
legend = c("Reject"), 
pch=c(22), 
pt.cex=1.5,
     pt.bg = c(gray(.7, alpha = .5)), 
     col =   c(gray(.7, alpha = .5)), bty="n")
# 3. plot 
hist(x = X, main=expression("Histogram of"~X==Y~"|"~(U<= f[X](Y))), #"Histogram of \n X=Y|(U<= f(Y))", 
     freq = FALSE, ylim=c(0,.85), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

::: {.callout-note icon=false} 

## 
::: {#thm-ARAlgo1}

## Accept-Reject Algorithm bounded density functions with compact support.

Let $f_X$ be a $d$-dimensional density function with 

* compact support $\operatorname{supp}(f)=[a_1,b_1]\times \dots\times[a_d,b_d]\subseteq\mathbb{R}^d,$ and 
* bounded from above $\sup_{x\in[a_1,b_1]\times \dots\times[a_d,b_d]}f_X(x)\leq m$ with $0<m<\infty.$  

Then the Accept-Reject algorithm as introduced in @sec-ARd1compact allows to simulate 
$$
X\sim f_X.
$$  
:::
:::

**Proof of @thm-ARAlgo1 for the case $d=1$:**

The following derivation shows that the Accept-Reject algorithm as introduced in @sec-ARd1compact allows to simulate random variables $X$ with the correct target distribution $F_X(x)=\int_a^xf_X(x)dx.$

$$
\begin{align*}
\mathbb{P}(X\leq x)
&=\mathbb{P}(Y\leq x|U\leq f_X(Y))\\[2ex]
&= \frac{\mathbb{P}(Y\leq x, U\leq f_X(Y))}{\mathbb{P}(U\leq f_X(Y))}\\[2ex]
&= \frac{\mathbb{P}(a\leq Y\leq x, \; 0\leq U\leq f_X(Y))}{\mathbb{P}(0\leq U\leq f_X(Y))}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,c\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,c\;du\,dy},
\end{align*}
$$
where the constant $c$ is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation 
$$
\int_a^{b} \int_0^{m}\,c\,du\,dy = 1,
$$
but which is irrelevant here since 
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{c\;\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{c\;\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}.\\
\end{align*}
$$
Now, using that $\int_{0}^{f_X(y)}1du=\big[x\big]^{f_X(y)}_0=f_X(y)$ yields
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{\int_a^x f_X(y)\,dy}{\int_a^b f_X(y)\,dy}\\[2ex]
& =\int_a^x f_X(y)dy = F_X(x),
\end{align*}
$$
which completes the proof. 


  




#### The case of densities with non-compact support {#sec-ARd1noncompact}

If $f_X$ has no compact support, the larger set $\mathcal{S}$ does not necessarily need to be a surrounding closed box as used above. In fact, it can be any "larger set" $\mathcal{S},$ enclosing the density function $f_X$, as long as simulating uniformly over this larger set $\mathcal{S}$ is feasible. This generalization allows for cases, where the support of $f_X$ is **not compact**. 

Let the larger set denote by
$$
\mathcal{S}=\{(y,u)\;|\;y\in\mathbb{R}^d\text{ and } 0\leq u\leq m(y)\},
$$
where: 

* simulation of a uniform on $\mathcal{S}$ is **feasible** and 


* $f_X(x)\leq m(x)$ for all $x\in\mathbb{R}^d.$


From the **feasibility-requirement** it follows that the function $m$ is necessarily integrable, i.e., that
$$
\int\dots \int m(x_1,\dots,x_d)dx_1\dots dx_d=M,
$$
or short with $x=(x_1,\dots,x_d)$ and $X\in\mathbb{R}^d$
$$
\int_{\mathbb{R}^d} m(x)dx=M,
$$
where $M$ exists and is finite (and positive), since otherwise, $\mathcal{S}$ would not have finite mass and a uniform distribution would not exists on $\mathcal{S}$.


Integrability of $m$ is crucial here, since it allows us to relate the function $m$ with a corresponding (auxiliary) density function $g$ as following: 
$$
m(x)=M\,g(x),
$$
where
$$
\int_{\mathbb{R}^d} m(x)\,dx=\int_{\mathbb{R}^d} M\,g(x)\,dx=M.
$$
Therefore, a uniform distribution over $\mathcal{S},$
$$
\mathcal{U}\big(\underbrace{\{(y,u)\;|\;y\in\mathbb{R}^d\text{ and } 0\leq u\leq m(y)\}}_{\mathcal{S}}\big)
$$ 
has a **density function** that is equal to 
$$
\frac{1}{M}
$$ 
for all points $(y,u)\in\mathcal{S}$ and zero else---otherwise the density would not integrate to one. 

Terminology:

* The density function $g$ is called the **instrumental density**. (Choose $g$ as a density from which it is easy to simulate!)

* The density function $f_X$ is called the **target density**.


**The Accept-Reject Algorithm for densities with non-compact support:**

To simulate 
$$
X\sim f_X
$$
simulate the $(d+1)$-dimensional random variable  
$$
(Y,U)\sim\mathcal{U}\big(\underbrace{\{(y,u)\;|\;y\in\mathbb{R}^d\text{ and } 0\leq u\leq m(y)\}}_{\mathcal{S}}\big)
$$ 
by simulating 

1. a realization of
$$
Y\sim g
$$
and
2. for a given realization $Y={\color{red}y},$ a realization of  
$$
U|Y={\color{red}y}\sim\mathcal{U}[0,M\,g({\color{red}y})].
$$

**Accept-Reject Step:**

**Accept** a simulated $Y$ as a simulation for $X$, i.e. set 
$$
X=Y
$$ 
**only if** 
$$
U\leq f_X(Y)
$$ 
and **reject** all other $Y$'s. 


::: {.callout-note icon=false} 

## 
::: {#thm-ARAlgo2}

## Accept-Reject Algorithm bounded density functions with non-compact support.

Let $f_X$ be a $d$-dimensional density function and let $m(x)=Mg(x)$ such that $g$ is a $d$-dimensional auxiliary density function with   

* $\int m(x)dx=\int Mg(x)dx = M$ with constant $0<M<\infty,$ and
* $f_X(x)\leq m(x)$ for all $x\in\mathbb{R^d}.$


Then the Accept-Reject algorithm as introduced in @sec-ARd1noncompact allows to simulate 
$$
X\sim f_X.
$$  
:::
:::

**Proof of @thm-ARAlgo2:**

The following derivation shows that the Appcept-Reject algorithm as introduced in @sec-ARd1noncompact allows to simulate random variables $X$ with the correct target distribution $f_X.$

First, note that $X\sim f_X$ if and only if 
$$
P(X\in A) = \int_A f_X(y)dy
$$
for every $A\subseteq \mathbb{R}^d.$ I.e., this is what we need to show. 


In a first step, we use that $X=Y$ only if $U\leq f_X(Y),$ which implies that  
$$
\begin{align*}
\mathbb{P}(X\in A) & =\mathbb{P}(Y\in A|U\leq f_X(Y)).
\end{align*}
$$
The definition of conditional probabilities yields that 
$$
\begin{align*}
\mathbb{P}(X\in A)
& = \frac{\mathbb{P}(Y\in A, \; U\leq f_X(Y))}{\mathbb{P}(U\leq f_X(Y))}.
\end{align*}
$$
Using that the density function of $(Y,U)\sim\mathcal{U}(\mathcal{S})$ equals $\frac{1}{M}$ for all $(y,u)\in\mathcal{S}$ and zero else we have that 

* $\mathbb{P}(Y\in{\color{red}A}, \; U\leq f_X(Y)) = \int_{\color{red}A}\int_0^{f(y)}\frac{1}{M}\,du\,dy\quad$ 

and that 

* $\mathbb{P}(U\leq f_X(Y)) = \int_{\mathbb{R}^d}\int_0^{f(y)}\frac{1}{M}\,du\,dy\quad$ by integrating out $y$ to get the marginal density.

Thus,
$$
\begin{align*}
\mathbb{P}(X\in A)
& =\frac{\int_A \int_0^{f_X(y)}\,\frac{1}{M}\,du\,dy}{\int_{\mathbb{R}^d}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}\\[2ex]
& =\frac{\frac{1}{M}\,\int_A \,\int_0^{f_X(y)}\,1\,du\,dy}{\frac{1}{M}\,\int_{\mathbb{R}^d}\,\int_0^{f(y)}\,1\,du\,dy}\\[2ex]
& =\frac{\int_A \,\int_0^{f_X(y)}\,1\,du\,dy}{\int_{\mathbb{R}^d}\,\int_0^{f(y)}\,1\,du\,dy}\\[2ex]
\end{align*}
$$
Using that $\int_{0}^{f_X(y)}\, 1\, du = f_X(y),$ we get that 
$$
\begin{align*}
\mathbb{P}(X\in A)
& =\frac{\int_A f_X(y)\,dy}{\int_{\mathbb{R}^d} f_X(y)\,dy}\\[2ex]
& =\int_A f_X(y)dy,
\end{align*}
$$
which applies to every set $A$ and thus shows the result. 

::: {.callout-note}
Note that the above derivation implies that we only need to know the density function $f_X$ **up to a multiplicative constant** $c>0,$ since constant factors cancel---just as the normalizing constant $1/M.$

That is, instead of working with $f_X,$ we can also work with a scaled version $\tilde{f}_X$ that is proportional to $f_X,$
$$
f_{X}(x) = c \tilde{f}_X(x)
$$ 
often written as 
$$
f_{X}(x)\propto \tilde{f}_X(x),
$$ 
since the constant $c$ cancels out anyways in the above quotient expression. (The sign "$\propto$" means "proportional to".)


This is a very useful property for implementing the Accept-Reject algorithm, since it implies that it is **irrelevant** whether we scale the **instrumental density** $g$ by some $1\leq M <\infty,$ or the **target density** $f_X$ by some constant $c>0.$ All we need is that 
$$
\frac{1}{c}f_X(x)=\tilde{f}_X(x) \leq M g(x) \text{ for all } x\in\mathbb{R}^d.
$$ 
for some some $c>0$ and some $1\leq M < \infty.$ This allows us to choose $c$ and $M$ such that simulating from $M g$ is as simple as possible. 
::: 


<!-- All this leads to a more general version of the Fundamental Theorem of Simulation: -->


<!-- ::: {.callout-note icon=false} 

## 
::: {#thm-ARAlgo2}

## Fundamental Theorem of Simulation (General Version)

Let $X\sim f_X$ and let $g$ be a density function such that 
$$
f_X(x)\leq M\,g(x)
$$ 
for some $M$ with $1\leq M<\infty$ and all $x\in\mathbb{R}^d.$ 

Then simulating $X\sim f_X$ is equivalent to simulating 
$$
Y\sim g\quad\text{and}\quad U|Y=y\sim\mathcal{U}[0,M\,g(y)]
$$
of one **accepts** $Y$ as a simulation for $X$ (i.e. sets $X=Y$) only if 
$U\leq f_X(Y),$ and **rejects** all other $Y$'s.
:::
::: -->


<!-- The Accept-Reject Algorithm (General Version):

```
# Accept-Reject Algorithm:
Y   <- generate n random numbers from g(.)

# Specify function m():
m <- function(y){YOUR CODE}

U   <- numeric(n)
for(i in 1:n){
  U[i] <- runif(n=1, min = 0, max = m(Y[i])) 
}

# A-R Step:
accept <- U <= f(Y)
X      <- Y[accept]
``` -->

::: {#exm-ARAlgo2Example} 

Let the target "density" be 
$$
f_X(x)\propto \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1).
$$

Given our considerations above, we can try to scaling such that the above "density" is dominated (i.e. $\tilde{f}_X(x) \leq g(x)$ for all $x$) by the standard normal density
$$
g(x)=\frac{1}{\sqrt{2\pi}}\exp(-x^2/2),
$$
which is obviously straightforward to simulate from.  

Thus, in this example we can set 
$$
m(x)=M\,g(x)\quad\text{with}\quad M=1,
$$ 
since we can simply scale the target "density" $f_X$ such that 
$$
\tilde{f}_X(x)\leq g(x)\quad\text{for all}\quad x.
$$ 
Specifically, we can set 
$$
\tilde{f}_X(x)=0.075 \cdot \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1).
$$ 

The following code provides a possible implementation of the Accept-Reject algorithm for this example. @fig-ARAlgo2Example shows the results.  

```{r}
# Target density
target_pdf <- function(x, c=.075){
  pdf <- c * exp(-x^2 / 2) * 
            (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1)
  return(pdf)
}

# Upper bound
m_fun <- function(x){
  m <- exp(-x^2 / 2)/sqrt(2*pi)
  return(m)  
}

# Accept-Reject Algo:
set.seed(32280)

## 1. Generate Y
Y   <- rnorm(n=1000)

## 2. Generate U|Y=y 
U   <- vector(length = length(Y), mode = "double")

for(i in 1:length(Y)){
  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) 
}

# A-R Step:
accept <- U <= target_pdf(Y)
X      <- Y[accept]
```

```{r}
#| echo: false
#| eval: true
#| label: fig-ARAlgo2Example
#| fig-cap: Accept-Reject algorithm for univariate $(d=1)$ densities with non-compact support---visualizing @exm-ARAlgo2Example.

# Target pdf
target_pdf <- function(x, c=.075){
  pdf <- c * (exp(-x^2 / 2) * (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1))
  return(pdf)
}

# Upper bound
m_fun <- function(x){
  m <- exp(-x^2 / 2)/sqrt(2*pi)
  return(m)  
}

# Accept-Reject Algo:
set.seed(32280)
Y   <- rnorm(n=1000) 
U   <- numeric(length(Y))
for(i in 1:length(Y)){
  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) 
}

# A-R Step:
accept <- U <= target_pdf(Y)
X      <- Y[accept]

# #######
# plots #
# #######
xx <- seq(-4, 4, len=500)

par(mfrow=c(1,3))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main=expression(f[X](x)), 
     xlab = "X", ylab = "Density", ylim=c(0,.40))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,0.40))
polygon(x=c(xx, rev(xx)), y=c(m_fun(x=xx), rep(0,length(xx))), col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U[1:100], x=Y[1:100], pch=19, cex=.4)
legend("topleft", legend = c("Accept","Reject"), pch=22, pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), 
     col =   c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), bty="n")
#legend("topright", legend = c("(Y,U) ~ Unif(L)"), pch=19, pt.cex=.4)
# 3. plot
hist(x = X, main=expression("Histogram of"~X==Y~"|"~(U<= f[X](Y))), 
     freq = FALSE, ylim=c(0,.40), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

:::


#### Efficiency of the Accept-Reject algorithm 

Simple statements with respect to the efficiency of the Accept-Reject algorithm can be made if $f_X$ and $g$ are both **normalized** such that they are **both density functions**. Then:

* The constant $M$ is necessarily larger than $1$.
* The probability of acceptance is $1/M$. (See Exercises.)
* $M$ is interpreted as the **efficiency** of the Accept-Reject algorithm. The closer $M$ is to $1$ the better. 
* $M$ quantifies how closely $g$ can imitate $f_X.$ 


::: {#exm-AREfficiency} 

## Normals from Double Exponentials

Consider simulating from $\mathcal{N}(0,1)$ using the Accept-Reject algorithm with the a double-exponential distribution $\mathcal{L}(\alpha),$ also called [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution), as the instrumental density, 
$$
g(x|b)=\frac{1}{2b}\exp\left(-\frac{|x|}{b}\right).
$$
<!-- here: location-param mu=0, scale.param=b\geq 0, precision=alpha=b^{-1} -->
It is then straightforward to show (see Exercises) that 
$$
\begin{align*}
\frac{f(x)}{g(x|b)}
%&=\frac{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right)}{\frac{1}{2b}\exp\left(-\frac{|x|}{b}\right)}\\[2ex]
%&=\sqrt{\frac{2}{\pi}}\,b\,\exp\left(-\frac{1}{2}x^2+\frac{|x|}{b}\right)\\[2ex]
&\leq\sqrt{\frac{2}{\pi}}\,b\,\exp\left(\frac{1}{2\,b^2}\right)
\end{align*}
$$
and that the minimum of the bound (in $b$) is attained for $b=1$. 

This leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental density:
$$
\begin{align*}
\frac{f(x)}{g(x|1)} \leq M 
& =\sqrt{\frac{2}{\pi}}\,\exp\left(\frac{1}{2}\right)\\[2ex]
& =\sqrt{\frac{2}{\pi} \,\exp\left(1\right)}.
\end{align*}
$$

The probability of acceptance is then 
$$
\frac{1}{M}=\sqrt{\frac{\pi}{2\exp(1)}}\approx 0.76.
$$ 
I.e., to produce one normal random variable, this Accept-Reject algorithm requires on average 
$$
\frac{1}{0.76}\approx 1.3
$$ 
uniform variables. 


This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is $1.$
:::


#### The instrumental density $g$ needs thicker tails than the target density $f_X$

Let $f_X$ and $g$ be both density functions, and let the instrumental density $g$ be such that 
$$
g(x)>0  
$$ 
for all $x\in\operatorname{supp}(f_X).$

::: {.callout-tip}
The defining properties of a density functions $f$ are: 

1. Non-negative: $f(x)\geq 0$ for all $x$
2. Normalized: $\int f(x)dx = 1 $
:::

Then, the inequality 
$$
f_X(x)\leq M\,g(x),
$$ 
with $1<M<\infty$ for all $x\in\mathbb{R}^d,$ implies that the quotient $f_X/g$ is bounded, i.e., that 
$$
0\leq \frac{f_X(x)}{g(x)}\leq M <\infty
$$
for all $x\in\operatorname{supp}(f_X).$


This implies that $g$ **has to have thicker tails** than $f_X.$ 

To see this, observe that both 
$$
f_X(|x|)\to 0 \quad\text{and}\quad g(|x|)\to 0
$$
for "extreme" values $|x|\to\infty,$ simply because both $f_X$ and $g$ are densities.  


If $g$ has **thicker tails** than $f_X,$ then 
$$
0 \leq \frac{f_X(|x|)}{g(|x|)} \leq 1 \leq M 
$$
for all sufficiently large values $|x|.$ Thus with $g$ having thicker tails than $f_X,$ we can be sure that the requirement $\frac{f_X(x)}{g(x)}\leq M <\infty$ holds for all sufficiently extreme values of $x.$ 

By constrast, if $g$ has strictly **thinner tails** than $f_X,$ then 
$$
0\leq \frac{f_X(|x|)}{g(|x|)} \to\infty,\quad\text{as}\quad|x|\to\infty, 
$$
which makes the requirement that $\frac{f_X(x)}{g(x)}\leq M <\infty$ impossible to hold for extreme values of $x.$

::: {.callout-note}
Therefore, it is, for instance, impossible to simulate from a Cauchy density $f_X$ using a normal density $g$. The reverse, however, works quite well.
::: 

<!-- ::: {.callout-tip}
Remember: Using the Cauchy as instrumental density $g$ does not harm the integrability/feasibility requirement! The fact that the Cauchy distribution has no mean, does not change the fact that the density of the Cauchy integrates to one.
::: -->


## Exercises {-}

#### Exercise 1. {-} 

Consider @exm-AREfficiency ("Normals from Double Exponentials"). Let $f$ be the density of the standard normal distribution $\mathcal{N}(0,1),$
$$
\begin{align*}
f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right),
\end{align*}
$$
and $g$ the density of the Laplace (double exponential) distribution
$$
\begin{align*}
g(x|b) = \frac{1}{2b}\exp\left(-\frac{|x|}{b}\right),\quad b>0.
\end{align*}
$$

(a) Show that 
$$
\begin{align*}
\frac{f(x)}{g(x|b)}\leq \sqrt{\frac{2}{\pi}} \; b\;\exp\left(\frac{1}{2b^2}\right).
\end{align*}
$$

(b) Show that the minimum of the bound (in $b>0$) is attained at $b=1.$

#### Exercise 2. {-} 

Let $X \sim f$ and $Y\sim g,$ where $f$ and $g$ are density functions of the random variables $X$ and $Y.$ Show that 

$$
\begin{align*}
P\left(X \leq h (Y)\right)
&=\int_{-\infty}^\infty \left(\int_{-\infty}^{h (y)} f(x) dx \right) g(y) dy.
\end{align*}
$$
**Note:** This was used, for instance, in the proofs of @thm-ARAlgo1 and @thm-ARAlgo2. 

**Hint:** Use that the probability of an event $A$ (e.g. $A=X \leq a$) can be written as 
$$
\begin{align*}
P(A) 
& = \;\;\;\;P(A) \;\;\cdot \;\;1 \;\;+\;\; P(\text{not}\;A) \;\;\cdot\;\; 0 \\[2ex]
& = P(1_{(A)} = 1) \cdot 1 + P(1_{(A)} = 0) \;\;\cdot 0 \\[2ex]
& = \mathbb{E}\left(1_{(A)}\right),
\end{align*}
$$
where $1_{(\texttt{TRUE})}=1$ and $1_{(\texttt{FALSE})}=0.$ This allows you to use the iterated law of expectations. 

#### Exercise 3. {-} 


Consider the Accept-Reject Algorithm for a target density function $f_X$ with compact support $[a,b]$ and $0\leq f(x)\leq m$ for all $x\in[a,b]$. What is the probability of accepting $Y$ from a simulation $(Y,U)$ as described in the lecture? 


#### Exercise 4. {-} 

Consider the Accept-Reject Algorithm for a target density function $f_X$ and an instrumental density function $g$ with 
$$
f_X(x) \leq Mg(x)
$$ 
with $M>0$ for all $x\in\operatorname{supp}(f_X).$ 
  
   (a) What is the probability of accepting $Y$ from a simulation 
   $$
   (Y,U)\sim\mathcal{U}\left(\left\{(y,u)|y\in \operatorname{supp}(f_X)\;\text{and}\;0\leq u\leq Mg(y)\right\}\right)?
   $$ 
   (b) Show that $M\geq 1.$


{{< include Ch1_Solutions.qmd >}}

## References {-}