# Random Variable Generation

### Literature {-}

In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:

* [Monte Carlo Statistical Methods](http://www.springer.com/us/book/9780387212395), Ch. 2, @RobertCasella1999 
* [Introducing Monte Carlo Methods with R](https://link.springer.com/book/10.1007/978-1-4419-1576-4), Ch. 2, @RobertCasella2010

<!-- * Further:
  * [Non-Uniform Random Variate Generation](http://luc.devroye.org/rnbookindex.html), Devroye, L. 
  * [Nonparametric Density Estimation: The L1 View](http://luc.devroye.org/L1bookBW.pdf), Devroye, L., Ch. 8
  * [Monte Carlo and Quasi-Monte Carlo Sampling](http://www.springer.com/us/book/9780387781648), Lemieux, C., Ch. 2 and 3 -->



### Introduction & Motivation {-}

In many complex statistical models parameter estimates can only be calculated numerically and their (statistical) behavior can only be investigated through a **simulation** on the computer: 

**Stylized Algorithm:**

* **Step 1** Generate artificial data from a statistical model and compute the estimate(s) of interest. 
* **Step 2** Repeat this 10000 (or more) times.
* **Step 3** From these 10000 "pseudo" realizations of the estimator we can directly compute, e.g., the bias or the mean squared error (MSE) of the estimator---without the need of complicated statistical theory.


```{r}
## Step 1
generate_mean_estimates <- function(n = 100){
  ## generate artificial data
  artificial_data <- rnorm(n)   
  ## compute the estimate
  estimate        <- mean(artificial_data) 
  return(estimate)
}

## Step 2 
set.seed(223)
simulated_estimates <- replicate(1000, 
                                 generate_mean_estimates(n = 100))

## Step 3
mean(simulated_estimates)
sd(simulated_estimates)

hist(simulated_estimates, 
     main = "Histogram of Simulated Mean Estimates", 
     xlab = "")
```


In order to conduct Steps 1 and 2 we need artificial realizations (here we used `rnorm()`) of a random variable---so-called **Pseudo Random Numbers**. The generation of such pseudo random numbers is the topic of the following chapters. 


## Uniform Simulation


**General procedure:**

* Usually, a random integer with values **uniformly** in $[0,m]$ with a large integer $m$ is generated. 
* To achieve a random number in $[0, 1]$, we divide this number by $m$. 
* From this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.


There are many different **Random Number Generators (RNGs)**, we consider the most simple class of RNGs:

::: {#def-LinConGen}

## Linear Congruential Generators 

Here the $i$th random integer $u_i$ is generated by
$$
u_i=(a u_{i-1}+c) \,\operatorname{mod}\, m,
$$
where the starting value $u_0$ is a chosen and fixed value called **seed**.

Furthermore:
    
* $m$, with $0<m$, is called the **modulus**
* $a$, with $0<a<m$, is called the **multiplier**
* $c$, with $0\leq c<m$, is called the **increment**
:::

::: {.callout-tip}

## The modulo operator: $\operatorname{mod}$

"$b\,\operatorname{mod}\,c$" denotes the remainder of the division of $b$ by $c$.

For instance 
$$
\begin{align*}
4\,&\operatorname{mod}\,2 = 0\\
5\,&\operatorname{mod}\,2 = 1\\
1\,&\operatorname{mod}\,2 = 1\\
\end{align*}
$$


```{r, eval=FALSE}
# Modulo computation using the modulo operator '%%'
5 %% 4
9 %% 4
4 %% 5

# own modulo-function:
my_mod <- function(x,m){
  t1 <- floor(x/m)
  return(x-t1*m)
}
```

:::


Some Facts:

* The above recursion generates a completely **nonrandom** sequence, therefore it is often called a ***pseudo* random** sequence. 
* Under appropriate choices of $u_0$ , $a$ and $m$ the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on $[0, m]$.
* The cycle length of linear congruential generators will never exceed modulus $m$, but can maximized with the three following conditions (see [Knuth (2002)](https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) for a proof):
    * The increment $c$ is [relatively prime](https://en.wikipedia.org/wiki/Coprime_integers) to $m$,
    * $a - 1$ is a multiple of every prime dividing $m$,
    * $a - 1$ is a multiple of $4$ when $m$ is a multiple of $4$.
    


<!--
Further reading:
Ripley (1987) provides a review of number generators. 
Devroye (1986) is very comprehensive book on non-uniform random variate generation.
-->



**Bad choice of parameters** for the linear congruential random number generator:
```{r}
m <- 64    # modulus
a <- 33    # multiplier
c <- 12    # increment
s <- 57    # seed
n <- 1000  # length of run (including seed)

r_vec    <- numeric(n) # initialize vector
r_vec[1] <- s # set seed

## Recursive generation 
for (i in 1:(n-1)){
 r_vec[i+1] <- (a * r_vec[i] + c) %% m
}

# scale result from [0,m] to [0,1]:
my_bad_runif_vec <- r_vec/m

# BUT! Very short cycle-length (here: period=16)
r_vec[ 1:16]
r_vec[17:32]
```

\

::: {#exm-GoodVsBadRNG}

## Good vs. Bad RNGs 

Average heads ratios 
$$
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i
$$ 
for $n=1,2,\dots$ simulated independent tosses of a fair coin $C_i$ with  
$$
C_{i}=\left\{\begin{array}{ll}
1&\text{if Head}\\
0&\text{if Tail}
\end{array}\right.
$$ 
and
$$
P(C_i=0)=P(C_i=1)=0.5.
$$ 
By the strong (or weak) law of large numbers the average should converge **stochastically** (i.e., almost surely or in probability) to $0.5$ as $n$ becomes large $(n\to\infty),$
$$
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i \to_p 0.5,\quad n \to\infty.
$$ 

```{r, fig.width=7, fig.height=4, fig.align='center'}
#| label: fig-goodvsbad
#| fig-cap: "Two sample paths showing the pseudo random convergence of $\\bar{C}_n$ to the limit 0.5---one based on a good RNG and the other based on a bad RNG."

# using the above bad RNG:
bar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)

# using R's high-quality RNG:
set.seed(223)
bar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)

# plotting the results:
plot(bar_x_bad, type="l", ylim=c(0.46,0.54), 
     xlab="", ylab="", main="Good vs. Bad RNG")
lines(bar_x_good, col="darkblue")
```
::: 

<!--
```{r, eval=FALSE}
## See also:
install.packages("randtoolbox")
library("randtoolbox")
?congruRand()
```
-->

::: {.callout-note}
IBM's [RANDU](https://en.wikipedia.org/wiki/RANDU) is a famous example of an miss-specified linear congruential RNG.
:::



## Generation of Discrete Random Variables

<!-- https://rpubs.com/maheshraje982/617094 -->

Assume the discrete random variable $X$ of interest takes on the values 
$$
X\in \{x_1, \dots , x_k \}
$${#eq-DiscRV}
with
$$
p_i = \mathbb{P}(X = x_i ), \quad i = 1,\dots , k,
$$
and 
$$
\sum_{i=1}^kp_i = 1.
$$

Assume that you can generate pseudo-random realizations $u\in[0,1]$ from a uniformly distributed random variable $U\sim\mathcal{U}[0, 1]$ using an RNG.


**General principle:** 

1. Subdivide $[0, 1]$ into $k$ intervals with
$$
I_i = (a_{i-1}, a_i],
$$
where 
$$
a_i = \sum_{j=1}^ip_j\quad\text{and}\quad a_0 = 0.
$$
2. Define the new discrete realizations 
$$x=\left\{
\begin{array}{cc}
          x_1&\quad\text{if}\quad u\in I_1\\
          \vdots& \vdots\\
          x_k&\quad\text{if}\quad u\in I_k
          \end{array}\right.
$${#eq-U2DiscrX}



::: {#lem-GenDiscrRV}

Let $u$ be a realization from $\mathcal{U}[0, 1]$ and if $u\in I_i$, set $x = x_i$ (see @eq-U2DiscrX). Then $x$ is a realizaton of the discrete random variable $X$ (see @eq-DiscRV).
      
::: 

<!-- **Proof:** Done in the lecture. -->


**Proof:** 

For any $i = 1, \dots, k$ we have that
$$
\begin{align*}
\mathbb{P}(X = x_i) 
& = \mathbb{P}(U \in I_i)  \\
& = F_\mathcal{U}(a_i) - F_\mathcal{U}(a_{i-1})\\
& = a_i - a_{i-1}\\
& = \sum_{j=1}^ip_j - \sum_{j=1}^{i-1}p_j  = p_i,
\end{align*}
$$
which shows the statement of @lem-GenDiscrRV. 


::: {#exm-Bernoulli}

## Bernoulli Distribution

Generate random numbers from 
$$
X\sim\mathrm{Bernoulli}(p),
$$
where $p$ is the probability of success, i.e., 
$$
\mathbb{P}(X=1)=p\quad\text{and}\quad\mathbb{P}(X=0)=1-p.
$$

**Algorithm:** If $U\sim\mathcal{U}[0,1]$ and $p$ is specified, define
$$
X=\left\{
  \begin{matrix}
  1 & \text{if }U\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then $X\sim\mathrm{Bernoulli}(p)$.


```{r}
set.seed(321)
# Generate one random number from Bernoulli(p) with p=0.5
p  <- 0.5
U  <- runif(1)

if(U<=p) X=1 else X=0

X
```
:::




::: {#exm-Binom}

## Binomial Distribution

Generate random numbers from 
$$
X\sim\mathrm{Binomial}(n,p),
$$
where $n$ is the number of trials and $p$ the probability of success such that 
$$
\mathbb{P}(X=i)=\binom{n}{i}p^i(1-p)^{n-1}
$$
for $i=1,\dots,n.$

::: {.callout-tip}
If $X_1,\dots,X_n\overset{i.i.d}{\sim}\mathrm{Bernoulli}(p),$ then 
$$
X=\sum_{i=1}^nX_i \sim\mathrm{Binomial}(n,p).
$$
::: 

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $n$ and $p$ are specified, define
$$
X_i=\left\{
  \begin{matrix}
  1 & \text{if }U_i\leq p\\
  0 & \text{otherwise}.\\
  \end{matrix}
\right.
$$ 
Then 
$$
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Binomial}(n,p).
$$

```{r}
set.seed(321)

# Generate one random number from B(n=10, p=0.5). 
n <- 10
X <- numeric(n)
p <- 0.5

for(i in 1:n){
  U <- runif(1)
  if(U<=p) X[i]=1 else X[i]=0
}
Y <- sum(X)
Y 
```
:::



::: {#exm-Poisson}

## Poisson Distribution

**Algorithm:** If $U_1,\dots,U_n$ are i.i.d. as $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=\min\left\{n=0,1,2\dots,\text{ such that }\prod_{i=1}^{n+1} U_i \leq e^{-\lambda}
\right\}. 
$$ 
Then 
$$
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Poisson}(\lambda).
$$

```{r}
set.seed(321)

# Generate one random number from Poisson(lambda) 
lambda <- 2

## Initializations
U <- 1
n <- 0

while(U > exp(-lambda)){
  U <- U * runif(1)
  n <- n + 1
}
n <- n-1
n
```
:::



## Generation of Continuous Random Variables

### The Inverse Method

A rather general method to generate continuous random variables is the **Inverse Method**.


::: {#thm-InverseMethod}

## Inverse Method

Let $U\sim\mathcal{U}[0,1],$ and let $F_X$ be an **invertible** distribution function. The transformed random variable   
$$
X=F_X^{-1}(U)
$$
has then the distribution function $F_X,$ 
$$
P(X\leq x) = F_X(x).
$$
:::

::: {.callout-important}
@thm-InverseMethod can only be used to generate random variables $X$ with **invertible** distribution functions $F_X.$
:::
      
**Proof:**

The distribution function of the transformed random variable 
$$
X=F^{-1}(U)
$$ 
can be derived as 
$$
\begin{align*}
\mathbb{P}(X\leq x) 
&= \mathbb{P}(F_X^{-1}(U)\leq x) \\
&= \mathbb{P}(U\leq F_X(x)) \\
&= F_U(F_X(x)) \\
& = F_X(x),
\end{align*}
$$
which shows the result of @thm-InverseMethod. The last (and important) equality follows since the distribution function of $U\sim\mathcal{U}[0,1]$ is 
$$
\mathbb{P}(U\leq u) = F_U(u) = u, \quad 0\leq u \leq 1
$$
since the distribution function $F_U$ of $U\sim\mathcal{U}[0,1]$ is
$$
F_U(u) = \left\{
  \begin{array}{ll}
  0 & \text{for } u < 0\\
  u & \text{for } 0 \leq  u \leq 1\\
  1 & \text{for } 1 < u.\\
  \end{array}
\right.  
$${#eq-FUnif}

<!-- https://statproofbook.github.io/P/cdf-itm.html#:~:text=Proof%3A%20Inverse%20transformation%20method%20using%20cumulative%20distribution%20function&text=has%20a%20probability%20distribution%20characterized,)%20F%20X%20(%20x%20)%20.&text=U%E2%88%BCU(0%2C1,U%E2%89%A4u)%3Du. -->


<!-- http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf -->


::: {#exm-Exponential}

## Exponential Distribution

Since 
$$
F(x)= 1 - \exp(-\lambda x),
$$ 
we have 
$$
F^{-1}(u) = - \frac{\ln(1-u)}{\lambda}.
$$

::: {.callout-tip}
Note that $1-U$ has the same distribution as $U$, if $U\sim U[0,1]$. Therefore also $-\frac{\ln(u)}{\lambda}$ leads to a value from $\mathrm{Exp}(\lambda).$
:::



**Algorithm:** If $U\sim \mathcal{U}[0,1]$ and $\lambda$ is specified, define
$$
X=-\frac{\ln(U)}{\lambda}.
$$
Then 
$$
X\sim \mathrm{Exp}(\lambda).
$$
:::


::: {.callout-note}

The inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods. 

The inverse method often cannot be applied or is often inefficient, because the inverse of many important distribution functions cannot be derived in [closed form](https://en.wikipedia.org/wiki/Closed-form_expression):

* The Gaussian distribution function $\Phi$ and therefore also its inverse $\Phi^{-1}$ is not available in closed form. 
* For discontinuous random variables we need efficient algorithms for computing the *generalized* inverse of their distribution function $F.$
:::


### Transformation Methods



**Idea:** Construct algorithms from [theoretical links](http://www.math.wm.edu/~leemis/2008amstat.pdf) between distributions. 

**Pro:** These methods can be advantageous if a distribution $f$ is linked (in a relatively simple way) to another distribution that is easy to simulate. 

**Con:** Generally, these methods are rather case-specific, and difficult to generalize. 



::: {#exm-BuildOnExp}

## Building on Exponential RVs

In @exm-Exponential, we learned to generate an exponential random variable $X\sim\operatorname{Exp}(\lambda)$ starting from a uniform random variable $U\sim\mathcal{U}[0,1].$ In the following we generate random variables starting from exponential random variables $X\sim\mathrm{Exp}(1):$

If the $X_1, X_2,\dots$ are i.i.d. as $X\sim\mathrm{Exp}(1),$ then

$$Y\sim \chi^2_{2\nu}\quad\text{if}       \quad Y= 2     \sum_{i=1}^\nu X_i,\quad\nu=1,2,\dots $$
$$Y\sim \Gamma(\alpha,\beta)\quad\text{if}\quad Y= \beta \sum_{i=1}^\alpha X_i,\quad \alpha=1,2,\dots $$
$$Y\sim \mathrm{Beta}(a,b)\quad\text{if}  \quad Y= \frac{\sum_{i=1}^a X_i}{\sum_{j=1}^{a+b} X_j},\quad a,b=1,2,\dots $$
:::


::: {.callout-note}

There are better algorithms to generate Gamma and Beta random variables.

We cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter $\alpha$.

* This implies that we cannot generate a $\chi^2_{1}$-distributed random variable, because the $\chi^2_{1}$ distribution is identical to the $\Gamma(\alpha, 2)$ distribution with $\alpha=\frac{1}{2}.$ 

* This then also implies that we cannot generate a $\mathcal{N}(0,1)$-distrbuted random variable, since $X^2\sim \chi^2_{1}$ for $X\sim\mathcal{N}(0,1)$.  
:::        


The well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of @BoxMuller1958:


::: {#thm-NormalVariableGeneration}

## Normal Variable Generation (Box and Muller, 1958)

If $U_1$ and $U_2$ are i.i.d. as $U\sim\mathcal{U}[0,1]$, then 
$$
X_1 =\sqrt{-2 \ln(U_1)}\, \cos(2\pi U_2)
$$
and 
$$
X_2=\sqrt{-2\ln(U_1)}\,\sin(2\pi U_2)
$$
are both i.i.d. as $X\sim\mathcal{N}(0,1).$
::: 

**Proof:** 

Define the random variables 
$$
R = \sqrt{-2 \ln(U_1)}\quad\text{and}\quad Q = 2\pi U_2,
$$
where
$$
R\in(0,\infty)\quad\text{and}\quad Q\in[0,2\pi].
$$

::: {.callout-tip}

**Idea of the proof:**

1. Derive the bivariate density of $(R, Q).$
2. Determine the functional connection $g$ between $(R, Q)$ and $(X_1, X_2)$ and note that $g$ is invertible.
3. Use the **transformation formula** for densities to derive the bivariate density of $(X_1,X_2)$ using $g^{-1}$ and the bivariate density of $(R, Q).$ 
4. The result follows, if the bivariate density of $(X_1,X_2)$ equals the product of two standard normal densities.  


**Transformation formula (bivariate case):**

Assume that the bivariate random variable 
$$
\left(\begin{matrix}R\\ Q\end{matrix}\right)
$$ 
has a bivariate density $f_{RQ}(r, q)$ and that there is a mapping $g$ between the bivariate random variables 
$$
\left(\begin{matrix}R\\ Q\end{matrix}\right)\text{ and }
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
$$ 
such that 
$$
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)=
g(R, Q)=
\left(\begin{matrix}g_1(R, Q)\\ g_2 (R, Q)\end{matrix}\right),
$$
where $g:\mathbb{R}^2\to\mathbb{R}^2$ is a differentiable and invertible function with inverse $g^{-1}.$ 

Then, the bivariate density of $\left(\begin{matrix}X_1\\X_2\end{matrix}\right)$ 
is given by 
$$
f_{X_1X_2}(x_1,x_2)=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\,\left|\det\left(J_{g^{-1}}(x_1,x_2)\right)\right|,
$${#eq-TransformFormula}
where 
$$
\det\left(J_{g^{-1}}(x_1,x_2)\right)
$$ 
denotes the determinant of the Jacobian matrix of $g^{-1}$ evaluated at $(x_1,x_2),$
$$
J_{g^{-1}}(x_1,x_2)=\left(\begin{matrix}
\frac{\partial g_1^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_1^{-1}}{\partial x_2}(x_1,x_2)\\
\frac{\partial g_2^{-1}}{\partial x_1}(x_1,x_2) & \frac{\partial g_2^{-1}}{\partial x_2}(x_1,x_2)\\
\end{matrix}\right).
$$
Note that the Jacobian of $g^{-1}$ equals the inverse of the Jacobian of $g,$
$$
J_{g^{-1}}(x_1,x_2) = \left(J_{g}(r,q)\right)^{-1},
$$
with points $(x_1,x_2)$ and $(r,q)$ such that 
$$
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right)=
g(r, q)=
\left(\begin{matrix}g_1(r, q)\\ g_2 (r, q)\end{matrix}\right),
$$
:::

We can derive the distribution function of $R$ as following
$$
\begin{align*}
F_R(r)=\mathbb{P}\left(R\leq r\right) 
& = \mathbb{P}\left(\sqrt{-2 \ln(U_1)}\leq r\right) \\
& = \mathbb{P}\left(\ln(U_1)\geq -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) < -\frac{r^2}{2}\right) \\
& = 1 - \mathbb{P}\left(\ln(U_1) \leq -\frac{r^2}{2}\right) \quad \text{(continous)}\\
& = 1 - \mathbb{P}\left(U_1 \leq \exp\left(-\frac{r^2}{2}\right)\right) \\
& = 1 - F_U\left(\exp\left(-\frac{r^2}{2}\right)\right)\\
& = 1 - \exp\left(-\frac{r^2}{2}\right),
\end{align*}
$$
where the last step follows from applying the distribution $F_U$ of $U\sim\mathcal{U}[0,1];$ see @eq-FUnif. 

For the density function $f_R$ of $R$ we get
$$
f_R(r)=F_R'(r)=\left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right)\cdot r&\text{for }r \in(0,\infty)\\
  0&\text{otherwise}\\
  \end{array}\right.
$$
Next, we derive the density function of 
$$
Q = 2\pi U_2.
$$
Since $U_2\sim\mathcal{U}[0,1],$ 
$$
Q\sim\mathcal{U}[0,2\pi].
$$
with density function 
$$
f_Q(q)=\left\{
  \begin{array}{ll}
  \frac{1}{2\pi}&\text{for } q\in [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
$$
Since $U_1$ and $U_2$ are independent, $R=\sqrt{-2 \ln(U_1)}$ and $Q = 2\pi U_2$ must also be independent, such that
$$
\begin{align*}
f_{RQ}(r,q) 
& = f_R(r)\cdot f_Q(q) \\
& = \left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right) r\cdot \frac{1}{2\pi} & \text{for } (r,q) \in (0,\infty)\times [0, 2\pi] \\
  0&\text{otherwise}.\\
  \end{array}\right.
\end{align*}
$$  
Now, as we know the bivariate density of $(R,Q)$ we can use the functional connection 
$$
\begin{align*}
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
& = g(R,Q) \\
& = \left(\begin{matrix}
       g_1(R,Q)\\
       g_2(R,Q)\end{matrix}\right)
 = \left(\begin{matrix}
  R\cos(Q)\\
  R\sin(Q)
  \end{matrix}\right)
% & = \left(\begin{matrix}
%   \sqrt{-2\ln(U_1)}\cos\left(2\pi U_2\right)\\
%   \sqrt{-2\ln(U_1)}\sin\left(2\pi U_2\right)\\
%   \end{matrix}\right)\\
\end{align*}
$$
with 
$$
R = \sqrt{-\ln(U_1)}\in (0,\infty)
$$ 
and 
$$
Q=2\pi U_2\in[0, 2\pi].
$$
<!-- $$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
&=  g(r,q)\\
& = \left(\begin{matrix}g_1(r,q)\\g_2(r,q)\end{matrix}\right)\\
& = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$
for all $r\in [0,\infty)$ and all $q\in[0, 2\pi].$ -->

::: {.callout-tip}
Note that, $g$ is just the one-to-one transformation that maps points $(r,q)$ of the polar coordinate system 

* **radius** $r\in (0,\infty)$ and 
* **angle** $q\in[0, 2\pi]$ 

to points $(x_1,x_2)$ of the Cartesian coordinate system:
$$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
& = g(r,q)\\ 
& = \left(\begin{matrix}g_1(r,q) \\ g_2(r,q)\end{matrix}\right) 
 = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$

Play around with this mapping here: 
[https://mathinsight.org/applet/polar_coordinates_map_rectangle](https://mathinsight.org/applet/polar_coordinates_map_rectangle)

The inverse mapping $g^{-1}$ maps points $(x_1,x_2)$ from the Cartesian coordinate system to points $(r,q)$ in the polar coordinate system
$$
\begin{align*}
\left(\begin{matrix}r\\ q\end{matrix}\right) 
& = g^{-1}(x_1,x_2)\\
& = \left(\begin{matrix}g_1^{-1}(x_1,x_2) \\ g_2^{-1}(x_1,x_2)\end{matrix}\right) 
 = \left(\begin{matrix}\sqrt{x_1^2 + x_2^2}\\ \operatorname{atan2}(x_1,x_2)
\end{matrix}\right), 
\end{align*}
$$
where the function $\operatorname{atan2}$ is the [2-argument arctangent](https://en.wikipedia.org/wiki/Atan2).
<!-- where the Jacobian determinant of $g^{-1}$ is given by (derivation is a little cumbersome)
$$
\left|J_{g^{-1}}(r,q)\right| = r.
$$ -->
:::

$$
\begin{align*}
J_{g^{-1}}(x_1,x_2)
&=\left(J_{g}(r,q)\right)^{-1}\\
&=\left(\begin{matrix}
\frac{\partial g_1}{\partial r}(r,q) & \frac{\partial g_1}{\partial q}(r,q)\\
\frac{\partial g_2}{\partial r}(r,q) & \frac{\partial g_2}{\partial q}(r,q)\\
\end{matrix}\right)^{-1}\\
&=\left(\begin{matrix}
\cos(q) & -r\sin(q)\\
\sin(q) & \phantom{-}r\cos(q)\\
\end{matrix}\right)^{-1}\\
&=
\frac{1}{r\cos^2(q) + r\sin^2(q)}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)\\
&=
\frac{1}{r}
\left(\begin{matrix}
r\cos(q) & r\sin(q)\\
-\sin(q) &  \cos(q)\\
\end{matrix}\right)
\end{align*},
$$
where the last step follows from [Pythagorean's identity](https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity) $\cos^2(q) + \sin^2(q)=1.$
So 
$$
\begin{align*}
\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|
&=\left|\operatorname{det}\left(\left(J_{g}(r,q)\right)^{-1}\right)\right|\\
&=
\left|\operatorname{det}\left(
  \begin{matrix}
            \cos(q) &            \sin(q)\\
-\frac{1}{r}\sin(q) & \frac{1}{r}\cos(q)\\
\end{matrix}\right)
\right|\\
&=
\left|\frac{1}{r}\cos^2(q) + \frac{1}{r}\sin^2(q)\right| = \frac{1}{r},
\end{align*}
$$
again using [Pythagorean's identity](https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity) $\cos^2(x_2) + \sin^2(x_2)=1$ and using that $r\in[0,\infty).$


Thus, by the transformation formula for bivariate densities (@eq-TransformFormula), we have 
$$
\begin{align*}
f_{X_1X_2}(x_1,x_2)
&=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|\\
&=f_{RQ}\Big(\underbrace{\sqrt{x_1^2+x_2^2}}_{=r},\underbrace{\operatorname{atan2}(x_1,x_2)}_{=q}\Big)\frac{1}{r}\\
&=\exp\left(-\frac{x_1^2+x_2^2}{2}\right) \sqrt{x_1^2+x_2^2} \cdot \frac{1}{2\pi} \frac{1}{r}\\
&=\exp\left(-\frac{x_1^2+x_2^2}{2}\right)  \frac{1}{2\pi},
\end{align*}
$$
where the last step uses that $r=\sqrt{x_1^2+x_2^2}.$ 

This shows the result of @thm-NormalVariableGeneration, since the bivariate density of $(X_1,X_2)$ equals the product of two marginal standard normal density functions,
$$
\begin{align*}
f_{X_1X_2}(x_1,x_2) 
& = \frac{1}{2\pi}\exp\left(-\frac{x_1^2+x_2^2}{2}\right) \\ 
& = \underbrace{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x_1^2}{2}\right)}_{=\phi(x_1)} \cdot 
    \underbrace{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x_2^2}{2}\right)}_{=\phi(x_2)}, 
\end{align*}
$$
where $\phi$ denotes the density function of the standard normal distribution $\mathcal{N}(0,1)$. 


**Implementation** of the Box-Muller algorithm:
```{r,fig.width=10, fig.height=5, out.width='\\textwidth', fig.align='center'}
# Implementation:
BM_Algo <- function(){
  # Generate U_1, U_2 iid U[0,1]
  U <- runif(2)
  # Transformation
  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])
  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])
  # Return result
  return(c(X1, X2))
}

# Generate n standard normal random variables:
set.seed(321)

n     <- 1000
X_vec <- numeric(n)

for(i in seq(1, n, by=2)){
  X_vec[c(i, i+1)] <- BM_Algo()
}

# Descriptive Plots
par(mfrow=c(1,2))
hist(X_vec, freq = FALSE, xlim=c(-4,4))
curve(dnorm, add = TRUE, col="blue", lwd=1.3)
qqnorm(X_vec)

# Testing for Normality using the Shapiro-Wilk Test 
# H0: Normality
shapiro.test(X_vec)
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

\


### Accept-Reject Methods


For many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the distribution function $F$ is somehow unusable. For instance, surprisingly often there is no explicit form of $F$ available or its inverse does not exists.

 
Accept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density $f$ of interest---actually, $f$ needs to be known only up to a multiplicative constant. No deep analytic study of $f$ is necessary.

::: {#thm-FundamentalThmSimulation}

## Fundamental Theorem of Simulation

Let $X\in\mathbb{R}^d$ be a random variable with density function $f_X.$ Then simulating $X$ is equivalent to simulating from a  
$$
(X,U)\sim\mathcal{U}(\mathcal{A}),
$$
where $\mathcal{U}(\mathcal{A})$ denotes the uniform distribution over the area
$$
\mathcal{A}=\left\{(x,u)\text{ such that } x\in\mathbb{R}^d, 0<u<f_X(x)\right\}.
$$
:::

@fig-FundThmSim visualizes the statement of @thm-FundamentalThmSimulation.


```{r}
#| echo: false
#| eval: true
#| label: fig-FundThmSim
#| fig-cap: To simulate a random variable $X,$ one can simulate $(X,U)$ uniformely over $\mathcal{A}$ and take those $X$'s as simulation results (see tick-marks at $x$-axis). 

set.seed(123)
# Accept-Reject Algorithm:
Y <- runif(500, min = -4.5, max = 4.5) 
U <- runif(500, min =    0, max = 0.4) 
# A-R Step:
accept <- U <  dnorm(Y) - 0.025
X      <- Y[accept]
U      <- U[accept]
par(mfrow=c(1,1))
# curve(dnorm, col="blue", lwd=1.3, xlim=c(-4.5,4.5), ylab="", xlab="x")
# legend("topright", legen=expression(f[X](x)), bty="n")
curve(dnorm, col="blue", lwd=1.3, 
xlim=c(-4.5,4.5), ylim=c(0,0.45), ylab="u", xlab="x")
legend("topright", 
lty=c(1, NA), pch=c(NA, 19), col=c("blue", "orange"), 
legend=c(expression(f[X](x)), "Points (X,U) uniformly over A"), bty="n")
points(y=U, x=X, col="orange", pch=19)
axis(side=1, at = X, 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkorange", 0.5))
par(mfrow=c(1,1))
```


It turns out that sampling $(X, U)$ uniformly over the set $\mathcal{A}$ is often challenging. However, one can consider some superset $\mathcal{S},$
$$
\mathcal{A}\subseteq \mathcal{S},
$$ 
such that simulating a random variable uniformly distributed over $\mathcal{S}$ is easy. 

A uniform distribution on $\mathcal{A}$ can then be obtained by drawing from a uniform distribution on $\mathcal{S},$ and rejecting samples that are in $\mathcal{S},$ but not in $\mathcal{A}.$



#### The case of densities with compact support 

The general principle of the accept-reject method is easily explained using a *bounded* density function $f$ with *compact support*. 

::: {.callout-tip}

* **Bounded** means that there exists a constant $m$ with $0<m<\infty$ such that 
$$
\sup_xf(x)\leq m
$$ 
Note that only degenerated density functions are not bounded.
* An interval $[a,b]$ is called **compact** if it is closed and the boundaries are finite. For instance, the Gaussian density $\phi$ has *not* a compact support, but $\mathrm{supp}(\phi)=(-\infty,\infty)$.
  

Example:
$$
f(x)=\frac{3}{4}\left(1-\left(x-1\right)^2\right)\,1_{(|x-1|\leq 1)},
$$
where the (compact) support of $f$ is $[a,b]=[-1,1]$ and its range is $[0,m]=[0,3/4]$, i.e., $f$ is bounded from above by $3/4$. \
<!-- (Yes, it's a stupid example as we do not necessarily need the Accept-Reject Method here.) -->
:::

To simulate 
$$
X\sim f_X
$$
with a bounded and compactly supported density function $f_X,$ simulate the random pair 
$$
(Y,U)\sim\mathcal{U}([a,b]\times[0,m])
$$ 
by simulating   
<!-- $$Y\sim \mathrm{Unif}[a,b]\quad\text{and}\quad U\sim \mathrm{Unif}[0,m],$$ -->
$$
Y\sim\mathcal{U}[a,b]\quad\text{and}\quad U|Y=y \sim \mathcal{U}[0,m]. 
$$
<!-- but with **accepting** the pair $(Y,U)$ only if $U\leq f_X(Y)$ and **rejecting** all others.  -->
Then **accept** a simulated $Y$ as a simulation for $X$, i.e. 
$$
X=Y, 
$$
**only if** 
$$
U\leq f_X(Y),
$$ 
and **reject** all other $Y$'s. 


::: {.callout-note}
Simulating 
$$
U|Y=y \sim \mathcal{U}[0,m]
$$ 
is particularly simple, since the distribution of $U|Y=y$ is here equal to the distribution of the unconditional random variable 
$$
U|Y=y \sim U\sim \mathcal{U}[0,m]
$$
for any possible realization $Y=y.$
:::


**The Accept-Reject Algorithm (for compact densities):**

```{r}
#| echo: true
#| eval: false
# Accept-Reject Algorithm:
Y <- runif(n, min = a, max = b) 
U <- runif(n, min = 0, max = m) 
# A-R Step:
accept <- U <= f(Y)
X      <- Y[accept]
```


The following derivation shows that the simulated random variable $X$ has the correct distribution $F_X(x)=\int_a^xf_X(x)dx.$
$$
\begin{align*}
\mathbb{P}(X\leq x)
&=\mathbb{P}(Y\leq x|U\leq f_X(Y))\\[2ex]
&= \frac{\mathbb{P}(Y\leq x, U\leq f_X(Y))}{\mathbb{P}(U\leq f_X(Y))}\\[2ex]
&= \frac{\mathbb{P}(a\leq Y\leq x, \; 0\leq U\leq f_X(Y))}{\mathbb{P}(0\leq U\leq f_X(Y))}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,c\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,c\;du\,dy},
\end{align*}
$$
where the constant $c$ is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation 
$$
\int_a^{b} \int_0^{m}\,c\,du\,dy = 1,
$$
but which is irrelevant here since 
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{c\;\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{c\;\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}\\[2ex]
& =\frac{\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}.\\
\end{align*}
$$
Now, using that $\int_{0}^{f_X(y)}1du=\big[x\big]^{f_X(y)}_0=f_X(y)$ yields
$$
\begin{align*}
\mathbb{P}(X\leq x)
& =\frac{\int_a^x f_X(y)\,dy}{\int_a^b f_X(y)\,dy}\\[2ex]
& =\int_a^x f_X(y)dy = F_X(x).
\end{align*}
$$


In the following you see a graphical illustration of this procedure:

```{r,echo=FALSE, out.width='\\textwidth', fig.align='center', fig.width=10, fig.height=5}
# target pdf
target_pdf <- function(x){
  pdf <- (3/4) * (1-(x)^2)
  pdf[(x)^2 > 1] <- 0
  ##
  return(pdf)
}

# Accept-Reject Algo:
Y <- runif(n=1000, min = -1, max = 1) 
U <- runif(n=1000, min =  0, max = 3/4) 
# A-R Step:
accept <- U <= target_pdf(Y)
X    <- Y[accept]

# #######
# plots #
# #######
library("scales")
xx <- seq(-1.2, 1.2, len=500)

par(mfrow=c(1,3))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main=expression(f[X](x)), 
     xlab = "X", ylab = "Density", ylim=c(0,.85))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,.85))
rect(xleft = -1, ybottom = 0, xright = 1, ytop = c((3/4)),      col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U, x=Y, pch=19, cex=.4)
legend("topleft", legend = c("Accept","Reject"), pch=22, pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), 
     col =   c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)))
legend("topright", legend = c("(Y,U) ~ Unif([-1,1]x[0,3/4])"), pch=19, pt.cex=.4)
# 3. plot 
hist(x = X, main="Histogram of \n X=Y|(U<= f(Y))", 
     freq = FALSE, ylim=c(0,.85), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```

The good thing is that we only need to evaluate the density function $f_X,$ nothing more.  




#### The case of densities with non-compact support {-}

The larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any "larger set", enclosing the pdf $f$, as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of $f$ is unbounded. 

Let the larger set denote by
$$
\mathscr{L}=\{(y,u):\, 0<u<m(y)\},
$$
where: 

* simulation of a uniform on $\mathscr{L}$ is **feasible** and 
\

* $m(x)\geq f(x)$ for all $x$.

\


From the **feasibility-requirement** it follows that $m(.)$ is necessarily integrable, i.e., that
$$\int_{\mathcal{X}}m(x)dx=M,$$
where $M$ exists and is finite (and positive), since otherwise, $\mathscr{L}$ would not have finite mass and a uniform distribution would not exists on $\mathscr{L}$.

\


Integrability of $m(.)$ is crucial here, since it allows us to relate $m(.)$ with a corresponding (auxiliary) pdf $g(.)$ as following: 
$$m(x)=M\,g(x),\quad\text{where}\quad\int_{\mathcal{X}}m(x)\,dx=\int_{\mathcal{X}}M\,g(x)\,dx=M.$$

Terminology:

* The pdf $g(.)$ is called the **instrumental density**. (Choose $g(.)$ as a pdf from which it is easy to simulate!)
* The pdf $f(.)$ is called the **target density**.

\


In order to simulate the pair $(Y,U)\sim\mathrm{Unif}(\mathscr{L})$ we can now simulate
$$Y\sim g\quad\text{and}\quad U|Y={\color{red}y}\sim\mathrm{Unif}[0,M\,g({\color{red}y})],$$
but **accept** the pair $(Y,U)$ only if $U\leq f(Y)$ and to **reject** all others.

This results in the correct distribution of the accepted value of $Y$, call it $X$, because
$$
\mathbb{P}(X\in A)=\mathbb{P}(Y\in A|U\leq f(Y))
=\frac{\int_{\color{red}A}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}{\int_\mathcal{X}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}
=\frac{\int_A f(y)\,dy}{\int_\mathcal{X} f(y)\,dy}
=\int_A f(y)dy,
$$
for every set $A$, 
<!-- measurable set $A$, i.e., is a member of the corresponding sigma algebra. -->
where we again used that $f(y)=\int_{0}^{f(y)}du$. 


Note that the above derivation implies that we only need to know the pdf $f(.)$ **up to an unkown multiplicative constant** $c>0$. I.e., it is enough to know $f(x)=c\,\tilde{f}_{\textrm{true}}(x)$, often written as $f(x)\propto \tilde{f}_{\textrm{true}}(x)$, since the unknown constant $c$ cancels out in the above quotient anyways. This is not so much of importance for us, but useful in **Bayesian Statistics**.

\

All this leads to a more general version of the Fundamental Theorem of Simulation:

*Fundamental Theorem of Simulation (General Version):*

:   Let $X\sim f$ and let $g(.)$ be a pdf s.t. $f(x)\leq M\,g(x)$ for some $M$ with $1\leq M<\infty$ and all $x$. 
    Then to simulate $X\sim f$ it is sufficient to generate
    $$Y\sim g\quad\text{and}\quad U|Y=y\sim\mathrm{Unif}[0,M\,g(y)]$$
    if one **accepts** the pair $(Y,U)$ only if $U\leq f(Y)$ and **rejects** all others.

\


The Accept-Reject Algorithm (General Version):

```
# Accept-Reject Algorithm:
Y   <- generate n random numbers from g(.)

# Specify function m():
m <- function(y){YOUR CODE}

U   <- numeric(n)
for(i in 1:n){
  U[i] <- runif(n=1, min = 0, max = m(Y[i])) 
}

# A-R Step:
accept <- U <= f(Y)
X      <- Y[accept]
```

\


**Example**

Let the target "density" be 
$$f(x)\propto \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)$$
with upper bound (or, rather, dominating density) the standard normal density
$$g(x)=\exp(-x^2/2)/\sqrt{2\pi},$$
which is obviously straightforward to generate. 

In this example we can set $m(x)=M\,g(x)$ with $M=1$, since we can simply scale the target "density" $f$ such that $f(x)\leq g(x)$ for all $x$. Specifically, we set $f(x)=0.075 \cdot \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)$. 

In the following you see the graphical illustration of this example:

```{r,echo=FALSE, out.width='\\textwidth', fig.align='center', fig.width=10, fig.height=5}
# Target pdf
target_pdf <- function(x, c=.075){
  pdf <- c * (exp(-x^2 / 2) * (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1))
  return(pdf)
}

# Upper bound
m_fun <- function(x){
  m <- exp(-x^2 / 2)/sqrt(2*pi)
  return(m)  
}

# Accept-Reject Algo:
set.seed(32280)
Y   <- rnorm(n=1000) 
U   <- numeric(length(Y))
for(i in 1:length(Y)){
  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) 
}

# A-R Step:
accept <- U <= target_pdf(Y)
X      <- Y[accept]

# #######
# plots #
# #######
xx <- seq(-4, 4, len=500)

par(mfrow=c(1,3))
# 1. plot #
plot(y = target_pdf(x=xx), x = xx, type="l", main="pdf f(x)", 
     xlab = "X", ylab = "Density", ylim=c(0,.40))
# 2. plot #
plot(y=target_pdf(x=xx), x=xx, type="l", 
     main="Accept-Reject Regions", xlab = "Y", ylab = "U", ylim=c(0,0.40))
polygon(x=c(xx, rev(xx)), y=c(m_fun(x=xx), rep(0,length(xx))), col=gray(0.8, alpha = 0.5))
polygon(x=c(xx, rev(xx)), y=c(target_pdf(x=xx), rep(0,length(xx))), col=alpha("darkgreen", 0.5))
points(y=U, x=Y, pch=19, cex=.4)
legend("topleft", legend = c("Accept","Reject"), pch=22, pt.cex=1.5,
     pt.bg = c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)), 
     col =   c(alpha("darkgreen", 0.5), gray(.7, alpha = .5)))
legend("topright", legend = c("(Y,U) ~ Unif(L)"), pch=19, pt.cex=.4)
# 3. plot
hist(x = X, main="Histogram of \n X=Y|(U<= f(Y))", 
     freq = FALSE, ylim=c(0,.40), xlim=range(xx), xlab="X", ylab="Density")
axis(side=1, at = X[seq(from = 1,to = length(X), by=3)], 
     tick = TRUE, labels = FALSE, col.ticks = alpha("darkgreen", 0.5))
lines(y=target_pdf(x=xx), x=xx, lty=2)
box()
```

```{r, echo=FALSE}
par(mfrow=c(1,1))
```



**Efficiency of the Accept-Reject algorithm:** 

Statements with respect to the efficiency of the Accept-Reject algorithm can be made if $f$ and $g$ are **normalized** such that they are **both pdfs**. Then:

* The constant $M$ is necessarily larger than $1$.
* The probability of acceptance is $1/M$. (See Exercises.)
* $M$ is interpreted as the efficiency of the Accept-Reject algorithm. (The closer $M$ is to $1$ the better.)
* $M$ is a function of how closely $g$ can imitate $f$. 

Note that, for such normalized $f$ and $g$ the inequality $f(x)\leq M\,g(x)$ with $1\leq M<\infty$ for all $x$ is equivalent to saying that the quotient $f/g$ is bounded, i.e., that 
$$
0\leq \frac{f(x)}{g(x)}\leq M <\infty\quad\text{for all}\quad x.
$$
That is, it is necessary for $g$ to have, e.g., thicker tails than $f$. This makes it, for instance, impossible to simulate a Cauchy distribution $f$ using a normal distribution $g$. The reverse, however, works quite well. 
<!--
Remember: Using the Cauchy as instrumental pdf $g$ does not harm the integrability/feasibility requirement! The fact that the Cauchy distribution has no mean, does not change the fact that the density of the Cauchy integrates to one!
-->


\

**Example: Normals from Double Exponentials**

Consider generating a $N(0,1)$ by the Accept-Reject algorithm using a double-exponential distribution $\mathcal{L}(\alpha)$, also called [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution), 
with density $g(x|b)=(1/(2b))\exp(-\,|x|/b)$. <!-- here: location-param mu=0, scale.param=b\geq 0, precision=alpha=b^{-1} -->
It is then straightforward to show that 
$$
\frac{f(x)}{g(x|b)}
%=\frac{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right)}{\frac{1}{2b}\exp\left(-\frac{|x|}{b}\right)}
%=\sqrt{\frac{2}{\pi}}\,b\,\exp\left(-\frac{1}{2}x^2+\frac{|x|}{b}\right)
\leq\sqrt{\frac{2}{\pi}}\,b\,\exp\left(\frac{1}{2\,b^2}\right)
$$
and that the minimum of the bound (in $b$) is attained for $b=1$. 

This leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf:
$$
\frac{f(x)}{g(x|1)}
\leq M=\sqrt{\frac{2}{\pi}}\,\exp\left(\frac{1}{2}\right).
$$

The probability of acceptance is then $\sqrt{\pi/(2e)}=0.76$. I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average $1/0.76\approx 1.3$ uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1.

