<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computational Statistics (M.Sc.) - 1&nbsp; Random Variable Generation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch2_Bootstrap.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Statistics (M.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Random_Variable_Generation.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_Bootstrap.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Bootstrap</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MaximumLikelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_EMAlgorithmus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Expectation Maximization (EM) Algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_NonparametricRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Nonparametric Regression Analysis</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#literature" id="toc-literature" class="nav-link active" data-scroll-target="#literature">Literature</a></li>
  <li><a href="#introduction-motivation" id="toc-introduction-motivation" class="nav-link" data-scroll-target="#introduction-motivation">Introduction &amp; Motivation</a></li>
  <li><a href="#uniform-simulation" id="toc-uniform-simulation" class="nav-link" data-scroll-target="#uniform-simulation"><span class="toc-section-number">1.1</span>  Uniform Simulation</a></li>
  <li><a href="#generation-of-discrete-random-variables" id="toc-generation-of-discrete-random-variables" class="nav-link" data-scroll-target="#generation-of-discrete-random-variables"><span class="toc-section-number">1.2</span>  Generation of Discrete Random Variables</a></li>
  <li><a href="#generation-of-continuous-random-variables" id="toc-generation-of-continuous-random-variables" class="nav-link" data-scroll-target="#generation-of-continuous-random-variables"><span class="toc-section-number">1.3</span>  Generation of Continuous Random Variables</a>
  <ul class="collapse">
  <li><a href="#the-inverse-method" id="toc-the-inverse-method" class="nav-link" data-scroll-target="#the-inverse-method"><span class="toc-section-number">1.3.1</span>  The Inverse Method</a></li>
  <li><a href="#transformation-methods" id="toc-transformation-methods" class="nav-link" data-scroll-target="#transformation-methods"><span class="toc-section-number">1.3.2</span>  Transformation Methods</a></li>
  <li><a href="#accept-reject-methods" id="toc-accept-reject-methods" class="nav-link" data-scroll-target="#accept-reject-methods"><span class="toc-section-number">1.3.3</span>  Accept-Reject Methods</a></li>
  </ul></li>
  <li><a href="#monte-carlo-method" id="toc-monte-carlo-method" class="nav-link" data-scroll-target="#monte-carlo-method"><span class="toc-section-number">2</span>  Monte Carlo Method</a>
  <ul class="collapse">
  <li><a href="#classical-monte-carlo-integration" id="toc-classical-monte-carlo-integration" class="nav-link" data-scroll-target="#classical-monte-carlo-integration"><span class="toc-section-number">2.1</span>  Classical Monte Carlo Integration</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling"><span class="toc-section-number">2.2</span>  Importance Sampling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="literature" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="literature">Literature</h3>
<p>In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:</p>
<ul>
<li><a href="http://www.springer.com/us/book/9780387212395">Monte Carlo Statistical Methods</a>, Ch. 2, <span class="citation" data-cites="RobertCasella1999">Robert and Casella (<a href="#ref-RobertCasella1999" role="doc-biblioref">2004</a>)</span></li>
<li><a href="https://link.springer.com/book/10.1007/978-1-4419-1576-4">Introducing Monte Carlo Methods with R</a>, Ch. 2, <span class="citation" data-cites="RobertCasella2010">Robert and Casella (<a href="#ref-RobertCasella2010" role="doc-biblioref">2010</a>)</span></li>
</ul>
<!-- * Further:
  * [Non-Uniform Random Variate Generation](http://luc.devroye.org/rnbookindex.html), Devroye, L. 
  * [Nonparametric Density Estimation: The L1 View](http://luc.devroye.org/L1bookBW.pdf), Devroye, L., Ch. 8
  * [Monte Carlo and Quasi-Monte Carlo Sampling](http://www.springer.com/us/book/9780387781648), Lemieux, C., Ch. 2 and 3 -->
</section>
<section id="introduction-motivation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="introduction-motivation">Introduction &amp; Motivation</h3>
<p>In many complex statistical models parameter estimates can only be calculated numerically and their (statistical) behavior can only be investigated through a <strong>simulation</strong> on the computer:</p>
<p><strong>Stylized Algorithm:</strong></p>
<ul>
<li><strong>Step 1</strong> Generate artificial data from a statistical model and compute the estimate(s) of interest.</li>
<li><strong>Step 2</strong> Repeat this 10000 (or more) times.</li>
<li><strong>Step 3</strong> From these 10000 “pseudo” realizations of the estimator we can directly compute, e.g., the bias or the mean squared error (MSE) of the estimator—without the need of complicated statistical theory.</li>
</ul>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-1_310d82b5672bf1add47868f68402abb7">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Step 1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>generate_mean_estimates <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> <span class="dv">100</span>){</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="do">## generate artificial data</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  artificial_data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)   </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="do">## compute the estimate</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  estimate        <span class="ot">&lt;-</span> <span class="fu">mean</span>(artificial_data) </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(estimate)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Step 2 </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">223</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>simulated_estimates <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                                 <span class="fu">generate_mean_estimates</span>(<span class="at">n =</span> <span class="dv">100</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Step 3</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(simulated_estimates)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.003045196</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(simulated_estimates)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.09714423</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(simulated_estimates, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Histogram of Simulated Mean Estimates"</span>, </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>In order to conduct Steps 1 and 2 we need artificial realizations (here we used <code>rnorm()</code>) of a random variable—so-called <strong>Pseudo Random Numbers</strong>. The generation of such pseudo random numbers is the topic of the following chapters.</p>
</section>
<section id="uniform-simulation" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="uniform-simulation"><span class="header-section-number">1.1</span> Uniform Simulation</h2>
<p><strong>General procedure:</strong></p>
<ul>
<li>Usually, a random integer with values <strong>uniformly</strong> in <span class="math inline">\([0,m]\)</span> with a large integer <span class="math inline">\(m\)</span> is generated.</li>
<li>To achieve a random number in <span class="math inline">\([0, 1]\)</span>, we divide this number by <span class="math inline">\(m\)</span>.</li>
<li>From this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.</li>
</ul>
<p>There are many different <strong>Random Number Generators (RNGs)</strong>, we consider the most simple class of RNGs:</p>
<div id="def-LinConGen" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Linear Congruential Generators) </strong></span>Here the <span class="math inline">\(i\)</span>th random integer <span class="math inline">\(u_i\)</span> is generated by <span class="math display">\[
u_i=(a u_{i-1}+c) \,\operatorname{mod}\, m,
\]</span> where the starting value <span class="math inline">\(u_0\)</span> is a chosen and fixed value called <strong>seed</strong>.</p>
<p>Furthermore:</p>
<ul>
<li><span class="math inline">\(m\)</span>, with <span class="math inline">\(0&lt;m\)</span>, is called the <strong>modulus</strong></li>
<li><span class="math inline">\(a\)</span>, with <span class="math inline">\(0&lt;a&lt;m\)</span>, is called the <strong>multiplier</strong></li>
<li><span class="math inline">\(c\)</span>, with <span class="math inline">\(0\leq c&lt;m\)</span>, is called the <strong>increment</strong></li>
</ul>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The modulo operator: <span class="math inline">\(\operatorname{mod}\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>“<span class="math inline">\(b\,\operatorname{mod}\,c\)</span>” denotes the remainder of the division of <span class="math inline">\(b\)</span> by <span class="math inline">\(c\)</span>.</p>
<p>For instance <span class="math display">\[
\begin{align*}
4\,&amp;\operatorname{mod}\,2 = 0\\
5\,&amp;\operatorname{mod}\,2 = 1\\
1\,&amp;\operatorname{mod}\,2 = 1\\
\end{align*}
\]</span></p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-2_27f357dacdc1175486533fbdd86713ca">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Modulo computation using the modulo operator '%%'</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span> <span class="sc">%%</span> <span class="dv">4</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span> <span class="sc">%%</span> <span class="dv">4</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span> <span class="sc">%%</span> <span class="dv">5</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># own modulo-function:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>my_mod <span class="ot">&lt;-</span> <span class="cf">function</span>(x,m){</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  t1 <span class="ot">&lt;-</span> <span class="fu">floor</span>(x<span class="sc">/</span>m)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">-</span>t1<span class="sc">*</span>m)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Some Facts:</p>
<ul>
<li>The above recursion generates a completely <strong>nonrandom</strong> sequence, therefore it is often called a <strong><em>pseudo</em> random</strong> sequence.</li>
<li>Under appropriate choices of <span class="math inline">\(u_0\)</span> , <span class="math inline">\(a\)</span> and <span class="math inline">\(m\)</span> the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on <span class="math inline">\([0, m]\)</span>.</li>
<li>The cycle length of linear congruential generators will never exceed modulus <span class="math inline">\(m\)</span>, but can maximized with the three following conditions (see <a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">Knuth (2002)</a> for a proof):
<ul>
<li>The increment <span class="math inline">\(c\)</span> is <a href="https://en.wikipedia.org/wiki/Coprime_integers">relatively prime</a> to <span class="math inline">\(m\)</span>,</li>
<li><span class="math inline">\(a - 1\)</span> is a multiple of every prime dividing <span class="math inline">\(m\)</span>,</li>
<li><span class="math inline">\(a - 1\)</span> is a multiple of <span class="math inline">\(4\)</span> when <span class="math inline">\(m\)</span> is a multiple of <span class="math inline">\(4\)</span>.</li>
</ul></li>
</ul>
<!--
Further reading:
Ripley (1987) provides a review of number generators. 
Devroye (1986) is very comprehensive book on non-uniform random variate generation.
-->
<p><strong>Bad choice of parameters</strong> for the linear congruential random number generator:</p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-3_c333ef6e5f657b66685a677a124cd3fc">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">64</span>    <span class="co"># modulus</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">33</span>    <span class="co"># multiplier</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="dv">12</span>    <span class="co"># increment</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">57</span>    <span class="co"># seed</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># length of run (including seed)</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>r_vec    <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n) <span class="co"># initialize vector</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>r_vec[<span class="dv">1</span>] <span class="ot">&lt;-</span> s <span class="co"># set seed</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Recursive generation </span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-1</span>)){</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a> r_vec[i<span class="sc">+</span><span class="dv">1</span>] <span class="ot">&lt;-</span> (a <span class="sc">*</span> r_vec[i] <span class="sc">+</span> c) <span class="sc">%%</span> m</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># scale result from [0,m] to [0,1]:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>my_bad_runif_vec <span class="ot">&lt;-</span> r_vec<span class="sc">/</span>m</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># BUT! Very short cycle-length (here: period=16)</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>r_vec[ <span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>r_vec[<span class="dv">17</span><span class="sc">:</span><span class="dv">32</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13</code></pre>
</div>
</div>
<p><br>
</p>
<div id="exm-GoodVsBadRNG" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Good vs.&nbsp;Bad RNGs) </strong></span>Average heads ratios <span class="math display">\[
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i
\]</span> for <span class="math inline">\(n=1,2,\dots\)</span> simulated independent tosses of a fair coin <span class="math inline">\(C_i\)</span> with<br>
<span class="math display">\[
C_{i}=\left\{\begin{array}{ll}
1&amp;\text{if Head}\\
0&amp;\text{if Tail}
\end{array}\right.
\]</span> and <span class="math display">\[
P(C_i=0)=P(C_i=1)=0.5.
\]</span> By the strong (or weak) law of large numbers the average should converge <strong>stochastically</strong> (i.e., almost surely or in probability) to <span class="math inline">\(0.5\)</span> as <span class="math inline">\(n\)</span> becomes large <span class="math inline">\((n\to\infty),\)</span> <span class="math display">\[
\bar{C}_n=\frac{1}{n}\sum_{i=1}^nC_i \to_p 0.5,\quad n \to\infty.
\]</span></p>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/fig-goodvsbad_958bdcb1072aef018e98ca723334ed2c">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using the above bad RNG:</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>bar_x_bad  <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(my_bad_runif_vec <span class="sc">&gt;</span> <span class="fl">0.5</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># using R's high-quality RNG:</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">223</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>bar_x_good <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(<span class="fu">runif</span>(n)  <span class="sc">&gt;</span> <span class="fl">0.5</span> )<span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the results:</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bar_x_bad, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.46</span>,<span class="fl">0.54</span>), </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">main=</span><span class="st">"Good vs. Bad RNG"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(bar_x_good, <span class="at">col=</span><span class="st">"darkblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-goodvsbad" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/fig-goodvsbad-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.1: Two sample paths showing the pseudo random convergence of <span class="math inline">\(\bar{C}_n\)</span> to the limit 0.5—one based on a good RNG and the other based on a bad RNG.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<!--

::: {.cell hash='Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-5_ec9f0e905fbcb657722022c2e0743a89'}

```{.r .cell-code}
## See also:
install.packages("randtoolbox")
library("randtoolbox")
?congruRand()
```
:::

-->
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>IBM’s <a href="https://en.wikipedia.org/wiki/RANDU">RANDU</a> is a famous example of an miss-specified linear congruential RNG.</p>
</div>
</div>
</section>
<section id="generation-of-discrete-random-variables" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="generation-of-discrete-random-variables"><span class="header-section-number">1.2</span> Generation of Discrete Random Variables</h2>
<!-- https://rpubs.com/maheshraje982/617094 -->
<p>Assume the discrete random variable <span class="math inline">\(X\)</span> of interest takes on the values <span id="eq-DiscRV"><span class="math display">\[
X\in \{x_1, \dots , x_k \}
\tag{1.1}\]</span></span> with <span class="math display">\[
p_i = \mathbb{P}(X = x_i ), \quad i = 1,\dots , k,
\]</span> and <span class="math display">\[
\sum_{i=1}^kp_i = 1.
\]</span></p>
<p>Assume that you can generate pseudo-random realizations <span class="math inline">\(u\in[0,1]\)</span> from a uniformly distributed random variable <span class="math inline">\(U\sim\mathcal{U}[0, 1]\)</span> using an RNG.</p>
<p><strong>General principle:</strong></p>
<ol type="1">
<li>Subdivide <span class="math inline">\([0, 1]\)</span> into <span class="math inline">\(k\)</span> intervals with <span class="math display">\[
I_i = (a_{i-1}, a_i],
\]</span> where <span class="math display">\[
a_i = \sum_{j=1}^ip_j\quad\text{and}\quad a_0 = 0.
\]</span></li>
<li>Define the new discrete realizations <span id="eq-U2DiscrX"><span class="math display">\[x=\left\{
\begin{array}{cc}
       x_1&amp;\quad\text{if}\quad u\in I_1\\
       \vdots&amp; \vdots\\
       x_k&amp;\quad\text{if}\quad u\in I_k
       \end{array}\right.
\tag{1.2}\]</span></span></li>
</ol>
<div id="lem-GenDiscrRV" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1.1 </strong></span>Let <span class="math inline">\(u\)</span> be a realization from <span class="math inline">\(\mathcal{U}[0, 1]\)</span> and if <span class="math inline">\(u\in I_i\)</span>, set <span class="math inline">\(x = x_i\)</span> (see <a href="#eq-U2DiscrX">Equation&nbsp;<span>1.2</span></a>). Then <span class="math inline">\(x\)</span> is a realizaton of the discrete random variable <span class="math inline">\(X\)</span> (see <a href="#eq-DiscRV">Equation&nbsp;<span>1.1</span></a>).</p>
</div>
<!-- **Proof:** Done in the lecture. -->
<p><strong>Proof:</strong></p>
<p>For any <span class="math inline">\(i = 1, \dots, k\)</span> we have that <span class="math display">\[
\begin{align*}
\mathbb{P}(X = x_i)
&amp; = \mathbb{P}(U \in I_i)  \\
&amp; = F_\mathcal{U}(a_i) - F_\mathcal{U}(a_{i-1})\\
&amp; = a_i - a_{i-1}\\
&amp; = \sum_{j=1}^ip_j - \sum_{j=1}^{i-1}p_j  = p_i,
\end{align*}
\]</span> which shows the statement of <a href="#lem-GenDiscrRV">Lemma&nbsp;<span>1.1</span></a>.</p>
<div id="exm-Bernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Bernoulli Distribution) </strong></span>Generate random numbers from <span class="math display">\[
X\sim\mathrm{Bernoulli}(p),
\]</span> where <span class="math inline">\(p\)</span> is the probability of success, i.e., <span class="math display">\[
\mathbb{P}(X=1)=p\quad\text{and}\quad\mathbb{P}(X=0)=1-p.
\]</span></p>
<p><strong>Algorithm:</strong> If <span class="math inline">\(U\sim\mathcal{U}[0,1]\)</span> and <span class="math inline">\(p\)</span> is specified, define <span class="math display">\[
X=\left\{
  \begin{matrix}
  1 &amp; \text{if }U\leq p\\
  0 &amp; \text{otherwise}.\\
  \end{matrix}
\right.
\]</span> Then <span class="math inline">\(X\sim\mathrm{Bernoulli}(p)\)</span>.</p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-6_ef7b417f0369cc4c519dec2c6dd96f95">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate one random number from Bernoulli(p) with p=0.5</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>p  <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>U  <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(U<span class="sc">&lt;=</span>p) X<span class="ot">=</span><span class="dv">1</span> <span class="cf">else</span> X<span class="ot">=</span><span class="dv">0</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
</div>
<div id="exm-Binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Binomial Distribution) </strong></span>Generate random numbers from <span class="math display">\[
X\sim\mathrm{Binomial}(n,p),
\]</span> where <span class="math inline">\(n\)</span> is the number of trials and <span class="math inline">\(p\)</span> the probability of success such that <span class="math display">\[
\mathbb{P}(X=i)=\binom{n}{i}p^i(1-p)^{n-1}
\]</span> for <span class="math inline">\(i=1,\dots,n.\)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(X_1,\dots,X_n\overset{i.i.d}{\sim}\mathrm{Bernoulli}(p),\)</span> then <span class="math display">\[
X=\sum_{i=1}^nX_i \sim\mathrm{Binomial}(n,p).
\]</span></p>
</div>
</div>
<p><strong>Algorithm:</strong> If <span class="math inline">\(U_1,\dots,U_n\)</span> are i.i.d. as <span class="math inline">\(U\sim \mathcal{U}[0,1]\)</span> and <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> are specified, define <span class="math display">\[
X_i=\left\{
  \begin{matrix}
  1 &amp; \text{if }U_i\leq p\\
  0 &amp; \text{otherwise}.\\
  \end{matrix}
\right.
\]</span> Then <span class="math display">\[
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Binomial}(n,p).
\]</span></p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-7_1c11cecb4085f854644b236fa6c3abfb">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate one random number from B(n=10, p=0.5). </span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n){</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(U<span class="sc">&lt;=</span>p) X[i]<span class="ot">=</span><span class="dv">1</span> <span class="cf">else</span> X[i]<span class="ot">=</span><span class="dv">0</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">sum</span>(X)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>Y </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7</code></pre>
</div>
</div>
</div>
<div id="exm-Poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Poisson Distribution) </strong></span><strong>Algorithm:</strong> If <span class="math inline">\(U_1,\dots,U_n\)</span> are i.i.d. as <span class="math inline">\(U\sim \mathcal{U}[0,1]\)</span> and <span class="math inline">\(\lambda\)</span> is specified, define <span class="math display">\[
X=\min\left\{n=0,1,2\dots,\text{ such that }\prod_{i=1}^{n+1} U_i \leq e^{-\lambda}
\right\}.
\]</span> Then <span class="math display">\[
X=\left(\sum_{i=1}^n X_i\right)\sim\mathrm{Poisson}(\lambda).
\]</span></p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-8_6aea75f5c24f06bcb921b2208222780d">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate one random number from Poisson(lambda) </span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Initializations</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(U <span class="sc">&gt;</span> <span class="fu">exp</span>(<span class="sc">-</span>lambda)){</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> U <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> n <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> n<span class="dv">-1</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3</code></pre>
</div>
</div>
</div>
</section>
<section id="generation-of-continuous-random-variables" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="generation-of-continuous-random-variables"><span class="header-section-number">1.3</span> Generation of Continuous Random Variables</h2>
<section id="the-inverse-method" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="the-inverse-method"><span class="header-section-number">1.3.1</span> The Inverse Method</h3>
<p>A rather general method to generate continuous random variables is the <strong>Inverse Method</strong>.</p>
<div id="thm-InverseMethod" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Inverse Method) </strong></span>Let <span class="math inline">\(U\sim\mathcal{U}[0,1],\)</span> and let <span class="math inline">\(F_X\)</span> be an <strong>invertible</strong> distribution function. The transformed random variable<br>
<span class="math display">\[
X=F_X^{-1}(U)
\]</span> has then the distribution function <span class="math inline">\(F_X,\)</span> <span class="math display">\[
P(X\leq x) = F_X(x).
\]</span></p>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="#thm-InverseMethod">Theorem&nbsp;<span>1.1</span></a> can only be used to generate random variables <span class="math inline">\(X\)</span> with <strong>invertible</strong> distribution functions <span class="math inline">\(F_X.\)</span></p>
</div>
</div>
<p><strong>Proof:</strong></p>
<p>The distribution function of the transformed random variable <span class="math display">\[
X=F^{-1}(U)
\]</span> can be derived as <span class="math display">\[
\begin{align*}
\mathbb{P}(X\leq x)
&amp;= \mathbb{P}(F_X^{-1}(U)\leq x) \\
&amp;= \mathbb{P}(U\leq F_X(x)) \\
&amp;= F_U(F_X(x)) \\
&amp; = F_X(x),
\end{align*}
\]</span> which shows the result of <a href="#thm-InverseMethod">Theorem&nbsp;<span>1.1</span></a>. The last (and important) equality follows since the distribution function of <span class="math inline">\(U\sim\mathcal{U}[0,1]\)</span> is <span class="math display">\[
\mathbb{P}(U\leq u) = F_U(u) = u, \quad 0\leq u \leq 1
\]</span> since the distribution function <span class="math inline">\(F_U\)</span> of <span class="math inline">\(U\sim\mathcal{U}[0,1]\)</span> is <span id="eq-FUnif"><span class="math display">\[
F_U(u) = \left\{
  \begin{array}{ll}
  0 &amp; \text{for } u &lt; 0\\
  u &amp; \text{for } 0 \leq  u \leq 1\\
  1 &amp; \text{for } 1 &lt; u.\\
  \end{array}
\right.  
\tag{1.3}\]</span></span></p>
<!-- https://statproofbook.github.io/P/cdf-itm.html#:~:text=Proof%3A%20Inverse%20transformation%20method%20using%20cumulative%20distribution%20function&text=has%20a%20probability%20distribution%20characterized,)%20F%20X%20(%20x%20)%20.&text=U%E2%88%BCU(0%2C1,U%E2%89%A4u)%3Du. -->
<!-- http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf -->
<div id="exm-Exponential" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Exponential Distribution) </strong></span>Since <span class="math display">\[
F(x)= 1 - \exp(-\lambda x),
\]</span> we have <span class="math display">\[
F^{-1}(u) = - \frac{\ln(1-u)}{\lambda}.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that <span class="math inline">\(1-U\)</span> has the same distribution as <span class="math inline">\(U\)</span>, if <span class="math inline">\(U\sim U[0,1]\)</span>. Therefore also <span class="math inline">\(-\frac{\ln(u)}{\lambda}\)</span> leads to a value from <span class="math inline">\(\mathrm{Exp}(\lambda).\)</span></p>
</div>
</div>
<p><strong>Algorithm:</strong> If <span class="math inline">\(U\sim \mathcal{U}[0,1]\)</span> and <span class="math inline">\(\lambda\)</span> is specified, define <span class="math display">\[
X=-\frac{\ln(U)}{\lambda}.
\]</span> Then <span class="math display">\[
X\sim \mathrm{Exp}(\lambda).
\]</span></p>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The inverse method is mainly a good &amp; general way to think about transforming random variables, in practice, however, we often use other methods.</p>
<p>The inverse method often cannot be applied or is often inefficient, because the inverse of many important distribution functions cannot be derived in <a href="https://en.wikipedia.org/wiki/Closed-form_expression">closed form</a>:</p>
<ul>
<li>The Gaussian distribution function <span class="math inline">\(\Phi\)</span> and therefore also its inverse <span class="math inline">\(\Phi^{-1}\)</span> is not available in closed form.</li>
<li>For discontinuous random variables we need efficient algorithms for computing the <em>generalized</em> inverse of their distribution function <span class="math inline">\(F.\)</span></li>
</ul>
</div>
</div>
</section>
<section id="transformation-methods" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="transformation-methods"><span class="header-section-number">1.3.2</span> Transformation Methods</h3>
<p><strong>Idea:</strong> Construct algorithms from <a href="http://www.math.wm.edu/~leemis/2008amstat.pdf">theoretical links</a> between distributions.</p>
<p><strong>Pro:</strong> These methods can be advantageous if a distribution <span class="math inline">\(f\)</span> is linked (in a relatively simple way) to another distribution that is easy to simulate.</p>
<p><strong>Con:</strong> Generally, these methods are rather case-specific, and difficult to generalize.</p>
<div id="exm-BuildOnExp" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Building on Exponential RVs) </strong></span>In <a href="#exm-Exponential">Example&nbsp;<span>1.5</span></a>, we learned to generate an exponential random variable <span class="math inline">\(X\sim\operatorname{Exp}(\lambda)\)</span> starting from a uniform random variable <span class="math inline">\(U\sim\mathcal{U}[0,1].\)</span> In the following we generate random variables starting from exponential random variables <span class="math inline">\(X\sim\mathrm{Exp}(1):\)</span></p>
<p>If the <span class="math inline">\(X_1, X_2,\dots\)</span> are i.i.d. as <span class="math inline">\(X\sim\mathrm{Exp}(1),\)</span> then</p>
<p><span class="math display">\[Y\sim \chi^2_{2\nu}\quad\text{if}       \quad Y= 2     \sum_{i=1}^\nu X_i,\quad\nu=1,2,\dots \]</span> <span class="math display">\[Y\sim \Gamma(\alpha,\beta)\quad\text{if}\quad Y= \beta \sum_{i=1}^\alpha X_i,\quad \alpha=1,2,\dots \]</span> <span class="math display">\[Y\sim \mathrm{Beta}(a,b)\quad\text{if}  \quad Y= \frac{\sum_{i=1}^a X_i}{\sum_{j=1}^{a+b} X_j},\quad a,b=1,2,\dots \]</span></p>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are better algorithms to generate Gamma and Beta random variables.</p>
<p>We cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter <span class="math inline">\(\alpha\)</span>.</p>
<ul>
<li><p>This implies that we cannot generate a <span class="math inline">\(\chi^2_{1}\)</span>-distributed random variable, because the <span class="math inline">\(\chi^2_{1}\)</span> distribution is identical to the <span class="math inline">\(\Gamma(\alpha, 2)\)</span> distribution with <span class="math inline">\(\alpha=\frac{1}{2}.\)</span></p></li>
<li><p>This then also implies that we cannot generate a <span class="math inline">\(\mathcal{N}(0,1)\)</span>-distrbuted random variable, since <span class="math inline">\(X^2\sim \chi^2_{1}\)</span> for <span class="math inline">\(X\sim\mathcal{N}(0,1)\)</span>.<br>
</p></li>
</ul>
</div>
</div>
<p>The well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of <span class="citation" data-cites="BoxMuller1958">Box and Muller (<a href="#ref-BoxMuller1958" role="doc-biblioref">1958</a>)</span>:</p>
<div id="thm-NormalVariableGeneration" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (Normal Variable Generation (Box and Muller, 1958)) </strong></span>If <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> are i.i.d. as <span class="math inline">\(U\sim\mathcal{U}[0,1]\)</span>, then <span class="math display">\[
X_1 =\sqrt{-2 \ln(U_1)}\, \cos(2\pi U_2)
\]</span> and <span class="math display">\[
X_2=\sqrt{-2\ln(U_1)}\,\sin(2\pi U_2)
\]</span> are both i.i.d. as <span class="math inline">\(X\sim\mathcal{N}(0,1).\)</span></p>
</div>
<p><strong>Proof:</strong></p>
<p>Define the random variables <span class="math display">\[
R = \sqrt{-2 \ln(U_1)}\quad\text{and}\quad Q = 2\pi U_2,
\]</span> where <span class="math display">\[
R\in[0,\infty)\quad\text{and}\quad Q\in[0,2\pi].
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Idea of the proof:</strong></p>
<ol type="1">
<li>Derive the bivariate density of <span class="math inline">\((R, Q).\)</span></li>
<li>Determine the functional connection <span class="math inline">\(g\)</span> between <span class="math inline">\((R, Q)\)</span> and <span class="math inline">\((X_1, X_2)\)</span> and note that <span class="math inline">\(g\)</span> is invertible.</li>
<li>Use the <strong>transformation formula</strong> for densities to derive the bivariate density of <span class="math inline">\((X_1,X_2)\)</span> using <span class="math inline">\(g^{-1}\)</span> and the bivariate density of <span class="math inline">\((R, Q).\)</span></li>
<li>The result follows, if the bivariate density of <span class="math inline">\((X_1,X_2)\)</span> equals the product of two standard normal densities.</li>
</ol>
<p><strong>Transformation formula (bivariate case):</strong></p>
<p>Assume that the bivariate random variable <span class="math display">\[
\left(\begin{matrix}R\\ Q\end{matrix}\right)
\]</span> has a bivariate density <span class="math inline">\(f_{RQ}(r, q)\)</span> and that there is a mapping <span class="math inline">\(g\)</span> between the bivariate random variables <span class="math display">\[
\left(\begin{matrix}R\\ Q\end{matrix}\right)\text{ and }
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
\]</span> such that <span class="math display">\[
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)=
g(R, Q)=
\left(\begin{matrix}g_1(R, Q)\\ g_2 (R, Q)\end{matrix}\right),
\]</span> where <span class="math inline">\(g:\mathbb{R}^2\to\mathbb{R}^2\)</span> is a differentiable and invertible function with inverse <span class="math inline">\(g^{-1}.\)</span></p>
<p>Then, the bivariate density of <span class="math inline">\(\left(\begin{matrix}X_1\\X_2\end{matrix}\right)\)</span> is given by <span id="eq-TransformFormula"><span class="math display">\[
f_{X_1X_2}(x_1,x_2)=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\,\left|\det\left(J_{g^{-1}}(x_1,x_2)\right)\right|,
\tag{1.4}\]</span></span> where <span class="math display">\[
\det\left(J_{g^{-1}}(x_1,x_2)\right)
\]</span> denotes the determinant of the Jacobian matrix of <span class="math inline">\(g^{-1}\)</span> evaluated at <span class="math inline">\((x_1,x_2),\)</span> <span class="math display">\[
J_{g^{-1}}(x_1,x_2)=\left(\begin{matrix}
\frac{\partial g_1^{-1}}{\partial x_1}(x_1,x_2) &amp; \frac{\partial g_1^{-1}}{\partial x_2}(x_1,x_2)\\
\frac{\partial g_2^{-1}}{\partial x_1}(x_1,x_2) &amp; \frac{\partial g_2^{-1}}{\partial x_2}(x_1,x_2)\\
\end{matrix}\right).
\]</span> Note that the Jacobian of <span class="math inline">\(g^{-1}\)</span> equals the inverse of the Jacobian of <span class="math inline">\(g,\)</span> <span class="math display">\[
J_{g^{-1}}(x_1,x_2) = \left(J_{g}(r,q)\right)^{-1},
\]</span> with points <span class="math inline">\((x_1,x_2)\)</span> and <span class="math inline">\((r,q)\)</span> such that <span class="math display">\[
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right)=
g(r, q)=
\left(\begin{matrix}g_1(r, q)\\ g_2 (r, q)\end{matrix}\right),
\]</span></p>
</div>
</div>
<p>We can derive the distribution function of <span class="math inline">\(R\)</span> as following <span class="math display">\[
\begin{align*}
F_R(r)=\mathbb{P}\left(R\leq r\right)
&amp; = \mathbb{P}\left(\sqrt{-2 \ln(U_1)}\leq r\right) \\
&amp; = \mathbb{P}\left(\ln(U_1)\geq -\frac{r^2}{2}\right) \\
&amp; = 1 - \mathbb{P}\left(\ln(U_1) &lt; -\frac{r^2}{2}\right) \\
&amp; = 1 - \mathbb{P}\left(\ln(U_1) \leq -\frac{r^2}{2}\right) \quad \text{(continous)}\\
&amp; = 1 - \mathbb{P}\left(U_1 \leq \exp\left(-\frac{r^2}{2}\right)\right) \\
&amp; = 1 - F_U\left(\exp\left(-\frac{r^2}{2}\right)\right)\\
&amp; = 1 - \exp\left(-\frac{r^2}{2}\right),
\end{align*}
\]</span> where the last step follows from applying the distribution <span class="math inline">\(F_U\)</span> of <span class="math inline">\(U\sim\mathcal{U}[0,1];\)</span> see <a href="#eq-FUnif">Equation&nbsp;<span>1.3</span></a>.</p>
<p>For the density function <span class="math inline">\(f_R\)</span> of <span class="math inline">\(R\)</span> we get <span class="math display">\[
f_R(r)=F_R'(r)=\left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right)\cdot r&amp;\text{for }r \in[0,\infty)\\
  0&amp;\text{for }r &lt; 0.\\
  \end{array}\right.
\]</span> Next, define the random variable <span class="math display">\[
Q = 2\pi U_2.
\]</span> Since <span class="math inline">\(U_2\sim\mathcal{U}[0,1],\)</span> <span class="math display">\[
Q\sim\mathcal{U}[0,2\pi].
\]</span> with density function <span class="math display">\[
f_Q(q)=\left\{
  \begin{array}{ll}
  \frac{1}{2\pi}&amp;\text{for } q\in [0, 2\pi] \\
  0&amp;\text{otherwise}.\\
  \end{array}\right.
\]</span> Since <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> are independent, <span class="math inline">\(R=\sqrt{-2 \ln(U_1)}\)</span> and <span class="math inline">\(Q = 2\pi U_2\)</span> must also be independent, such that <span class="math display">\[
\begin{align*}
f_{RQ}(r,q)
&amp; = f_R(r)\cdot f_Q(q) \\
&amp; = \left\{
  \begin{array}{ll}
  \exp\left(-\frac{r^2}{2}\right) r\cdot \frac{1}{2\pi} &amp; \text{for } (r,q) \in [0,\infty)\times [0, 2\pi] \\
  0&amp;\text{otherwise}.\\
  \end{array}\right.
\end{align*}
\]</span><br>
Now, as we know the bivariate density of <span class="math inline">\((R,Q)\)</span> we can use the functional connection <span class="math display">\[
\begin{align*}
\left(\begin{matrix}X_1\\ X_2\end{matrix}\right)
&amp; = g(R,Q) \\
&amp; = \left(\begin{matrix}
       g_1(R,Q)\\
       g_2(R,Q)\end{matrix}\right)
= \left(\begin{matrix}
  R\cos(Q)\\
  R\sin(Q)
  \end{matrix}\right)
% &amp; = \left(\begin{matrix}
%   \sqrt{-2\ln(U_1)}\cos\left(2\pi U_2\right)\\
%   \sqrt{-2\ln(U_1)}\sin\left(2\pi U_2\right)\\
%   \end{matrix}\right)\\
\end{align*}
\]</span> with <span class="math display">\[
R = \sqrt{-\ln(U_1)}\in [0,\infty)
\]</span> and <span class="math display">\[
Q=2\pi U_2\in[0, 2\pi].
\]</span> <!-- $$
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right) 
&=  g(r,q)\\
& = \left(\begin{matrix}g_1(r,q)\\g_2(r,q)\end{matrix}\right)\\
& = \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right) 
\end{align*}
$$
for all $r\in [0,\infty)$ and all $q\in[0, 2\pi].$ --></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that, <span class="math inline">\(g\)</span> is just the one-to-one transformation that maps points <span class="math inline">\((r,q)\)</span> of the polar coordinate system</p>
<ul>
<li><strong>radius</strong> <span class="math inline">\(r\in [0,\infty)\)</span> and</li>
<li><strong>angle</strong> <span class="math inline">\(q\in[0, 2\pi]\)</span></li>
</ul>
<p>to points <span class="math inline">\((x_1,x_2)\)</span> of the Cartesian coordinate system: <span class="math display">\[
\begin{align*}
\left(\begin{matrix}x_1\\ x_2\end{matrix}\right)
&amp; = g(r,q)\\
&amp; = \left(\begin{matrix}g_1(r,q) \\ g_2(r,q)\end{matrix}\right)
= \left(\begin{matrix}r\cos(q)\\r\sin(q)\end{matrix}\right)
\end{align*}
\]</span></p>
<p>Play around with this mapping here: <a href="https://mathinsight.org/applet/polar_coordinates_map_rectangle">https://mathinsight.org/applet/polar_coordinates_map_rectangle</a></p>
<p>The inverse mapping <span class="math inline">\(g^{-1}\)</span> maps points <span class="math inline">\((x_1,x_2)\)</span> from the Cartesian coordinate system to polar-points <span class="math inline">\((r,q)\)</span> in the polar coordinate system <span class="math display">\[
\begin{align*}
\left(\begin{matrix}r\\ q\end{matrix}\right)
&amp; = g^{-1}(x_1,x_2)\\
&amp; = \left(\begin{matrix}g_1^{-1}(x_1,x_2) \\ g_2^{-1}(x_1,x_2)\end{matrix}\right)
= \left(\begin{matrix}\sqrt{x_1^2 + x_2^2}\\ \operatorname{atan2}(x_1,x_2)
\end{matrix}\right),
\end{align*}
\]</span> <!-- where the Jacobian determinant of $g^{-1}$ is given by (derivation is a little cumbersome)
$$
\left|J_{g^{-1}}(r,q)\right| = r.
$$ --></p>
</div>
</div>
<p><span class="math display">\[
\begin{align*}
J_{g^{-1}}(x_1,x_2)
&amp;=\left(J_{g}(r,q)\right)^{-1}\\
&amp;=\left(\begin{matrix}
\frac{\partial g_1}{\partial r}(r,q) &amp; \frac{\partial g_1}{\partial q}(r,q)\\
\frac{\partial g_2}{\partial r}(r,q) &amp; \frac{\partial g_2}{\partial q}(r,q)\\
\end{matrix}\right)^{-1}\\
&amp;=\left(\begin{matrix}
\cos(q) &amp; -r\sin(q)\\
\sin(q) &amp; \phantom{-}r\cos(q)\\
\end{matrix}\right)^{-1}\\
&amp;=
\frac{1}{r\cos^2(q) + r\sin^2(q)}
\left(\begin{matrix}
r\cos(q) &amp; r\sin(q)\\
-\sin(q) &amp;  \cos(q)\\
\end{matrix}\right)\\
&amp;=
\frac{1}{r}
\left(\begin{matrix}
r\cos(q) &amp; r\sin(q)\\
-\sin(q) &amp;  \cos(q)\\
\end{matrix}\right)
\end{align*},
\]</span> where the last step follows from Pythagorean’s identity <span class="math inline">\(\cos^2(q) + \sin^2(q)=1.\)</span> So <span class="math display">\[
\begin{align*}
\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|
&amp;=\left|\operatorname{det}\left(\left(J_{g}(r,q)\right)^{-1}\right)\right|\\
&amp;=
\left|\operatorname{det}\left(
  \begin{matrix}
            \cos(q) &amp;            \sin(q)\\
-\frac{1}{r}\sin(q) &amp; \frac{1}{r}\cos(q)\\
\end{matrix}\right)
\right|\\
&amp;=
\left|\frac{1}{r}\cos^2(q) + \frac{1}{r}\sin^2(q)\right| = \frac{1}{r},
\end{align*}
\]</span> again using Pythagorean’s identity <span class="math inline">\(\cos^2(x_2) + \sin^2(x_2)=1\)</span> and using that <span class="math inline">\(r\in[0,\infty).\)</span></p>
<p>Thus, by the transformation formula for bivariate densities (<a href="#eq-TransformFormula">Equation&nbsp;<span>1.4</span></a>), we have <span class="math display">\[
\begin{align*}
f_{X_1X_2}(x_1,x_2)
&amp;=f_{RQ}\left(g^{-1}(x_1,x_2)\right)\left|\operatorname{det}\left(J_{g^{-1}}(x_1,x_2)\right)\right|\\
&amp;=f_{RQ}\Big(\underbrace{\sqrt{x_1^2+x_2^2}}_{=r},\underbrace{\operatorname{atan2}(x_1,x_2)}_{=q}\Big)\frac{1}{r}\\
&amp;=\exp\left(-\frac{x_1^2+x_2^2}{2}\right) \sqrt{x_1^2+x_2^2} \cdot \frac{1}{2\pi} \frac{1}{r}\\
&amp;=\exp\left(-\frac{x_1^2+x_2^2}{2}\right)  \frac{1}{2\pi},
\end{align*}
\]</span> where the last step uses that <span class="math inline">\(r=\sqrt{x_1^2+x_2^2}.\)</span></p>
<p>This shows the result of <a href="#thm-NormalVariableGeneration">Theorem&nbsp;<span>1.2</span></a>, since <span class="math display">\[
f_{X_1X_2}(x_1,x_2) = \frac{1}{2\pi}\exp\left(-\frac{x_1^2+x_2^2}{2}\right)
\]</span> is <a href="https://online.stat.psu.edu/stat505/lesson/4/4.2">known to be</a> the bivariate standard normal density for two uncorrelated (thus independent) standard normal random variables with marginal distributions <span class="math inline">\(X_1\sim\mathcal{N}(0,1)\)</span> and <span class="math inline">\(X_2\sim\mathcal{N}(0,1).\)</span></p>
<p><strong>Implementation</strong> of the Box-Muller algorithm:</p>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-9_c52b6f599ca7ef9dc596ebeb338e4bb8">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Implementation:</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>BM_Algo <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Generate U_1, U_2 iid U[0,1]</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">2</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Transformation</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  X1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">log</span>(U[<span class="dv">1</span>])) <span class="sc">*</span> <span class="fu">cos</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> U[<span class="dv">2</span>])</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  X2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">log</span>(U[<span class="dv">1</span>])) <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> U[<span class="dv">2</span>])</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Return result</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(X1, X2))</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate n standard normal random variables:</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>X_vec <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, n, <span class="at">by=</span><span class="dv">2</span>)){</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>  X_vec[<span class="fu">c</span>(i, i<span class="sc">+</span><span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fu">BM_Algo</span>()</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Descriptive Plots</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(X_vec, <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(dnorm, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.3</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(X_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing for Normality using the Shapiro-Wilk Test </span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># H0: Normality</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(X_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Shapiro-Wilk normality test

data:  X_vec
W = 0.99882, p-value = 0.7667</code></pre>
</div>
</div>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-10_1e525beb0a6dc15dc682b810ed933d9a">

</div>
<p><br>
</p>
</section>
<section id="accept-reject-methods" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="accept-reject-methods"><span class="header-section-number">1.3.3</span> Accept-Reject Methods</h3>
<p>For many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the distribution function <span class="math inline">\(F\)</span> is somehow unusable. For instance, surprisingly often there is no explicit form of <span class="math inline">\(F\)</span> available or its inverse does not exists.</p>
<p>Accept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density <span class="math inline">\(f\)</span> of interest—actually, <span class="math inline">\(f\)</span> needs to be known only up to a multiplicative constant. No deep analytic study of <span class="math inline">\(f\)</span> is necessary.</p>
<div id="thm-FundamentalThmSimulation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.3 (Fundamental Theorem of Simulation) </strong></span>Let <span class="math inline">\(X\in\mathbb{R}^d\)</span> be a random variable with density function <span class="math inline">\(f_X.\)</span> Then simulating <span class="math inline">\(X\)</span> is equivalent to simulating from a<br>
<span class="math display">\[
(X,U)\sim\mathcal{U}(\mathcal{A}),
\]</span> where <span class="math inline">\(\mathcal{U}(\mathcal{A})\)</span> denotes the uniform distribution over the area <span class="math display">\[
\mathcal{A}=\left\{(x,u)\text{ such that } x\in\mathbb{R}^d, 0&lt;u&lt;f_X(x)\right\}.
\]</span></p>
</div>
<p><a href="#fig-FundThmSim">Figure&nbsp;<span>1.2</span></a> visualizes the statement of <a href="#thm-FundamentalThmSimulation">Theorem&nbsp;<span>1.3</span></a>.</p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/fig-FundThmSim_0b604e52954a1f6c1ecf94b14506298e">
<div class="cell-output-display">
<div id="fig-FundThmSim" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/fig-FundThmSim-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.2: To simulate a random variable <span class="math inline">\(X,\)</span> one can simulate <span class="math inline">\((X,U)\)</span> uniformely over <span class="math inline">\(\mathcal{A}\)</span> and take those <span class="math inline">\(X\)</span>’s as simulation results (see tick-marks at <span class="math inline">\(x\)</span>-axis).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It turns out that sampling <span class="math inline">\((X, U)\)</span> uniformly over the set <span class="math inline">\(\mathcal{A}\)</span> is often challenging. However, one can consider some superset <span class="math inline">\(\mathcal{S},\)</span> <span class="math display">\[
\mathcal{A}\subseteq \mathcal{S},
\]</span> such that simulating a random variable uniformly distributed over <span class="math inline">\(\mathcal{S}\)</span> is easy.</p>
<p>A uniform distribution on <span class="math inline">\(\mathcal{A}\)</span> can then be obtained by drawing from a uniform distribution on <span class="math inline">\(\mathcal{S},\)</span> and rejecting samples that are in <span class="math inline">\(\mathcal{S},\)</span> but not in <span class="math inline">\(\mathcal{A}.\)</span></p>
<section id="the-case-of-densities-with-compact-support" class="level4" data-number="1.3.3.1">
<h4 data-number="1.3.3.1" class="anchored" data-anchor-id="the-case-of-densities-with-compact-support"><span class="header-section-number">1.3.3.1</span> The case of densities with compact support</h4>
<p>The general principle of the accept-reject method is easily explained using a <em>bounded</em> density function <span class="math inline">\(f\)</span> with <em>compact support</em>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Bounded</strong> means that there exists a constant <span class="math inline">\(m\)</span> with <span class="math inline">\(0&lt;m&lt;\infty\)</span> such that <span class="math display">\[
\sup_xf(x)\leq m
\]</span> Note that only degenerated density functions are not bounded.</li>
<li>An interval <span class="math inline">\([a,b]\)</span> is called <strong>compact</strong> if it is closed and the boundaries are finite. For instance, the Gaussian density <span class="math inline">\(\phi\)</span> has <em>not</em> a compact support, but <span class="math inline">\(\mathrm{supp}(\phi)=(-\infty,\infty)\)</span>.</li>
</ul>
<p>Example: <span class="math display">\[
f(x)=\frac{3}{4}\left(1-\left(x-1\right)^2\right)\,1_{(|x-1|\leq 1)},
\]</span> where the (compact) support of <span class="math inline">\(f\)</span> is <span class="math inline">\([a,b]=[-1,1]\)</span> and its range is <span class="math inline">\([0,m]=[0,3/4]\)</span>, i.e., <span class="math inline">\(f\)</span> is bounded from above by <span class="math inline">\(3/4\)</span>.<br>
<!-- (Yes, it's a stupid example as we do not necessarily need the Accept-Reject Method here.) --></p>
</div>
</div>
<p>To simulate <span class="math display">\[
X\sim f_X
\]</span> with a bounded and compactly supported density function <span class="math inline">\(f_X,\)</span> simulate the random pair <span class="math display">\[
(Y,U)\sim\mathcal{U}([a,b]\times[0,m])
\]</span> by simulating<br>
<!-- $$Y\sim \mathrm{Unif}[a,b]\quad\text{and}\quad U\sim \mathrm{Unif}[0,m],$$ --> <span class="math display">\[
Y\sim\mathcal{U}[a,b]\quad\text{and}\quad U|Y=y \sim \mathcal{U}[0,m].
\]</span> <!-- but with **accepting** the pair $(Y,U)$ only if $U\leq f_X(Y)$ and **rejecting** all others.  --> Then <strong>accept</strong> a simulated <span class="math inline">\(Y\)</span> as a simulation for <span class="math inline">\(X\)</span>, i.e.&nbsp; <span class="math display">\[
X=Y,
\]</span> <strong>only if</strong> <span class="math display">\[
U\leq f_X(Y),
\]</span> and <strong>reject</strong> all other <span class="math inline">\(Y\)</span>’s.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Simulating <span class="math display">\[
U|Y=y \sim \mathcal{U}[0,m]
\]</span> is particularly simple, since the distribution of <span class="math inline">\(U|Y=y\)</span> is here equal to the distribution of the unconditional random variable <span class="math display">\[
U|Y=y \sim U\sim \mathcal{U}[0,m]
\]</span> for any possible realization <span class="math inline">\(Y=y.\)</span></p>
</div>
</div>
<p><strong>The Accept-Reject Algorithm (for compact densities):</strong></p>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-12_601f246536999ca82b0e9548eaeeb99b">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Accept-Reject Algorithm:</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> a, <span class="at">max =</span> b) </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> m) </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># A-R Step:</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>accept <span class="ot">&lt;-</span> U <span class="sc">&lt;=</span> <span class="fu">f</span>(Y)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>X      <span class="ot">&lt;-</span> Y[accept]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The following derivation shows that the simulated random variable <span class="math inline">\(X\)</span> has the correct distribution <span class="math inline">\(F_X(x)=\int_a^xf_X(x)dx.\)</span> <span class="math display">\[
\begin{align*}
\mathbb{P}(X\leq x)
&amp;=\mathbb{P}(Y\leq x|U\leq f_X(Y))\\[2ex]
&amp;= \frac{\mathbb{P}(Y\leq x, U\leq f_X(Y))}{\mathbb{P}(U\leq f_X(Y))}\\[2ex]
&amp;= \frac{\mathbb{P}(a\leq Y\leq x, \; 0\leq U\leq f_X(Y))}{\mathbb{P}(0\leq U\leq f_X(Y))}\\[2ex]
&amp; =\frac{\int_a^{x} \int_0^{f_X(y)}\,c\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,c\;du\,dy},
\end{align*}
\]</span> where the constant <span class="math inline">\(c\)</span> is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation <span class="math display">\[
\int_a^{b} \int_0^{m}\,c\,du\,dy = 1,
\]</span> but which is irrelevant here since <span class="math display">\[
\begin{align*}
\mathbb{P}(X\leq x)
&amp; =\frac{c\;\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{c\;\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}\\[2ex]
&amp; =\frac{\int_a^{x} \int_0^{f_X(y)}\,1\;du\,dy}{\int_a^{b}\int_0^{f_X(y)}\,1\;du\,dy}.\\
\end{align*}
\]</span> Now, using that <span class="math inline">\(\int_{0}^{f_X(y)}1du=\big[x\big]^{f_X(y)}_0=f_X(y)\)</span> yields <span class="math display">\[
\begin{align*}
\mathbb{P}(X\leq x)
&amp; =\frac{\int_a^x f_X(y)\,dy}{\int_a^b f_X(y)\,dy}\\[2ex]
&amp; =\int_a^x f_X(y)dy = F_X(x).
\end{align*}
\]</span></p>
<p>In the following you see a graphical illustration of this procedure:</p>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-13_b2cc18373c0b13664aa4aad5a2474879">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-14_1b182821e01dc2f6d52d2d82a24d27bb">

</div>
<p>The good thing is that we only need to evaluate the density function <span class="math inline">\(f_X,\)</span> nothing more.</p>
</section>
<section id="the-case-of-densities-with-non-compact-support" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-case-of-densities-with-non-compact-support">The case of densities with non-compact support</h4>
<p>The larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set”, enclosing the pdf <span class="math inline">\(f\)</span>, as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of <span class="math inline">\(f\)</span> is unbounded.</p>
<p>Let the larger set denote by <span class="math display">\[
\mathscr{L}=\{(y,u):\, 0&lt;u&lt;m(y)\},
\]</span> where:</p>
<ul>
<li><p>simulation of a uniform on <span class="math inline">\(\mathscr{L}\)</span> is <strong>feasible</strong> and<br>
</p></li>
<li><p><span class="math inline">\(m(x)\geq f(x)\)</span> for all <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p><br>
</p>
<p>From the <strong>feasibility-requirement</strong> it follows that <span class="math inline">\(m(.)\)</span> is necessarily integrable, i.e., that <span class="math display">\[\int_{\mathcal{X}}m(x)dx=M,\]</span> where <span class="math inline">\(M\)</span> exists and is finite (and positive), since otherwise, <span class="math inline">\(\mathscr{L}\)</span> would not have finite mass and a uniform distribution would not exists on <span class="math inline">\(\mathscr{L}\)</span>.</p>
<p><br>
</p>
<p>Integrability of <span class="math inline">\(m(.)\)</span> is crucial here, since it allows us to relate <span class="math inline">\(m(.)\)</span> with a corresponding (auxiliary) pdf <span class="math inline">\(g(.)\)</span> as following: <span class="math display">\[m(x)=M\,g(x),\quad\text{where}\quad\int_{\mathcal{X}}m(x)\,dx=\int_{\mathcal{X}}M\,g(x)\,dx=M.\]</span></p>
<p>Terminology:</p>
<ul>
<li>The pdf <span class="math inline">\(g(.)\)</span> is called the <strong>instrumental density</strong>. (Choose <span class="math inline">\(g(.)\)</span> as a pdf from which it is easy to simulate!)</li>
<li>The pdf <span class="math inline">\(f(.)\)</span> is called the <strong>target density</strong>.</li>
</ul>
<p><br>
</p>
<p>In order to simulate the pair <span class="math inline">\((Y,U)\sim\mathrm{Unif}(\mathscr{L})\)</span> we can now simulate <span class="math display">\[Y\sim g\quad\text{and}\quad U|Y={\color{red}y}\sim\mathrm{Unif}[0,M\,g({\color{red}y})],\]</span> but <strong>accept</strong> the pair <span class="math inline">\((Y,U)\)</span> only if <span class="math inline">\(U\leq f(Y)\)</span> and to <strong>reject</strong> all others.</p>
<p>This results in the correct distribution of the accepted value of <span class="math inline">\(Y\)</span>, call it <span class="math inline">\(X\)</span>, because <span class="math display">\[
\mathbb{P}(X\in A)=\mathbb{P}(Y\in A|U\leq f(Y))
=\frac{\int_{\color{red}A}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}{\int_\mathcal{X}\int_0^{f(y)}\,\frac{1}{M}\,du\,dy}
=\frac{\int_A f(y)\,dy}{\int_\mathcal{X} f(y)\,dy}
=\int_A f(y)dy,
\]</span> for every set <span class="math inline">\(A\)</span>, <!-- measurable set $A$, i.e., is a member of the corresponding sigma algebra. --> where we again used that <span class="math inline">\(f(y)=\int_{0}^{f(y)}du\)</span>.</p>
<p>Note that the above derivation implies that we only need to know the pdf <span class="math inline">\(f(.)\)</span> <strong>up to an unkown multiplicative constant</strong> <span class="math inline">\(c&gt;0\)</span>. I.e., it is enough to know <span class="math inline">\(f(x)=c\,\tilde{f}_{\textrm{true}}(x)\)</span>, often written as <span class="math inline">\(f(x)\propto \tilde{f}_{\textrm{true}}(x)\)</span>, since the unknown constant <span class="math inline">\(c\)</span> cancels out in the above quotient anyways. This is not so much of importance for us, but useful in <strong>Bayesian Statistics</strong>.</p>
<p><br>
</p>
<p>All this leads to a more general version of the Fundamental Theorem of Simulation:</p>
<dl>
<dt><em>Fundamental Theorem of Simulation (General Version):</em></dt>
<dd>
<p>Let <span class="math inline">\(X\sim f\)</span> and let <span class="math inline">\(g(.)\)</span> be a pdf s.t. <span class="math inline">\(f(x)\leq M\,g(x)\)</span> for some <span class="math inline">\(M\)</span> with <span class="math inline">\(1\leq M&lt;\infty\)</span> and all <span class="math inline">\(x\)</span>. Then to simulate <span class="math inline">\(X\sim f\)</span> it is sufficient to generate <span class="math display">\[Y\sim g\quad\text{and}\quad U|Y=y\sim\mathrm{Unif}[0,M\,g(y)]\]</span> if one <strong>accepts</strong> the pair <span class="math inline">\((Y,U)\)</span> only if <span class="math inline">\(U\leq f(Y)\)</span> and <strong>rejects</strong> all others.</p>
</dd>
</dl>
<p><br>
</p>
<p>The Accept-Reject Algorithm (General Version):</p>
<pre><code># Accept-Reject Algorithm:
Y   &lt;- generate n random numbers from g(.)

# Specify function m():
m &lt;- function(y){YOUR CODE}

U   &lt;- numeric(n)
for(i in 1:n){
  U[i] &lt;- runif(n=1, min = 0, max = m(Y[i])) 
}

# A-R Step:
accept &lt;- U &lt;= f(Y)
X      &lt;- Y[accept]</code></pre>
<p><br>
</p>
<p><strong>Example</strong></p>
<p>Let the target “density” be <span class="math display">\[f(x)\propto \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)\]</span> with upper bound (or, rather, dominating density) the standard normal density <span class="math display">\[g(x)=\exp(-x^2/2)/\sqrt{2\pi},\]</span> which is obviously straightforward to generate.</p>
<p>In this example we can set <span class="math inline">\(m(x)=M\,g(x)\)</span> with <span class="math inline">\(M=1\)</span>, since we can simply scale the target “density” <span class="math inline">\(f\)</span> such that <span class="math inline">\(f(x)\leq g(x)\)</span> for all <span class="math inline">\(x\)</span>. Specifically, we set <span class="math inline">\(f(x)=0.075 \cdot \exp(-x^2/2)\,(\sin(6x)^2 + 3\cos(x)^2\,\sin(4x)^2 + 1)\)</span>.</p>
<p>In the following you see the graphical illustration of this example:</p>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-15_125f15979c6b31c34e61e657d2dffc51">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-16_5bed6dd3b951b654b9193528441a7851">

</div>
<p><strong>Efficiency of the Accept-Reject algorithm:</strong></p>
<p>Statements with respect to the efficiency of the Accept-Reject algorithm can be made if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are <strong>normalized</strong> such that they are <strong>both pdfs</strong>. Then:</p>
<ul>
<li>The constant <span class="math inline">\(M\)</span> is necessarily larger than <span class="math inline">\(1\)</span>.</li>
<li>The probability of acceptance is <span class="math inline">\(1/M\)</span>. (See Exercises.)</li>
<li><span class="math inline">\(M\)</span> is interpreted as the efficiency of the Accept-Reject algorithm. (The closer <span class="math inline">\(M\)</span> is to <span class="math inline">\(1\)</span> the better.)</li>
<li><span class="math inline">\(M\)</span> is a function of how closely <span class="math inline">\(g\)</span> can imitate <span class="math inline">\(f\)</span>.</li>
</ul>
<p>Note that, for such normalized <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> the inequality <span class="math inline">\(f(x)\leq M\,g(x)\)</span> with <span class="math inline">\(1\leq M&lt;\infty\)</span> for all <span class="math inline">\(x\)</span> is equivalent to saying that the quotient <span class="math inline">\(f/g\)</span> is bounded, i.e., that <span class="math display">\[
0\leq \frac{f(x)}{g(x)}\leq M &lt;\infty\quad\text{for all}\quad x.
\]</span> That is, it is necessary for <span class="math inline">\(g\)</span> to have, e.g., thicker tails than <span class="math inline">\(f\)</span>. This makes it, for instance, impossible to simulate a Cauchy distribution <span class="math inline">\(f\)</span> using a normal distribution <span class="math inline">\(g\)</span>. The reverse, however, works quite well. <!--
Remember: Using the Cauchy as instrumental pdf $g$ does not harm the integrability/feasibility requirement! The fact that the Cauchy distribution has no mean, does not change the fact that the density of the Cauchy integrates to one!
--></p>
<p><br>
</p>
<p><strong>Example: Normals from Double Exponentials</strong></p>
<p>Consider generating a <span class="math inline">\(N(0,1)\)</span> by the Accept-Reject algorithm using a double-exponential distribution <span class="math inline">\(\mathcal{L}(\alpha)\)</span>, also called <a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace distribution</a>, with density <span class="math inline">\(g(x|b)=(1/(2b))\exp(-\,|x|/b)\)</span>. <!-- here: location-param mu=0, scale.param=b\geq 0, precision=alpha=b^{-1} --> It is then straightforward to show that <span class="math display">\[
\frac{f(x)}{g(x|b)}
%=\frac{\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right)}{\frac{1}{2b}\exp\left(-\frac{|x|}{b}\right)}
%=\sqrt{\frac{2}{\pi}}\,b\,\exp\left(-\frac{1}{2}x^2+\frac{|x|}{b}\right)
\leq\sqrt{\frac{2}{\pi}}\,b\,\exp\left(\frac{1}{2\,b^2}\right)
\]</span> and that the minimum of the bound (in <span class="math inline">\(b\)</span>) is attained for <span class="math inline">\(b=1\)</span>.</p>
<p>This leads to the following optimal (i.e.&nbsp;most efficient) specification of the double-exponential distribution as instrumental pdf: <span class="math display">\[
\frac{f(x)}{g(x|1)}
\leq M=\sqrt{\frac{2}{\pi}}\,\exp\left(\frac{1}{2}\right).
\]</span></p>
<p>The probability of acceptance is then <span class="math inline">\(\sqrt{\pi/(2e)}=0.76\)</span>. I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average <span class="math inline">\(1/0.76\approx 1.3\)</span> uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1.</p>
<p><br>
</p>
</section>
</section>
</section>
<section id="monte-carlo-method" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Monte Carlo Method</h1>
<p>Monte Carlo methods take advantage of the availability of:</p>
<ol type="i">
<li>computer generated RVs,</li>
<li>the Laws of Large Numbers, and</li>
<li>the Central Limit Theorems.</li>
</ol>
<p><strong>Notions:</strong></p>
<ul>
<li><strong>Monte Carlo Method</strong>: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.</li>
<li><strong>Monte Carlo Integration</strong>: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a <strong>univariate and multivariate integral</strong>. (Integrals are everywhere in statistics!)</li>
<li><strong>Stochastic Simulation</strong> (or <strong>Monte Carlo Simulation</strong>): The application of the Monte Carlo method.</li>
</ul>
<p><br>
</p>
<section id="classical-monte-carlo-integration" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="classical-monte-carlo-integration"><span class="header-section-number">2.1</span> Classical Monte Carlo Integration</h2>
<p>The generic problem here is the evaluation of integrals. (Be aware: Integrals are everywhere in statistics!). For instance, <span class="math display">\[
\mathbb{E}_f\left(h(X)\right)=\mathbb{E}\left(h(X)\right)=\int_\mathcal{X}h(x)\,f(x)\,dx.
\]</span></p>
<p><strong>Convergence:</strong></p>
<p>Given our previous developments, it is natural to propose using a realization <span class="math inline">\(x_1,\dots,x_m\)</span> from a (pseudo random) i.i.d. sample <span class="math inline">\(X_1,\dots,X_m\)</span> with each <span class="math inline">\(X_j\)</span> distributed as <span class="math inline">\(X\sim f\)</span> to approximate the above integral by the empirical mean <span class="math display">\[
\bar{h}_m=\frac{1}{m}\sum_{j=1}^m h(x_j).
\]</span> By the <a href="http://www.statlect.com/asylln1.htm">Strong Law of Large Numbers</a> we know that the empirical mean <span class="math inline">\(\bar{h}_m\)</span> converges almost surely (a.s.) to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span> as <span class="math inline">\(m\to\infty\)</span>. (The only prerequisits are that <span class="math inline">\(f\)</span> has finite first moments, i.e., <span class="math inline">\(\mathbb{E}\left(h(X)\right)&lt;\infty\)</span>, and that <span class="math inline">\(\bar{h}_m\)</span> is constructed from an i.i.d. sample <span class="math inline">\(X_1,\dots,X_m\)</span>.)</p>
<p>As we can use the computer to produce realizations from the i.i.d. sample <span class="math inline">\(X_1,\dots,X_m\)</span>, we can in principle choose an arbitrary <strong>large sample</strong> size <span class="math inline">\(m\)</span> such that <span class="math inline">\(\bar{h}_m\)</span> can (in principle) be <strong>arbitrarily close</strong> to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span>.</p>
<p><strong>Though,</strong> …</p>
<ul>
<li>… which sample size <span class="math inline">\(m\)</span> is large enough?</li>
<li>Or “equivalently”: How fast converges <span class="math inline">\(\bar{h}_m\)</span> to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span>?</li>
</ul>
<p><br>
</p>
<p><strong>Speed of Convergence:</strong></p>
<p>OK, we know now that <span class="math inline">\(\bar{h}_m\)</span> reaches its limit (here in the “almost surely” sense) as <span class="math inline">\(m\to\infty\)</span> under some rather loose conditions on the random sample <span class="math inline">\(X_1,\dots,X_m\)</span>.</p>
<p>If we are willing to additionally assume that <span class="math inline">\(f\)</span> has finite second moments, i.e., <span class="math inline">\(\mathbb{E}(h(X)^2)&lt;\infty\)</span>, we can additionally say something about <strong>how fast</strong> <span class="math inline">\(\bar{h}_m\)</span> converges (a.s.) to <span class="math inline">\(\mathbb{E}(h(X))\)</span>.</p>
<p>The <strong>speed of convergence</strong> of the stochastic sequence <span class="math inline">\(\{\bar{h}_m\}\)</span> (i.e., now we think of <span class="math inline">\(\bar{h}_m\)</span> as the {RV} <span class="math inline">\(\bar{h}_m=\frac{1}{m}\sum_{j=1}^m h({\color{red}{X_{j}}})\)</span>) to its limit <span class="math inline">\(\mathbb{E}(h(X))\)</span> can be assessed by answering the question how fast the standard deviation (which is a function of <span class="math inline">\(m\)</span>) of the stochastic sequence converges to zero as <span class="math inline">\(m\to\infty\)</span>.</p>
<ul>
<li><p>The variance of <span class="math inline">\(\bar{h}_m\)</span> is given by <span class="math display">\[
\mathbb{V}_f\left(\bar{h}_m\right)=
\mathbb{V}\left(\frac{1}{m}\sum_{j=1}^m h(X_j)\right)=
\frac{1}{m}\mathbb{V}\left(h(X)\right)
\]</span></p></li>
<li><p>Note that assuming finite second moments <span class="math inline">\(\mathbb{E}(h(X)^2)&lt;\infty\)</span> is equivalent to assuming finite variance <span class="math inline">\(\mathbb{V}\left(h(X)\right)&lt;\infty\)</span>. Consequently, we can set <span class="math inline">\(\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)}\)</span> with <span class="math inline">\(0&lt;\mathtt{const}&lt;\infty\)</span> such that <span class="math display">\[
\sqrt{\mathbb{V}\left(\bar{h}_m\right)}=m^{-1/2}\mathtt{const}\propto m^{-1/2}.
\]</span></p></li>
</ul>
<p>I.e., the speed of convergence (or rate) of the stochastic sequence <span class="math inline">\(\{\bar{h}_m\}\)</span> is proportional to the deterministic sequence <span class="math inline">\(\{m^{-1/2}\}\)</span>.</p>
<p><br>
</p>
<p><strong>Remark:</strong> Even if we would not know the value of <span class="math inline">\(\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)}\)</span>, we know now that the improvement from <span class="math inline">\(m=10\)</span> to <span class="math inline">\(m=100\)</span> will be <em>much</em> higher than from <span class="math inline">\(m=110\)</span> to <span class="math inline">\(m=200\)</span>. In practice, a typical choice is <span class="math inline">\(m=10000\)</span>; for moderate standard deviations this choice will guarantee a very good approximation.</p>
<p><br>
</p>
<p><strong>Limit Distribution:</strong></p>
<p>Of course, we can estimate the variance of the estimator <span class="math inline">\(\mathbb{V}\left(\bar{h}_m\right)\)</span> by its empirical version <span class="math display">\[
v_m=\frac{1}{m}\left(\frac{1}{m}\sum_{j=1}^m\left(h(x_j)-\bar{h}_m\right)^2\right),
\]</span> where again by the <a href="http://www.statlect.com/asylln1.htm">Strong Law of Large Numbers (SLLN)</a> <span class="math display">\[
\left(\frac{1}{m}\sum_{j=1}^m\left(h(x_j)-\bar{h}_m\right)^2\right)\to_{\text{a.s.}}\mathbb{V}\left(h(X)\right).
\]</span> <!--
I.e., after some rewriting, we have that: 
$$
\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{v_m}}\to_{\text{a.s.}}
\sqrt{m}\left(\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right).
$$
--> By the <a href="http://www.statlect.com/central_limit_theorem.htm">Central Limit Theorem (CLT)</a> we have <span class="math display">\[
\sqrt{m}\left(\frac{\bar{h}_m - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right)\to_d Z,
\]</span> where <span class="math inline">\(Z\sim N(0,1)\)</span>. Note that the the above sequence <span class="math inline">\(\{\sqrt{m}\}\)</span> <strong>just hinders</strong> the convergence of the sequence <span class="math inline">\(\bar{h}_m - \mathbb{E}\left(h(X)\right)\to_{a.s.}0\)</span> such that the quotient converges to a “stable” distribution.</p>
<p>The above result can now be used for the construction of (asymptotically valid) <strong>convergence tests</strong> and <strong>confidence intervals</strong> with respect to <span class="math inline">\(\bar{h}_m\)</span>, since for large <span class="math inline">\(m\)</span> <span class="math display">\[
\bar{h}_m\,\overset{d}{\approx} N\left(\mathbb{E}\left(h(X)\right),\frac{\mathbb{V}\left(h(X)\right)}{m}\right).
\]</span></p>
<p>And as we can use the computer to generate realizations of the i.i.d. sample <span class="math inline">\(X_1,\dots,X_m\)</span> from a generic <span class="math inline">\(X\sim f\)</span>, we can easily approximate the mean <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span> and the variance <span class="math inline">\(\mathbb{V}\left(h(X)\right)\)</span> with arbitrary accuracy as <span class="math inline">\(m\to\infty\)</span>; by the SLLN (or the WLLN).</p>
<p><br>
</p>
<p><strong>Example: A first Monte Carlo Integration</strong></p>
<p>Let’s say we want to integrate the function <span class="math inline">\(h(x)=\left(\cos(50\,x)+\sin(20\,x)\right)^2\)</span>. Although this function could be integrated analytically it is a good first test case. The left plot below shows the graph of the function <span class="math inline">\(h(.)\)</span>.</p>
<p>To approximate the integral <span class="math display">\[
\int_\mathcal{X}h(x)dx\quad\text{with}\quad\mathcal{X}=[0,1]
\]</span> we can use that <span class="math display">\[
\int_\mathcal{X}h(x)dx=\int_\mathcal{[0,1]}1\cdot h(x)dx =\mathbb{E}_{f_\text{Unif[0,1]}}(h(X)).
\]</span></p>
<p>Thus, we generate a realization <span class="math inline">\((u_1,\dots,u_n)\)</span> from the i.i.d. random sample <span class="math inline">\(U_1,\dots,U_n\sim[0,1]\)</span> and approximate <span class="math display">\[
\int_\mathcal{X}h(x)dx\approx \bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(u_i).
\]</span></p>
<p>In order to assess how good this approximation is, we need to consider the stochastic propoerties of the RV <span class="math display">\[
\frac{1}{n}\sum_{i=1}^n h(U_i).
\]</span> This is done using the above (review of) results on the limit distribution of the sample mean which allows us to construct an approximative <span class="math inline">\(95\%\)</span> confidence interval, since for large <span class="math inline">\(n\)</span> <span class="math display">\[
\left[\bar{h}_n - 1.96\frac{\mathtt{std.error}_n}{\sqrt{n}}, \bar{h}_n + 1.96\frac{\mathtt{std.error}_n}{\sqrt{n}}\right]\approx
\left[\bar{h}_n - 1.96  \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}, \bar{h}_n + 1.96  \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}\right],
\]</span> where <span class="math inline">\(\mathtt{std.error}_n^2=n^{-1}\sum_{i=1}^n(h(u_i)-\bar{h}_n)^2\)</span>.</p>
<p>The right plot below shows one realization of the stochastic sequence <span class="math inline">\(\{\bar{h}_1,\dots,\bar{h}_n\}\)</span> with <span class="math inline">\(n=10000\)</span>, where the realized value of <span class="math inline">\(\bar{h}_n\)</span> is <span class="math inline">\(0.966\)</span>. This compares favorably with the with the exact value of <span class="math inline">\(0.965\)</span>.</p>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-17_5498f4df9b8ed5b83400505c1dc9a7ad">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-18_3825b6cf40949a306fe5721ef1ca45a6">

</div>
<p><br>
</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>The approach followed in the above example can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency through numerical methods (e.g., <a href="https://en.wikipedia.org/wiki/Riemann_sum">Riemann Sum</a>, <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule">Trapezoidal Rule</a>, <a href="https://en.wikipedia.org/wiki/Simpson%27s_rule">Simpson’s Rule</a>, etc.) in dimensions 1 or 2.</li>
<li>The approach is particularly useful for approximating integrals over higher dimensional sets.</li>
</ul>
<p><br>
</p>
<p><strong>Example: Approximation of Normal Distribution Tables</strong></p>
<p>A possible way to construct normal distribution tables is to use MC simulations.</p>
<p>Generate a realization <span class="math inline">\((x_1,\dots,x_n)\)</span> from an i.i.d. standard normal random sample, e.g., using the Box-Muller algorithm.</p>
<p>The approximation of the standard normal cdf <span class="math display">\[
\Phi(t)=\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy
\]</span> by the Monte Carlo method is thus <span class="math display">\[
\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n 1_{(x_i\leq t)}.
\]</span> The corresponding RV <span class="math inline">\(\hat{\Phi}_n(t)=\frac{1}{n}\sum_{i=1}^n1_{(X_i\leq t)}\)</span> has (exact) variance <span class="math display">\[
\mathbb{V}(\hat{\Phi}_n(t))=\frac{\Phi(t)(1-\Phi(t))}{n},
\]</span> since the single RVs <span class="math inline">\(1_{(X_i\leq t)}\)</span> are independent Bernoulli with success probability <span class="math inline">\(\Phi(t)\)</span>.</p>
<p>For values of <span class="math inline">\(t\)</span> around <span class="math inline">\(t=0\)</span>, the variance is thus approximately <span class="math inline">\(1/4n\)</span>.</p>
<p>To achieve a precision of <strong>four decimals</strong> by means of a <span class="math inline">\(99.9\%\)</span> confidence interval, the approximation requires on average <span class="math inline">\(n\approx 10^8\)</span> simulations.</p>
<p>The table below gives the evolution of this approximation for several values of <span class="math inline">\(t\)</span> and shows a very accurate evaluation for <span class="math inline">\(n=10^8\)</span>.</p>
<p><br>
</p>
<p><span class="math display">\[
\begin{array}{cccccccccc}
\hline
n   &amp;t=0.0  &amp;t=0.67 &amp;t=0.84 &amp;t=1.28 &amp;t=1.65 &amp;t=2.32 &amp;t=2.58 &amp;t=3.09 &amp;t=3.72 \\
\hline
10^2 &amp;0.485  &amp;0.74   &amp;0.77   &amp;0.9    &amp;0.945  &amp;0.985  &amp;0.995  &amp;1      &amp;1      \\
10^3 &amp;0.4925 &amp;0.7455 &amp;0.801  &amp;0.902  &amp;0.9425 &amp;0.9885 &amp;0.9955 &amp;0.9985 &amp;1      \\
10^4 &amp;0.4962 &amp;0.7425 &amp;0.7941 &amp;0.9    &amp;0.9498 &amp;0.9896 &amp;0.995  &amp;0.999  &amp;0.9999 \\
10^5 &amp;0.4995 &amp;0.7489 &amp;0.7993 &amp;0.9003 &amp;0.9498 &amp;0.9898 &amp;0.995  &amp;0.9989 &amp;0.9999 \\
10^6 &amp;0.5001 &amp;0.7497 &amp;0.8    &amp;0.9002 &amp;0.9502 &amp;0.99   &amp;0.995  &amp;0.999  &amp;0.9999 \\
10^7 &amp;0.5002 &amp;0.7499 &amp;0.8    &amp;0.9001 &amp;0.9501 &amp;0.99   &amp;0.995  &amp;0.999  &amp;0.9999 \\
10^8 &amp;0.5    &amp;0.75   &amp;0.8    &amp;0.9    &amp;0.95   &amp;0.99   &amp;0.995  &amp;0.999  &amp;0.9999 \\
\end{array}
\]</span></p>
<p><br>
</p>
<p><strong>Remarks:</strong></p>
<ul>
<li>To achieve a precision of <strong>two decimals</strong> by means of a <span class="math inline">\(99.9\%\)</span> confidence interval, already <span class="math inline">\(n=10^4\)</span> leads to satisfactory results.</li>
<li>Note that <strong>greater accuracy is achieved in the tails</strong> and that more efficient simulation methods could be used (e.g., Importance Sampling).</li>
</ul>
<p><br>
</p>
</section>
<section id="importance-sampling" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">2.2</span> Importance Sampling</h2>
<p>Importance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it’s refereed to as a <strong>variance reduction</strong> technique. This variance reduction is achieved by weighting functions, so-called <strong>importance functions</strong>.</p>
<p>As in the case of Monte Carlo integration the focus lies on evaluating the integral <span class="math display">\[
\mathbb{E}_f(h(X))=\int_\mathcal{X}h(x)f(x)\,dx.
\]</span></p>
<ul>
<li><p>Though, it turns out that the above approach, i.e., sampling from <span class="math inline">\(f\)</span> is often suboptimal.</p></li>
<li><p>Observe that the value of the above integral can be represented by infinitely many alternative choices of the triplet <span class="math inline">\((\mathcal{X}, h, f)\)</span>. Therefore, the search for an optimal estimator should encompass all these possible representations.</p></li>
</ul>
<p>Let’s illustrate this with a simple example.</p>
<p><strong>Example: Cauchy Tail Probability (from <a href="http://onlinelibrary.wiley.com/book/10.1002/9780470316726">Ripley 1987</a>)</strong></p>
<p>Suppose that the quantity of interest is the probability, say <span class="math inline">\(p\)</span>, that a <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy</a> <span class="math inline">\(\mathrm{C}(0,1)\)</span> RV is larger than <span class="math inline">\(2\)</span>, i.e.: <span class="math display">\[
p=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx.
\]</span></p>
<p><strong>1. Naive Approach:</strong> If <span class="math inline">\(p\)</span> is approximated through the empirical mean <span class="math display">\[
\hat{p}_{1}=\frac{1}{m}\sum_{j=1}^m1_{(X_j&gt;2)}
\]</span> of an i.i.d. sample <span class="math inline">\(X_1,\dots,X_m\sim\mathrm{C}(0,1)\)</span>, then the variance of this estimator, a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial</a> RV scaled by <span class="math inline">\(1/m\)</span>, is <span class="math display">\[
\mathbb{V}(\hat{p}_{1})=\frac{1}{m^2}\mathbb{V}\left(\sum_{j=1}^m1_{(X_j&gt;2)}\right)=\frac{p(1-p)}{m},
\]</span> which is equal to <span class="math inline">\(0.1275/m\)</span>, since (we already know that) <span class="math inline">\(p=0.15\)</span>.</p>
<p><br>
</p>
<p><strong>2. Accounting for Symmetry (i.e., using the ‘Adjusting Screws’ <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(h\)</span>):</strong> We can achieve a <strong>more efficient estimator</strong> (i.e., an estimator with lower variance for a given same sample size <span class="math inline">\(n\)</span>) if we take into account the symmetric nature of <span class="math inline">\(\mathrm{C}(0,1)\)</span>. Obviously, our target integral can be equivalently written as <span class="math display">\[
p=\frac{1}{2}\left(\int_{-\infty}^{-2}\frac{1}{\pi(1+x^2)}\,dx + \int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx \right).
\]</span> This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean: <span class="math display">\[
\hat{p}_{2}=
\frac{1}{2}\left(\frac{1}{m}\sum_{j=1}^m1_{(X_j&lt;-2)}+ \frac{1}{m}\sum_{j=1}^m1_{(X_j&gt;2)}\right)\;=\;
\frac{1}{2m}\sum_{j=1}^m1_{(|X_i|&gt;2)}.
\]</span> The variance of this new estimator, <span class="math display">\[
\mathbb{V}(\hat{p}_{2})=\frac{1}{4m^2}\mathbb{V}\left(\sum_{j=1}^m1_{(|X_i|&gt;2)}\right)=\frac{2p(1-2p)}{4m},
\]</span> is equal to <span class="math inline">\(0.0525/m\)</span>, i.e., lower than in the naive approach.</p>
<p><br>
</p>
<p><strong>3. Using all ‘Adjusting Screws’ <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(h\)</span>, and <span class="math inline">\(f\)</span>:</strong> The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, <span class="math inline">\([2,+\infty)\)</span>, which are in some sense irrelevant for the approximation of <span class="math inline">\(p\)</span>. This motivates the following reformulation of <span class="math inline">\(p\)</span>:</p>
<p>By symmetry of <span class="math inline">\(f\)</span>: <span class="math display">\[
\frac{1}{2}=\int_{0}^2\frac{1}{\pi(1+x^2)}dx + \underbrace{\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}dx}_{=p}
\]</span> <span class="math display">\[
\Leftrightarrow \; p=\frac{1}{2}-\int_{0}^2\frac{1}{\pi(1+x^2)}dx.
\]</span> Furthermore, we can re-arrange the last integral a bit such that <span class="math display">\[
\int_{0}^2\;\left(\frac{1}{2}\cdot 2\right)\;\frac{1}{\pi(1+x^2)}\,dx =
\int_{0}^2\;\underbrace{\frac{1}{2}}_{f_{\mathrm{Unif}[0,2]}}\;\underbrace{\frac{2}{\pi(1+x^2)}}_{=h(x)}\,dx =
\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,2].
\]</span></p>
<p>Therefore a new alternative method for evaluating <span class="math inline">\(p\)</span> is: <span class="math display">\[
\hat{p}_{3}=\frac{1}{2} - \frac{1}{m}\sum_{j=1}^m h(U_j),\quad\text{where}\quad U_j\sim\mathrm{Unif}[0,2].
\]</span> Using integration by parts, it can be shown that <span class="math inline">\(\mathbb{V}(\hat p_3)=0.0285/m\)</span>. (Compare this to the former results: <span class="math inline">\(\mathbb{V}(\hat{p}_{2})=0.0525/m\)</span> and <span class="math inline">\(\mathbb{V}(\hat{p}_{1})=0.1275/m\)</span>.)</p>
<p><br>
</p>
<p><strong>A More General Point of View:</strong></p>
<p>The idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating <span class="math display">\[
\theta=\mathbb{E}_f(h(X))=\int h(x)f(x)dx.
\]</span></p>
<p>Some outcomes of <span class="math inline">\(X\sim f\)</span> may be more important than others in determining <span class="math inline">\(\theta\)</span> and we wish to select such values more frequently.</p>
<p>For instance, if <span class="math inline">\(\theta\)</span> denotes the probability of the occurrence of a very rare event, then the only way to estimate <span class="math inline">\(\theta\)</span> at all accurately may be to produce the rare events more frequently.</p>
<p>To achieve this, we can simulate a model which gives pdf <span class="math inline">\(g\)</span> to <span class="math inline">\(X\)</span> instead of the correct pdf <span class="math inline">\(f\)</span>, where both pdfs need to be known. This can be easily done, since <span class="math display">\[
\theta=\mathbb{E}_f(h(X))=\int h(x)\left(\frac{g(x)}{g(x)}\right)\;f(x)dx=
\int \underbrace{\left(h(x)\frac{f(x)}{g(x)}\right)}_{=\psi(x)}\;g(x)dx=
\int \psi(x)\;g(x)dx=
\mathbb{E}_g(\psi(X)).
\]</span></p>
<p>This leads to the following unbiased estimator for <span class="math inline">\(\theta\)</span> based on sampling from <span class="math inline">\(g\)</span>: <span class="math display">\[
\hat{\theta}_g=\frac{1}{n}\sum_{i=1}^n\psi(X_i)\quad\text{with}\quad X_i\sim g,
\]</span> which is a weighted mean of the <span class="math inline">\(h(X_i)\)</span> with weights inversely proportional to the “<strong>selection factor</strong>” <span class="math inline">\(\frac{g(X_i)}{f(X_i)}\)</span>. <!-- 
For appropriately chosen pdfs f and g: 
The *selection factor* refers to "how more likely is it to select a 'rare event'?".
The inverse weight re-scales these 'too often' chosen rare events. 
--></p>
<p>For the variance of the estimator <span class="math inline">\(\hat{\theta}_g\)</span> we have <span class="math display">\[
\mathbb{V}(\hat{\theta}_g)=\frac{1}{n}\mathbb{V}(\psi(X_i))=
\frac{1}{n}\int\left(\psi(x)-\theta\right)^2g(x)dx=
\frac{1}{n}\int\left(\frac{h(x)\,f(x)}{g(x)}-\theta\right)^2g(x)dx,
\]</span> which, depending on the choice of <span class="math inline">\(g(.)\)</span>, can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean. <!-- 
$$
\mathbb{V}(\hat{\theta}_{\text{naive}})=
\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n 1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=
\frac{1}{n}\mathbb{V}\left(1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=\frac{\theta(1-\theta)}{n}.
$$ 
--></p>
<p><br>
<br>
</p>
<dl>
<dt><em>Minimum Variance Theorem</em></dt>
<dd>
<p>The <strong>importance function</strong> <span class="math inline">\(g(.)\)</span> which minimizes the variance <span class="math inline">\(\mathbb{V}(\psi(X_i))\)</span>, and therefore the variance <span class="math inline">\(\mathbb{V}(\hat{\theta}_g)\)</span>, is given by <span class="math display">\[
g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
\]</span></p>
</dd>
</dl>
<p><strong>Proof:</strong> Done in the lecture.</p>
<p><br>
</p>
<p>Though, this result is rather formal (in the sense of “impractical”), since, e.g., if <span class="math inline">\(h(x)&gt;0\)</span> then <span class="math inline">\(g^\ast\)</span> requires us to know <span class="math inline">\(\int h(z)f(z)dz\)</span>, which is just the integral of interest!</p>
<p><strong>Remarks:</strong></p>
<p>The above minimum variance result is still useful:</p>
<ul>
<li>It tells us that a good choice of <span class="math inline">\(g(x)\)</span> shall mimic the shape of <span class="math inline">\(|h(x)|f(x)\)</span>, since the optimal <span class="math inline">\(g^\ast(x)\propto |h(x)|f(x)\)</span>.</li>
<li>Furthermore, <span class="math inline">\(g(x)\)</span> should be chosen such that it has a thicker tail than <span class="math inline">\(f(x)\)</span>, since the variance <span class="math inline">\(\mathbb{V}(\hat{\theta}_g)\)</span> crucially depends on the quotient <span class="math inline">\(f(x)/g(x)\)</span> which would “explode” for <span class="math inline">\(g(x)\approx 0\)</span>.</li>
</ul>
<p><br>
</p>
<p>Let’s apply our new insights to the above example on the Cauchy tail probability <span class="math inline">\(p\)</span>.</p>
<p><strong>Example: Cauchy Tail Probability (cont.)</strong></p>
<p>Above we had:</p>
<ol type="1">
<li><span class="math inline">\(f(x)=\frac{1}{\pi(1+x^2)}\)</span>, the pdf of <span class="math inline">\(\mathrm{C}(0,1)\)</span> and</li>
<li><span class="math inline">\(h(x)=1_{(x&gt;2)}\)</span>, i.e., here <span class="math inline">\(|h(x)|=h(x)\)</span>.</li>
</ol>
<p>Therefore <span class="math display">\[
p=\mathbb{E}_f(h(X))=\int h(x)f(x)dx=\int_{2}^{\infty}f(x)dx=\int_{2}^{\infty}\underbrace{\frac{f(x)}{g(x)}}_{=\psi(x)}\;g(x)dx=\mathbb{E}_g(\psi(X)),
\]</span> where the <span class="math inline">\(h\)</span> function is absorbed by the formulation of the definite integral.</p>
<p>A possibly good (and simple) choice of <span class="math inline">\(g\)</span> is, e.g., <span class="math inline">\(g(x)=2/(x^2)\)</span>, since this function:</p>
<ul>
<li>“closely” matches <span class="math inline">\(h(x)f(x)\)</span> and</li>
<li><span class="math inline">\(g\)</span> has thicker tails than <span class="math inline">\(f\)</span>.</li>
</ul>
<div class="cell" data-layout-align="center" data-hash="Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-19_5c5d83688073d3a1e8d3f7e75314484f">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch1_Random_Variable_Generation_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><br>
</p>
<p><strong>Caution:</strong> It is not straight forward to directly sample from <span class="math inline">\(g\)</span>, therefore we need some further steps:</p>
<p><br>
</p>
<p>The choice of <span class="math inline">\(g\)</span> leads to <span class="math display">\[
p=\mathbb{E}_g(\psi(X))=
\int_{2}^{+\infty}\left(\frac{x^2}{2\,\pi(1+x^2)}\right)\,\frac{2}{x^2}\,dx=
\int_{2}^{+\infty}\left(\frac{1}{\pi(1+x^{-2})}\right)\,x^{-2}\,dx.
\]</span></p>
<p><br>
</p>
<p>Now we can apply some additional (rather case-specific) re-arrangements:</p>
<p>Integration by substitution (substituting <span class="math inline">\(u=x^{-1}\)</span>) yields: <span class="math display">\[
p=\int_{0}^{1/2}\frac{1}{\pi(1+u^2)}du.
\]</span> Again, we can re-arrange the last integral a bit such that <span class="math display">\[
p=\int_{0}^{1/2}\underbrace{2}_{f_{\mathrm{Unif}[0,1/2]}}\;\underbrace{\frac{1}{2\,\pi(1+u^2)}}_{=h(u)}\,du=\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
\]</span> Therefore, we have a final fourth version of the estimator of <span class="math inline">\(p\)</span>: <span class="math display">\[
\hat{p}_4=\sum_{j=1}^m h(U_j),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2]
\]</span> and <span class="math inline">\(h(u)=1/(2\pi(1+u^2))\)</span>.</p>
<p>The variance of <span class="math inline">\(\hat{p}_4\)</span> is <span class="math inline">\((\mathbb{E}(h(U)^2)-\mathbb{E}(h(U))^2)/m\)</span> and an integration by parts shows that <span class="math inline">\(\mathbb{V}(\hat{p}_4)=0.95\cdot 10^{-4}/m\)</span>. Compare this to the former results: <span class="math inline">\(\mathbb{V}(\hat p_3)=0.0285/m\)</span>, <span class="math inline">\(\mathbb{V}(\hat{p}_{2})=0.0525/m\)</span> and <span class="math inline">\(\mathbb{V}(\hat{p}_{1})=0.1275/m\)</span>. The variance of <span class="math inline">\(\hat{p}_4\)</span> is by a factor of <span class="math inline">\(10^{-3}\)</span> lower than the variance of the original <span class="math inline">\(\hat{p}_1\)</span>.</p>
<!--
**Version 2:** It can be shown that (see [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))
$$
p=\int_0^{1/2}\frac{y^{-2}}{\pi(1+y^{-2})}dy,
$$
where this integral can also be seen as the expectation of 
$$
\frac{1}{4}h(U)=\frac{1}{2\pi(1+U^2)},\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
-->

<!--
## The 'Real Need' of Monte Carlo Simulation 

**Monte Carlo Simulation applied to Hypothesis Tests**

* **Problem:** Very often, statistical test procedures (or estimators) rely on asymptotic arguments. Asymptotic arguments ease the live of a statistician. In practice, however, we never have something like a diverging sample size of $n\to\infty$, but need to deal with a finite sample size $n$. All we can hope for is that the asymptotic results (e.g., on the level of significance of a test statistic and its power) are good approximations to the finite $n$ case. 

* **Solution:** Monte Carlo simulation the classical tool to investigate the finite $n$ performance of statistical test procedures (or estimators). It helps to answer questions like: How good are the asymptotic results given finite sample size scenarios of $n=100$, $n=500$, etc.

For instance, the **likelihood ratio (LR)** test statistic  
$$
-2\,\log\left[\ell(\hat\theta|x)/\ell(\hat\theta_0|x)\right]=-2\,\left\{\log\ell(\hat\theta|x)-\log\ell(\hat\theta_0|x)\right\}\to_d\chi^2_r
$$
is distributed as $\chi^2_r$ generally only in the limiting case as $n\to\infty$; and under some regularity constraints on the likelihood function. In the formula above $\ell(\theta|x)$ denotes the likelihood function, $\hat\theta\in\mathbb{R}^k$ is the estimated (via maximum likelihood) parameter vector from the unconstrained model and $\hat\theta_0\in\mathbb{R}^k$ is the estimated parameter vector from the constrained model with $r\leq k$ restrictions.

\

**Example: Contingency Tables**

The following table gives the results of a study comparing radiation therapy with surgery in treating cancer of the larynx. 


$$
\begin{array}{c|cc|c}
                   &  \text{Cancer}  &  \text{Cancer not}&           \\
                   &  \text{Controlled}  &  \text{Controlled}&           \\
\hline                   
\text{Surgery}     &  y_{11}=21      &   y_{12}=2        &  n_{1.}=23\\
\text{Radiation}   &  y_{21}=15      &   y_{22}=3        &  n_{2.}=18\\
\hline
                   &  n_{.1}=36      &   n_{.2}=5        &  n=41
\end{array}
$$

\

Let's condition on a fix number of total observations $n$. Then the random vector $(Y_{11},Y_{12},Y_{21},Y_{22})^\top$ comes from a [multinomial distribiton](https://en.wikipedia.org/wiki/Multinomial_distribution) with 4 cells and cell probabilities 
$$
p=(p_{11}, p_{12}, p_{21}, p_{22}),\quad\text{with}\quad\sum_{ij}p_{ij}=1,
$$
that is, 
$$
(Y_{11},Y_{12},Y_{21},Y_{22})^\top\sim\mathcal{M}_4(n, p).
$$
With $y_{ij}$ denoting the number of realizations in cell $ij$, the likelihood function can be written as 
$$
\ell(p|y)\propto\prod_{ij}p_{ij}^{y_{ij}},
$$
where the 4-dimensional parameter space can be displayed as following:
$$
\begin{array}{cc|c}
p_{11} & p_{12} & p_{1.}\\
p_{21} & p_{22} & p_{2.}\\
\hline
p_{.1} & p_{.2} & 1\\
\end{array}
$$



**Null Hypothesis:** The null hypothesis to be tested is one of independence, which is to say that the surgery treatment has no bearing on the control of cancer. Translated into a parameter statement this means
$$\text{H}_0: p_{11}=p_{1.}\,p_{.1}\quad\text{against}\quad\text{H}_1: p_{11}\neq p_{1.}\,p_{.1}.$$


The likelihood ratio statistic for testing this hypothesis is 
$$
\lambda=\frac{\max_{p\text{ s.t. }p_{11}=p_{1.}p_{.1}}\ell(p|y)}{\max_{p}\ell(p|y)}.
$$
It is "straightforward" to show that the denominator maximum is attained at:
$$
\hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all}\quad ij. 
$$
and the numerator maximum at
$$
\hat{p}_{11}=\hat{p}_{1.}\hat{p}_{.1}\quad\text{with}\quad \hat{p}_{1.}=\frac{n_{1.}}{n}\quad\text{and}\quad\hat{p}_{2.}=\frac{n_{2.}}{n},
$$
$$
\text{and} \quad \hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all other}\quad ij. 
$$

As mentioned above, under H$_0$, $-2\log \lambda$ is asymptotically distributed as $\chi^2_1$. However, with only $n=42$ observations, the asymptotics do not necessarily apply. One alternative is to use devise a **Monte Carlo experiment** to simulate the null distribution of $-2\log \lambda$ or equivalently of $\lambda$ in order to obtain a cutoff point for a hypothesis test. A more sophisticated approach is that of [Mehta at el. (2000)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10473906).  

**Description of the Procedure:**

* Let's denote the finite $n$ null distribution of $\lambda$ by $f_{0,n}(.)$. As we are interested in an $\alpha$ level test, we need to specify $\alpha$ (e.g., $\alpha=0.05$) and to solve the following integral for the $1-\alpha$ quantile $\lambda_\alpha$:
$$
\int_0^{\lambda_\alpha}f_{0,n}(u)du=1-\alpha.
$$

* The standard Monte Carlo approach to this problem is to generate random variables $\lambda_k\sim f_{0,n}$, $k=1,\dots,m$, then order the sample 
$$
\lambda_{(1)}\leq \lambda_{(2)}\leq\dots\leq \lambda_{(m)}
$$
and finally calculate the empirical $1-\alpha$ quantile $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$, where $\lfloor x \rfloor$ is the next lower integer to $x$, e.g., $\lfloor 2.9 \rfloor=2$.

* Similarly to the above integration example which builds on the SLLN, the central idea here is to use the so-to-say SLLN for quantiles:
$$
\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}\to_{a.s.} \lambda_{\alpha}\quad\text{as}\quad m \to\infty;
$$
see, e.g., the classical book "Approximation Theorems of Mathematical Statistics" of R. Serfling Ch. 2.3.1. As a computer is doing this job for us, we can in principle choose an arbitrary large $m$ such that the above approximation of $\lambda_{\alpha}$ by $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$ can be arbitrarily good. 

To run the 


::: {.cell hash='Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-20_e510920566ce01c62e5ef2accdd98d9d'}

```{.r .cell-code}
set.seed(123)
#
p_init <- runif(4)
#
p_11   <- p_init[1]
p_12   <- p_init[2]
p_21   <- p_init[3]
p_22   <- p_init[4]
#
p_1.   <- p_11 + p_12
p_.1   <- p_11 + p_21
#
p_11   <- p_1. * p_.1

# probabilities under H0:
p_0 <- c(p_11, p_21, p_12, p_22)


Y_vec <- rmultinom(n=1, size=4, prob=p_0)
```
:::

-->
<!-- 
**Example: LR Test**

Let's assume we want to test the following regression model 
$$
Y_t=\beta_0 + \beta_1 t +\varepsilon_{t}
$$
against the constrained model without a time trend:
$$
Y_t=\beta_0 + \varepsilon_{t},
$$
where $\varepsilon_{t}\sim N(0,\sigma_\varepsilon^2)$ with $0<\varepsilon^2<\infty$.



::: {.cell hash='Ch1_Random_Variable_Generation_cache/html/unnamed-chunk-21_4c84cde5c586a52ff1cd404a9cd41466'}

```{.r .cell-code}
LL_fun <- function(beta0, beta1, sigma) {
      # residuals
      res_vec   <-  y - x * beta1 - beta0
      # log-transformed probabilities of observing the vector of residuals res_vec: 
      Log_probs <- dnorm(x = R, mean = 0, sd = sigma, log = TRUE)
      # 
      result <- - sum(Log_probs)
      return(result)
}
```
:::

-->
<p><br>
<br>
<br>
<br>
<!-- 
Alternative Header-Syntax:
  ## A level-two header
  ### A level-three header ###
--></p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-BoxMuller1958" class="csl-entry" role="doc-biblioentry">
Box, George EP, and Mervin E Muller. 1958. <span>“A Note on the Generation of Random Normal Deviates.”</span> <em>The Annals of Mathematical Statistics</em> 29 (2): 610–11. <a href="https://projecteuclid.org/euclid.aoms/1177706645">https://projecteuclid.org/euclid.aoms/1177706645</a>.
</div>
<div id="ref-RobertCasella1999" class="csl-entry" role="doc-biblioentry">
Robert, Christian P, and George Casella. 2004. <em>Monte Carlo Statistical Methods</em>. 2nd ed. Springer Texts in Statistics. Springer.
</div>
<div id="ref-RobertCasella2010" class="csl-entry" role="doc-biblioentry">
———. 2010. <em>Introducing Monte Carlo Methods with r</em>. 1st ed. Use r! Springer.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Organization of the Course</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch2_Bootstrap.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Bootstrap</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>