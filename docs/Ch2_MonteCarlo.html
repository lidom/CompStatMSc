<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computational Statistics (M.Sc.) - 2&nbsp; Monte Carlo Integration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch3_Bootstrap.html" rel="next">
<link href="./Ch1_Random_Variable_Generation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Statistics (M.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Random_Variable_Generation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_MonteCarlo.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Bootstrap.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_MaximumLikelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_EMAlgorithmus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Expectation Maximization (EM) Algorithm</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#literature" id="toc-literature" class="nav-link active" data-scroll-target="#literature">Literature</a></li>
  <li><a href="#classical-monte-carlo-integration" id="toc-classical-monte-carlo-integration" class="nav-link" data-scroll-target="#classical-monte-carlo-integration"><span class="toc-section-number">2.1</span>  Classical Monte Carlo Integration</a>
  <ul class="collapse">
  <li><a href="#speed-of-convergence" id="toc-speed-of-convergence" class="nav-link" data-scroll-target="#speed-of-convergence">Speed of Convergence</a></li>
  <li><a href="#distributional-properties" id="toc-distributional-properties" class="nav-link" data-scroll-target="#distributional-properties">Distributional Properties</a></li>
  <li><a href="#classical-monte-carlo-integration-using-proportions" id="toc-classical-monte-carlo-integration-using-proportions" class="nav-link" data-scroll-target="#classical-monte-carlo-integration-using-proportions"><span class="toc-section-number">2.1.1</span>  Classical Monte Carlo Integration using Proportions</a></li>
  </ul></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling"><span class="toc-section-number">2.2</span>  Importance Sampling</a>
  <ul class="collapse">
  <li><a href="#a-more-general-point-of-view" id="toc-a-more-general-point-of-view" class="nav-link" data-scroll-target="#a-more-general-point-of-view"><span class="toc-section-number">2.2.1</span>  A More General Point of View</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="literature" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="literature">Literature</h3>
<p>In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:</p>
<ul>
<li><a href="http://www.springer.com/us/book/9780387212395">Monte Carlo Statistical Methods</a>, Ch. 3, <span class="citation" data-cites="RobertCasella1999">Robert and Casella (<a href="#ref-RobertCasella1999" role="doc-biblioref">2004</a>)</span></li>
<li><a href="https://link.springer.com/book/10.1007/978-1-4419-1576-4">Introducing Monte Carlo Methods with R</a>, Ch. 3, <span class="citation" data-cites="RobertCasella2010">Robert and Casella (<a href="#ref-RobertCasella2010" role="doc-biblioref">2010</a>)</span></li>
<li><a href="https://mitpress.mit.edu/9780262547741/numerical-methods-in-economics/">Numerical Methods in Economics</a>, Ch. 8.2 Monte Carlo Integration, <span class="citation" data-cites="Kenneth_Judd_Book_1998">Judd (<a href="#ref-Kenneth_Judd_Book_1998" role="doc-biblioref">1998</a>)</span></li>
</ul>
<p>Monte Carlo methods take advantage of the availability of:</p>
<ol type="i">
<li>computer generated random variables</li>
<li>the law of large numbers</li>
<li>the central limit theorem</li>
</ol>
<p><strong>Terminology:</strong></p>
<ul>
<li><strong>Monte Carlo Method</strong>: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.</li>
<li><strong>Monte Carlo Integration</strong>: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a <strong>univariate and multivariate integral</strong>. (Integrals are everywhere in statistics!)</li>
<li><strong>Stochastic Simulation</strong> (or <strong>Monte Carlo Simulation</strong>): The application of the Monte Carlo method.</li>
</ul>
<p>This chapter is about <strong>Monte Carlo Integration</strong> which is a stochastic alternative to deterministic numerical integration methods such as numerical quadrature.</p>
<p><a href="#fig-MCIntPublishedExample">Figure&nbsp;<span>2.1</span></a> shows a screenshot of a published example (see <span class="citation" data-cites="Bourreau2021market">Bourreau, Sun, and Verboven (<a href="#ref-Bourreau2021market" role="doc-biblioref">2021</a>)</span>), where the authors use Monte Carlo simulation to solve a hard-to-compute integral.</p>
<div id="fig-MCIntPublishedExample" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/MCIntPublishedExample2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.1: Market share function in <span class="citation" data-cites="Bourreau2021market">Bourreau, Sun, and Verboven (<a href="#ref-Bourreau2021market" role="doc-biblioref">2021</a>)</span>, which involves a hard-to-compute integral. Here Monte Carlo integration is used to solve the integral.</figcaption><p></p>
</figure>
</div>
</section>
<section id="classical-monte-carlo-integration" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="classical-monte-carlo-integration"><span class="header-section-number">2.1</span> Classical Monte Carlo Integration</h2>
<p>The generic problem here is the evaluation of integrals. For instance, <span id="eq-MeanIntegral"><span class="math display">\[
\mathbb{E}_{f_{X}}\left(h(X)\right)=\mathbb{E}\left(h(X)\right)=\int_\mathcal{X}h(x)\,f_X(x)\,dx,
\tag{2.1}\]</span></span> where <span class="math inline">\(\mathcal{X}\)</span> denotes the domain of the random variable <span class="math inline">\(X\in\mathcal{X}\subseteq\mathbb{R}^d,\)</span> and where <span class="math inline">\(h\)</span> is some transformation function, e.g., <span class="math display">\[
h(x)=x^2,\;\;h(x)=\ln(x),\;\;h(x)=x,\;\;\text{etc.}
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Computing means means computing integrals. To stress that one computes the integral with respect to the distribution characterized by the density function <span class="math inline">\(f_X,\)</span> one can write <span class="math display">\[
\mathbb{E}_{f_{X}}\left(h(X)\right)
\]</span> instead of <span class="math display">\[
\mathbb{E}\left(h(X)\right).
\]</span> We will use this notation more often below.</p>
</div>
</div>
<p>Often, analytical solutions for integrals such as in <a href="#eq-MeanIntegral">Equation&nbsp;<span>2.1</span></a> are not readily available and one needs to use some numerical approaches/computational. Given our previous developments, it is kind of natural to propose using a realization <span class="math display">\[
x_1,\dots,x_n
\]</span> from a (pseudo) random sample <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X
\]</span> to approximate the integral in <a href="#eq-MeanIntegral">Equation&nbsp;<span>2.1</span></a> using the empirical mean <span class="math display">\[
\mathbb{E}\left(h(X)\right)\approx\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(x_i).
\]</span> By the <a href="http://www.statlect.com/asylln1.htm">Strong Law of Large Numbers (SLLN)</a> we know that the empirical mean <span class="math inline">\(\bar{h}_n\)</span> converges almost surely (a.s.), and thus also “in probabiliuty” to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span> as the sample size <span class="math inline">\(n\)</span> becomes large, i.e., as <span class="math inline">\(n\to\infty\)</span>. Prerequisites for the SLLN:</p>
<ol type="1">
<li><span class="math inline">\(h(X)\)</span> has finite first moment, i.e., <span class="math inline">\(\mathbb{E}\left(h(X)\right)&lt;\infty\)</span> and</li>
<li><span class="math inline">\(\bar{h}_n\)</span> is constructed from a random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f_X.\)</span></li>
</ol>
<p>As we can use the computer to produce realizations from the i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span>, we can in principle choose an arbitrary <strong>large sample</strong> size <span class="math inline">\(n\)</span> such that <span class="math inline">\(\bar{h}_n\)</span> can, in principle, be <strong>arbitrarily close</strong> to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span>.</p>
<p><strong>Though,</strong> …</p>
<ul>
<li>… which sample size <span class="math inline">\(n\)</span> is large enough?</li>
<li>or “equivalently”, how fast converges <span class="math inline">\(\bar{h}_n\)</span> to the desired limit <span class="math inline">\(\mathbb{E}\left(h(X)\right)\)</span>?</li>
</ul>
<section id="speed-of-convergence" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="speed-of-convergence">Speed of Convergence</h3>
<p>OK, we know now that <span class="math inline">\(\bar{h}_n\)</span> reaches its limit (here in the “almost surely” sense, but likewise in the “in probability” case) as <span class="math inline">\(n\to\infty\)</span> under some rather loose conditions on the random sample <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
<p>If we are willing to additionally assume that <span class="math inline">\(h(X)\)</span> has finite <em>second</em> moments, i.e. <span class="math display">\[
\mathbb{E}(h(X)^2)&lt;\infty,
\]</span> then we can additionally say something about <strong>how fast</strong> <span class="math display">\[
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(X_i)\to_{p} \mathbb{E}(h(X)).
\]</span></p>
<p>The <strong>speed of convergence</strong> of the stochastic sequence <span class="math display">\[
\{\bar{h}_n\}_{n=1,2,\dots}= \bar{h}_1,\;\bar{h}_2,\;\;\bar{h}_3,\;\dots
\]</span> to its limit <span class="math inline">\(\mathbb{E}(h(X))\)</span> can be quantified by the rate at which the standard error <span class="math display">\[
\operatorname{SE}\left(\bar{h}_n\right)=\sqrt{\mathbb{V}\left(\bar{h}_n\right)}
\]</span> converges to zero as <span class="math inline">\(n\to\infty\)</span>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We think of <span class="math inline">\(\{\bar{h}_n\}_{n=1,2,\dots}\)</span> as the sequence of <strong>random variables</strong><br>
<span class="math display">\[
\bar{h}_n=\frac{1}{n}\sum_{i=1}^n h({\color{red}{X_{i}}})
\]</span> with <span class="math inline">\({\color{red}{X_1}},\dots,{\color{red}{X_n}}\overset{\text{i.i.d.}}{\sim}f_X.\)</span></p>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that assuming finite second moments <span class="math inline">\(\mathbb{E}(h(X)^2)&lt;\infty\)</span> is equivalent to assuming finite variance <span class="math inline">\(\mathbb{V}\left(h(X)\right)&lt;\infty,\)</span> since <span class="math display">\[
\mathbb{V}\left(h(X)\right) = \mathbb{E}(h(X)^2) - \left(\mathbb{E}(h(X))\right)^2,
\]</span> and since if higher moments, like <span class="math inline">\(\mathbb{E}(h(X)^2),\)</span> are finite, also the lower moments, like <span class="math inline">\(\mathbb{E}(h(X)),\)</span> are finite.</p>
</div>
</div>
<p>The standard error of <span class="math inline">\(\bar{h}_n\)</span> is just the square root of the variance of <span class="math inline">\(\bar{h}_n.\)</span> The variance of <span class="math inline">\(\bar{h}_n\)</span> is given by <span class="math display">\[
\begin{align*}
\mathbb{V}\left(\bar{h}_n\right)
&amp;=\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\right) \\[2ex]
&amp;=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n h(X_i)\right) \\[2ex]
&amp;=\frac{n}{n^2}\mathbb{V}\left(h(X_1)\right)\quad \text{(since i.i.d.)} \\[2ex]
&amp;=\frac{1}{n}  \mathbb{V}\left(h(X_1)\right)
\end{align*}
\]</span> The square root of <span class="math inline">\(\mathbb{V}\left(h(X_1)\right)\)</span> equals some finite, positive constant <span class="math inline">\(0&lt;\mathtt{const}&lt;\infty\)</span>,<br>
<span class="math display">\[
\mathtt{const}=\sqrt{\mathbb{V}\left(h(X_1)\right)}
\]</span> such that <span class="math display">\[
\sqrt{\mathbb{V}\left(\bar{h}_n\right)}=n^{-1/2}\mathtt{const}%\propto n^{-1/2}.
\]</span> I.e., the speed of convergence (or rate) of the stochastic sequence <span class="math inline">\(\{\bar{h}_n\}\)</span> is proportional to the deterministic sequence <span class="math inline">\(\{n^{-1/2}\}.\)</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even if we would not know the value of <span class="math inline">\(\mathtt{const}=\sqrt{\mathbb{V}\left(h(X)\right)},\)</span> we know now that the improvement from <span class="math inline">\(n=10\)</span> to <span class="math inline">\(n=100\)</span> will be <em>much</em> higher than from <span class="math inline">\(n=110\)</span> to <span class="math inline">\(n=200\)</span>. In practice, a typical choice is <span class="math inline">\(n=10,000;\)</span> for moderate standard errors this choice will guarantee a very good approximation.</p>
</div>
</div>
</section>
<section id="distributional-properties" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="distributional-properties">Distributional Properties</h3>
<p>Besides the speed of convergence of <span class="math inline">\(\{\bar{h}_n\}_{n=1,2,\dots}\)</span> for <span class="math inline">\(n\to\infty,\)</span> we can also say something about the distribution of the random variable <span class="math inline">\(\bar{h}_n\)</span> for large sample sizes <span class="math inline">\(n.\)</span></p>
<p>We can estimate the variance of the estimator <span class="math inline">\(\mathbb{V}\left(\bar{h}_n\right)\)</span> by its empirical version <span class="math display">\[
v_n^2=\frac{1}{n}\sum_{i=1}^n\left(h(x_i)-\bar{h}_n\right)^2,
\]</span> where by the SLLN, which also implies convergence in probability, <span class="math display">\[
v_n^2\to_{p}\mathbb{V}\left(h(X)\right),\quad n\to\infty.
\]</span> <!--
I.e., after some rewriting, we have that: 
$$
\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{v_n}}\to_{\text{a.s.}}
\sqrt{m}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{\sqrt{\mathbb{V}\left(h(X)\right)}}\right).
$$
--> Then, by the <a href="https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#cmt">Continuous Mapping Theorem (CMT)</a>, the <a href="http://www.statlect.com/central_limit_theorem.htm">Central Limit Theorem (CLT)</a>, and <a href="https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html#slutsky">Slutsky’s theorem</a>, we have that <span class="math display">\[
\sqrt{n}\left(\frac{\bar{h}_n - \mathbb{E}\left(h(X)\right)}{v_n}\right)\to_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> <!-- Note that the the above sequence $\{\sqrt{m}\}$ **just hinders** the convergence of the sequence $\bar{h}_n - \mathbb{E}\left(h(X)\right)\to_{a.s.}0$ such that the quotient converges to a "stable" distribution.  --></p>
<p>The above result can now be used for the construction of (asymptotically valid) <strong>convergence tests</strong> and <strong>confidence intervals</strong> with respect to <span class="math inline">\(\bar{h}_n\)</span>, since for large <span class="math inline">\(n\)</span> <span class="math display">\[
\bar{h}_n\,\overset{d}{\approx}\mathcal{N}\left(\mathbb{E}\left(h(X)\right),\frac{\mathbb{V}\left(h(X)\right)}{n}\right).
\]</span></p>
<p>Since we can use the computer to generate realizations of the i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\)</span> with <span class="math inline">\(X\sim f_X,\)</span> we can easily approximate the mean <span class="math display">\[
\mathbb{E}\left(h(X)\right)\approx \bar{h}_n
\]</span> and the variance <span class="math display">\[
\mathbb{V}\left(h(X)\right)\approx v_n^2
\]</span> with arbitrary accuracy as <span class="math inline">\(n\to\infty\)</span> (justification: strong/weak law of large numbers).</p>
<div id="exm-MCInt1" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (A first Monte Carlo Integration) </strong></span>Let’s say we want to compute the integral <span class="math display">\[
\int_0^1h(x)dx
\]</span> with <span class="math display">\[
h(x)=\left(\cos(50\,x)+\sin(20\,x)\right)^2
\]</span> over <span class="math inline">\(x\in[0,1].\)</span> Although this integral could also be computed analytically, it is a good first test case. The following code computes the analytic result that <span class="math inline">\(\int_0^1h(x)dx = 0.9652009.\)</span></p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/unnamed-chunk-1_54804e3ecaaf05a6aa3264dfb1361bf7">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("mosaicCalc")</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"mosaicCalc"</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Symbolic (= analytic) integration </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>F <span class="ot">&lt;-</span> <span class="fu">antiD</span>( (<span class="fu">cos</span>(<span class="dv">50</span><span class="sc">*</span>x)<span class="sc">+</span><span class="fu">sin</span>(<span class="dv">20</span><span class="sc">*</span>x))<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> x)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">F</span>(<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">F</span>(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9652009</code></pre>
</div>
</div>
<p><a href="#fig-MCInt1_1">Figure&nbsp;<span>2.2</span></a> shows the graph of the function <span class="math inline">\(h\)</span>.</p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/fig-MCInt1_1_dc51bbbf89d1fefee7571c153e27e80e">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>h_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">&lt;-</span> (<span class="fu">cos</span>(<span class="dv">50</span><span class="sc">*</span>x)<span class="sc">+</span><span class="fu">sin</span>(<span class="dv">20</span><span class="sc">*</span>x))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(result)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>xx  <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="dv">0</span>, <span class="at">to=</span><span class="dv">1</span>, <span class="at">len=</span><span class="dv">500</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> xx, </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> <span class="fu">h_fun</span>(xx), </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Function h"</span>, </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="st">"h(x)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MCInt1_1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Ch2_MonteCarlo_files/figure-html/fig-MCInt1_1-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.2: Function <span class="math inline">\(h\)</span> of <a href="#exm-MCInt1">Example&nbsp;<span>2.1</span></a>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To approximate the integral <span class="math display">\[
\int_0^1 h(x)dx
\]</span> using Monte Carlo integration, we can use that <span class="math display">\[
\begin{align*}
\int_0^1 h(x)dx
&amp;=\int_0^11\cdot h(x)dx \\[2ex]
&amp;=\int_0^1f_{\mathcal{U}\text{[0,1]}}(x)\cdot h(x)dx \\[2ex]
&amp;= \mathbb{E}_{f_{\mathcal{U}\text{[0,1]}}}(h(X)),
\end{align*}
\]</span> where <span class="math inline">\(f_{\mathcal{U}\text{[0,1]}}\)</span> denotes the density function of the standard uniform distribution <span class="math inline">\(\mathcal{U}\text{[0,1]}.\)</span></p>
<p>Thus, to compute <span class="math inline">\(\int_0^1 h(x)dx\)</span> we generate a realization <span class="math inline">\((u_1,\dots,u_n)\)</span> from the random sample <span class="math inline">\(U_1,\dots,U_n\sim \mathcal{U}[0,1]\)</span> and approximate <span class="math display">\[
\int_0^1 h(x)dx\approx \bar{h}_n=\frac{1}{n}\sum_{i=1}^n h(u_i).
\]</span></p>
<p>In order to assess how good this approximation is, we need to consider the stochastic properties of the random variable <span class="math display">\[
\bar{h}_n = \frac{1}{n}\sum_{i=1}^n h(U_i).
\]</span> This is done using the above (review of) results on the limit distribution of the sample mean <span class="math inline">\(\bar{h}_n\)</span> which allows us to construct an approximate <span class="math inline">\(95\%\)</span> confidence interval, since for large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\operatorname{CI}^{95\%}_n
=&amp;\left[\bar{h}_n - z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}, \bar{h}_n + z_{1-\alpha/2}\sqrt{\frac{v_n^2}{n}}\right]\\[2ex]
\approx&amp;
\left[\bar{h}_n - z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}, \bar{h}_n + z_{1-\alpha/2} \sqrt{\frac{\mathbb{V}(h(U_i))}{n}}\right],
\end{align*}
\]</span> where <span class="math inline">\(z_{1-\alpha/2}\approx 1.96\)</span> denotes the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of <span class="math inline">\(\mathcal{N}(0,1),\)</span> <span class="math inline">\(v_n^2=n^{-1}\sum_{i=1}^n(h(u_i)-\bar{h}_n)^2,\)</span> and where <span class="math display">\[
P\left(\int_0^1 h(x)dx  \in \operatorname{CI}^{95\%}_n \right) \to 0.95,\quad n \to\infty,
\]</span> by the CLT.</p>
<p><a href="#fig-MCInt1_2">Figure&nbsp;<span>2.3</span></a> shows <em>one</em> realization of the stochastic sequence <span class="math display">\[
\bar{h}_1,\dots,\bar{h}_n
\]</span> with <span class="math inline">\(n=10000\)</span>, where the realized value of <span class="math inline">\(\bar{h}_n\)</span> is <span class="math inline">\(0.966\)</span>. This compares favorably with the with the exact value of <span class="math inline">\(\int_0^1h(x)dx = 0.9652009.\)</span></p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/fig-MCInt1_2_ba10bfdb31c3b0451eee9cedcc2e2cbe">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scales"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># h(x):</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>h_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">&lt;-</span> (<span class="fu">cos</span>(<span class="dv">50</span><span class="sc">*</span>x)<span class="sc">+</span><span class="fu">sin</span>(<span class="dv">20</span><span class="sc">*</span>x))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(result)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># sample size</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample of uniforms</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>u_vec <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="at">n=</span>n)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximation of the integral </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>h_bar_n <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(<span class="fu">h_fun</span>(u_vec))<span class="sc">/</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># True value:</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>true.value <span class="ot">&lt;-</span> <span class="fl">0.9652009</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% CI</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error of the estimator using the "algebraic </span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># formula" for the variance (german: 'verschiebungssatz')</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>st.error_n <span class="ot">&lt;-</span>  <span class="fu">sqrt</span>((<span class="fu">cumsum</span>(<span class="fu">h_fun</span>(u_vec)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>n) <span class="sc">-</span> </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                     <span class="fu">cumsum</span>(<span class="fu">h_fun</span>(u_vec))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>CI_u       <span class="ot">&lt;-</span>  h_bar_n <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> st.error_n <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>CI_l       <span class="ot">&lt;-</span>  h_bar_n <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> st.error_n <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n), <span class="at">y =</span> h_bar_n, <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.7</span>,<span class="fl">1.2</span>), </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"n"</span>, </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">""</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="fu">rev</span>(<span class="dv">1</span><span class="sc">:</span>n)), </span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> <span class="fu">c</span>(CI_u, <span class="fu">rev</span>(CI_l)), </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="at">col    =</span> <span class="fu">alpha</span>(<span class="st">"blue"</span>, <span class="fl">0.5</span>), </span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="at">border =</span> <span class="fu">alpha</span>(<span class="st">"blue"</span>, <span class="fl">0.5</span>))</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n), </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> h_bar_n, <span class="at">type=</span><span class="st">"l"</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n), </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">rep</span>(true.value, n), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">expression</span>(<span class="fu">bar</span>(h)[n]), </span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"True Value"</span>, <span class="st">"95% CI"</span>), </span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty   =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>), <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>), <span class="at">pt.cex=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>),</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">pt.bg =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="fu">alpha</span>(<span class="st">"blue"</span>, <span class="fl">0.5</span>)), </span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">col   =</span>   <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="fu">alpha</span>(<span class="st">"blue"</span>, <span class="fl">0.5</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-MCInt1_2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Ch2_MonteCarlo_files/figure-html/fig-MCInt1_2-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.3: One realization of the stochastic sequence <span class="math inline">\(\bar{h}_1,\dots,\bar{h}_n\)</span> with <span class="math inline">\(n=10000\)</span>, where the realized value of <span class="math inline">\(\bar{h}_{n=10000}\)</span> is <span class="math inline">\(0.966\)</span>. The blue band shows the point-wise (i.e.&nbsp;for each given sample size <span class="math inline">\(n\)</span>) confidence intervals <span class="math inline">\(\operatorname{CI}^{95\%}_n.\)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The approach of <a href="#exm-MCInt1">Example&nbsp;<span>2.1</span></a> can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency/accuracy through numerical methods (e.g., <a href="https://en.wikipedia.org/wiki/Riemann_sum">Riemann Sum</a>, <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule">Trapezoidal Rule</a>, <a href="https://en.wikipedia.org/wiki/Simpson%27s_rule">Simpson’s Rule</a>, etc.) in dimensions 1 or 2; see the following code example:</p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/unnamed-chunk-4_cbfbccb62ac2a53f3937a23b91d220ad">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>numericalIntegration <span class="ot">&lt;-</span> <span class="fu">integrate</span>(<span class="at">f     =</span> h_fun, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lower =</span> <span class="dv">0</span>, </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>numericalIntegration</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9652009 with absolute error &lt; 1.9e-10</code></pre>
</div>
</div>
<p>However, the Monte Carlo integration approach is particularly useful for approximating integrals over higher dimensional sets <span class="math inline">\(\mathcal{X}\subseteq\mathbb{R}^d.\)</span></p>
</div>
</div>
</section>
<section id="classical-monte-carlo-integration-using-proportions" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="classical-monte-carlo-integration-using-proportions"><span class="header-section-number">2.1.1</span> Classical Monte Carlo Integration using Proportions</h3>
<div id="exm-ApproxNormTables" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Approximation of Normal Distribution Tables) </strong></span>A possible way to construct normal distribution tables, i.e., table for the values of the distribution function <span class="math inline">\(0\leq \Phi(x)\leq 1,\)</span> is to use Monte Carlo integration. Besides this specific use case, this example showcases the treatment of integrals that can be approximated using propoartions <span class="math display">\[
\hat{p} =\frac{\text{Number of successes}}{\text{Sample size}},
\]</span> where we choose the sample size <span class="math inline">\(n\)</span> to achieve a certain level of precision for our integral approximation.</p>
<p>Observe that the distribution function <span class="math inline">\(\Phi(x)\)</span> can be written as the mean of a binary (taking values 0 and 1) random variable, <span class="math display">\[
\begin{align*}
\Phi(x)
&amp;=\int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy\\[2ex]
&amp;=P\left(X \leq x\right)\\[2ex]
&amp;=P\left(1_{(X \leq x)} = 1\right)\\[2ex]
&amp;=P\left(1_{(X \leq x)} = 1\right)\cdot 1 + P\left(1_{(X \leq x)} = 0\right)\cdot 0\\[2ex]
&amp;=\mathbb{E}\left(1_{(X \leq x)}\right),
\end{align*}
\]</span> where <span class="math inline">\(X\sim\mathcal{N}(0,1),\)</span> and where <span class="math inline">\(1_{(\texttt{TRUE})}=1\)</span> and <span class="math inline">\(1_{(\texttt{FALSE})}=0.\)</span></p>
<p>That is, we can write the integration problem as the mean of a Bernoulli random variable <span class="math display">\[
1_{(X \leq x)}\in\{0,1\}
\]</span> <span class="math display">\[
1_{(X \leq x)}\sim\mathcal{Bern}\left(p=\Phi(x)\right),
\]</span> with parameter <span class="math display">\[
p=P\left(X \leq x\right) = \Phi(x).
\]</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that <span class="math inline">\(1_{(X \leq x)}\)</span> is a Bernoulli random variable no matter the distribution of <span class="math inline">\(X.\)</span> Here <span class="math inline">\(X\)</span> is a standard normal random variable; however, in other use cases <span class="math inline">\(X\)</span> can have, of course, another distribution, but <span class="math inline">\(1_{(X \leq x)}\)</span> remains a Bernoulli random variable. We use this feature below when we think about the distributional properties of our estimator for approximating the integral value.</p>
</div>
</div>
<p>Monte Carlo integration, allows us to approximate the integral <span class="math inline">\(\Phi(x)\)</span> using the empirical mean <span class="math display">\[
\hat{p}_n(x)=\frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> with <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,1),
\]</span> and <span class="math display">\[
\hat{p}_n(x)\in[0,1]
\]</span> for all <span class="math inline">\(x\in\mathbb{R},\)</span> and all “sample sizes” (chosen by us) <span class="math inline">\(n=1,2,\dots\)</span></p>
<section id="distributional-properties-of-proportions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="distributional-properties-of-proportions">Distributional Properties of Proportions</h3>
<p>Now, to assess the accuracy of our Monte Carlo integration for approximating <span class="math inline">\(\Phi_n(x),\)</span> we need to derive the distributional properties of our estimator <span class="math inline">\(\hat{p}_n(x).\)</span></p>
<p>For this we consider, firstly, the transformed random variable <span class="math display">\[
n \hat{p}_n(x)=\sum_{i=1}^n1_{(X_i\leq x)},
\]</span> which is just the sum of independent Bernoulli distributed random variables with success probability <span class="math inline">\(p=\Phi(x),\)</span> <span class="math display">\[
1_{(X_1\leq x)},\dots,1_{(X_n\leq x)}\overset{\text{i.i.d.}}{\sim} \mathcal{Bern}\left(p=\Phi(x)\right)
\]</span> Thus, the transformed random variable <span class="math display">\[
n\hat{p}_n(x)=\sum_{i=1}^n1_{(X_i\leq x)}
\]</span> is <strong>binomial distributed</strong> with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p=\Phi(x),\)</span> <span class="math display">\[
n \hat{p}_n(x)\sim \mathcal{Binom}\left(n, p=\Phi(x)\right).
\]</span> Thus, we know the <em>exact</em> variance which is <span class="math display">\[
\begin{align*}
\mathbb{V}\left(n\hat{p}_n(x)\right)&amp;=\Phi(x)(1-\Phi(x)).
%\Leftrightarrow \mathbb{V}\left(\hat{p}_n(t)\right) &amp;=\frac{\Phi(x)(1-\Phi(x))}{n}
\end{align*}
\]</span> The standard error for our estimator <span class="math inline">\(\hat{p}_n(x)\)</span> thus is equal to <span class="math display">\[
\begin{align*}
\operatorname{SE}\left(\hat{p}_n(x)\right)
&amp;=\sqrt{\frac{\Phi(x)(1-\Phi(x))}{n}}\\[2ex]
&amp;=\texttt{const}\cdot \frac{1}{\sqrt{n}}
\end{align*}
\]</span> That is, the Monte Carlo integration algorithm has the typical parametric convergence rate of <span class="math display">\[
\frac{1}{\sqrt{n}}.
\]</span></p>
<!-- For values of $x$ around $x=0$, the variance is thus approximately $1/4n,$
$$
\mathbb{V}(\hat{\Phi}_n(t))\approx \frac{\Phi(0)(1-\Phi(0))}{n}=\frac{0.5^2}{n},\quad t\approx 0.
$$   -->
<p><a href="#tbl-CIProp">Table&nbsp;<span>2.1</span></a> gives the evolution of the Monte Carlo integration results <span class="math display">\[
\hat{p}_n(x)\approx \Phi(x)
\]</span> for several values of <span class="math inline">\(x\)</span> and <span class="math inline">\(n.\)</span> Very accurate approximations are achieved for <span class="math inline">\(n=10^8.\)</span></p>
<div id="tbl-CIProp" class="anchored">
<table class="table">
<caption>Table&nbsp;2.1: Monte Carlo integration results <span class="math inline">\(\hat{p}_n(x)\approx \Phi(x)\)</span> for different values of <span class="math inline">\(x\)</span> and sample sizes <span class="math inline">\(n.\)</span></caption>
<thead>
<tr class="header">
<th><span class="math inline">\(n\)</span></th>
<th><span class="math inline">\(x=0.0\)</span></th>
<th><span class="math inline">\(x=0.84\)</span></th>
<th><span class="math inline">\(x=3.72\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(10^2\)</span></td>
<td><span class="math inline">\(0.4850\)</span></td>
<td><span class="math inline">\(0.7700\)</span></td>
<td><span class="math inline">\(1.0000\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(10^3\)</span></td>
<td><span class="math inline">\(0.4925\)</span></td>
<td><span class="math inline">\(0.8010\)</span></td>
<td><span class="math inline">\(1.0000\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(10^4\)</span></td>
<td><span class="math inline">\(0.4962\)</span></td>
<td><span class="math inline">\(0.7941\)</span></td>
<td><span class="math inline">\(0.9999\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(10^5\)</span></td>
<td><span class="math inline">\(0.4995\)</span></td>
<td><span class="math inline">\(0.7993\)</span></td>
<td><span class="math inline">\(0.9999\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(10^6\)</span></td>
<td><span class="math inline">\(0.5001\)</span></td>
<td><span class="math inline">\(0.8000\)</span></td>
<td><span class="math inline">\(0.9999\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(10^7\)</span></td>
<td><span class="math inline">\({\color{red}0.5002}\)</span></td>
<td><span class="math inline">\(0.8000\)</span></td>
<td><span class="math inline">\(0.9999\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(10^8\)</span></td>
<td><span class="math inline">\(0.5000\)</span></td>
<td><span class="math inline">\(0.8000\)</span></td>
<td><span class="math inline">\(0.9999\)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that <strong>greater accuracy is achieved in the tails</strong>.</p>
</div>
</div>
<section id="confidence-intervals-for-proportions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confidence-intervals-for-proportions">Confidence Intervals for Proportions</h4>
<p>As an additional tool for showing the accuracy of our Monte Carlo integration, we can report confidence intervals.</p>
<p>For large <span class="math inline">\(n,\)</span> we have by the CLT that <span id="eq-AsymDistrProp"><span class="math display">\[
\begin{align*}
\frac{\hat{p}_n(x) -  \Phi(x)}{\operatorname{SE}\left(\hat{p}_n(x)\right)} \overset{d}{\approx}\mathcal{N}(0,1).
\end{align*}
\tag{2.2}\]</span></span> Thus, for large <span class="math inline">\(n,\)</span> we can provide an approximate confidence interval <span id="eq-CIProp1"><span class="math display">\[
\begin{align*}
\operatorname{CI}^{95\%}_n\left(\Phi(x)\right)
=&amp;\left[
  \hat{p}_n(x) \pm z_{1-\alpha/2}\operatorname{SE}\left(\hat{p}_n(x)\right)
  \right].
\end{align*}
\tag{2.3}\]</span></span> This confidence interval is an approximate one, since the quantile <span class="math inline">\(z_{1-\alpha/2}\)</span> uses the approximate asymptotic <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution in <a href="#eq-AsymDistrProp">Equation&nbsp;<span>2.2</span></a>. To make the confidence interval in <a href="#eq-CIProp1">Equation&nbsp;<span>2.3</span></a> usable in practice, we need to plug-in an estimate for the unknown <span class="math inline">\(\operatorname{SE}\left(\hat{p}_n(x)\right),\)</span> <span class="math display">\[
\widehat{\operatorname{SE}}\left(\hat{p}_n(x)\right) = \sqrt{\frac{\hat{p}_n(x)(1-\hat{p}_n(x))}{n}}.
\]</span></p>
<p>However, it turns out that going the indirect way via the central limit theorem to derive a confidence interval is not optimal here. A more efficient <strong>confidence interval for proportions</strong> is, for instance, the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Clopper%E2%80%93Pearson_interval">Clopper-Pearson confidence interval</a> (<span class="citation" data-cites="Clopper_Pearson_1934">Clopper and Pearson (<a href="#ref-Clopper_Pearson_1934" role="doc-biblioref">1934</a>)</span>).</p>
<p>The following code snippet computes the 99% Clopper-Pearson confidence interval for the <span style="color:red">red marked</span> Monte Carlo integration result, <span class="math inline">\(\hat{p}_n(0)=0.5002,\)</span> in <a href="#tbl-CIProp">Table&nbsp;<span>2.1</span></a>.</p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/unnamed-chunk-5_2f6319a78a68e973f59df089514e5a26">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("PropCIs")</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"PropCIs"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Clopper-Pearson confidence interval</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fl">10e7</span>       <span class="co"># "sample" size (chosen by use)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fl">0.5002</span> <span class="sc">*</span> n <span class="co"># observed value of n * \hat{p}_n(x) </span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>CI <span class="ot">&lt;-</span> <span class="fu">exactci</span>(<span class="at">x          =</span> x, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">n          =</span> n, </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">conf.level =</span> <span class="fl">0.99</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Lower and upper CI-border              </span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(CI<span class="sc">$</span>conf.int[<span class="dv">1</span>], CI<span class="sc">$</span>conf.int[<span class="dv">2</span>]) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5001089 0.5002911</code></pre>
</div>
</div>
<!-- $n$    |$x=0.0$  |$x=0.84$ |$x=1.65$ |$x=2.58$ |$x=3.72$ 
-------|---------|---------|---------|---------|---------
$10^2$ |$0.485$  |$0.77$   |$0.945$  |$0.995$  |$1$      
$10^3$ |$0.4925$ |$0.801$  |$0.9425$ |$0.9955$ |$1$      
$10^4$ |$0.4962$ |$0.7941$ |$0.9498$ |$0.995$  |$0.9999$ 
$10^5$ |$0.4995$ |$0.7993$ |$0.9498$ |$0.995$  |$0.9999$ 
$10^6$ |$0.5001$ |$0.8$    |$0.9502$ |$0.995$  |$0.9999$ 
$10^7$ |$0.5002$ |$0.8$    |$0.9501$ |$0.995$  |$0.9999$ 
$10^8$ |$0.5$    |$0.8$    |$0.95$   |$0.995$  |$0.9999$  -->
<!--                  X                   X                   X                   X
$n$    |$x=0.0$  |$x=0.67$ |$x=0.84$ |$x=1.28$ |$x=1.65$ |$x=2.32$ |$x=2.58$ |$x=3.09$ |$x=3.72$ 
-----  |-------  |-------  |-------  |-------  |-------  |-------  |-------  |-------  |---------
$10^2$ |$0.485$  |$0.74$   |0.77     |0.9      |0.945    |0.985    |0.995    |1        |1      
$10^3$ |$0.4925$ |$0.7455$ |0.801    |0.902    |0.9425   |0.9885   |0.9955   |0.9985   |1      
$10^4$ |$0.4962$ |$0.7425$ |0.7941   |0.9      |0.9498   |0.9896   |0.995    |0.999    |0.9999 
$10^5$ |$0.4995$ |$0.7489$ |0.7993   |0.9003   |0.9498   |0.9898   |0.995    |0.9989   |0.9999 
$10^6$ |$0.5001$ |$0.7497$ |0.8      |0.9002   |0.9502   |0.99     |0.995    |0.999    |0.9999 
$10^7$ |$0.5002$ |$0.7499$ |0.8      |0.9001   |0.9501   |0.99     |0.995    |0.999    |0.9999 
$10^8$ |$0.5$    |$0.75$   |0.8      |0.9      |0.95     |0.99     |0.995    |0.999    |0.9999  
-->
<!-- $$
\begin{array}{cccccccccc}
\hline
n   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\
\hline
10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\
10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\
10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\
10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\
10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\
10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\
10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\
\end{array}
$$ -->
<!-- * To achieve a precision of **four decimals** by means of a $99.9\%$ confidence interval, the approximation requires on average $n\approx 10^8$ simulations. 

* To achieve a precision of **two decimals** by means of a $99.9\%$  confidence interval, already $n=10^4$ leads to satisfactory results.  -->
<!-- * Note that **greater accuracy is achieved in the tails** 

* More efficient simulation methods could be used (e.g., Importance Sampling).  -->
<p>That is, we need a sample size of <span class="math inline">\(n^7\)</span> to achieve a precision of <strong>three decimal places</strong> by means of a 99% Clopper-Pearson confidence interval.</p>
</section>
</section>
</div>
</section>
</section>
<section id="importance-sampling" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">2.2</span> Importance Sampling</h2>
<p>As demonstrated in <a href="#exm-ApproxNormTables">Example&nbsp;<span>2.2</span></a>, the accuracy of the Monte Carlo integration method as a tool for approximating integral values depends on the variance of the estimate that approximates the integral value.</p>
<p>“Importance sampling” aims to reduce the variance of the Monte Carlo integral estimator. Therefore, importance sampling is also refereed to as a <strong>variance reduction</strong> technique. This variance reduction is achieved by weighting functions, so-called <strong>importance functions</strong>.</p>
<p>As in the case of Monte Carlo integration the focus lies on evaluating the integral <span class="math display">\[
\mathbb{E}_{f_X}(h(X))=\int_\mathcal{X}h(x)f(x)\,dx.
\]</span></p>
<p>However, it turns out that the above approach, i.e., sampling from <span class="math inline">\(f\)</span> is often suboptimal.</p>
<p>Observe that the value of the above integral can be represented by infinitely many alternative choices of the triplet <span class="math display">\[
(\mathcal{X}, h, f_X).
\]</span> Therefore, the search for an optimal estimator should encompass all these possible representations.</p>
<p>Let’s illustrate this with a simple example.</p>
<div id="exm-CauchTailProb" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (<a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy</a> Tail Probability) </strong></span>This example is from <span class="citation" data-cites="Ripley_2009">Ripley (<a href="#ref-Ripley_2009" role="doc-biblioref">2009</a>)</span>.</p>
<p>Suppose that the quantity of interest is the probability, say <span class="math inline">\(p\)</span>, that a <span class="math inline">\(\mathcal{Cauchy}(0,1)\)</span>-distributed random variable is larger than <span class="math inline">\(2\)</span>, i.e. <span id="eq-CauchIntegral"><span class="math display">\[
p=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx,
\tag{2.4}\]</span></span> where <span class="math inline">\(1/(\pi(1+x^2))\)</span> is the density function of the <span class="math inline">\(\mathcal{Cauchy}(0,1)\)</span>-distribution.</p>
<p>This is a nice example. One the one hand, it allows us to showcase possibilities to improve efficiency of Monte Carlo integration. On the other hand, we know already the result of <a href="#eq-CauchIntegral">Equation&nbsp;<span>2.4</span></a>; namely, <span class="math inline">\(p=0.1476\)</span> <!-- $$
\begin{align*}
p
&=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx\\[2ex]
&=\frac{1}{\pi}\int_{2}^{+\infty}\frac{1}{1+x^2}\,dx\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left[\frac{1}{\tan(x)}\right]_2^b\\[2ex]
&=\frac{1}{\pi}\lim_{b\to\infty}\left(\frac{1}{\tan(b)}-\frac{1}{\tan(2)}\right)\\[2ex]
\end{align*}
$$ --></p>
<div class="cell" data-hash="Ch2_MonteCarlo_cache/html/unnamed-chunk-6_910b12e9c35fd9721643eb44f1b988db">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pcauchy</span>(<span class="dv">2</span>), <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1476</code></pre>
</div>
</div>
<p><strong>1. Approach: The Naive Approach</strong></p>
<p>The most direct approach would be to use the following mean expression for the integral of interest: <span class="math display">\[
\begin{align*}
p
&amp;=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx\\[2ex]
&amp;=P\left(X \leq 2\right)\\[2ex]
&amp;=P\left(1_{(X \leq 2)}=1\right)\\[2ex]
&amp;= P\left(1_{(X \leq 2)}=1\right)\cdot 1 + P\left(1_{(X \leq 2)}=0\right)\cdot 0\\[2ex]
&amp;=\mathbb{E}\left(1_{(X \leq 2)}\right)
\end{align*}
\]</span></p>
<p>So, we can approximate <span class="math inline">\(p\)</span> using the empirical mean <span class="math display">\[
\hat{p}_{1}=\frac{1}{n}\sum_{i=1}^n 1_{(X_i&gt;2)}
\]</span> of a random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{Cauchy}(0,1),\)</span> where <span class="math display">\[
1_{(X_1&gt;2)},\dots,1_{(X_n&gt;2)}\overset{\text{i.i.d.}}{\sim}\mathcal{Bernoulli}(p).
\]</span> The variance of <span class="math inline">\(\hat{p}_1\)</span> is thus <span class="math display">\[
\begin{align*}
\mathbb{V}(\hat{p}_{1})
&amp;=\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(X_i&gt;2)}\right)\\[2ex]
&amp;=\frac{n}{n^2}\mathbb{V}\left(1_{(X_1&gt;2)}\right)\quad\text{(i.i.d.)}\\[2ex]
&amp;=\frac{p(1-p)}{n},
\end{align*}
\]</span> which is equal to <span class="math inline">\(0.126/n,\)</span> since we know that <span class="math inline">\(p=0.1476\)</span>.</p>
<p><strong>2. Approach: Accounting for Symmetry</strong></p>
<p>In this approach, we use the “adjusting Screws” <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(h.\)</span></p>
<p>We can achieve a <strong>more efficient estimator</strong> (i.e., an estimator with lower variance for a given same sample size <span class="math inline">\(n\)</span>) if we take into account the symmetric nature of <span class="math inline">\(\mathcal{Cauchy}(0,1).\)</span></p>
<p>Obviously, due to the symmetry of our target integrand, can do the following rearrangement <span class="math display">\[
\begin{align*}
p
&amp;=\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx\\[2ex]
&amp;=\frac{1}{2}\left(\int_{-\infty}^{-2}\frac{1}{\pi(1+x^2)}\,dx + \int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}\,dx \right)\\[2ex]
&amp;=\frac{1}{2}\left(\mathbb{E}\left(1_{(X \leq -2)}\right) + \mathbb{E}\left(1_{(X \geq 2)}\right)\right)\\[2ex]
&amp;=\frac{1}{2} \; \mathbb{E}\left(1_{(|X| \geq 2)}\right)
\end{align*}
\]</span> This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean <span class="math display">\[
\begin{align*}
\hat{p}_{2}
%&amp;=\frac{1}{2}\left(\frac{1}{n}\sum_{i=1}^n1_{(X_i \leq -2)}+ \frac{1}{n}\sum_{i=1}^n 1_{(X_i \geq 2)}\right)\\[2ex]
&amp;=\frac{1}{2n}\sum_{i=1}^n 1_{(|X_i|\geq 2)}
\end{align*}
\]</span> again of a random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{Cauchy}(0,1),\)</span> where <span class="math display">\[
1_{(|X_1|&gt;2)},\dots,1_{(|X_n|&gt;2)}\overset{\text{i.i.d.}}{\sim}\mathcal{Bernoulli}(2\,p).
\]</span> The variance of this new estimator, <span class="math display">\[
\begin{align*}
\mathbb{V}(\hat{p}_{2})
&amp;=\frac{1}{4n^2}\mathbb{V}\left(\sum_{i=1}^n1_{(|X_i|\geq 2)}\right)\\[2ex]
&amp;=\frac{1}{4n}\mathbb{V}\left(1_{(|X_1|\geq 2)}\right)\\[2ex]
&amp;=\frac{2p(1-2p)}{4n},
\end{align*}
\]</span> which is equal to <span class="math inline">\(0.052/n,\)</span> since we know that <span class="math inline">\(p=0.1476\)</span>. This is clearly lower than in the naive approach, where we had <span class="math inline">\(0.126/n.\)</span></p>
<p><strong>3. Approach:</strong></p>
<p>In this approach, we use all the “adjusting screws” <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(h\)</span>, and <span class="math inline">\(f.\)</span></p>
<p>The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, <span class="math inline">\([2,+\infty)\)</span>, which are in some sense irrelevant for the approximation of <span class="math inline">\(p\)</span>. This motivates the following reformulation of <span class="math inline">\(p\)</span>:</p>
<p>By symmetry of <span class="math inline">\(f,\)</span> we have that <span class="math display">\[
\begin{align*}
\frac{1}{2}
&amp; =\int_{0}^\infty\frac{1}{\pi(1+x^2)}dx.
\end{align*}
\]</span> This can be used to do the following: <span class="math display">\[
\begin{align*}
\frac{1}{2}
&amp; =\int_{0}^2\frac{1}{\pi(1+x^2)}dx + \underbrace{\int_{2}^{+\infty}\frac{1}{\pi(1+x^2)}dx}_{=p}\\[2ex]
\Leftrightarrow \; p&amp; =\frac{1}{2}-\int_{0}^2\frac{1}{\pi(1+x^2)}dx.
\end{align*}
\]</span> Furthermore, we can rearrange the integral a bit such that <span class="math display">\[
\begin{align*}
&amp;\int_{0}^2\;\left(\frac{1}{2}\cdot 2\right)\;\frac{1}{\pi(1+x^2)}\,dx \\[2ex]
=&amp;\int_{0}^2\;\underbrace{\frac{1}{2}}_{f_{\mathcal{U}[0,2]}}\;\underbrace{\frac{2}{\pi(1+x^2)}}_{=h(x)}\,dx \\[2ex]
=&amp;\mathbb{E}_{f_{\mathcal{U}[0,2]}}(h(U)),
\end{align*}
\]</span> where <span class="math inline">\(U\sim\mathcal{U}[0,2].\)</span></p>
<p>Therefore a new alternative method for evaluating <span class="math inline">\(p\)</span> is: <span class="math display">\[
\hat{p}_{3}=\frac{1}{2} - \frac{1}{n}\sum_{i=1}^n h(U_i),
\]</span> where <span class="math display">\[
U_1,\dots,U_n\overset{\text{i.i.d.}}{\sim}\mathcal{U}[0,2].
\]</span> Computing the variance is here cumbersome. <span class="math display">\[
\begin{align*}
\mathbb{V}\left(\hat{p}_{3}\right)
&amp; =\frac{1}{n^2}\mathbb{V}\left(\sum_{i=1}^n h(U_i)\right)\\[2ex]
&amp; =\frac{1}{n}\mathbb{V}\left(h(U_1)\right)\\[2ex]
\end{align*}
\]</span> Using integration by parts and that <span class="math inline">\(p=0.1476\)</span>, it can be shown that <span class="math display">\[
\mathbb{V}(\hat p_3)=0.029/n,
\]</span> which is lower than both previous approaches, where we had that <span class="math inline">\(\mathbb{V}(\hat{p}_{2})=0.052/n\)</span> and <span class="math inline">\(\mathbb{V}(\hat{p}_{1})=0.126/n\)</span>.</p>
</div>
<section id="a-more-general-point-of-view" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="a-more-general-point-of-view"><span class="header-section-number">2.2.1</span> A More General Point of View</h3>
<p>The idea of <strong>importance sampling</strong> is related to weighted and stratified sampling ideas, when estimating <span class="math display">\[
\theta=\mathbb{E}_{f}(h(X))=\int h(x)f(x)dx,
\]</span> as already illustrated in <a href="#exm-CauchTailProb">Example&nbsp;<span>2.3</span></a>.</p>
<p>Some outcomes of <span class="math inline">\(X\sim f\)</span> may be more important than others in determining <span class="math inline">\(\theta\)</span> and we wish to select such values more frequently.</p>
<p>For instance, if <span class="math inline">\(\theta\)</span> denotes the probability of the occurrence of a very rare event, then the only way to estimate <span class="math inline">\(\theta\)</span> at all accurately may be to produce the rare events more frequently.</p>
<p>To achieve this, we can simulate a model which gives a density <span class="math inline">\(g\)</span> to <span class="math inline">\(X\)</span> instead of the correct density <span class="math inline">\(f,\)</span> where both density functions need to be known. This can be easily done, since <span class="math display">\[
\begin{align*}
\theta
&amp;=\mathbb{E}_{f}(h(X))\\[2ex]
&amp;=\int h(x)\left(\frac{g(x)}{g(x)}\right)\;f(x)dx\\[2ex]
&amp;=\int \underbrace{\left(h(x)\frac{f(x)}{g(x)}\right)}_{=:\psi(x)}\;g(x)dx\\[2ex]
&amp;=\int \psi(x)\;g(x)dx\\[2ex]
&amp;=\mathbb{E}_g(\psi(X)).
\end{align*}
\]</span></p>
<p>This leads to the following unbiased estimator for <span class="math inline">\(\theta\)</span> based on sampling from <span class="math inline">\(g\)</span>: <span id="eq-ImportSample"><span class="math display">\[
\begin{align*}
\hat{\theta}_g
&amp;=\frac{1}{n}\sum_{i=1}^n\psi(X_i) \\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n\underbrace{\left(\frac{f(X_i)}{g(X_i)}\right)}_{=:W_i} h(X_i)
\end{align*}
\tag{2.5}\]</span></span> with <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} g.
\]</span></p>
<p>Note that <a href="#eq-ImportSample">Equation&nbsp;<span>2.5</span></a> can be seen as a weighted mean of the transformed random variables <span class="math inline">\(h(X_i)\)</span> with weights <span class="math display">\[
W_i = \frac{f(X_i)}{g(X_i)}
\]</span> that are inversely proportional to the so-called <strong>selection factor</strong> <span class="math display">\[
\frac{g(X_i)}{f(X_i)}.
\]</span> <!-- 
For appropriately chosen pdfs f and g: 
The *selection factor* refers to "how more likely is it to select a 'rare event'?".
The inverse weight re-scales these 'too often' chosen rare events. 
--></p>
<p>For the variance of the estimator <span class="math inline">\(\hat{\theta}_g\)</span> we have <span class="math display">\[
\begin{align*}
\mathbb{V}_g(\hat{\theta}_g)
&amp;=\mathbb{V}_g\left(\frac{1}{n}\sum_{i=1}^n\psi(X_i)\right) \\[2ex]
&amp;=\frac{1}{n}\mathbb{V}_g(\psi(X_i))\quad\text{(i.i.d)}\\[2ex]
&amp;=\frac{1}{n}\int\left(\psi(x)-\theta\right)^2g(x)\;dx\\[2ex]
&amp;=\frac{1}{n}\int\left(\frac{h(x)\,f(x)}{g(x)}-\theta\right)^2g(x)\;dx,
\end{align*}
\]</span> which, depending on the choice of <span class="math inline">\(g\)</span>, can be <em>much</em> smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary (unweighted) empirical mean. <!-- 
$$
\mathbb{V}(\hat{\theta}_{\text{naive}})=
\mathbb{V}\left(\frac{1}{n}\sum_{i=1}^n 1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=
\frac{1}{n}\mathbb{V}\left(1_{(X_i\in\{\mathtt{Rare.Event}\})}\right)=\frac{\theta(1-\theta)}{n}.
$$ 
--></p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-MinVarTheorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Minimum Variance Theorem) </strong></span>The <strong>importance function</strong> <span class="math inline">\(g\)</span> which minimizes the variance <span class="math inline">\(\mathbb{V}_g(\psi(X_i))\)</span>, and therefore also the variance <span class="math inline">\(\mathbb{V}_g(\hat{\theta}_g),\)</span> is given by <span class="math display">\[
g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
\]</span></p>
</div>
</div>
</div>
<p><strong>Proof of <a href="#thm-MinVarTheorem">Theorem&nbsp;<span>2.1</span></a>:</strong></p>
<p>Note that <span class="math display">\[
\begin{align*}
\mathbb{V}_g(\psi(X_i))
&amp;=\quad\quad \mathbb{E}_g(\psi(X_i)^2)\quad\quad  - \quad\quad \left(\mathbb{E}_g(\psi(X_i))\right)^2\\[2ex]
&amp;=\int\left(\frac{h(x)f(x)}{g(x)}\right)^2g(x)dx -
  \left(\int\frac{h(x)f(x)}{g(x)}g(x)dx\right)^2\\[2ex]
  &amp;=\int\left(\frac{h(x)f(x)}{g(x)}\right)^2g(x)dx -
  \left(\int h(x)f(x)dx\right)^2.
\end{align*}
\]</span> Thus, <span class="math inline">\(\left(\mathbb{E}_g(\psi(X_i))\right)^2\)</span> does not depend on <span class="math inline">\(g.\)</span> Consequently, the density <span class="math inline">\(g\)</span> that minimizizes the variance <span class="math inline">\(\mathbb{V}_g(\psi(X_i))\)</span> is also the density <span class="math inline">\(g\)</span> that minimizes <span class="math inline">\(\mathbb{E}_g(\psi(X_i)^2).\)</span> We use this in the following since minimizing <span class="math inline">\(\mathbb{E}_g(\psi(X_i)^2)\)</span> is simpler than minimizing <span class="math inline">\(\mathbb{V}_g(\psi(X_i)).\)</span></p>
<p>We begin with a small trick by including an absolute value function as following: <span class="math display">\[
\begin{align*}
\mathbb{E}_g(\psi(X_i)^2)
&amp;=\mathbb{E}_g({\color{red}|}\psi(X_i){\color{red}|}^2).
\end{align*}
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong><a href="https://www.statlect.com/fundamentals-of-probability/Jensen-inequality">Jensen’s Inequality</a>:</strong> Let <span class="math inline">\(u\)</span> be a “convex function” such as, for instance, <span class="math inline">\(u(x)=x^2,\)</span> and let <span class="math inline">\(X\)</span> be some random variable. Then, <span class="math display">\[
\mathbb{E}\left(u(X)\right) \geq u\left(\mathbb{E}\left(X\right)\right),
\]</span> where we assume that the moments exist.</p>
</div>
</div>
<p>Applying Jensen’s inequality yields, the following lower bound for <span class="math inline">\(\mathbb{E}_g(\psi(X_i)^2):\)</span> <span class="math display">\[
\begin{align*}
\mathbb{E}_g(\psi(X_i)^2)
&amp;=\mathbb{E}_g({\color{red}|}\psi(X_i){\color{red}|}^2)\\[2ex]
&amp;\geq \left(\mathbb{E}_g({\color{red}|}\psi(X_i){\color{red}|})\right)^2\\[2ex]
&amp;=\left(\int\left|\frac{h(x)f(x)}{g(x)}\right|g(x)dx\right)^2\\[2ex]
&amp;=\left(\int\frac{\left|h(x)\right|f(x)}{g(x)}g(x)dx\right)^2\quad\text{(since $f$ and $g$ are non-negative)}\\[2ex]
&amp;=\underbrace{\left(\int\left|h(x)\right|f(x)dx\right)^2}_{\text{Lower bound of $\mathbb{E}_g(\psi(X_i)^2)$}},
\end{align*}
\]</span> where this lower bound is independent of <span class="math inline">\(g.\)</span></p>
<p>It is now straight forward to show that this lower bound is achieved if <span class="math display">\[
g(x) = g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}.
\]</span> For this, observe that <span class="math display">\[
\begin{align*}
\mathbb{E}_g(\psi(X_i)^2)
&amp; = \int\left(\frac{h(x)f(x)}{g(x)}\right)^2g(x)dx\\[2ex]
&amp; = \int\frac{h^2(x)f^2(x)}{g(x)}dx.
\end{align*}
\]</span> Now, setting <span class="math inline">\(g(x)=g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz}\)</span> yields <span class="math display">\[
\begin{align*}
\mathbb{E}_{g^\ast}(\psi(X_i)^2)
&amp; = \int\frac{h^2(x)f^2(x)}{|h(x)|f(x)} \left(\int |h(z)|f(z)dz\right) dx\\[2ex]
&amp; = \int\frac{\left|h(x)\right|^2f(x)}{|h(x)|} \; \left(\int |h(z)|f(z)dz\right) dx\\
&amp; = \left(\int\left|h(x)\right|f(x) dx\right) \; \left(\int |h(z)|f(z)dz\right) \\[2ex]
&amp; = \underbrace{\left(\int\left|h(x)\right|f(x) dx\right)^2}_{\text{Lower bound of $\mathbb{E}_g(\psi(X_i)^2)$}}
\end{align*}
\]</span> This shows that the lower bound of <span class="math inline">\(\mathbb{E}_g(\psi(X_i)^2)\)</span> is achieved for <span class="math inline">\(g(x)=g^\ast(x)=\frac{|h(x)|f(x)}{\int |h(z)|f(z)dz},\)</span> which shows the statement of <a href="#thm-MinVarTheorem">Theorem&nbsp;<span>2.1</span></a>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that <a href="#thm-MinVarTheorem">Theorem&nbsp;<span>2.1</span></a> may appear as rather impractical, since, for instance, if <span class="math inline">\(h(x)&gt;0\)</span> then <span class="math inline">\(g^\ast\)</span> requires us to know <span class="math inline">\(\theta=\int h(z)f(z)dz,\)</span> which, however, is just the integral of interest!</p>
<p>The minimum variance result of <a href="#thm-MinVarTheorem">Theorem&nbsp;<span>2.1</span></a> is, however, still useful:</p>
<ul>
<li>It tells us that a good choice of <span class="math inline">\(g(x)\)</span> shall mimic the shape of <span class="math inline">\(|h(x)|f(x)\)</span>, since the optimal <span class="math inline">\(g^\ast(x)\propto |h(x)|f(x)\)</span>.</li>
<li>Furthermore, <span class="math inline">\(g(x)\)</span> should be chosen such that it has a thicker tail than <span class="math inline">\(f(x)\)</span>, since the variance <span class="math inline">\(\mathbb{V}(\hat{\theta}_g)\)</span> crucially depends on the quotient <span class="math inline">\(f(x)/g(x)\)</span> which would “explode” for <span class="math inline">\(g(x)\approx 0\)</span>.</li>
</ul>
</div>
</div>
<p>Let’s apply our new insights to the above <a href="#exm-CauchTailProb">Example&nbsp;<span>2.3</span></a> on the Cauchy tail probability <span class="math inline">\(p\)</span>.</p>
<div id="exm-CauchTailProb2" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Cauchy Tail Probability (continued)) </strong></span>Above we had:</p>
<ol type="1">
<li><span class="math inline">\(f(x)=\frac{1}{\pi(1+x^2)}\)</span>, the density of <span class="math inline">\(\mathcal{Cauchy}(0,1)\)</span> and</li>
<li><span class="math inline">\(h(x)=1_{(x&gt;2)}\)</span>, i.e., here <span class="math inline">\(|h(x)|=h(x)\)</span>.</li>
</ol>
<p>Therefore <span class="math display">\[
\begin{align*}
p
&amp; =\mathbb{E}_f(h(X)) \\[2ex]
&amp; =\int h(x)f(x)dx    \\[2ex]
&amp; =\int_{2}^{\infty}f(x)dx \\[2ex]
&amp; =\int_{2}^{\infty}\underbrace{\frac{f(x)}{g(x)}}_{=\psi(x)}\;g(x)dx \\[2ex]
&amp; =\mathbb{E}_g(\psi(X)),
\end{align*}
\]</span> where the <span class="math inline">\(h\)</span> function is absorbed by the formulation of the definite integral.</p>
<p>A possibly good (and simple) choice of <span class="math inline">\(g\)</span> is, e.g., <span class="math inline">\(g(x)=2/(x^2)\)</span>, since this function:</p>
<ul>
<li>“closely” matches <span class="math inline">\(h(x)f(x)\)</span> and</li>
<li><span class="math inline">\(g\)</span> has thicker tails than <span class="math inline">\(f\)</span>.</li>
</ul>
<div class="cell" data-layout-align="center" data-hash="Ch2_MonteCarlo_cache/html/unnamed-chunk-7_beb39e0cb94c451b649d5c5a219ab27f">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Ch2_MonteCarlo_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><br>
</p>
<p><strong>Caution:</strong> It is not straight forward to directly sample from <span class="math inline">\(g\)</span>, therefore we need some further steps:</p>
<p><br>
</p>
<p>The choice of <span class="math inline">\(g\)</span> leads to <span class="math display">\[
p=\mathbb{E}_g(\psi(X))=
\int_{2}^{+\infty}\left(\frac{x^2}{2\,\pi(1+x^2)}\right)\,\frac{2}{x^2}\,dx=
\int_{2}^{+\infty}\left(\frac{1}{\pi(1+x^{-2})}\right)\,x^{-2}\,dx.
\]</span></p>
<p><br>
</p>
<p>Now we can apply some additional (rather case-specific) re-arrangements:</p>
<p>Integration by substitution (substituting <span class="math inline">\(u=x^{-1}\)</span>) yields: <span class="math display">\[
p=\int_{0}^{1/2}\frac{1}{\pi(1+u^2)}du.
\]</span> Again, we can re-arrange the last integral a bit such that <span class="math display">\[
p=\int_{0}^{1/2}\underbrace{2}_{f_{\mathrm{Unif}[0,1/2]}}\;\underbrace{\frac{1}{2\,\pi(1+u^2)}}_{=h(u)}\,du=\mathbb{E}(h(U)),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
\]</span> Therefore, we have a final fourth version of the estimator of <span class="math inline">\(p\)</span>: <span class="math display">\[
\hat{p}_4=\sum_{i=1}n h(U_i),\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2]
\]</span> and <span class="math inline">\(h(u)=1/(2\pi(1+u^2))\)</span>.</p>
<p>The variance of <span class="math inline">\(\hat{p}_4\)</span> is <span class="math inline">\((\mathbb{E}(h(U)^2)-\mathbb{E}(h(U))^2)/n\)</span> and an integration by parts shows that <span class="math inline">\(\mathbb{V}(\hat{p}_4)=0.95\cdot 10^{-4}/n\)</span>. Compare this to the former results: <span class="math inline">\(\mathbb{V}(\hat p_3)=0.0285/n\)</span>, <span class="math inline">\(\mathbb{V}(\hat{p}_{2})=0.0525/n\)</span> and <span class="math inline">\(\mathbb{V}(\hat{p}_{1})=0.1275/n\)</span>. The variance of <span class="math inline">\(\hat{p}_4\)</span> is by a factor of <span class="math inline">\(10^{-3}\)</span> lower than the variance of the original <span class="math inline">\(\hat{p}_1\)</span>.</p>
</div>
<!--
**Version 2:** It can be shown that (see [Ripley 1987](http://onlinelibrary.wiley.com/book/10.1002/9780470316726))
$$
p=\int_0^{1/2}\frac{y^{-2}}{\pi(1+y^{-2})}dy,
$$
where this integral can also be seen as the expectation of 
$$
\frac{1}{4}h(U)=\frac{1}{2\pi(1+U^2)},\quad\text{where}\quad U\sim\mathrm{Unif}[0,1/2].
$$
-->

<!--
## The 'Real Need' of Monte Carlo Simulation 

**Monte Carlo Simulation applied to Hypothesis Tests**

* **Problem:** Very often, statistical test procedures (or estimators) rely on asymptotic arguments. Asymptotic arguments ease the live of a statistician. In practice, however, we never have something like a diverging sample size of $n\to\infty$, but need to deal with a finite sample size $n$. All we can hope for is that the asymptotic results (e.g., on the level of significance of a test statistic and its power) are good approximations to the finite $n$ case. 

* **Solution:** Monte Carlo simulation the classical tool to investigate the finite $n$ performance of statistical test procedures (or estimators). It helps to answer questions like: How good are the asymptotic results given finite sample size scenarios of $n=100$, $n=500$, etc.

For instance, the **likelihood ratio (LR)** test statistic  
$$
-2\,\log\left[\ell(\hat\theta|x)/\ell(\hat\theta_0|x)\right]=-2\,\left\{\log\ell(\hat\theta|x)-\log\ell(\hat\theta_0|x)\right\}\to_d\chi^2_r
$$
is distributed as $\chi^2_r$ generally only in the limiting case as $n\to\infty$; and under some regularity constraints on the likelihood function. In the formula above $\ell(\theta|x)$ denotes the likelihood function, $\hat\theta\in\mathbb{R}^k$ is the estimated (via maximum likelihood) parameter vector from the unconstrained model and $\hat\theta_0\in\mathbb{R}^k$ is the estimated parameter vector from the constrained model with $r\leq k$ restrictions.

\

**Example: Contingency Tables**

The following table gives the results of a study comparing radiation therapy with surgery in treating cancer of the larynx. 


$$
\begin{array}{c|cc|c}
                   &  \text{Cancer}  &  \text{Cancer not}&           \\
                   &  \text{Controlled}  &  \text{Controlled}&           \\
\hline                   
\text{Surgery}     &  y_{11}=21      &   y_{12}=2        &  n_{1.}=23\\
\text{Radiation}   &  y_{21}=15      &   y_{22}=3        &  n_{2.}=18\\
\hline
                   &  n_{.1}=36      &   n_{.2}=5        &  n=41
\end{array}
$$

\

Let's condition on a fix number of total observations $n$. Then the random vector $(Y_{11},Y_{12},Y_{21},Y_{22})^\top$ comes from a [multinomial distribiton](https://en.wikipedia.org/wiki/Multinomial_distribution) with 4 cells and cell probabilities 
$$
p=(p_{11}, p_{12}, p_{21}, p_{22}),\quad\text{with}\quad\sum_{ij}p_{ij}=1,
$$
that is, 
$$
(Y_{11},Y_{12},Y_{21},Y_{22})^\top\sim\mathcal{M}_4(n, p).
$$
With $y_{ij}$ denoting the number of realizations in cell $ij$, the likelihood function can be written as 
$$
\ell(p|y)\propto\prod_{ij}p_{ij}^{y_{ij}},
$$
where the 4-dimensional parameter space can be displayed as following:
$$
\begin{array}{cc|c}
p_{11} & p_{12} & p_{1.}\\
p_{21} & p_{22} & p_{2.}\\
\hline
p_{.1} & p_{.2} & 1\\
\end{array}
$$



**Null Hypothesis:** The null hypothesis to be tested is one of independence, which is to say that the surgery treatment has no bearing on the control of cancer. Translated into a parameter statement this means
$$\text{H}_0: p_{11}=p_{1.}\,p_{.1}\quad\text{against}\quad\text{H}_1: p_{11}\neq p_{1.}\,p_{.1}.$$


The likelihood ratio statistic for testing this hypothesis is 
$$
\lambda=\frac{\max_{p\text{ s.t. }p_{11}=p_{1.}p_{.1}}\ell(p|y)}{\max_{p}\ell(p|y)}.
$$
It is "straightforward" to show that the denominator maximum is attained at:
$$
\hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all}\quad ij. 
$$
and the numerator maximum at
$$
\hat{p}_{11}=\hat{p}_{1.}\hat{p}_{.1}\quad\text{with}\quad \hat{p}_{1.}=\frac{n_{1.}}{n}\quad\text{and}\quad\hat{p}_{2.}=\frac{n_{2.}}{n},
$$
$$
\text{and} \quad \hat{p}_{ij}=\frac{n_{ij}}{n}\quad\text{for all other}\quad ij. 
$$

As mentioned above, under H$_0$, $-2\log \lambda$ is asymptotically distributed as $\chi^2_1$. However, with only $n=42$ observations, the asymptotics do not necessarily apply. One alternative is to use devise a **Monte Carlo experiment** to simulate the null distribution of $-2\log \lambda$ or equivalently of $\lambda$ in order to obtain a cutoff point for a hypothesis test. A more sophisticated approach is that of [Mehta at el. (2000)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2000.10473906).  

**Description of the Procedure:**

* Let's denote the finite $n$ null distribution of $\lambda$ by $f_{0,n}(.)$. As we are interested in an $\alpha$ level test, we need to specify $\alpha$ (e.g., $\alpha=0.05$) and to solve the following integral for the $1-\alpha$ quantile $\lambda_\alpha$:
$$
\int_0^{\lambda_\alpha}f_{0,n}(u)du=1-\alpha.
$$

* The standard Monte Carlo approach to this problem is to generate random variables $\lambda_k\sim f_{0,n}$, $k=1,\dots,m$, then order the sample 
$$
\lambda_{(1)}\leq \lambda_{(2)}\leq\dots\leq \lambda_{(m)}
$$
and finally calculate the empirical $1-\alpha$ quantile $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$, where $\lfloor x \rfloor$ is the next lower integer to $x$, e.g., $\lfloor 2.9 \rfloor=2$.

* Similarly to the above integration example which builds on the SLLN, the central idea here is to use the so-to-say SLLN for quantiles:
$$
\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}\to_{a.s.} \lambda_{\alpha}\quad\text{as}\quad m \to\infty;
$$
see, e.g., the classical book "Approximation Theorems of Mathematical Statistics" of R. Serfling Ch. 2.3.1. As a computer is doing this job for us, we can in principle choose an arbitrary large $n$ such that the above approximation of $\lambda_{\alpha}$ by $\lambda_{(\lfloor (1-\alpha)\,m\rfloor)}$ can be arbitrarily good. 

To run the 


::: {.cell hash='Ch2_MonteCarlo_cache/html/unnamed-chunk-8_007bedb57ed10c2dfe97d45b96ef5c0c'}

```{.r .cell-code}
set.seed(123)
#
p_init <- runif(4)
#
p_11   <- p_init[1]
p_12   <- p_init[2]
p_21   <- p_init[3]
p_22   <- p_init[4]
#
p_1.   <- p_11 + p_12
p_.1   <- p_11 + p_21
#
p_11   <- p_1. * p_.1

# probabilities under H0:
p_0 <- c(p_11, p_21, p_12, p_22)


Y_vec <- rmultinom(n=1, size=4, prob=p_0)
```
:::

-->
<!-- 
**Example: LR Test**

Let's assume we want to test the following regression model 
$$
Y_t=\beta_0 + \beta_1 t +\varepsilon_{t}
$$
against the constrained model without a time trend:
$$
Y_t=\beta_0 + \varepsilon_{t},
$$
where $\varepsilon_{t}\sim N(0,\sigma_\varepsilon^2)$ with $0<\varepsilon^2<\infty$.



::: {.cell hash='Ch2_MonteCarlo_cache/html/unnamed-chunk-9_ca81624119e21fc2b128118e96c78cb5'}

```{.r .cell-code}
LL_fun <- function(beta0, beta1, sigma) {
      # residuals
      res_vec   <-  y - x * beta1 - beta0
      # log-transformed probabilities of observing the vector of residuals res_vec: 
      Log_probs <- dnorm(x = R, mean = 0, sd = sigma, log = TRUE)
      # 
      result <- - sum(Log_probs)
      return(result)
}
```
:::

-->
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<!-- #### Exercise 3. {-} 

Consider the following faster version of the Box-Muller algorithm:<br> 
  **1. Step:** Generate 
  $$
  U_1, U_2\overset{\text{i.i.d.}}{\sim}\mathcal{U}[0,1] 
  $$
  until $S = U_1^2+ U_2^2 \leq 1.$<br>
  **2. Step:** Define 
  $$
  Z = \sqrt{-2\log(S)/S}
  $$
  and take 
  $$
  X_1 = Z U_1\quad\text{and}\quad X_2 = Z U_2
  $$

   (a) Show that $(U_1,U_2)$ is uniformly distributed over the unit sphere. 
   (b) Show that $X_1$ and $X_2$ are independent.  -->
<!-- MC Stat Methods. Problem 2.10. -->
<!-- 1. A possible algorithm for generating standard normal random variables from $n$ standard uniformly distributed random variables $U_1,\dots,U_n\sim\mathcal{U}[0,1],$ would be to use the Central Limit Theorem (CLT), i.e.
$$
Y_n=\frac{\sqrt{n}\left(\bar{U}_n - \mu\right)}{\sigma}\to_d \mathcal{N}(0,1).
$$
as $n\to\infty,$ where $\mathbb{E}(U_1)=\mu$ and $\mathbb{V}(U_1)=\sigma^2.$

Asses this "algorithm" critically by answering the following questions: 
 (a) What is the maximal range of values of the generated random variables $Y_n$?  
 (b) Compare the moments of $Y_n$ with those of $\mathcal{N}(0,1)$ using the moment generating function 
$$
 M_X(t) = \mathbb{E}\left(\exp\left(t X\right)\right)
$$ 
 Hint: For the case of $U\sim\mathcal{U}[0,1],$ the moment generating function is 
$$
 M_U(t) = \mathbb{E}\left(\exp\left(t U\right)\right)=\frac{\exp(t) - 1}{t}.
$$ 

::: {.callout-tip}
The distribution of a random variable $X$ can be characterized (completely) using any of the following functions (provided they exist):

* Density function $f_X$

* Distribution function $F_X(x)=\int_{-\infty}^x f_X(u)\,du$

* Moment generating function $M_X(t)=\mathbb{E}\left(\exp\left(t X\right)\right),$ where $t\in\mathbb{R}.$ The $j$th derivative of $M_X$ evaluated at $t=0$ gives the $j$th moment  
  $$
  \left.M_X^{(j)}(t)\right|_{t=0} = \mathbb{E}(X^j),\quad j=1,2,\dots 
  $$
:::
-->
<!-- {{< include Ch2_Solutions.qmd >}} -->
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Bourreau2021market" class="csl-entry" role="doc-biblioentry">
Bourreau, Marc, Yutec Sun, and Frank Verboven. 2021. <span>“Market Entry, Fighting Brands, and Tacit Collusion: Evidence from the French Mobile Telecommunications Market.”</span> <em>American Economic Review</em> 111 (11): 3459–99.
</div>
<div id="ref-Clopper_Pearson_1934" class="csl-entry" role="doc-biblioentry">
Clopper, Charles J, and Egon S Pearson. 1934. <span>“The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.”</span> <em>Biometrika</em> 26 (4): 404–13.
</div>
<div id="ref-Kenneth_Judd_Book_1998" class="csl-entry" role="doc-biblioentry">
Judd, Kenneth L. 1998. <em>Numerical Methods in Economics</em>. MIT press.
</div>
<div id="ref-Ripley_2009" class="csl-entry" role="doc-biblioentry">
Ripley, Brian D. 2009. <em>Stochastic Simulation</em>. John Wiley &amp; Sons. <a href="http://onlinelibrary.wiley.com/book/10.1002/9780470316726">http://onlinelibrary.wiley.com/book/10.1002/9780470316726</a>.
</div>
<div id="ref-RobertCasella1999" class="csl-entry" role="doc-biblioentry">
Robert, Christian P, and George Casella. 2004. <em>Monte Carlo Statistical Methods</em>. 2nd ed. Springer Texts in Statistics. Springer.
</div>
<div id="ref-RobertCasella2010" class="csl-entry" role="doc-biblioentry">
———. 2010. <em>Introducing Monte Carlo Methods with r</em>. 1st ed. Use r! Springer.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch1_Random_Variable_Generation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch3_Bootstrap.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>