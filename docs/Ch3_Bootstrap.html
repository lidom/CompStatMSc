<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computational Statistics (M.Sc.) - 3&nbsp; The Bootstrap</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch4_MaximumLikelihood.html" rel="next">
<link href="./Ch2_MonteCarlo.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Statistics (M.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Random_Variable_Generation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_MonteCarlo.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Bootstrap.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_MaximumLikelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_EMAlgorithmus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Expectation Maximization (EM) Algorithm</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-empirical-distribution-function" id="toc-the-empirical-distribution-function" class="nav-link active" data-scroll-target="#the-empirical-distribution-function"><span class="toc-section-number">3.1</span>  The Empirical Distribution Function</a></li>
  <li><a href="#basic-idea-of-the-bootstrap" id="toc-basic-idea-of-the-bootstrap" class="nav-link" data-scroll-target="#basic-idea-of-the-bootstrap"><span class="toc-section-number">3.2</span>  Basic Idea of the Bootstrap</a></li>
  <li><a href="#the-nonparametric-standard-bootstrap" id="toc-the-nonparametric-standard-bootstrap" class="nav-link" data-scroll-target="#the-nonparametric-standard-bootstrap"><span class="toc-section-number">3.3</span>  The Nonparametric (Standard) Bootstrap</a>
  <ul class="collapse">
  <li><a href="#the-bootstrap-algorithm" id="toc-the-bootstrap-algorithm" class="nav-link" data-scroll-target="#the-bootstrap-algorithm">The Bootstrap Algorithm</a></li>
  <li><a href="#bootstrap-consistency" id="toc-bootstrap-consistency" class="nav-link" data-scroll-target="#bootstrap-consistency">Bootstrap Consistency</a></li>
  <li><a href="#example-inference-about-the-population-mean" id="toc-example-inference-about-the-population-mean" class="nav-link" data-scroll-target="#example-inference-about-the-population-mean"><span class="toc-section-number">3.3.1</span>  Example: Inference About the Population Mean</a></li>
  <li><a href="#example-inference-about-a-population-proportion" id="toc-example-inference-about-a-population-proportion" class="nav-link" data-scroll-target="#example-inference-about-a-population-proportion"><span class="toc-section-number">3.3.2</span>  Example: Inference about a Population Proportion</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="toc-section-number">3.3.3</span>  Confidence Intervals</a></li>
  </ul></li>
  <li><a href="#pivot-statistics-and-the-bootstrap-t-method" id="toc-pivot-statistics-and-the-bootstrap-t-method" class="nav-link" data-scroll-target="#pivot-statistics-and-the-bootstrap-t-method"><span class="toc-section-number">3.4</span>  Pivot Statistics and the Bootstrap-<span class="math inline">\(t\)</span> Method</a>
  <ul class="collapse">
  <li><a href="#bootstrap-t-confidence-interval" id="toc-bootstrap-t-confidence-interval" class="nav-link" data-scroll-target="#bootstrap-t-confidence-interval"><span class="toc-section-number">3.4.1</span>  Bootstrap-t Confidence Interval</a></li>
  <li><a href="#accuracy-of-the-bootstrap-t-method" id="toc-accuracy-of-the-bootstrap-t-method" class="nav-link" data-scroll-target="#accuracy-of-the-bootstrap-t-method"><span class="toc-section-number">3.4.2</span>  Accuracy of the Bootstrap-<span class="math inline">\(t\)</span> method</a></li>
  </ul></li>
  <li><a href="#regression-analysis-bootstrapping-pairs" id="toc-regression-analysis-bootstrapping-pairs" class="nav-link" data-scroll-target="#regression-analysis-bootstrapping-pairs"><span class="toc-section-number">3.5</span>  Regression Analysis: Bootstrapping Pairs</a>
  <ul class="collapse">
  <li><a href="#bootstrapping-pairs-bootstrap-under-random-design" id="toc-bootstrapping-pairs-bootstrap-under-random-design" class="nav-link" data-scroll-target="#bootstrapping-pairs-bootstrap-under-random-design"><span class="toc-section-number">3.5.1</span>  Bootstrapping Pairs: Bootstrap under Random Design</a></li>
  </ul></li>
  <li><a href="#regression-analysis-residual-bootstrap" id="toc-regression-analysis-residual-bootstrap" class="nav-link" data-scroll-target="#regression-analysis-residual-bootstrap"><span class="toc-section-number">3.6</span>  Regression Analysis: Residual bootstrap</a>
  <ul class="collapse">
  <li><a href="#bootstrap-confidence-intervals-for-the-regression-coefficients" id="toc-bootstrap-confidence-intervals-for-the-regression-coefficients" class="nav-link" data-scroll-target="#bootstrap-confidence-intervals-for-the-regression-coefficients"><span class="toc-section-number">3.6.1</span>  Bootstrap confidence intervals for the regression coefficients</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- TO-DO: 
1. Rework this chapter using the overview article of Horowitz
BOOTSTRAP METHODS IN ECONOMETRICS 
2. Remove the fraction estimator parts 
-->
<p>The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.</p>
<p>Some literature:</p>
<ul>
<li><span class="citation" data-cites="Hall_1992">Hall (<a href="#ref-Hall_1992" role="doc-biblioref">1992</a>)</span>: The Bootstrap and Edgeworth Expansion</li>
<li><span class="citation" data-cites="Efron_Tibshirani_1994">Efron and Tibshirani (<a href="#ref-Efron_Tibshirani_1994" role="doc-biblioref">1994</a>)</span>: An Introduction to the Bootstrap</li>
<li><span class="citation" data-cites="Shao_Tu_1996">Shao and Tu (<a href="#ref-Shao_Tu_1996" role="doc-biblioref">1996</a>)</span>: The Jackknife and Bootstrap</li>
<li><span class="citation" data-cites="Horowitz_2001">Horowitz (<a href="#ref-Horowitz_2001" role="doc-biblioref">2001</a>)</span>: The Bootstrap. In: Handbook of Econometrics</li>
<li><span class="citation" data-cites="Davison_Hinkley_2013">Davison and Hinkley (<a href="#ref-Davison_Hinkley_2013" role="doc-biblioref">2013</a>)</span>: Bootstrap Methods and their Applications</li>
</ul>
<section id="the-empirical-distribution-function" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-empirical-distribution-function"><span class="header-section-number">3.1</span> The Empirical Distribution Function</h2>
<p>The distribution of a real-valued random variable <span class="math inline">\(X\)</span> can be completely described by its distribution function <span class="math display">\[
F(x)=P(X\leq x)\quad \text{for all } x\in\mathbb{R}.
\]</span></p>
<p>For given data, the sample analogue of <span class="math inline">\(F\)</span> is the so-called <strong>empirical distribution function</strong>, which is an important tool of statistical inference.</p>
<p><strong>Data:</strong> i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> from <span class="math inline">\(X\sim F\)</span></p>
<p>Let <span class="math inline">\(1_{(\cdot)}\)</span> denote the indicator function, i.e., <span class="math inline">\(1_{(x\leq t)}=1\)</span> if <span class="math inline">\(x\leq t\)</span>, and <span class="math inline">\(1_{(x\leq t)}=0\)</span> if <span class="math inline">\(x&gt;t.\)</span></p>
<div id="def-ecdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Empirical distribution function) </strong></span><span class="math display">\[
F_n(x)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> I.e <span class="math inline">\(F_n(x)\)</span> is the proportion of observations with <span class="math inline">\(X_i\le x,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
<p><strong>Properties:</strong></p>
<ul>
<li><span class="math inline">\(0\le F_n(x)\le 1\)</span></li>
<li><span class="math inline">\(F_n(x)=0,\)</span> if <span class="math inline">\(x&lt;X_{(1)}\)</span>, where <span class="math inline">\(X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}\)</span> denotes the <strong>order-statistic</strong>.</li>
<li><span class="math inline">\(F_n(x)=1,\)</span> if <span class="math inline">\(x\ge X_{(n)}\)</span>, where <span class="math inline">\(X_{(n)}\)</span> is largest observation</li>
<li><span class="math inline">\(F_n\)</span> is a monotonically increasing step function</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Structurally, <span class="math inline">\(F_n\)</span> itself is a distribution function. <span class="math inline">\(F_n\)</span> is the distribution function of a <strong>discrete random variable</strong> <span class="math inline">\(X^*\)</span></p>
<ul>
<li>with possible values <span class="math inline">\(X^*\in\{X_1,\dots,X_n\}\)</span> and</li>
<li>with <span class="math inline">\(P(X^*=X_i)=\frac{1}{n}\)</span> for each <span class="math inline">\(i=1,\dots,n.\)</span></li>
</ul>
</div>
</div>
<div id="exm-ecdfexample" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Empirical distribution function) </strong></span><br></p>
<p>Some data:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.20</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.80</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.40</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.60</td>
</tr>
<tr class="odd">
<td>5</td>
<td>6.10</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.40</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.80</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
</tr>
</tbody>
</table>
<p>Corresponding empirical distribution function using <code>R</code>:</p>
<div class="cell" data-hash="Ch3_Bootstrap_cache/html/ecdfPlot_93f9239fda49e47f1862b4eb6bd0cc52">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">5.20</span>, <span class="fl">4.80</span>, <span class="fl">5.30</span>, <span class="fl">4.60</span>, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">6.10</span>, <span class="fl">5.40</span>, <span class="fl">5.80</span>, <span class="fl">5.50</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>myecdf_fun     <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(observedSample)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myecdf_fun, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/ecdfPlot-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<p><span class="math inline">\(F_n(x)\)</span> depends on the i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> and thus is itself a <strong>random function</strong>.</p>
<p>We obtain</p>
<ul>
<li>For every <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
nF_n(x)\sim B(n, p=F(x))
\]</span> I.e., <span class="math inline">\(nF_n(x)\)</span> has a binomial distribution with parameters <span class="math inline">\(n\)</span> (“number of trials”) and <span class="math inline">\(p=F(x)\)</span> (“probability of success on a single trial”).</li>
<li><span class="math inline">\(\mathbb{E}(F_n(x))=F(x)\)</span></li>
<li><span class="math inline">\(\mathbb{V}(F_n(x))=\frac{F(x)(1-F(x))}{n}\)</span></li>
</ul>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-Clivenko-Cantelli" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Theorem of Glivenko-Cantelli) </strong></span><span class="math display">\[
P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1
\]</span></p>
</div>
</div>
</div>
</section>
<section id="basic-idea-of-the-bootstrap" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="basic-idea-of-the-bootstrap"><span class="header-section-number">3.2</span> Basic Idea of the Bootstrap</h2>
<p>The basic idea of the bootstrap is to replace random sampling from the true (unknown) population <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation) by random sampling from the empirical distribution <span class="math inline">\(F_n\)</span> (feasible Monte Carlo simulation).</p>
<p><strong>Population distribution <span class="math inline">\(F\)</span>:</strong> The random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}F\)</span> is generated by drawing observations independently and with replacement from the unknown population distribution function <span class="math inline">\(F\)</span>. That is, for each interval <span class="math inline">\([a,b]\)</span> the probability of drawing an observation in <span class="math inline">\([a,b]\)</span> is given by <span class="math display">\[
P(X\in [a,b])=F(b)-F(a).
\]</span></p>
<p><strong>Empirical distribution <span class="math inline">\(F_n\)</span>:</strong> For large <span class="math inline">\(n,\)</span> the empirical distribution <span class="math inline">\(F_n\)</span> of the sample values is “close” to the unknown distribution <span class="math inline">\(F\)</span> (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli">Theorem&nbsp;<span>3.1</span></a>). That is, for <span class="math inline">\(n\rightarrow\infty\)</span> the relative frequency of observations <span class="math inline">\(X_i\)</span> in <span class="math inline">\([a,b]\)</span> converges to <span class="math inline">\(P(X\in [a,b])\)</span><br>
<span class="math display">\[
  \begin{align*}
  \underbrace{\frac{1}{n}\sum_{i=1}^n1_{(X_i\in[a,b])}}_{=F_n(b)-F_n(a)}&amp;\to_p \underbrace{P(X\in [a,b])}_{=F(b)-F(a)}
  \end{align*}
\]</span></p>
<p><strong>The idea of the bootstrap</strong> consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution <span class="math inline">\(F,\)</span> the bootstrap uses random sampling from the known <span class="math inline">\(F_n.\)</span> This is justified by the insight that the empirical distribution <span class="math inline">\(F_n\)</span> of the observed data is “similar” to the true distribution <span class="math inline">\(F\)</span> (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli">Theorem&nbsp;<span>3.1</span></a>).</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bradley Efron
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap method is attributed to <a href="https://statistics.stanford.edu/people/bradley-efron">Bradley Efron</a>, who received the <em><a href="https://statsandstories.net/methods/2018/9/28/bootstrapping-an-international-prize">International Prize in Statistics</a></em> (the Nobel price of statistics) for his seminal works on the bootstrap method.</p>
</div>
</div>
</section>
<section id="the-nonparametric-standard-bootstrap" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="the-nonparametric-standard-bootstrap"><span class="header-section-number">3.3</span> The Nonparametric (Standard) Bootstrap</h2>
<p><strong>Setup:</strong></p>
<ul>
<li>i.i.d. sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> from <span class="math inline">\(X\sim F\)</span>.</li>
<li>The distribution <span class="math inline">\(X_i\sim F,\)</span> <span class="math inline">\(i=1,\dots,n,\)</span> depends on an unknown parameter (vector) <span class="math inline">\(\theta.\)</span></li>
<li>The data <span class="math inline">\(X_1,\dots,X_n\)</span> is used to estimate an element <span class="math inline">\(\theta\)</span> of the parameter vector <span class="math inline">\(\theta.\)</span><br>
</li>
<li>Thus, the estimator is a function of the random sample <span class="math display">\[
\hat\theta\equiv \hat\theta(X_1,\dots,X_n).
\]</span></li>
</ul>
<p><strong>Inference:</strong> We are interested in evaluating the distribution of <span class="math display">\[
\hat\theta-\theta
\]</span> in order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis.</p>
<section id="the-bootstrap-algorithm" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-bootstrap-algorithm">The Bootstrap Algorithm</h3>
<ol type="1">
<li><strong>Draw a bootstrap sample:</strong> Generate a new random sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> by drawing observations independently and with replacement from the available sample <span class="math inline">\(X_1,\dots,X_n.\)</span></li>
<li><strong>Compute bootstrap estimate:</strong> Compute the estimate <span class="math display">\[
\hat\theta^*\equiv \hat\theta(X_1^*,\dots,X_n^*)
\]</span></li>
<li><strong>Bootstrap replications:</strong> Repeat Steps 1 and 2 <span class="math inline">\(m\)</span> times (e.g.&nbsp;<span class="math inline">\(m=2000\)</span>) leading to <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*
\]</span></li>
</ol>
<p>For large <span class="math inline">\(m\)</span>, the estimates <span class="math inline">\(\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*\)</span> allow to approximate the <strong>bootstrap distribution</strong> of <span class="math inline">\(\hat\theta^*-\hat\theta\)</span> arbitrarily well.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap distribution of <span class="math display">\[
\hat\theta^*-\hat\theta
\]</span> is used to approximate the unknown distribution of <span class="math display">\[
\hat\theta-\theta.
\]</span></p>
<p>Note: For the bootstrap distribution <span class="math inline">\(\hat\theta\)</span> is a “population parameter”.</p>
</div>
</div>
<p>The theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are <strong>more accurate</strong> than those based on standard asymptotic approximations.</p>
</section>
<section id="bootstrap-consistency" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="bootstrap-consistency">Bootstrap Consistency</h3>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Caution: The bootstrap does <em>not</em> always work
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap does <strong>not always work</strong>. A necessary condition for the use of the bootstrap is the <strong>consistency of the bootstrap approximation</strong>.</p>
</div>
</div>
<p>The bootstrap is called <strong>consistent</strong> if, for large <span class="math inline">\(n\)</span>, the bootstrap distribution of <span class="math inline">\(\hat{\theta}^* -\hat{\theta}\)</span> is a good approximation of the underlying distribution of <span class="math inline">\(\hat{\theta}-\theta\)</span>, i.e. <span class="math display">\[
\text{distribution}(\hat{\theta}^* -\hat{\theta}\ |{\cal S}_n)\approx
\text{distribution}(\hat{\theta}-\theta).
\]</span> The following definition states this more precisely.</p>
<div id="def-BootstrapConsistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Bootstrap consistency) </strong></span><em>If for some <span class="math inline">\(\gamma&gt;0\)</span> (usually: <span class="math inline">\(\gamma=1/2\)</span>) we have <span class="math inline">\(n^\gamma(\hat{\theta}-\theta)\rightarrow_d Z\)</span> for some random variable <span class="math inline">\(Z\)</span> with a non-degenerate distribution, then the bootstrap is <strong>consistent</strong> if and only if <span class="math display">\[
\sup_\delta \left|
   P\left(n^\gamma(\hat\theta^*-\hat\theta)\le \delta|{\cal S}_n\right)
  -P\left(n^\gamma(\hat\theta      -\theta)\le \delta\right)
  \right|\rightarrow_p 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></em></p>
</div>
<p>Luckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are <strong>some crucial requirements</strong>:</p>
<ol type="1">
<li>Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).</li>
<li>The distribution of the estimator <span class="math inline">\(\hat\theta\)</span> needs to be asymptotically normal.</li>
</ol>
<p>The standard bootstrap <strong>will usually fail</strong> if one of the above conditions 1 or 2 is violated. For instance,</p>
<ul>
<li>The bootstrap will not work if the i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(X_1,\dots,X_n\)</span> does not properly reflect the way how the <span class="math inline">\(X_1,\dots,X_n\)</span> are generated when <span class="math inline">\(X_1,\dots,X_n\)</span> is a time-series with auto-correlated data.</li>
<li>The distribution of the estimator <span class="math inline">\(\hat\theta\)</span> is not asymptotically normal. (For instance, in case of extreme value problems.)</li>
</ul>
<p><strong>Note:</strong> In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g.&nbsp;the block-bootstrap in case of time-series data).</p>
</section>
<section id="example-inference-about-the-population-mean" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="example-inference-about-the-population-mean"><span class="header-section-number">3.3.1</span> Example: Inference About the Population Mean</h3>
<p><strong>Setup:</strong></p>
<ul>
<li>i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span> from <span class="math inline">\(X\sim F.\)</span></li>
<li>Continuous random variable <span class="math inline">\(X\sim F\)</span> with unknown <span class="math inline">\(F\)</span> and thus unknown mean <span class="math display">\[
\mu = \int x f(x) dx = \int x d F(x),\]</span> where <span class="math inline">\(f=F'.\)</span></li>
<li>Estimator: empirical mean <span class="math display">\[
\begin{align*}
\bar{X}
&amp;\equiv \bar{X}(X_1,\dots,X_n) \\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n X_i \\[2ex]
&amp;=\int x d F_n(x)
\end{align*}
\]</span></li>
</ul>
<p><strong>Inference Problem:</strong> What is the distribution of <span class="math display">\[
\bar{X} -\mu?
\]</span></p>
<p>Now assume that <span class="math inline">\(n=8\)</span> and that the <strong>observed sample</strong> is</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>-0.6</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.4</td>
</tr>
<tr class="even">
<td>4</td>
<td>-0.8</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.6</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>-0.1</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<div class="cell" data-hash="Ch3_Bootstrap_cache/html/unnamed-chunk-1_0f5f5a4c86b5e9cf771df83d09ea6ca5">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.6</span>, <span class="fl">1.0</span>,  <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.8</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                     <span class="fl">1.6</span>, <span class="fl">1.9</span>, <span class="sc">-</span><span class="fl">0.1</span>,  <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
So the sample mean is
<center>
<span class="math inline">\(\bar X =\)</span> <code>mean(observedSample)</code> <span class="math inline">\(=\)</span> 0.6375
</center>
<p><br></p>
<p><strong>Bootstrap:</strong></p>
<p>The observed sample <span class="math display">\[
{\cal S}_n=\{X_1,\dots,X_n\}
\]</span> is taken as underlying empirical “population” in order to generate the <strong>bootstrap sample</strong> <span class="math inline">\(X_1^*,\dots,X_n^*\)</span>:</p>
<ul>
<li>i.i.d. samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> are generated by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span>.</li>
</ul>
<div class="cell" data-hash="Ch3_Bootstrap_cache/html/unnamed-chunk-2_e9d8265e6352bcf0b8d248d8f6f56895">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generating a bootstrap sample</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>bootSample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">size    =</span> <span class="fu">length</span>(observedSample), </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>The distribution of <span class="math display">\[
\bar X -\mu
\]</span> is approximated by the conditional distribution of <span class="math display">\[
\bar X^* -\bar X,
\]</span> given the original sample <span class="math inline">\({\cal S}_n,\)</span> i.e.&nbsp;more formally <span class="math display">\[
\underbrace{P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)}_{\text{approximable}}
\approx
\underbrace{P\left(\bar{X}-\mu&lt;\delta\right)}_{\text{unknown}}.
\]</span></li>
</ul>
<p>For the given data with <span class="math inline">\(n=8\)</span> observations, there are <span class="math display">\[
n^n=8^8=16,777,216
\]</span> possible bootstrap samples which are all equally probable.</p>
<p>The conditional distribution function of <span class="math inline">\(\bar{X}^*-\bar{X}\)</span> given <span class="math inline">\(\mathcal{S}_n\)</span> <span class="math display">\[
P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)
\]</span> can be approximated using a Monte-Carlo simulation. For this, we draw new data <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(F_n,\)</span> i.e., we sample with replacement data points from the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p>
<p>Using a large number <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) of simulation runs allows us to generate bootstrap estimates <span class="math display">\[
\bar{X}^*_1,\bar{X}^*_2,\dots,\bar{X}^*_m
\]</span></p>
<!-- Simul. | $X_1^*$|  $X_2^*$| $X_3^*$| $X_4^*$|  $X_5^*$|  $X_6^*$|  $X_7^*$|   $X_8^*$ | $\bar X^*-\bar{X}$
----|----:|----:|----:|----:|----:|----:|----:|----:|:----:
1 | 1.9| -0.8| 1.9|  -0.6| 1.4| -0.1| -0.8| 1.0 | -0.15
2 | 0.7| -0.8| -0.8| 1.0 | 1.6| 1.0| -0.1| -0.8 | -0.4125  
3 | -0.1| 1.9| 0.7|  1.0| -0.1| 1.6| 1.0| -0.6 |  0.0375 
4 | 1.4| 1.0| 1.4|  -0.1| 1.9| -0.8| 1.9| 1.0 | 0.325 
5 | 1.0| 0.7| -0.1|  0.7| 1.4| -0.8| 1.0| 1.6 |  0.05
... ||||||||| -->
<p>These bootstrap estimates are then used to approximate the bootstrap distribution <span class="math display">\[
%\overbrace{
  \underbrace{P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right)}_{\text{bootstrap distribution}}%}^{=P^*\left(\bar X^*-\bar X\leq \delta\right)}
\approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
\]</span> where this approximation will be <strong>arbitrarily precise</strong> as <span class="math inline">\(m\to\infty.\)</span> (So, we can effectively ignore this type of approximation error.)</p>
<div class="cell" data-hash="Ch3_Bootstrap_cache/html/unnamed-chunk-3_4d454b59edc79e6b5c2a5832a627004d">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n                <span class="ot">&lt;-</span> <span class="fu">length</span>(observedSample)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Xbar             <span class="ot">&lt;-</span> <span class="fu">mean</span>(observedSample)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>m                <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># number of bootstrap samples </span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>Xbar_boot        <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"double"</span>, <span class="at">length =</span> m)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="fu">seq_len</span>(m)){</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a> bootSample          <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">size    =</span> n, </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a> Xbar_boot[k]        <span class="ot">&lt;-</span> <span class="fu">mean</span>(bootSample)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>( Xbar_boot <span class="sc">-</span> Xbar ), </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Approximate Bootstrap Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate, for instance, the standard error of <span class="math inline">\(\bar{X},\)</span> we can now simply use the <strong>empirical standard deviation</strong> of <span class="math inline">\(\bar{X}^*_k,\)</span> <span class="math inline">\(k=1,\dots,m.\)</span></p>
<div class="cell" data-hash="Ch3_Bootstrap_cache/html/unnamed-chunk-4_6c9d31826ede111a8b435799f8bace68">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(Xbar_boot), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.34</code></pre>
</div>
</div>
<!-- to test the null-hypothesis 
<center>
H$_0:$ $\mu = \mu_0$
</center>
against the alternative hypothesis that 
<center>
H$_1:$ $\mu \neq \mu_0$
</center>
we can use the simulated distribution of $\bar{X}^*_k,$ $k=1,\dots,m.$ 

Under H$_0,$ we know that the true (unknown) mean equals the hypothetical mean $\mu = \mu_0,$ and we know that $\bar{X}\approx \mu$ if $n$ is large.   -->
<!-- 

m         <- 10000 # number of bootstrap samples
Xbar_boot <- vector(model = "double", length = m)

for(k in 1:m){
  bootSample   <- sample(x       = observedSample, 
                         size    = length(observedSample), 
                         replace = TRUE)
  Xbar_boot[k] <- mean(bootSample)
}

## Some (approximate) quantiles of the bootstrap distribution 
quantile(Xbar_boot, probs = seq(from = 0, to = 1,  by = 0.25))
``` -->
<section id="theory-the-bootstrap-distribution-of-bar-x--barx" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="theory-the-bootstrap-distribution-of-bar-x--barx">Theory: The Bootstrap Distribution of <span class="math inline">\(\bar X^*- \bar{X}\)</span></h4>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation <span class="math inline">\(\mathbb{E}^*(\cdot),\)</span> <span class="math inline">\(\mathbb{V}^*(\cdot),\)</span> and <span class="math inline">\(P^*(\cdot)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the bootstrap literature one frequently finds the notation <span class="math display">\[
\mathbb{E}^*(\cdot),\;\mathbb{V}^*(\cdot),\;\text{and}\;P^*(\cdot)
\]</span> to denote the <strong>conditional</strong> expectation <span class="math display">\[
\mathbb{E}^*(\cdot)=\mathbb{E}(\cdot|\mathcal{S}_n),
\]</span> the <strong>conditional</strong> variance <span class="math display">\[
\mathbb{V}^*(\cdot)=\mathbb{V}(\cdot|\mathcal{S}_n),
\]</span> and the <strong>conditional</strong> probability <span class="math display">\[
P^*(\cdot)=P(\cdot|\mathcal{S}_n),
\]</span> given the sample <span class="math inline">\({\cal S}_n.\)</span></p>
</div>
</div>
<p>The bootstrap focuses on the <strong>conditional</strong> distribution of <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> given the observed sample <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span> and the resulting conditional distribution of <span class="math display">\[
(\bar X^* -\bar X)|\mathcal{S}_n.
\]</span> These conditional distributions are usually called <strong>bootstrap distributions</strong>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
We know the distribution of <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can analyze the bootstrap distribution of <span class="math inline">\(\bar X^* -\bar X\)</span>, since <strong>we <em>know</em> 🤟 the discrete distribution</strong> of the conditional random variables <span class="math display">\[
X_i^*|\mathcal{S}_n,\;i=1,\dots,n,
\]</span> even though, we do <strong>not know</strong> the distribution of <span class="math inline">\(X_i\sim F,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
<!-- The discrete distribution of the conditional random variables 
$X_i^*|\mathcal{S}_n,$ is 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\}
$$
with  -->
</div>
</div>
<p>For each <span class="math inline">\(i=1,\dots,n\)</span>, the possible values of the discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> are <span class="math display">\[
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
\]</span> and each of these values is equally probable <span class="math display">\[
\begin{align*}
P^*(X_i^*=X_1)&amp;= P(X_i^*=X_1|{\cal S}_n) = \frac{1}{n} \\[2ex]
P^*(X_i^*=X_2)&amp;= P(X_i^*=X_2|{\cal S}_n) = \frac{1}{n} \\[2ex]
&amp;\vdots\\[2ex]
P^*(X_i^*=X_n)&amp;= P(X_i^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
\]</span></p>
<p>Thus, <strong>we know the whole distribution</strong> of the (conditional) discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> and, therefore, can compute, for instance, easily its conditional mean and its variance.</p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*(X_i^*)
&amp;=\mathbb{E}(X_i^*|{\cal S}_n)\\[2ex]
&amp;=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_n\\[2ex]
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{V}^*(X_i^*)
&amp;=\mathbb{V}(X_i^*|{\cal S}_n)\\[2ex]
&amp;=\mathbb{E}((X_i^* - \mathbb{E}(X_i^*|{\cal S}_n))^2|{\cal S}_n)\\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\\[2ex]
&amp;=\hat\sigma^2
\end{align*}
\]</span></p></li>
</ul>
<p>That is, in the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> the “population” mean and the “population” variance are equal to the empirical mean, <span class="math inline">\(\bar{X},\)</span> and the empirical variance, <span class="math inline">\(\hat{\sigma}^2,\)</span> of the original sample <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
General case: Conditional moments of transformed <span class="math inline">\(g(X_i^*)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any (measurable) function <span class="math inline">\(g\)</span> we have <span class="math display">\[
\mathbb{E}^*(g(X_i^*))=\mathbb{E}(g(X_i^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
\]</span> For instance, <span class="math inline">\(g(X_i)=1_{(X_i\leq \delta)}.\)</span></p>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Caution: Conditioning on <span class="math inline">\(\mathcal{S}_n\)</span> in important!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Conditioning on the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> is very important.</p>
<p>The unconditional distribution of <span class="math inline">\(X_i^*\)</span> is equal to the <strong>unknown distribution</strong> <span class="math inline">\(F\)</span> of <span class="math inline">\(X_i.\)</span> This can be seen from the following derivation: <span class="math display">\[
\begin{align*}
P(X_i^*\leq \delta)
&amp;= P(1_{(X_i^*\leq \delta)}=1) \\[2ex]
&amp;= P(1_{(X_i^*\leq \delta)}=1) \cdot 1 + P(1_{(X_i^*\leq \delta)}=0) \cdot 0\\[2ex]
&amp;= E\left(1_{\left(X_i^*\leq \delta\right)}\right)\\[2ex]
&amp;= E\left[E\left(1_{\left(X_i^*\leq \delta\right)}|\mathcal{S}_n\right)\right]\\[2ex]
&amp;= E\left[\frac{1}{n}\sum_{i=1}^n 1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&amp;= \frac{n}{n}E\left[1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&amp;= P\left(X_i\leq \delta\right)=F(\delta)
\end{align*}
\]</span></p>
</div>
</div>
<p><strong>Now consider the bootstrap distribution of <span class="math inline">\(\bar X^*\)</span></strong></p>
<!-- We know the distribution of the i.i.d. random variables 
$$
X_i^*|\mathcal{S}_n, \quad i=1,\dots,n,
$$
it is straight forward to derive the asymptotic distribution of 
$$
(\bar X^*-\bar{X})|\mathcal{S}_n
$$ 
using the central limit theorem.  -->
<p>Firstly, let us derive the conditional mean and variance of <span class="math display">\[
\bar X^* = \frac{1}{n}\sum_{i=1}^nX_i^*.
\]</span></p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*(\bar X^*)
&amp;=\mathbb{E}(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n\mathbb{E}(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n \bar X\\
&amp;=\frac{n}{n}\bar X \\
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{V}^*(\bar X^*)
&amp;=\mathbb{V}(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n \mathbb{V}(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n \hat\sigma^2\\
&amp;=\frac{n}{n^2}\hat\sigma^2\\
&amp;=\frac{1}{n}\hat\sigma^2,
\end{align*}
\]</span> where <span class="math inline">\(\hat{\sigma}=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(X_i - \bar{X}\right)^2}.\)</span></p></li>
</ul>
<!-- Next, we can check whether we can apply the classical Lindeberg-Lévy CLT. 

::: {.callout-tip}

## Good news: We can apply the CLT to $\bar X^*|\mathcal{S}_n$

Conditionally on ${\cal S}_n=\{X_1,\dots,X_n\}$,  

* the random variables $X_1^*,\dots,X_n^*$ are i.i.d.
* with mean $\mathbb{E}^*(X_i^*)=\bar X$ 
* and variance $\mathbb{V}^*(X^*)=\hat\sigma^2$

Thus, we can apply the [central limit theorem (Lindeberg-Lévy)](https://www.statlect.com/asymptotic-theory/central-limit-theorem) to the appropriately standardized bootstrap sample mean $\hat{X}^*$ conditionally on ${\cal S}_n$
$$
\left(\left.\frac{\sqrt{n}(\bar X^* - \bar X)}{\hat\sigma}\right|{\cal S}_n\right)
$$
::: -->
<p>An appropriate central limit theorem argument implies that <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>Moreover, <span class="math inline">\(\hat\sigma^2\)</span> is a consistent estimator of <span class="math inline">\(\sigma^2,\)</span> and thus asymptotically <span class="math inline">\(\hat\sigma^2\)</span> may be replaced by <span class="math inline">\(\sigma\)</span>. Therefore, <span class="math display">\[
\begin{align*}
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X^* -\bar X)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
\]</span></p>
<p>On the other hand, by the CLT, we also have that <span class="math display">\[
\begin{align*}
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right)
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X - \mu)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
\]</span></p>
<p>This means that the bootstrap is <strong>consistent</strong>, since the bootstrap distribution of <span class="math display">\[
\sqrt{n}(\bar X^* -\bar X)|{\cal S}_n
\]</span> asymptotically <span class="math inline">\((n\rightarrow\infty)\)</span> coincides with the distribution of <span class="math display">\[
\sqrt{n}(\bar X-\mu).
\]</span> In other words, for large <span class="math inline">\(n\)</span>, <span class="math display">\[
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
\]</span></p>
<p>This bootstrap consistency result justifies using the bootstrap distribution <span class="math display">\[
P(\bar{X}^*-\bar{X}\leq \delta|\mathcal{S}_n) \approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
\]</span> which we can approximate (arbitrary precise as <span class="math inline">\(m\to\infty\)</span>) using the bootstrap realizations <span class="math display">\[
\bar{X}^*_1,\;\bar{X}^*_2, \dots, \bar{X}^*_m.
\]</span></p>
</section>
</section>
<section id="example-inference-about-a-population-proportion" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="example-inference-about-a-population-proportion"><span class="header-section-number">3.3.2</span> Example: Inference about a Population Proportion</h3>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n,\)</span> where <span class="math inline">\(X_i\in\{0,1\}\)</span> is dichotomous and <span class="math inline">\(P(X_i=1)=p\)</span>, <span class="math inline">\(P(X_i=0)=1-p\)</span>.</li>
<li><strong>Estimator:</strong> Let <span class="math display">\[
S=\sum_{i=1}^n 1_{(X_i = 1)}
\]</span> denote the number of <span class="math inline">\(X_i\)</span> which are equal to <span class="math inline">\(1.\)</span> Then, the maximum likelihood estimate of <span class="math inline">\(p\)</span> is <span class="math display">\[
\hat p=\frac{1}{n}S.
\]</span></li>
<li><strong>Inference Problem:</strong> What is the distribution of <span class="math display">\[
(\hat{p} - p)?
\]</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall Asymptotics:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><span class="math inline">\(n\hat p=S\sim \mathcal{Binom}(n,p)\)</span></li>
<li>As <span class="math inline">\(n\rightarrow\infty,\)</span> the central limit theorem implies that <span class="math display">\[
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
\]</span> Thus for <span class="math inline">\(n\)</span> large, the distributions of <span class="math inline">\(\sqrt{n}(\hat p -p)\)</span> and <span class="math inline">\(\hat p -p\)</span> can be approximated by <span class="math inline">\(\mathcal{N}(0,p(1-p))\)</span> and <span class="math inline">\(\mathcal{N}(0,p(1-p)/n)\)</span>, respectively.</li>
</ul>
</div>
</div>
<p><strong>Bootstrap Approach:</strong></p>
<ul>
<li>Random sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> generated by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{X_1,\dots,X_n\}.
\]</span></li>
<li>Let <span class="math display">\[
S^*=\sum_{i=1}^n 1_{(X_i^* = 1)}
\]</span><br>
denote the number of <span class="math inline">\(X_i^*\)</span> which are equal to <span class="math inline">\(1.\)</span></li>
<li>Bootstrap estimate of <span class="math inline">\(p\)</span>: <span class="math display">\[
\hat p^*=\frac{1}{n}S^*
\]</span></li>
</ul>
<!-- The distribution of $\hat p^*$ depends on the observed sample ${\cal S}_n:=\{X_1,\dots,X_n\}$. That is, a different sample ${\cal S}_n$ will lead to a different distribution.  -->
<p>The bootstrap now tries to approximate the true distribution of <span class="math inline">\(\hat p - p\)</span> by the <strong>conditional</strong> distribution of <span class="math inline">\((\hat p^*-\hat p)|\mathcal{S}_n\)</span> given the observed sample <span class="math inline">\({\cal S}_n,\)</span> where the latter can be approximated arbitrarily well <span class="math inline">\((m\to\infty)\)</span> using the bootstrap estimators <span class="math display">\[
p^*_1,p^*_2,\dots,p^*_m;
\]</span> namely by <span class="math display">\[
P\left(\hat{p}^* - \hat{p} \leq \delta|\mathcal{S}_n\right)\approx \frac{1}{m}\sum_{k=1}^m 1_{(\hat{p}^*_k - \hat{p} \leq\delta )}.
\]</span></p>
<p>The bootstrap is called <strong>consistent</strong> if asymptotically <span class="math inline">\((n\rightarrow \infty)\)</span> the conditional distribution of <span class="math inline">\((\hat p^*-\hat p)|{\cal S}_n\)</span> coincides with the true distribution of <span class="math inline">\(\hat p - p.\)</span> (Note: a proper scaling is required!)</p>
<p><strong>The distribution of <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span></strong></p>
<p>The conditional random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> is a binary random variable <span class="math display">\[
X_i^*|\mathcal{S}_n\in\{0,1\}.
\]</span> Since <span class="math inline">\(X_i^*\)</span> is drawn independently and with replacement from <span class="math inline">\(\mathcal{S}_n,\)</span> we obtain for each <span class="math inline">\(i=1,\dots,n,\)</span> <span class="math display">\[
\begin{align*}
&amp; P^*(X_i^*=1)=P(X_i^*=1|{\cal S}_n)=\hat p, \\[2ex]  
&amp; P^*(X_i^*=0)=P(X_i^*=0|{\cal S}_n)=1-\hat p.
\end{align*}
\]</span> Thus, <span class="math inline">\(X_i^*|{\cal S}_n\)</span> is a Bernoulli distributed random variable with parameter <span class="math inline">\(p=\hat{p}\)</span> <span class="math display">\[
X_i^*|{\cal S}_n \sim\mathcal{Bern}(p=\hat p), \quad i=1,\dots,n.\\[5ex]
\]</span></p>
<p><strong>The distribution of <span class="math inline">\(\hat{p}^*|\mathcal{S}_n\)</span></strong></p>
<p>The above implies that <span class="math inline">\(n \hat{p}^*|{\cal S}_n\)</span> has a Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p=\hat{p},\)</span><br>
<span class="math display">\[
\underbrace{n \hat{p}_i^*}_{=S^*}|{\cal S}_n \sim\mathcal{Binom}(n, p=\hat p), \quad i=1,\dots,n.
\]</span></p>
<p>Therefore, <span class="math display">\[
\begin{align*}
\mathbb{E}^*(n \hat p^*)
&amp;=\mathbb{E}(n \hat p^*|\ {\cal S}_n)\\[2ex]
&amp; = n \hat{p}\\[2ex]
\Rightarrow \mathbb{E}^*(\hat p^*) &amp; = \hat{p}
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
\mathbb{V}^*(n \hat p^*)
&amp;=\mathbb{V}(n \hat p^*|\ {\cal S}_n)\\[2ex]
&amp; = n \hat{p} (1- \hat{p})\\[2ex]
\Rightarrow \mathbb{V}^*(\hat p^*) &amp; = \frac{\hat{p}(1-\hat{p})}{n}
\end{align*}
\]</span></p>
<p>An appropriate central limit theorem argument implies that <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>Moreover, <span class="math inline">\(\hat p\)</span> is a consistent estimator of <span class="math inline">\(p,\)</span> and thus <span class="math display">\[
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
\]</span> Therefore, <span class="math inline">\(\hat p(1-\hat p)\)</span> can be replaced asymptotically by <span class="math inline">\(p(1-p)\)</span>, and <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> So, we can conclude that, <span class="math display">\[
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0
\]</span> as <span class="math inline">\(n\rightarrow\infty,\)</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e.&nbsp;for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
\]</span> and therefore also <span class="math display">\[
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx
\text{distribution}(\hat p -p).%\approx N(0,p(1-p)/n)
\]</span></p>
</section>
<section id="confidence-intervals" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">3.3.3</span> Confidence Intervals</h3>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall: The traditional (non-bootstrap) approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if</p>
<ul>
<li><span class="math inline">\(\theta\in\mathbb{R}\)</span> and</li>
<li><span class="math inline">\(\sqrt{n}(\hat\theta-\theta)\rightarrow_d\mathcal{N}(0,v^2)\)</span> as <span class="math inline">\(n\to\infty,\)</span></li>
</ul>
<p>then one traditionally tries to determine an approximation <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> (the standard error of <span class="math inline">\(\hat\theta\)</span>) from the data. An approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval is then given by <span class="math display">\[
\left[
\hat{\theta}-z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}},
\hat{\theta}+z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}}
\right]
\]</span></p>
<p>In some cases it is, however, very difficult to obtain approximations <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span>. Statistical inference is then usually based on the bootstrap.</p>
</div>
</div>
<p>In contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed <strong>more precise</strong> than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)</p>
<section id="the-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-bootstrap-approach">The Bootstrap Approach</h4>
<!-- General approach: Basic bootstrap $(1-\alpha)\times 100\%$ confidence interval -->
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> with <span class="math inline">\(X_i\sim F\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, where the distribution <span class="math inline">\(F\)</span> depends on the unknown parameter (vector) <span class="math inline">\(\theta.\)</span></li>
<li><strong>Problem:</strong> Construct a confidence interval for <span class="math inline">\(\theta.\)</span></li>
</ul>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption: Bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the following, we will assume that the bootstrap is consistent; i.e.&nbsp;that <span class="math display">\[
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^* -\hat{\theta})|{\cal S}_n)
&amp;\approx
\text{distribution}(\sqrt{n}(\hat{\theta}-\theta))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*-\hat{\theta})|{\cal S}_n
&amp;\overset{d}{\approx} \sqrt{n}(\hat{\theta}-\theta)
\end{align*}
\]</span> if <span class="math inline">\(n\)</span> is sufficiently large.</p>
<p>Caution: This is not always the case and in cases of doubt one needs to show this property.</p>
</div>
</div>
<p><strong>Derivation of the nonparametric bootstrap confidence intervals:</strong></p>
<ul>
<li><p>We can generate <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_k^*\equiv\hat\theta(X_{1k}^*,\dots,X_{nk}^*),\quad k=1,\dots,m,
\]</span> by drawing bootstrap samples <span class="math inline">\(X_{1k}^*,\dots,X_{nk}^*\)</span> independently and with replacement from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p></li>
<li><p>The <span class="math inline">\(m\)</span> bootstrap estimates allow us to approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and the <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> of the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}.\)</span> This can be done with negligible approximation error (for <span class="math inline">\(m\)</span> large) using the empirical quantiles <span id="eq-empiricalQuantile"><span class="math display">\[
\hat t_{p}=\left\{
\begin{array}{ll}
\hat\theta^*_{(\lfloor mp\rfloor+1)},         &amp; mp \text{ not a whole number}\\
(\hat\theta^*_{(mp)}+\hat\theta^*_{(mp+1)})/2,&amp; mp \text{ a whole number}
\end{array}\right.
\tag{3.1}\]</span></span> for <span class="math inline">\(p=\frac{\alpha}{2}\)</span> or <span class="math inline">\(p=1-\frac{\alpha}{2},\)</span> where <span class="math inline">\(\hat\theta_{(i)}^*\)</span> denotes the order statistic <span class="math display">\[
\hat\theta_{(1)}^* \leq \hat\theta_{(2)}^*\leq \dots\leq \hat\theta_{(m)}^*,
\]</span> and <span class="math inline">\(\lfloor mp\rfloor\)</span> denotes the greatest whole number less than or equal to <span class="math inline">\(mp\)</span> (e.g.&nbsp;<span class="math inline">\(\lfloor 4.9\rfloor = 4\)</span>).</p></li>
</ul>
<p>Then <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat t_\frac{\alpha}{2} \leq \hat{\theta}^* \leq \hat t_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp; P^*\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}^*-\hat{\theta} \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp; P^*\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)
\approx 1-\alpha,
\end{align*}
\]</span> where the approximation becomes arbitrarily precise for <span class="math inline">\(m\to\infty.\)</span> Here, <span class="math inline">\(P^*\)</span> denotes probabilities with respect to the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span>.</p>
<p>Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> <span class="math display">\[
{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}-\theta)}.
\]</span> Therefore, for large <span class="math inline">\(n,\)</span> <span class="math display">\[
\begin{align*}
&amp;P\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{blue}\sqrt{n}(\hat{\theta}-\theta)}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}-\theta \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(\hat{\theta}-(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\le \theta\le \hat{\theta}-
(\hat t_\frac{\alpha}{2}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}\le \theta\le 2\hat{\theta}-
\hat t_\frac{\alpha}{2}\right)\approx 1-\alpha.
\end{align*}
\]</span></p>
<p>Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) <strong>bootstrap confidence interval</strong> is given by <span id="eq-NPBootCI"><span class="math display">\[
\left[2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}, 2\hat{\theta}-\hat t_\frac{\alpha}{2}\right],
\tag{3.2}\]</span></span> where <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> are the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles of the bootstrap distribution approximated by the empirical quantiles of the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math inline">\(\hat{\theta}^*_1, \hat{\theta}^*_2,\dots, \hat{\theta}^*_m.\)</span></p>
</section>
<section id="example-confidence-intervals-for-the-population-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-confidence-intervals-for-the-population-mean">Example: Confidence Intervals for the Population Mean</h4>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> Let <span class="math inline">\(X_1,\dots,X_n\)</span> denote an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2.\)</span> <!-- * In the following $F$ will denote the corresponding distribution function; i.e., $X_i\sim F$ for all $i=1,\dots,n.$ --></li>
<li><strong>Estimator:</strong> <span class="math inline">\(\bar X=\frac{1}{n} \sum_{i=1}^n X_i\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span></li>
<li><strong>Inference Problem:</strong> Construct a confidence interval for <span class="math inline">\(\mu.\)</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall: The traditional (non-bootstrap) approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional, non-bootstrap approach for constructing a <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval:</p>
<ul>
<li>By the CLT: <span class="math inline">\(\bar X\overset{a}{\sim} \mathcal{N}(\mu,\frac{\sigma^2}{n})\)</span> for large <span class="math inline">\(n\)</span></li>
<li>Estimation of <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2\)</span></li>
<li>This implies: <span class="math inline">\(\sqrt{n}((\bar X -\mu)/s)\overset{a}{\sim} t_{n-1}\)</span>, and hence <span class="math display">\[
\begin{align*}
&amp;P\left(-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right)\approx 1-\alpha\\
\Rightarrow
&amp;P\left(\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \mu\le
      \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}
\right)\approx 1-\alpha
\end{align*}
\]</span></li>
<li><span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval: <span class="math display">\[
\left[\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}},
    \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(t_{n-1,1-\frac{\alpha}{2}}\)</span> denotes the <span class="math inline">\(1-\frac{\alpha}{2}\)</span>-quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This traditional construction relies on the assumption that <span class="math inline">\(\bar X\)</span> is exactly normal distributed, also for small <span class="math inline">\(n,\)</span> which requires that the random sample <span class="math inline">\(X_1,\dots,X_n\)</span> is i.i.d. <em>normally</em> distributed.</p>
<p>If the underlying distribution is <strong>not normal</strong>, then this normal distribution holds <em>approximately</em> if the sample size <span class="math inline">\(n\)</span> is sufficiently large (central limit theorem), i.e., <span class="math display">\[
\bar X \overset{a}{\sim}\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right).
\]</span> In this case the constructed confidence interval is an <em>approximate</em> <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval.</p>
</div>
</div>
</div>
</div>
</section>
<section id="the-nonparametric-standard-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-nonparametric-standard-bootstrap-approach">The Nonparametric (Standard) Bootstrap Approach</h4>
<p>The bootstrap offers an alternative method for constructing approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals. We already know that the bootstrap is consistent in this situation.</p>
<p><strong>Construction of the nonparametric (standard) bootstrap confidence interval:</strong></p>
<ul>
<li>Draw <span class="math inline">\(m\)</span> bootstrap samples (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) and calculate the corresponding estimates <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span>.</li>
<li>Compute the empirical quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> from <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span></li>
<li>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>3.2</span></a>: <span class="math display">\[
\left[2\bar X-\hat t_{1-\frac{\alpha}{2}},
    2\bar X-\hat t_\frac{\alpha}{2}\right]
\]</span></li>
</ul>
</section>
</section>
</section>
<section id="pivot-statistics-and-the-bootstrap-t-method" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="pivot-statistics-and-the-bootstrap-t-method"><span class="header-section-number">3.4</span> Pivot Statistics and the Bootstrap-<span class="math inline">\(t\)</span> Method</h2>
<p>In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-<span class="math inline">\(t\)</span> method (one also speaks of the “studentized bootstrap”). The construction relies on so-called pivotal statistics.</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be an i.i.d. random sample and assume that the distribution of <span class="math inline">\(X\)</span> depends on an unknown parameter (or parameter vector) <span class="math inline">\(\theta\)</span>.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-pivotal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (Asymptotically pivotal statistics) </strong></span><em>A statistic <span class="math display">\[
T_n\equiv T(X_1,\dots,X_n)
\]</span> is called (exact) <strong>pivotal</strong>, if the distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter. A statistic <span class="math inline">\(T_n\)</span> is called <strong>asymptotically pivotal</strong>, if the asymptotic distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter.</em></p>
</div>
</div>
</div>
<p><em>Exact</em> pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an <em>asymptotically pivotal</em> statistic. Assume that an estimator <span class="math inline">\(\hat{\theta}\)</span> satisfies <span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d\mathcal{N}(0,v^2),
\]</span> where <span class="math inline">\(v^2\)</span> denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator <span class="math display">\[
\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2
\]</span> of <span class="math inline">\(v\)</span> such that <span class="math display">\[
\hat v_n^2 \rightarrow_p v^2.
\]</span> Then, of course, also <span class="math inline">\(\hat v_n\rightarrow_p v\)</span>, and <span class="math display">\[
T_n:= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> This means that <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
\]</span> is <strong>asymptotically pivotal</strong>.</p>
<section id="example-barx-is-asymptotically-pivotal" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-barx-is-asymptotically-pivotal">Example: <span class="math inline">\(\bar{X}\)</span> is Asymptotically Pivotal</h4>
<p>Let <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> be a i.i.d. random sample with <span class="math inline">\(X_i\sim X\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> with mean <span class="math inline">\(\mathbb{E}(X)=\mu\)</span>, variance <span class="math inline">\(\mathbb{V}(X)=\sigma^2&gt;0\)</span>, and <span class="math inline">\(\mathbb{E}(|X|^4)=\beta&lt;\infty\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is normally distributed, we obtain <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\sim t_{n-1}
\]</span> with <span class="math inline">\(s^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\)</span>, where <span class="math inline">\(t_{n-1}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. We can conclude that <span class="math inline">\(T_n\)</span> is pivotal.</p>
<p>If <span class="math inline">\(X\)</span> is <em>not</em> normally distributed, the central limit theorem implies that <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span> In this case <span class="math inline">\(T_n\)</span> is an asymptotically pivotal statistics.</p>
</section>
<section id="bootstrap-t-consistency" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-consistency">Bootstrap-<span class="math inline">\(t\)</span> Consistency</h4>
<p>The general idea of the bootstrap-<span class="math inline">\(t\)</span> method relies on approximating the unknown distribution of <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
\]</span> by the approximable (via bootstrap resampling) conditional distribution of <span class="math display">\[
T_n^*=\sqrt{n}\frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*},
\]</span> given <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where the variance estimate <span class="math inline">\(v_n^*\)</span> is computed from the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*,\)</span> i.e. <span class="math display">\[
\hat v_n^*=v_n(X_1^*,\dots,X_n^*).
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Good news: Bootstrap-<span class="math inline">\(t\)</span> consistency follows if the standard nonparametric bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the standard nonparametric bootstrap is consistent, i.e.&nbsp;if the conditional distribution of <span class="math inline">\(\sqrt{n}(\hat{\theta}^*-\hat{\theta})|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, yields a consistent estimate of <span class="math inline">\(\mathcal{N}(0,v^2)\)</span>, then also the bootstrap-<span class="math inline">\(t\)</span> method is consistent. That is, then the conditional distribution of <span class="math inline">\(T_n^*|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, provides a consistent estimate of the asymptotic distribution of <span class="math inline">\(T_n\rightarrow_d \mathcal{N}(0,1)\)</span> such that <span class="math display">\[
\sup_\delta \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution.</p>
</div>
</div>
</section>
<section id="bootstrap-t-confidence-interval" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="bootstrap-t-confidence-interval"><span class="header-section-number">3.4.1</span> Bootstrap-t Confidence Interval</h3>
<p>Let <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> be an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with unknown parameter (vector) <span class="math inline">\(\theta.\)</span> Assume that the bootstrap is consistent and that the estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is asymptotically normal. Furthermore, suppose that a <strong>consistent</strong> estimator <span class="math display">\[
\hat v\equiv \hat v(X_1,\dots,X_n)
\]</span> of the asymptotic standard error, <span class="math inline">\(v,\)</span> of <span class="math inline">\(\hat{\theta}\)</span> is available.</p>
<p><strong>Derivation of the bootstrap-<span class="math inline">\(t\)</span> confidence interval:</strong></p>
<ul>
<li><p>Based on an i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> calculate the bootstrap estimates <span class="math display">\[
\hat{\theta}^*\equiv \hat{\theta}^*(X_1^*,\dots,X_n^*)
\]</span> and <span class="math display">\[
v^*\equiv v^*(X_1^*,\dots,X_n^*)
\]</span> and the bootstrap statistic <span class="math display">\[
\begin{align*}
T^*&amp;=T^*(X_1^*,\dots,X_n^*)\\
&amp;=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}.
\end{align*}
\]</span> Repeating this yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) many bootstrap statistics <span class="math display">\[
T_1^*,T_2^*, \dots, T_m^*
\]</span> which allow us to approximate the bootstrap distribution of <span class="math inline">\(T^*=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*},\)</span> conditionally on <span class="math inline">\(\mathcal{S}_n,\)</span> arbitrarily precise as <span class="math inline">\(m\to\infty.\)</span></p></li>
<li><p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> of the bootstrap distribution of <span class="math display">\[
\left.\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}\right|\mathcal{S}_n
\]</span> using the empirical quantiles based on <span class="math inline">\(T_1^*,T_2^*, \dots, T_m^*\)</span> (see <a href="#eq-empiricalQuantile">Equation&nbsp;<span>3.1</span></a>).</p></li>
</ul>
<p>This implies, for large <span class="math inline">\(m,\)</span> <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat \tau_\frac{\alpha}{2}\leq {\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha
\end{align*}
\]</span> Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> that <span class="math display">\[
\left.{\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}}\right|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\frac{\hat{\theta}-\theta}{\hat v}}.
\]</span> Therefore, for lage <span class="math inline">\(n,\)</span> <span class="math display">\[
\begin{align*}
&amp; P\left(\hat \tau_\frac{\alpha}{2}\leq {\color{blue}\frac{\hat{\theta}-\theta}{\hat v}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta-\hat{\theta} \leq -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(\hat{\theta}-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta \leq \hat{\theta} -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha
\end{align*}
\]</span> Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) bootstrap-<span class="math inline">\(t\)</span> confidence interval is given by <span id="eq-Boot_tCI"><span class="math display">\[
\left[\hat{\theta}-\hat \tau_{1-\frac{\alpha}{2}}\hat v,\;
      \hat{\theta}-\hat \tau_{  \frac{\alpha}{2}}\hat v\right]
\tag{3.3}\]</span></span></p>
<section id="example-bootstrap-t-confidence-interval-for-the-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-bootstrap-t-confidence-interval-for-the-mean">Example: Bootstrap-<span class="math inline">\(t\)</span> Confidence Interval for the Mean</h4>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Draw i.i.d. random samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\({\cal S}_n\)</span> and calculate <span class="math inline">\(\bar X^*\)</span> as well as <span class="math inline">\(s^*=\sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2}\)</span> to generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap realizations <span class="math display">\[
\frac{\bar X^*_1-\bar X}{s^*_1},\dots,\frac{\bar X^*_m-\bar X}{s^*_m}
\]</span></li>
<li>Determine <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> from <span class="math inline">\(\frac{\bar X^*_1-\bar X}{s^*_1},\dots,\frac{\bar X^*_m-\bar X}{s^*_m}\)</span> using <a href="#eq-empiricalQuantile">Equation&nbsp;<span>3.1</span></a>.</li>
<li>This yields the <span class="math inline">\(1-\alpha\)</span> confidence interval (using <a href="#eq-Boot_tCI">Equation&nbsp;<span>3.3</span></a>): <span class="math display">\[
\left[\bar X-\hat \tau_{1-\frac{\alpha}{2}}s,
    \bar X-\hat \tau_{\frac{\alpha}{2}}s\right],
\]</span> where <span class="math inline">\(s\)</span> is computed from the original sample, i.e., <span class="math display">\[
s=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2}.
\]</span></li>
</ul>
</section>
</section>
<section id="accuracy-of-the-bootstrap-t-method" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="accuracy-of-the-bootstrap-t-method"><span class="header-section-number">3.4.2</span> Accuracy of the Bootstrap-<span class="math inline">\(t\)</span> method</h3>
<p>Usually, the bootstrap-<span class="math inline">\(t\)</span> provides a <strong>gain in accuracy</strong> over the standard nonparametric bootstrap. The reason is that the approximation of the law of <span class="math inline">\(T_n\)</span> by the bootstrap law of <span class="math display">\[
\left.\frac{\sqrt{n}(\hat{\theta}^*-\hat{\theta})}{v^*_n}\right|\mathcal{S}_n
\]</span> is more direct and hence more accurate (<span class="math inline">\(v^*_n\)</span> depends on the bootstrap sample — not the original sample) than by the bootstrap law of <span class="math display">\[
\left.\sqrt{n}(\hat{\theta}^*-\hat{\theta})\right|\mathcal{S}_n.
\]</span></p>
<p>The use of pivotal statistics and the corresponding construction of bootstrap-<span class="math inline">\(t\)</span> confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-<span class="math inline">\(t\)</span> methods are <strong>second order accurate</strong>.</p>
<p>Consider generally <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals of the form <span class="math inline">\([L_n,U_n]\)</span> of <span class="math inline">\(\theta\)</span>. The lower, <span class="math inline">\(L_n\)</span>, and upper bounds, <span class="math inline">\(U_n\)</span>, of such intervals are determined from the data and are thus random, <span class="math display">\[
L_n\equiv L(X_1,\dots,X_n)
\]</span> <span class="math display">\[
U_n\equiv U(X_1,\dots,X_n)
\]</span> and their accuracy depends on the particular procedure applied (e.g.&nbsp;nonparametric bootstrap vs.&nbsp;bootstrap-<span class="math inline">\(t\)</span>).</p>
<ul>
<li>(Symmetric) confidence intervals are said to be <strong>first-order accurate</strong> if there exist some constants <span class="math inline">\(c_1,c_2&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_1}{\sqrt{n}}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_2}{\sqrt{n}}
\end{align*}
\]</span></li>
<li>(Symmetric) confidence intervals are said to be <strong>second-order accurate</strong> if there exist some constants <span class="math inline">\(c_3,c_4&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_3}{n}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_4}{n}
\end{align*}
\]</span></li>
</ul>
<p>If the distribution of <span class="math inline">\(\hat\theta\)</span> is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that</p>
<ul>
<li>Standard confidence intervals based on asymptotic approximations are <strong>first-order</strong> accurate.</li>
<li>Nonparametric (standard) boostrap confidence intervals are <strong>first-order</strong> accurate.</li>
<li>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals are <strong>second-order</strong> accurate.</li>
</ul>
<p>The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to <em>much</em> better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Proofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field.</p>
</div>
</div>
</section>
</section>
<section id="regression-analysis-bootstrapping-pairs" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="regression-analysis-bootstrapping-pairs"><span class="header-section-number">3.5</span> Regression Analysis: Bootstrapping Pairs</h2>
<p>Consider the linear regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
\]</span> where <span class="math inline">\(Y_i\in\mathbb{R}\)</span> denotes the response (or “dependent”) variable and <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
\]</span> denotes the vector of predictor variables. In the following, we differentiate between a <strong>random design</strong> and a <strong>fixed design</strong>.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-RandomFixedDesign" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 (Random and fixed design) </strong></span><br></p>
<p><em><strong>Random Design:</strong> <span class="math display">\[
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
\]</span> are i.i.d. random variables and <span class="math inline">\(\mathbb{E}(\varepsilon_i|X_i)=0\)</span> with either</em></p>
<ul>
<li><em><strong>homoscedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, for a constant <span class="math inline">\(\sigma^2&lt;\infty\)</span> or</em></li>
<li><em><strong>heteroscedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2(X_i)&lt;\infty\)</span>, <span class="math inline">\(i=1,\dots,n.\)</span></em></li>
</ul>
<p><em><strong>Fixed Design:</strong> <span class="math display">\[
X_1, X_2, \dots, X_n
\]</span> are deterministic vectors in <span class="math inline">\(\mathbb{R}^p\)</span> and <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. random variables with zero mean <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span> and <strong>homoscedastic errors</strong> <span class="math inline">\(\mathbb{E}(\varepsilon_i^2)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></em></p>
</div>
</div>
</div>
<p>The least squares estimator <span class="math inline">\(\hat\beta\in\mathbb{R}^p\)</span> is given by <span class="math display">\[
\begin{align*}
\hat\beta
&amp;=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\\
&amp;=\beta+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
\]</span></p>
<section id="bootstrapping-pairs-bootstrap-under-random-design" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="bootstrapping-pairs-bootstrap-under-random-design"><span class="header-section-number">3.5.1</span> Bootstrapping Pairs: Bootstrap under Random Design</h3>
<p>Under the random design, we additionally assume that there exists a positive definite (thus invertible) matrix <span class="math inline">\(M\)</span> <span class="math display">\[
M=\mathbb{E}(X_iX_i^T)
\]</span> and a positive semi-definite matrix <span class="math inline">\(Q\)</span> such that <span class="math display">\[
Q=\mathbb{E}(\varepsilon_i^2X_iX_i^T)=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>For homoscedastic errors we have <span class="math display">\[
\begin{align*}
Q
&amp;=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)\\
&amp;=\sigma^2\mathbb{E}(X_iX_i^T)\, =\sigma^2 M.
\end{align*}
\]</span></p>
</div>
</div>
<p>The law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\rightarrow_d\mathcal{N}(0,M^{-1}QM^{-1}),\quad n\to\infty.
\]</span></p>
<p>Bootstrapping regression estimates <span class="math inline">\(\hat\beta\)</span> is straightforward under a <strong>random design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;<span>3.4</span></a>).</p>
<p>Under a random design, <span class="math inline">\((Y_i,X_i)\)</span> are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of the estimation errors <span class="math display">\[
\hat\beta-\beta.
\]</span> In the literature this procedure is usually called <strong>bootstrapping pairs</strong>, namely, <span class="math inline">\((Y_i, X_i)\)</span>-pairs.</p>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Original data: i.i.d. sample <span class="math inline">\({\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}\)</span></li>
<li>Generate bootstrap samples <span class="math display">\[
(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
\]</span> by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n.\)</span></li>
<li>Each bootstrap sample <span class="math inline">\((Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)\)</span> leads to a bootstrap realization of the least squares estimator <span class="math display">\[
\hat\beta^*=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
\]</span></li>
</ul>
<p>It can be shown that bootstrapping pairs is <strong>consistent</strong>; i.e.&nbsp;that for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx\mathcal{N}(0,M^{-1}QM^{-1})
\]</span></p>
<section id="confidence-intervals-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confidence-intervals-1">Confidence Intervals</h4>
<p>This allows to construct basic bootstrap confidence intervals for the <span class="math inline">\(j\)</span>th regression coefficient <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>:</p>
<ul>
<li><p>Generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap realizations <span class="math display">\[
\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*
\]</span></p></li>
<li><p>Determine the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span><br>
from the bootstrap realizations <span class="math inline">\(\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*\)</span> using <a href="#eq-empiricalQuantile">Equation&nbsp;<span>3.1</span></a>.</p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>3.2</span></a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
    2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}\right]
\]</span></p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are <strong>heteroscedastic</strong>. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages.</p>
</div>
</div>
</section>
</section>
</section>
<section id="regression-analysis-residual-bootstrap" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="regression-analysis-residual-bootstrap"><span class="header-section-number">3.6</span> Regression Analysis: Residual bootstrap</h2>
<p>If the sample <span class="math display">\[
(Y_1,X_1),\dots,(Y_n,X_n)
\]</span> is <strong>not</strong> an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for <strong>fixed designs</strong> and also generally not in time-series regression contexts. However, if error terms are <strong>homoscedastic</strong>, then it is possible to rely on the <strong>residual bootstrap</strong>.</p>
<p>In the following we will formally assume a regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i, \quad i=1,\dots,n,
\]</span> with <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
\]</span> under <strong>fixed design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;<span>3.4</span></a>), i.e., where <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. with zero mean <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span> and <strong>homoscedastic</strong> errors <span class="math display">\[
\mathbb{E}(\varepsilon_i^2)=\sigma^2.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Applicability of the Residual Bootstrap
</div>
</div>
<div class="callout-body-container callout-body">
<p>Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated <span class="math inline">\(X\)</span>-variables (time-series). In these cases all arguments are meant conditionally on the given <span class="math inline">\(X_1,\dots,X_n\)</span>. The above assumptions on the error terms then of course have to be satisfied conditionally on <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
</div>
</div>
<p>The idea of the residual bootstrap is very simple: The model implies that the error terms <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n
\]</span> are i.i.d which suggests a bootstrap based on resampling the error terms.</p>
<p>These errors are, of course, unobserved, but they can be approximated by the corresponding residuals <span class="math display">\[
\hat \varepsilon_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,
\]</span> where again <span class="math display">\[
\hat\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
\]</span> denotes the least squares estimator.</p>
<p>It is well known that <span class="math display">\[
\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2
\]</span> provides an unbiased, consistent estimator of the error variance <span class="math inline">\(\sigma^2\)</span>. That is, <span class="math display">\[
\mathbb{E}(\hat\sigma^2)=\sigma^2 \quad \text{and}\quad \hat\sigma^2\rightarrow_p \sigma^2.
\]</span></p>
<section id="the-residual-bootstrap-algorithm" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-residual-bootstrap-algorithm">The Residual Bootstrap Algorithm</h4>
<p>Based on the original data <span class="math inline">\((Y_i,X_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, and the least squares estimate <span class="math inline">\(\hat\beta\)</span>, calculate the residuals <span class="math inline">\(\hat\varepsilon_1,\dots,\hat \varepsilon_n\)</span>.</p>
<ol type="1">
<li>Generate random bootstrap samples <span class="math inline">\(\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*\)</span> of residuals by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
\]</span></li>
<li>Calculate new depend variables <span class="math display">\[
Y_i^*=X_i^T\hat\beta+\hat\varepsilon_i^*,\quad i=1,\dots,n
\]</span></li>
<li>Bootstrap estimators <span class="math inline">\(\hat\beta^*\)</span> are determined by least squares estimation from the data <span class="math inline">\((Y_1^*,X_1),\dots,(Y_n^*,X_n)\)</span>: <span class="math display">\[
\hat\beta^*=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
\]</span></li>
</ol>
<p>Repeating Steps 1-3 <span class="math inline">\(m\)</span> many times yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap estimators <span class="math display">\[
\hat\beta^*_1,\hat\beta^*_2,\dots,\hat\beta^*_m
\]</span> which allow us to approximate the bootstrap distribution <span class="math inline">\(\hat\beta^*-\hat\beta|\mathcal{S}_n\)</span> arbitrarily well as <span class="math inline">\(m\to\infty.\)</span></p>
</section>
<section id="motivating-the-residual-bootstrap" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="motivating-the-residual-bootstrap">Motivating the Residual Bootstrap</h4>
<p>It is not difficult to understand why the residual bootstrap generally works for <em>homoscedastic</em> (!) errors. We have <span class="math display">\[
\hat\beta-\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
\]</span> and for large <span class="math inline">\(n\)</span> the distribution of <span class="math inline">\(\sqrt{n}(\hat\beta-\beta)\)</span> is approximately normal with mean 0 and covariance matrix <span class="math inline">\(\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\)</span> <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\to_d\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)
\]</span></p>
<p>On the other hand (the bootstrap world), we have construction <span class="math display">\[
\hat\beta^*-\hat\beta
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*
\]</span> Conditional on <span class="math inline">\({\cal S}_n,\)</span> the bootstrap error terms are i.i.d with <span class="math display">\[
\mathbb{E}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
\]</span> and <span class="math display">\[
\mathbb{V}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2.
\]</span> An appropriate central limit theorem argument implies that <span class="math display">\[
\left.\sqrt{n}(\hat\beta^*-\hat\beta)\right|\mathcal{S}_n\to_d\mathcal{N}\left(0,\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right),
\]</span> for <span class="math inline">\(n\to\infty.\)</span></p>
<p>Since<br>
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2\rightarrow_p \sigma^2
\]</span> as <span class="math inline">\(n\rightarrow\infty\)</span>, the bootstrap is consistent. That is, for large <span class="math inline">\(n\)</span>, we have approximately <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta-\beta))}_{\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)}
\]</span></p>
</section>
<section id="bootstrap-confidence-intervals-for-the-regression-coefficients" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="bootstrap-confidence-intervals-for-the-regression-coefficients"><span class="header-section-number">3.6.1</span> Bootstrap confidence intervals for the regression coefficients</h3>
<section id="nonparametric-bootstrap-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="nonparametric-bootstrap-confidence-intervals">Nonparametric bootstrap confidence intervals</h4>
<p>Basic <strong>nonparametric bootstrap</strong> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span> of the conditional distribution of <span class="math inline">\(\hat\beta_j^*|\mathcal{S}_n\)</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles based on the <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\beta_{j1}^*,\hat\beta_{j2}^*, \dots, \hat\beta_{jm}^*.
\]</span> <!-- This approximation step is with arbitrary accuracy as $m\to\infty.$ --></p>
<p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>3.2</span></a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
      2\hat\beta_j-\hat t_{ \frac{\alpha}{2},j }\right]
\]</span></p>
</section>
<section id="bootstrap-t-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-confidence-intervals">Bootstrap-<span class="math inline">\(t\)</span> confidence intervals</h4>
<p><strong>Bootstrap-<span class="math inline">\(t\)</span></strong> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Let <span class="math inline">\(\gamma_{jj}\)</span> denote the <span class="math inline">\(j\)</span>-th diagonal element of the matrix <span class="math inline">\((\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\)</span>, i.e., <span class="math display">\[
\gamma_{jj}:=\left[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right]_{jj}.
\]</span> Then <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}
\]</span> with <span class="math display">\[
\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}
\]</span> is an asymptotically pivotal statistic, since <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>A bootstrap-<span class="math inline">\(t\)</span> interval for <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>, can thus be constructed as follows:</p>
<p>Additionally compute <span class="math display">\[
\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^{*2},
\]</span> and approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2},j}\)</span> of the bootstrap distribution of <span class="math display">\[
T^*=\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}
\]</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles (see <a href="#eq-empiricalQuantile">Equation&nbsp;<span>3.1</span></a>) based on the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math display">\[
T^*_1,T_2^*,\dots, T_m^*.
\]</span></p>
<p>Compute the <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval as in <a href="#eq-Boot_tCI">Equation&nbsp;<span>3.3</span></a>: <span class="math display">\[
\left[
  \hat\beta_j-\hat \tau_{1-\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}},\;
  \hat\beta_j-\hat \tau_{\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}}
\right],
\]</span> where <span class="math inline">\(\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}.\)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are many more bootstrap procedures. In case of heteroscedastic errors, for instance, there’s also the “Wild Bootstrap.”</p>
<p>For high-dimensional problems (<span class="math inline">\(p\)</span> as large as <span class="math inline">\(n\)</span> or larger), one can use (under certain regularity assumptions) the “Multiplier Bootstrap”.</p>
</div>
</div>
</section>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<section id="exercise-1." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-1.">Exercise 1.</h4>
<p>Consider the empirical distribution function <span class="math display">\[
F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> for a random sample <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} F.
\]</span></p>
<ol type="a">
<li><p>Derive the exact distribution of <span class="math inline">\(nF_n(x)\)</span> and the asymptotic distribution of the empirical distribution function <span class="math inline">\(F_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span></p></li>
<li><p>Show that <span class="math inline">\(F_n(x)\)</span> is a point-wise (weakly) consistent estimator of <span class="math inline">\(F(x)\)</span> for each given <span class="math inline">\(x\in\mathbb{R}\)</span>.</p></li>
</ol>
</section>
<section id="exercise-2." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-2.">Exercise 2.</h4>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Exercise 1 shows that the empirical distribution function is a <strong>point-wise</strong> consistent estimator for each given <span class="math inline">\(x\in\mathbb{R}.\)</span> However, point-wise consistency generally does not imply <strong>uniformly</strong> consistency for all <span class="math inline">\(x\in\mathbb{R},\)</span> and therefore the Clivenko-Cantelli (<a href="#thm-Clivenko-Cantelli">Theorem&nbsp;<span>3.1</span></a>) is so famous.</p>
<p>This exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.</p>
</div>
</div>
<p>Point-wise convergence of a function <span class="math inline">\(g_n(x),\)</span> i.e., <span class="math display">\[
|g_n(x) - g(x)|\to 0
\]</span> for each <span class="math inline">\(x\in\mathcal{X}\subset\mathbb{R}\)</span> as <span class="math inline">\(n\to\infty\)</span> generally does not imply uniform convergence, i.e., <span class="math display">\[
\sup_{x\in\mathcal{X}}|g_n(x) - g(x)|\to 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Show this by providing an example for <span class="math inline">\(g_n\)</span> which converges point-wise, but not uniformly for <span class="math inline">\(x\in\mathcal{X}\)</span>.</p>
<!-- 
http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln03.pdf 
-->
</section>
<section id="exercise-3." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-3.">Exercise 3.</h4>
<p>Consider the following setup:</p>
<ul>
<li>iid data <span class="math inline">\(X_1,\dots,X_n\)</span> with <span class="math inline">\(X_i\sim F\)</span></li>
<li><span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span></li>
<li><span class="math inline">\(\mathbb{V}(X_i)=\sigma^2&lt;\infty\)</span></li>
<li>Estimator: <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^nX_i\)</span></li>
</ul>
<ol type="a">
<li>Derive the classic confidence interval for <span class="math inline">\(\mu\)</span> using the asymptotic normality of the estimator <span class="math inline">\(\bar{X}.\)</span> Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of <span class="math inline">\(n=30\)</span> and,</li>
</ol>
<ul>
<li>first, for <span class="math inline">\(F\)</span> being the normal distribution with <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=2\)</span>, and</li>
<li>second, for <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom.</li>
</ul>
<ol start="2" type="a">
<li><p>Reconsider the case of <span class="math inline">\(n=30\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.</p></li>
<li><p>Reconsider the case of <span class="math inline">\(n=30\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-<span class="math inline">\(t\)</span> confidence interval.</p></li>
</ol>
<!-- {{< include Ch3_Solutions.qmd >}} -->
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Davison_Hinkley_2013" class="csl-entry" role="doc-biblioentry">
Davison, Anthony Christopher, and David Victor Hinkley. 2013. <em>Bootstrap Methods and Their Application</em>. Cambridge University Press.
</div>
<div id="ref-Efron_Tibshirani_1994" class="csl-entry" role="doc-biblioentry">
Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.
</div>
<div id="ref-Hall_1992" class="csl-entry" role="doc-biblioentry">
Hall, Peter. 1992. <em>The Bootstrap and Edgeworth Expansion</em>. Springer Science.
</div>
<div id="ref-Horowitz_2001" class="csl-entry" role="doc-biblioentry">
Horowitz, Joel L. 2001. <span>“The Bootstrap.”</span> In <em>Handbook of Econometrics</em>, 5:3159–3228.
</div>
<div id="ref-Shao_Tu_1996" class="csl-entry" role="doc-biblioentry">
Shao, Jun, and Dongsheng Tu. 1996. <em>The Jackknife and Bootstrap</em>. Springer Science.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2_MonteCarlo.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch4_MaximumLikelihood.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>