[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    12:15-13:45 \n    Jur / Hörsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/computational-statistics-script-MSc/\nWe’ll use an eWhiteboard for derivations and some extra explanations.\nBasic material from our econometrics course:\n\nIntroduction to R\nProbability\n\n\n\n\n\n\n\n\nYou can use the Zulip-Chat CompStat (M.Sc.) to post questions, share codes, etc. Happy sharing and discussing!\n\n\n\n\n\nConsider using git/github for your personal course notes.\n\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html",
    "href": "Ch1_Random_Variable_Generation.html",
    "title": "1  Random Variable Generation",
    "section": "",
    "text": "In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:\n\nMonte Carlo Statistical Methods, Ch. 2, Robert and Casella (2004)\nIntroducing Monte Carlo Methods with R, Ch. 2, Robert and Casella (2010)"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "href": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "title": "1  Random Variable Generation",
    "section": "1.1 Uniform Simulation",
    "text": "1.1 Uniform Simulation\nGeneral procedure:\n\nUsually, a random integer with values uniformly in \\([0,m]\\) with a large integer \\(m\\) is generated.\nTo achieve a random number in \\([0, 1]\\), we divide this number by \\(m\\).\nFrom this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.\n\nThere are many different Random Number Generators (RNGs), we consider the most simple class of RNGs:\n\nDefinition 1.1 (Linear Congruential Generators) Here the \\(i\\)th random integer \\(u_i\\) is generated by \\[\nu_i=(a u_{i-1}+c) \\,\\operatorname{mod}\\, m,\n\\] where the starting value \\(u_0\\) is a chosen and fixed value called seed.\nFurthermore:\n\n\\(m\\), with \\(0<m\\), is called the modulus\n\\(a\\), with \\(0<a<m\\), is called the multiplier\n\\(c\\), with \\(0\\leq c<m\\), is called the increment\n\n\n\n\n\n\n\n\nThe modulo operator: \\(\\operatorname{mod}\\)\n\n\n\n“\\(b\\,\\operatorname{mod}\\,c\\)” denotes the remainder of the division of \\(b\\) by \\(c\\).\nFor instance \\[\n\\begin{align*}\n4\\,&\\operatorname{mod}\\,2 = 0\\\\\n5\\,&\\operatorname{mod}\\,2 = 1\\\\\n1\\,&\\operatorname{mod}\\,2 = 1\\\\\n\\end{align*}\n\\]\n\n# Modulo computation using the modulo operator '%%'\n5 %% 4\n9 %% 4\n4 %% 5\n\n# own modulo-function:\nmy_mod <- function(x,m){\n  t1 <- floor(x/m)\n  return(x-t1*m)\n}\n\n\n\nSome Facts:\n\nThe above recursion generates a completely nonrandom sequence, therefore it is often called a pseudo random sequence.\nUnder appropriate choices of \\(u_0\\) , \\(a\\) and \\(m\\) the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on \\([0, m]\\).\nThe cycle length of linear congruential generators will never exceed modulus \\(m\\), but can maximized with the three following conditions (see Knuth (2002) for a proof):\n\nThe increment \\(c\\) is relatively prime to \\(m\\),\n\\(a - 1\\) is a multiple of every prime dividing \\(m\\),\n\\(a - 1\\) is a multiple of \\(4\\) when \\(m\\) is a multiple of \\(4\\).\n\n\n\nBad choice of parameters for the linear congruential random number generator:\n\nm <- 64    # modulus\na <- 33    # multiplier\nc <- 12    # increment\ns <- 57    # seed\nn <- 1000  # length of run (including seed)\n\nr_vec    <- numeric(n) # initialize vector\nr_vec[1] <- s # set seed\n\n## Recursive generation \nfor (i in 1:(n-1)){\n r_vec[i+1] <- (a * r_vec[i] + c) %% m\n}\n\n# scale result from [0,m] to [0,1]:\nmy_bad_runif_vec <- r_vec/m\n\n# BUT! Very short cycle-length (here: period=16)\nr_vec[ 1:16]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\nr_vec[17:32]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\n\n\nExample 1.1 (Good vs. Bad RNGs) Average heads ratios \\[\n\\bar{C}_n=\\frac{1}{n}\\sum_{i=1}^nC_i\n\\] for \\(n=1,2,\\dots\\) simulated independent tosses of a fair coin \\(C_i\\) with\n\\[\nC_{i}=\\left\\{\\begin{array}{ll}\n1&\\text{if Head}\\\\\n0&\\text{if Tail}\n\\end{array}\\right.\n\\] and \\[\nP(C_i=0)=P(C_i=1)=0.5.\n\\] By the strong (or weak) law of large numbers the average should converge stochastically (i.e., almost surely or in probability) to \\(0.5\\) as \\(n\\) becomes large \\((n\\to\\infty),\\) \\[\n\\bar{C}_n=\\frac{1}{n}\\sum_{i=1}^nC_i \\to_p 0.5,\\quad n \\to\\infty.\n\\]\n\n# using the above bad RNG:\nbar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)\n\n# using R's high-quality RNG:\nset.seed(223)\nbar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)\n\n# plotting the results:\nplot(bar_x_bad, type=\"l\", ylim=c(0.46,0.54), \n     xlab=\"\", ylab=\"\", main=\"Good vs. Bad RNG\")\nlines(bar_x_good, col=\"darkblue\")\n\n\n\n\nFigure 1.1: Two sample paths showing the pseudo random convergence of \\(\\bar{C}_n\\) to the limit 0.5—one based on a good RNG and the other based on a bad RNG.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIBM’s RANDU is a famous example of an miss-specified linear congruential RNG."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "title": "1  Random Variable Generation",
    "section": "1.2 Generation of Discrete Random Variables",
    "text": "1.2 Generation of Discrete Random Variables\n\nAssume the discrete random variable \\(X\\) of interest takes on the values \\[\nX\\in \\{x_1, \\dots , x_k \\}\n\\tag{1.1}\\] with \\[\np_i = \\mathbb{P}(X = x_i ), \\quad i = 1,\\dots , k,\n\\] and \\[\n\\sum_{i=1}^kp_i = 1.\n\\]\nAssume that you can generate pseudo-random realizations \\(u\\in[0,1]\\) from a uniformly distributed random variable \\(U\\sim\\mathcal{U}[0, 1]\\) using an RNG.\nGeneral principle:\n\nSubdivide \\([0, 1]\\) into \\(k\\) intervals with \\[\nI_i = (a_{i-1}, a_i],\n\\] where \\[\na_i = \\sum_{j=1}^ip_j\\quad\\text{and}\\quad a_0 = 0.\n\\]\nDefine the new discrete realizations \\[x=\\left\\{\n\\begin{array}{cc}\n       x_1&\\quad\\text{if}\\quad u\\in I_1\\\\\n       \\vdots& \\vdots\\\\\n       x_k&\\quad\\text{if}\\quad u\\in I_k\n       \\end{array}\\right.\n\\tag{1.2}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nLemma 1.1 Let \\(u\\) be a realization from \\(\\mathcal{U}[0, 1]\\) and if \\(u\\in I_i\\), set \\(x = x_i\\) (see Equation 1.2). Then \\(x\\) is a realizaton of the discrete random variable \\(X\\) (see Equation 1.1).\n\n\n\n\nProof of Lemma 1.1:\nFor any \\(i = 1, \\dots, k\\) we have that \\[\n\\begin{align*}\n\\mathbb{P}(X = x_i)\n& = \\mathbb{P}(U \\in I_i)  \\\\\n& = F_\\mathcal{U}(a_i) - F_\\mathcal{U}(a_{i-1})\\\\\n& = a_i - a_{i-1}\\\\\n& = \\sum_{j=1}^ip_j - \\sum_{j=1}^{i-1}p_j  = p_i,\n\\end{align*}\n\\] which shows the statement of Lemma 1.1.\n\nExample 1.2 (Bernoulli Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Bernoulli}(p),\n\\] where \\(p\\) is the probability of success, i.e., \\[\n\\mathbb{P}(X=1)=p\\quad\\text{and}\\quad\\mathbb{P}(X=0)=1-p.\n\\]\nAlgorithm: If \\(U\\sim\\mathcal{U}[0,1]\\) and \\(p\\) is specified, define \\[\nX=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\(X\\sim\\mathrm{Bernoulli}(p)\\).\n\nset.seed(321)\n# Generate one random number from Bernoulli(p) with p=0.5\np  <- 0.5\nU  <- runif(1)\n\nif(U<=p) X=1 else X=0\n\nX\n\n[1] 0\n\n\n\n\nExample 1.3 (Binomial Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Binomial}(n,p),\n\\] where \\(n\\) is the number of trials and \\(p\\) the probability of success such that \\[\n\\mathbb{P}(X=i)=\\binom{n}{i}p^i(1-p)^{n-1}\n\\] for \\(i=1,\\dots,n.\\)\n\n\n\n\n\n\nTip\n\n\n\nIf \\(X_1,\\dots,X_n\\overset{i.i.d}{\\sim}\\mathrm{Bernoulli}(p),\\) then \\[\nX=\\sum_{i=1}^nX_i \\sim\\mathrm{Binomial}(n,p).\n\\]\n\n\nAlgorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(n\\) and \\(p\\) are specified, define \\[\nX_i=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U_i\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Binomial}(n,p).\n\\]\n\nset.seed(321)\n\n# Generate one random number from B(n=10, p=0.5). \nn <- 10\nX <- numeric(n)\np <- 0.5\n\nfor(i in 1:n){\n  U <- runif(1)\n  if(U<=p) X[i]=1 else X[i]=0\n}\nY <- sum(X)\nY \n\n[1] 7\n\n\n\n\nExample 1.4 (Poisson Distribution) Algorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=\\min\\left\\{n=0,1,2\\dots,\\text{ such that }\\prod_{i=1}^{n+1} U_i \\leq e^{-\\lambda}\n\\right\\}.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Poisson}(\\lambda).\n\\]\n\nset.seed(321)\n\n# Generate one random number from Poisson(lambda) \nlambda <- 2\n\n## Initializations\nU <- 1\nn <- 0\n\nwhile(U > exp(-lambda)){\n  U <- U * runif(1)\n  n <- n + 1\n}\nn <- n-1\nn\n\n[1] 3"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "title": "1  Random Variable Generation",
    "section": "1.3 Generation of Continuous Random Variables",
    "text": "1.3 Generation of Continuous Random Variables\n\n1.3.1 The Inverse Method\nA rather general method to generate continuous random variables is the Inverse Method.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Inverse Method) Let \\(U\\sim\\mathcal{U}[0,1],\\) and let \\(F_X\\) be an invertible distribution function. The transformed random variable\n\\[\nX=F_X^{-1}(U)\n\\] has then the distribution function \\(F_X,\\) \\[\nP(X\\leq x) = F_X(x).\n\\]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTheorem 1.1 can only be used to generate random variables \\(X\\) with invertible distribution functions \\(F_X.\\)\n\n\nProof of Theorem 1.1:\nThe distribution function of the transformed random variable \\[\nX=F^{-1}(U)\n\\] can be derived as \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&= \\mathbb{P}(F_X^{-1}(U)\\leq x) \\\\\n&= \\mathbb{P}(U\\leq F_X(x)) \\\\\n&= F_U(F_X(x)) \\\\\n& = F_X(x),\n\\end{align*}\n\\] which shows the result of Theorem 1.1. The last (and important) equality follows since the distribution function of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\n\\mathbb{P}(U\\leq u) = F_U(u) = u, \\quad 0\\leq u \\leq 1\n\\] since the distribution function \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\nF_U(u) = \\left\\{\n  \\begin{array}{ll}\n  0 & \\text{for } u < 0\\\\\n  u & \\text{for } 0 \\leq  u \\leq 1\\\\\n  1 & \\text{for } 1 < u.\\\\\n  \\end{array}\n\\right.  \n\\tag{1.3}\\]\n\n\n\nExample 1.5 (Exponential Distribution) Since \\[\nF(x)= 1 - \\exp(-\\lambda x),\n\\] we have \\[\nF^{-1}(u) = - \\frac{\\ln(1-u)}{\\lambda}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that \\(1-U\\) has the same distribution as \\(U\\), if \\(U\\sim U[0,1]\\). Therefore also \\(-\\frac{\\ln(u)}{\\lambda}\\) leads to a value from \\(\\mathrm{Exp}(\\lambda).\\)\n\n\nAlgorithm: If \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=-\\frac{\\ln(U)}{\\lambda}.\n\\] Then \\[\nX\\sim \\mathrm{Exp}(\\lambda).\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods.\nThe inverse method often cannot be applied or is often inefficient, because the inverse of many important distribution functions cannot be derived in closed form:\n\nThe Gaussian distribution function \\(\\Phi\\) and therefore also its inverse \\(\\Phi^{-1}\\) is not available in closed form.\nFor discontinuous random variables we need efficient algorithms for computing the generalized inverse of their distribution function \\(F.\\)\n\n\n\n\n\n1.3.2 Transformation Methods\nIdea: Construct algorithms from theoretical links between distributions.\nPro: These methods can be advantageous if a distribution \\(f\\) is linked (in a relatively simple way) to another distribution that is easy to simulate.\nCon: Generally, these methods are rather case-specific, and difficult to generalize.\n\nExample 1.6 (Building on Exponential RVs) In Example 1.5, we learned to generate an exponential random variable \\(X\\sim\\operatorname{Exp}(\\lambda)\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from exponential random variables \\(X\\sim\\mathrm{Exp}(1):\\)\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\n\n\n\n\n\n\n\nNote\n\n\n\nThere are better algorithms to generate Gamma and Beta random variables.\nWe cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter \\(\\alpha\\).\n\nThis implies that we cannot generate a \\(\\chi^2_{1}\\)-distributed random variable, because the \\(\\chi^2_{1}\\) distribution is identical to the \\(\\Gamma(\\alpha, 2)\\) distribution with \\(\\alpha=\\frac{1}{2}.\\)\nThis then also implies that we cannot generate a \\(\\mathcal{N}(0,1)\\)-distrbuted random variable, since \\(X^2\\sim \\chi^2_{1}\\) for \\(X\\sim\\mathcal{N}(0,1)\\).\n\n\n\n\nThe well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of Box and Muller (1958):\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Normal Variable Generation (Box and Muller, 1958)) If \\(U_1\\) and \\(U_2\\) are i.i.d. as \\(U\\sim\\mathcal{U}[0,1]\\), then \\[\nX_1 =\\sqrt{-2 \\ln(U_1)}\\, \\cos(2\\pi U_2)\n\\] and \\[\nX_2=\\sqrt{-2\\ln(U_1)}\\,\\sin(2\\pi U_2)\n\\] are both i.i.d. as \\(X\\sim\\mathcal{N}(0,1).\\)\n\n\n\nProof of Theorem 1.2:\nDefine the random variables \\[\nR = \\sqrt{-2 \\ln(U_1)}\\quad\\text{and}\\quad Q = 2\\pi U_2,\n\\] where \\[\nR\\in(0,\\infty)\\quad\\text{and}\\quad Q\\in[0,2\\pi].\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIdea of the proof:\n\nDerive the bivariate density of \\((R, Q).\\)\nDetermine the functional connection \\(g\\) between \\((R, Q)\\) and \\((X_1, X_2)\\) and note that \\(g\\) is invertible.\nUse the transformation formula for densities to derive the bivariate density of \\((X_1,X_2)\\) using \\(g^{-1}\\) and the bivariate density of \\((R, Q).\\)\nThe result follows, if the bivariate density of \\((X_1,X_2)\\) equals the product of two standard normal densities.\n\nTransformation formula (bivariate case):\nAssume that the bivariate random variable \\[\n\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\n\\] has a bivariate density \\(f_{RQ}(r, q)\\) and that there is a mapping \\(g\\) between the bivariate random variables \\[\n\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\\text{ and }\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\n\\] such that \\[\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)=\ng(R, Q)=\n\\left(\\begin{matrix}g_1(R, Q)\\\\ g_2 (R, Q)\\end{matrix}\\right),\n\\] where \\(g:\\mathbb{R}^2\\to\\mathbb{R}^2\\) is a differentiable and invertible function with inverse \\(g^{-1}.\\)\nThen, the bivariate density of \\(\\left(\\begin{matrix}X_1\\\\X_2\\end{matrix}\\right)\\) is given by \\[\nf_{X_1X_2}(x_1,x_2)=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\,\\left|\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|,\n\\tag{1.4}\\] where \\[\n\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\n\\] denotes the determinant of the Jacobian matrix of \\(g^{-1}\\) evaluated at \\((x_1,x_2),\\) \\[\nJ_{g^{-1}}(x_1,x_2)=\\left(\\begin{matrix}\n\\frac{\\partial g_1^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_1^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\frac{\\partial g_2^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_2^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\end{matrix}\\right).\n\\] Note that the Jacobian of \\(g^{-1}\\) equals the inverse of the Jacobian of \\(g,\\) \\[\nJ_{g^{-1}}(x_1,x_2) = \\left(J_{g}(r,q)\\right)^{-1},\n\\] with points \\((x_1,x_2)\\) and \\((r,q)\\) such that \\[\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)=\ng(r, q)=\n\\left(\\begin{matrix}g_1(r, q)\\\\ g_2 (r, q)\\end{matrix}\\right),\n\\]\n\n\nWe can derive the distribution function of \\(R\\) as following \\[\n\\begin{align*}\nF_R(r)=\\mathbb{P}\\left(R\\leq r\\right)\n& = \\mathbb{P}\\left(\\sqrt{-2 \\ln(U_1)}\\leq r\\right) \\\\\n& = \\mathbb{P}\\left(\\ln(U_1)\\geq -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) < -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) \\leq -\\frac{r^2}{2}\\right) \\quad \\text{(continous)}\\\\\n& = 1 - \\mathbb{P}\\left(U_1 \\leq \\exp\\left(-\\frac{r^2}{2}\\right)\\right) \\\\\n& = 1 - F_U\\left(\\exp\\left(-\\frac{r^2}{2}\\right)\\right)\\\\\n& = 1 - \\exp\\left(-\\frac{r^2}{2}\\right),\n\\end{align*}\n\\] where the last step follows from applying the distribution \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1];\\) see Equation 1.3.\nFor the density function \\(f_R\\) of \\(R\\) we get \\[\nf_R(r)=F_R'(r)=\\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(-\\frac{r^2}{2}\\right)\\cdot r&\\text{for }r \\in(0,\\infty)\\\\\n  0&\\text{otherwise}\\\\\n  \\end{array}\\right.\n\\] Next, we derive the density function of \\[\nQ = 2\\pi U_2.\n\\] Since \\(U_2\\sim\\mathcal{U}[0,1],\\) \\[\nQ\\sim\\mathcal{U}[0,2\\pi].\n\\] with density function \\[\nf_Q(q)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{1}{2\\pi}&\\text{for } q\\in [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\] Since \\(U_1\\) and \\(U_2\\) are independent, \\(R=\\sqrt{-2 \\ln(U_1)}\\) and \\(Q = 2\\pi U_2\\) must also be independent, such that \\[\n\\begin{align*}\nf_{RQ}(r,q)\n& = f_R(r)\\cdot f_Q(q) \\\\\n& = \\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(-\\frac{r^2}{2}\\right) r\\cdot \\frac{1}{2\\pi} & \\text{for } (r,q) \\in (0,\\infty)\\times [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\end{align*}\n\\]\nNow, as we know the bivariate density of \\((R,Q)\\) we can use the functional connection \\[\n\\begin{align*}\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\n& = g(R,Q) \\\\\n& = \\left(\\begin{matrix}\n       g_1(R,Q)\\\\\n       g_2(R,Q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\n  R\\cos(Q)\\\\\n  R\\sin(Q)\n  \\end{matrix}\\right)\n% & = \\left(\\begin{matrix}\n%   \\sqrt{-2\\ln(U_1)}\\cos\\left(2\\pi U_2\\right)\\\\\n%   \\sqrt{-2\\ln(U_1)}\\sin\\left(2\\pi U_2\\right)\\\\\n%   \\end{matrix}\\right)\\\\\n\\end{align*}\n\\] with \\[\nR = \\sqrt{-\\ln(U_1)}\\in (0,\\infty)\n\\] and \\[\nQ=2\\pi U_2\\in[0, 2\\pi].\n\\] \n\n\n\n\n\n\nTip\n\n\n\nNote that, \\(g\\) is just the one-to-one transformation that maps points \\((r,q)\\) of the polar coordinate system\n\nradius \\(r\\in (0,\\infty)\\) and\nangle \\(q\\in[0, 2\\pi]\\)\n\nto points \\((x_1,x_2)\\) of the Cartesian coordinate system: \\[\n\\begin{align*}\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)\n& = g(r,q)\\\\\n& = \\left(\\begin{matrix}g_1(r,q) \\\\ g_2(r,q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}r\\cos(q)\\\\r\\sin(q)\\end{matrix}\\right)\n\\end{align*}\n\\]\nPlay around with this mapping here: https://mathinsight.org/applet/polar_coordinates_map_rectangle\nThe inverse mapping \\(g^{-1}\\) maps points \\((x_1,x_2)\\) from the Cartesian coordinate system to points \\((r,q)\\) in the polar coordinate system \\[\n\\begin{align*}\n\\left(\\begin{matrix}r\\\\ q\\end{matrix}\\right)\n& = g^{-1}(x_1,x_2)\\\\\n& = \\left(\\begin{matrix}g_1^{-1}(x_1,x_2) \\\\ g_2^{-1}(x_1,x_2)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\\sqrt{x_1^2 + x_2^2}\\\\ \\operatorname{atan2}(x_1,x_2)\n\\end{matrix}\\right),\n\\end{align*}\n\\] where the function \\(\\operatorname{atan2}\\) is the 2-argument arctangent. \n\n\n\\[\n\\begin{align*}\nJ_{g^{-1}}(x_1,x_2)\n&=\\left(J_{g}(r,q)\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\frac{\\partial g_1}{\\partial r}(r,q) & \\frac{\\partial g_1}{\\partial q}(r,q)\\\\\n\\frac{\\partial g_2}{\\partial r}(r,q) & \\frac{\\partial g_2}{\\partial q}(r,q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\cos(q) & -r\\sin(q)\\\\\n\\sin(q) & \\phantom{-}r\\cos(q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\n\\frac{1}{r\\cos^2(q) + r\\sin^2(q)}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\\\\\n&=\n\\frac{1}{r}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\n\\end{align*},\n\\] where the last step follows from Pythagorean’s identity \\(\\cos^2(q) + \\sin^2(q)=1.\\) So \\[\n\\begin{align*}\n\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\n&=\\left|\\operatorname{det}\\left(\\left(J_{g}(r,q)\\right)^{-1}\\right)\\right|\\\\\n&=\n\\left|\\operatorname{det}\\left(\n  \\begin{matrix}\n            \\cos(q) &            \\sin(q)\\\\\n-\\frac{1}{r}\\sin(q) & \\frac{1}{r}\\cos(q)\\\\\n\\end{matrix}\\right)\n\\right|\\\\\n&=\n\\left|\\frac{1}{r}\\cos^2(q) + \\frac{1}{r}\\sin^2(q)\\right| = \\frac{1}{r},\n\\end{align*}\n\\] again using Pythagorean’s identity \\(\\cos^2(x_2) + \\sin^2(x_2)=1\\) and using that \\(r\\in(0,\\infty).\\)\nThus, by the transformation formula for bivariate densities (Equation 1.4), we have \\[\n\\begin{align*}\nf_{X_1X_2}(x_1,x_2)\n&=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\\\\\n&=f_{RQ}\\Big(\\underbrace{\\sqrt{x_1^2+x_2^2}}_{=r},\\underbrace{\\operatorname{atan2}(x_1,x_2)}_{=q}\\Big)\\frac{1}{r}\\\\\n&=\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right) \\sqrt{x_1^2+x_2^2} \\cdot \\frac{1}{2\\pi} \\frac{1}{r}\\\\\n&=\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right)  \\frac{1}{2\\pi},\n\\end{align*}\n\\] where the last step uses that \\(r=\\sqrt{x_1^2+x_2^2}.\\)\nThis shows the result of Theorem 1.2, since the bivariate density of \\((X_1,X_2)\\) equals the product of two marginal standard normal density functions, \\[\n\\begin{align*}\nf_{X_1X_2}(x_1,x_2)\n& = \\frac{1}{2\\pi}\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right) \\\\\n& = \\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x_1^2}{2}\\right)}_{=\\phi(x_1)} \\cdot\n    \\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x_2^2}{2}\\right)}_{=\\phi(x_2)},\n\\end{align*}\n\\] where \\(\\phi\\) denotes the density function of the standard normal distribution \\(\\mathcal{N}(0,1)\\).\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  # Return result\n  return(c(X1, X2))\n}\n\n# Generate n standard normal random variables:\nset.seed(321)\n\nn     <- 1000\nX_vec <- vector(length = n, mode = \"double\")\n\nfor(i in seq(1, n, by=2)){\n  X_vec[c(i, i+1)] <- BM_Algo()\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE, xlim=c(-4,4))\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test \n# H0: Normality\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99882, p-value = 0.7667\n\n\n\n\n\nThat is, the Shapiro Wilk is not able to reject its null-hypothesis that the data comes from a normal distribution. However, this was only a single test result and the results may differ in other samples. So, let’s check the test results across \\(B=5000\\) samples.\n\n## Monte Carlo (MC) replications\nB     <- 5000\n\n## Sample size\nn     <- 1000\n\n## Significance level\nalpha <- 0.05\n\n## Container vectors\nX_vec <- vector(length = n, mode = \"double\")\nT_vec <- vector(length = B, mode = \"logical\")\n\n## MC-Simulation\nfor(b in 1:B){ \n\n  for(i in seq(1, n, by=2)){\n    X_vec[c(i, i+1)] <- BM_Algo()\n  }\n\n  ## Collect test decisions\n  tmp      <- shapiro.test(X_vec)\n  T_vec[b] <- tmp$p.value < alpha\n}\n\nround(mean(T_vec), 2)\n\n[1] 0.05\n\n\nIt turns out that the false positive rate (empirical frequency of Type I errors) of the Shapiro Wilk test equals the chosen significance level \\(\\alpha.\\) That is, the test is not able to reject its null-hypothesis (“normal distribution”) beyond the expected frequency of a Type I error.\n\n\n1.3.3 Accept-Reject Methods\nFor many distributions \\(F\\) it is difficult (or impossible) to apply the Inverse Method or some Transformation Method, since the distribution function \\(F\\) is somehow “unsuitable”. For instance, surprisingly often there is no explicit form of \\(F\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest—actually, \\(f\\) needs to be known only up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.3 (Fundamental Theorem of Simulation) Let \\(X\\in\\mathbb{R}^d\\) be a random variable with density function \\(f_X,\\) \\[\nX\\sim f_X.\n\\] Then simulating \\(X\\) is equivalent to simulating \\((X,U)\\in\\mathbb{R}^{d+1},\\)\n\\[\n(X,U)\\sim\\mathcal{U}(\\mathcal{A}),\n\\] where \\(\\mathcal{U}(\\mathcal{A})\\) denotes the uniform distribution over the area \\[\n\\mathcal{A}=\\left\\{(x,u)\\;|\\; x\\in\\mathbb{R}^d \\text{ and } 0<u<f_X(x)\\right\\}.\n\\]\n\n\n\nFigure 1.2 visualizes the statement of Theorem 1.3 for the univariate \\((d=1)\\) case.\n\n\n\n\n\nFigure 1.2: To simulate a random variable \\(X,\\) one can simulate \\((X,U)\\) uniformely over \\(\\mathcal{A},\\) and then take those \\(X\\) realizations (tick-marks at \\(x\\)-axis) as simulation results.\n\n\n\n\nIt turns out that sampling \\((X, U)\\) uniformly over the set \\(\\mathcal{A}\\) is often challenging. However, one can consider some superset \\(\\mathcal{S},\\) \\[\n\\mathcal{A}\\subseteq \\mathcal{S},\n\\] such that simulating a random variable uniformly distributed over \\(\\mathcal{S}\\) is easy.\nA uniform distribution on \\(\\mathcal{A}\\) can then be obtained by\n\ndrawing from a uniform distribution on \\(\\mathcal{S}\\) and\nrejecting samples that are in \\(\\mathcal{S},\\) but not in \\(\\mathcal{A}.\\)\n\n\n1.3.3.1 The case of bounded densities with compact support\nThe general principle of the accept-reject method is easily explained using a bounded density function \\(f\\) with compact support.\n\n\n\n\n\n\nTip\n\n\n\n\nBounded (from above) means that there exists a constant \\(m\\) with \\(0<m<\\infty\\) such that \\[\n\\sup_xf(x)\\leq m\n\\]\nAn interval \\([a,b]\\) is called compact if it is closed and the boundaries are finite. For instance, the Gaussian density \\(\\phi\\) has not a compact support, but \\(\\mathrm{supp}(\\phi)=(-\\infty,\\infty)\\).\nExamples:\n\nA bounded density function with compact support: \\[\nf_c(x)=\\frac{3}{4}\\left(1-\\left(x-c\\right)^2\\right)\\,1_{(|x-c|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[c-1,c+1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f_c\\) is bounded from above by \\(3/4\\) for all \\(c\\in\\mathbb{R}.\\) \nAn unbounded density function with compact support: \\[\nf(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{x}{b^2}\\left(1-\\frac{x^2}{b^2}\\right)^{-1/2}&\\text{ for }x\\in(0,b)\\\\\n0&\\text{ otherwise}\n\\end{array}\n\\right.\n\\] \\(f(x)\\) is \\(0\\) for \\(x=0,\\) but diverges to positive infinity for \\(x\\to b.\\)\n\n\n\n\nThe Accept-Reject Algorithm for bounded densities with compact support:\n\nCompact support: Let \\[\n\\operatorname{supp}(f)=[a_1,b_1]\\times \\dots\\times[a_d,b_d],\n\\] where \\([a_j,b_j]\\subseteq\\mathbb{R}\\) are compact intervals for each \\(j=1,\\dots,d.\\)\nBounded (from above): Let \\[\n\\sup_{x\\in[a_1,b_1]\\times \\dots\\times[a_d,b_d]} f(x)\\leq m,\n\\] where \\(m\\) is a constant with \\(0<m<\\infty.\\)\n\nTo simulate \\[\nX\\sim f_X\n\\] simulate the random pair \\[\n(Y,U)\\sim\\mathcal{U}\\big(\\underbrace{[a_1,b_1]\\times \\dots\\times[a_d,b_d] \\times[0,m]}_{=\\mathcal{S}}\\big)\n\\] by simulating independently \\[\nY\\sim\\mathcal{U}([a_1,b_1]\\times \\dots\\times[a_d,b_d])\n\\] and \\[\nU \\sim \\mathcal{U}[0,m].\n\\]\nAccept-Reject Step:\nAccept a simulated \\(Y\\) as a simulation for \\(X\\), i.e. set \\[\nX=Y,\n\\] only if \\[\nU\\leq f_X(Y),\n\\] and reject all other \\(Y\\)’s.\n\n\nExample 1.7 Let the target density be \\[\nf_X(x)=\\frac{3}{4} \\left(1-x^2\\right)\\mathbf{1}_{(|x|\\leq 1)},\n\\] where \\[\n\\operatorname{supp}(f_X)=[-1,1]\n\\] and \\[\n\\sup_{x\\in[-1,1]}f_X(x)\\leq\\frac{3}{4}.\n\\] Thus the Accept-Reject algorithm for simulating \\(X\\sim f_X\\) can be implemented by generating\n\\[\n(Y,X)\\sim\\mathcal{U}([-1,1]\\times[0,3/4])\n\\] and accepting \\(Y\\) as realizations of \\(X\\) only if \\(U\\leq f_X(Y).\\)\n\nLook at the following code. It’s a very simple algorithm, where we effectively only need to evaluate the density function \\(f_X,\\) target_pdf:\n\n# Target pdf f_X\ntarget_pdf <- function(x){\n  pdf <- (3/4) * (1-(x)^2)\n  pdf[(x)^2 > 1] <- 0\n  ##\n  return(pdf)\n}\n\n# Accept-Reject Algo:\nY <- runif(n=1000, min = -1, max = 1) \nU <- runif(n=1000, min =  0, max = 3/4) \n# A-R Step:\naccept <- U <= target_pdf(Y)\nX      <- Y[accept]\n\nFigure 1.3 illustrates Example 1.7.\n\n\n\n\n\nFigure 1.3: Accept-Reject algorithm for univariate \\((d=1)\\) bounded densities with compact support.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.4 (Accept-Reject Algorithm bounded density functions with compact support.) Let \\(f_X\\) be a \\(d\\)-dimensional density function with\n\ncompact support \\(\\operatorname{supp}(f)=[a_1,b_1]\\times \\dots\\times[a_d,b_d]\\subseteq\\mathbb{R}^d,\\) and\nbounded from above \\(\\sup_{x\\in[a_1,b_1]\\times \\dots\\times[a_d,b_d]}f_X(x)\\leq m\\) with \\(0<m<\\infty.\\)\n\nThen the Accept-Reject algorithm as introduced in Section 1.3.3.1 allows to simulate \\[\nX\\sim f_X.\n\\]\n\n\n\nProof of Theorem 1.4 for the case \\(d=1\\):\nThe following derivation shows that the Accept-Reject algorithm as introduced in Section 1.3.3.1 allows to simulate random variables \\(X\\) with the correct target distribution \\(F_X(x)=\\int_a^xf_X(x)dx.\\)\n\\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&=\\mathbb{P}(Y\\leq x|U\\leq f_X(Y))\\\\[2ex]\n&= \\frac{\\mathbb{P}(Y\\leq x, U\\leq f_X(Y))}{\\mathbb{P}(U\\leq f_X(Y))}\\\\[2ex]\n&= \\frac{\\mathbb{P}(a\\leq Y\\leq x, \\; 0\\leq U\\leq f_X(Y))}{\\mathbb{P}(0\\leq U\\leq f_X(Y))}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,c\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,c\\;du\\,dy},\n\\end{align*}\n\\] where the constant \\(c\\) is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation \\[\n\\int_a^{b} \\int_0^{m}\\,c\\,du\\,dy = 1,\n\\] but which is irrelevant here since \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{c\\;\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{c\\;\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}.\\\\\n\\end{align*}\n\\] Now, using that \\(\\int_{0}^{f_X(y)}1du=\\big[x\\big]^{f_X(y)}_0=f_X(y)\\) yields \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{\\int_a^x f_X(y)\\,dy}{\\int_a^b f_X(y)\\,dy}\\\\[2ex]\n& =\\int_a^x f_X(y)dy = F_X(x),\n\\end{align*}\n\\] which completes the proof.\n\n\n1.3.3.2 The case of densities with non-compact support\nIf \\(f_X\\) has no compact support, the larger set \\(\\mathcal{S}\\) does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set” \\(\\mathcal{S},\\) enclosing the density function \\(f_X\\), as long as simulating uniformly over this larger set \\(\\mathcal{S}\\) is feasible. This generalization allows for cases, where the support of \\(f_X\\) is not compact (i.e. unbounded).\nLet the larger set denote by \\[\n\\mathcal{S}=\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathcal{S}\\) is feasible and\n\\(f_X(x)\\leq m(x)\\) for all \\(x\\in\\mathbb{R}^d.\\)\n\nFrom the feasibility-requirement it follows that the function \\(m\\) is necessarily integrable, i.e., that \\[\n\\int\\dots \\int m(x_1,\\dots,x_d)dx_1\\dots dx_d=M,\n\\] or short with \\(x=(x_1,\\dots,x_d)\\) and \\(X\\in\\mathbb{R}^d\\) \\[\n\\int_{\\mathbb{R}^d} m(x)dx=M,\n\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathcal{S}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathcal{S}\\).\nIntegrability of \\(m\\) is crucial here, since it allows us to relate the function \\(m\\) with a corresponding (auxiliary) density function \\(g\\) as following: \\[\nm(x)=M\\,g(x),\n\\] where \\[\n\\int_{\\mathbb{R}^d} m(x)\\,dx=\\int_{\\mathbb{R}^d} M\\,g(x)\\,dx=M.\n\\] Therefore, a uniform distribution over \\(\\mathcal{S},\\) \\[\n\\mathcal{U}\\big(\\underbrace{\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\}}_{\\mathcal{S}}\\big)\n\\] has a density function that is equal to \\[\n\\frac{1}{M}\n\\] for all points \\((y,u)\\in\\mathcal{S}\\) and zero else—otherwise the density would not integrate to one.\nTerminology:\n\nThe density function \\(g\\) is called the instrumental density. (Choose \\(g\\) as a density from which it is easy to simulate!)\nThe density function \\(f_X\\) is called the target density.\n\nThe Accept-Reject Algorithm for densities with non-compact support:\nTo simulate \\[\nX\\sim f_X\n\\] simulate the \\((d+1)\\)-dimensional random variable\n\\[\n(Y,U)\\sim\\mathcal{U}\\big(\\underbrace{\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\}}_{\\mathcal{S}}\\big)\n\\] by simulating\n\na realization of \\[\nY\\sim g\n\\] and\nfor a given realization \\(Y={\\color{red}y},\\) a realization of\n\\[\nU|Y={\\color{red}y}\\sim\\mathcal{U}[0,M\\,g({\\color{red}y})].\n\\]\n\nAccept-Reject Step:\nAccept a simulated \\(Y\\) as a simulation for \\(X\\), i.e. set \\[\nX=Y\n\\] only if \\[\nU\\leq f_X(Y)\n\\] and reject all other \\(Y\\)’s.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.5 (Accept-Reject Algorithm bounded density functions with non-compact support.) Let \\(f_X\\) be a \\(d\\)-dimensional density function and let \\(m(x)=Mg(x)\\) such that \\(g\\) is a \\(d\\)-dimensional auxiliary density function with\n\n\\(\\int m(x)dx=\\int Mg(x)dx = M\\) with constant \\(0<M<\\infty,\\) and\n\\(f_X(x)\\leq m(x)\\) for all \\(x\\in\\mathbb{R^d}.\\)\n\nThen the Accept-Reject algorithm as introduced in Section 1.3.3.2 allows to simulate \\[\nX\\sim f_X.\n\\]\n\n\n\nProof of Theorem 1.5:\nThe following derivation shows that the Appcept-Reject algorithm as introduced in Section 1.3.3.2 allows to simulate random variables \\(X\\) with the correct target distribution \\(f_X.\\)\nFirst, note that \\(X\\sim f_X\\) if and only if \\[\nP(X\\in A) = \\int_A f_X(y)dy\n\\] for every \\(A\\subseteq \\mathbb{R}^d.\\) I.e., this is what we need to show.\nIn a first step, we use that \\(X=Y\\) only if \\(U\\leq f_X(Y),\\) which implies that\n\\[\n\\begin{align*}\n\\mathbb{P}(X\\in A) & =\\mathbb{P}(Y\\in A|U\\leq f_X(Y)).\n\\end{align*}\n\\] The definition of conditional probabilities yields that \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& = \\frac{\\mathbb{P}(Y\\in A, \\; U\\leq f_X(Y))}{\\mathbb{P}(U\\leq f_X(Y))}.\n\\end{align*}\n\\] Using that the density function of \\((Y,U)\\sim\\mathcal{U}(\\mathcal{S})\\) equals \\(\\frac{1}{M}\\) for all \\((y,u)\\in\\mathcal{S}\\) and zero else we have that\n\n\\(\\mathbb{P}(Y\\in{\\color{red}A}, \\; U\\leq f_X(Y)) = \\int_{\\color{red}A}\\int_0^{f(y)}\\frac{1}{M}\\,du\\,dy\\quad\\)\n\nand that\n\n\\(\\mathbb{P}(U\\leq f_X(Y)) = \\int_{\\mathbb{R}^d}\\int_0^{f(y)}\\frac{1}{M}\\,du\\,dy\\quad\\) by integrating out \\(y\\) to get the marginal density.\n\nThus, \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& =\\frac{\\int_A \\int_0^{f_X(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_{\\mathbb{R}^d}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\\\\[2ex]\n& =\\frac{\\frac{1}{M}\\,\\int_A \\,\\int_0^{f_X(y)}\\,1\\,du\\,dy}{\\frac{1}{M}\\,\\int_{\\mathbb{R}^d}\\,\\int_0^{f(y)}\\,1\\,du\\,dy}\\\\[2ex]\n& =\\frac{\\int_A \\,\\int_0^{f_X(y)}\\,1\\,du\\,dy}{\\int_{\\mathbb{R}^d}\\,\\int_0^{f(y)}\\,1\\,du\\,dy}\\\\[2ex]\n\\end{align*}\n\\] Using that \\(\\int_{0}^{f_X(y)}\\, 1\\, du = f_X(y),\\) we get that \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& =\\frac{\\int_A f_X(y)\\,dy}{\\int_{\\mathbb{R}^d} f_X(y)\\,dy}\\\\[2ex]\n& =\\int_A f_X(y)dy,\n\\end{align*}\n\\] which applies to every set \\(A\\) and thus shows the result.\n\n\n\n\n\n\nNote\n\n\n\nNote that the above derivation implies that we only need to know the density function \\(f_X\\) up to a multiplicative constant \\(c>0,\\) since constant factors cancel—just as the normalizing constant \\(1/M.\\)\nThat is, instead of working with \\(f_X,\\) we can also work with a scaled version \\(\\tilde{f}_X\\) that is proportional to \\(f_X,\\) \\[\nf_{X}(x) = c \\tilde{f}_X(x)\n\\] often written as \\[\nf_{X}(x)\\propto \\tilde{f}_X(x),\n\\] since the constant \\(c\\) cancels out anyways in the above quotient expression. (The sign “\\(\\propto\\)” means “proportional to”.)\nThis is a very useful property for implementing the Accept-Reject algorithm, since it implies that it is irrelevant whether we scale the instrumental density \\(g\\) by some \\(1\\leq M <\\infty,\\) or the target density \\(f_X\\) by some constant \\(c>0.\\) All we need is that \\[\n\\frac{1}{c}f_X(x)=\\tilde{f}_X(x) \\leq M g(x) \\text{ for all } x\\in\\mathbb{R}^d.\n\\] for some some \\(c>0\\) and some \\(1\\leq M < \\infty.\\) This allows us to choose \\(c\\) and \\(M\\) such that simulating from \\(M g\\) is as simple as possible.\n\n\n\n\n\n\nExample 1.8 Let the target “density” be \\[\nf_X(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1).\n\\]\nGiven our considerations above, we can try to scaling such that the above “density” is dominated (i.e. \\(\\tilde{f}_X(x) \\leq g(x)\\) for all \\(x\\)) by the standard normal density \\[\ng(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2),\n\\] which is obviously straightforward to simulate from.\nThus, in this example we can set \\[\nm(x)=M\\,g(x)\\quad\\text{with}\\quad M=1,\n\\] since we can simply scale the target “density” \\(f_X\\) such that \\[\n\\tilde{f}_X(x)\\leq g(x)\\quad\\text{for all}\\quad x.\n\\] Specifically, we can set \\[\n\\tilde{f}_X(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1).\n\\]\nThe following code provides a possible implementation of the Accept-Reject algorithm for this example. Figure 1.4 shows the results.\n\n# Target density\ntarget_pdf <- function(x, c=.075){\n  pdf <- c * exp(-x^2 / 2) * \n            (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1)\n  return(pdf)\n}\n\n# Upper bound\nm_fun <- function(x){\n  m <- exp(-x^2 / 2)/sqrt(2*pi)\n  return(m)  \n}\n\n# Accept-Reject Algo:\nset.seed(32280)\n\n## 1. Generate Y\nY   <- rnorm(n=1000)\n\n## 2. Generate U|Y=y \nU   <- vector(length = length(Y), mode = \"double\")\n\nfor(i in 1:length(Y)){\n  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= target_pdf(Y)\nX      <- Y[accept]\n\n\n\n\n\n\nFigure 1.4: Accept-Reject algorithm for univariate \\((d=1)\\) densities with non-compact support—visualizing Example 1.8.\n\n\n\n\n\n\n\n\n\n\n1.3.3.3 Efficiency of the Accept-Reject algorithm\nSimple statements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f_X\\) and \\(g\\) are both normalized such that they are both density functions. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. The closer \\(M\\) is to \\(1\\) the better.\n\\(M\\) quantifies how closely \\(g\\) can imitate \\(f_X.\\)\n\n\nExample 1.9 (Normals from Double Exponentials) Consider simulating from \\(\\mathcal{N}(0,1)\\) using the Accept-Reject algorithm with the a double-exponential distribution \\(\\mathcal{L}(\\alpha),\\) also called Laplace distribution, as the instrumental density, \\[\ng(x|b)=\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right).\n\\]  It is then straightforward to show (see Exercises) that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n%&=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n%&=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\\\\[2ex]\n&\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\end{align*}\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental density: \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|1)} \\leq M\n& =\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right)\\\\[2ex]\n& =\\sqrt{\\frac{2}{\\pi} \\,\\exp\\left(1\\right)}.\n\\end{align*}\n\\]\nThe probability of acceptance is then \\[\n\\frac{1}{M}=\\sqrt{\\frac{\\pi}{2\\exp(1)}}\\approx 0.76.\n\\] I.e., to produce one normal random variable, this Accept-Reject algorithm requires on average \\[\n\\frac{1}{0.76}\\approx 1.3\n\\] uniform variables.\nThis is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is \\(1.\\)\n\n\n\n1.3.3.4 The instrumental density \\(g\\) needs thicker tails than the target density \\(f_X\\)\nLet \\(f_X\\) and \\(g\\) be both density functions, and let the instrumental density \\(g\\) be such that \\[\ng(x)>0  \n\\] for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\n\n\n\n\n\nTip\n\n\n\nThe defining properties of a density functions \\(f\\) are:\n\nNon-negative: \\(f(x)\\geq 0\\) for all \\(x\\)\nNormalized: $f(x)dx = 1 $\n\n\n\nThen, the inequality \\[\nf_X(x)\\leq M\\,g(x),\n\\] with \\(1<M<\\infty\\) for all \\(x\\in\\mathbb{R}^d,\\) implies that the quotient \\(f_X/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f_X(x)}{g(x)}\\leq M <\\infty\n\\] for all \\(x\\in\\operatorname{supp}(f_X).\\)\nThis implies that \\(g\\) has to have thicker tails than \\(f_X.\\)\nTo see this, observe that both \\[\nf_X(|x|)\\to 0 \\quad\\text{and}\\quad g(|x|)\\to 0\n\\] for “extreme” values \\(|x|\\to\\infty,\\) simply because both \\(f_X\\) and \\(g\\) are densities.\nIf \\(g\\) has thicker tails than \\(f_X,\\) then \\[\n0 \\leq \\frac{f_X(|x|)}{g(|x|)} \\leq 1 \\leq M\n\\] for all sufficiently large values \\(|x|.\\) Thus with \\(g\\) having thicker tails than \\(f_X,\\) we can be sure that the requirement \\(\\frac{f_X(x)}{g(x)}\\leq M <\\infty\\) holds for all sufficiently extreme values of \\(x.\\)\nBy constrast, if \\(g\\) has strictly thinner tails than \\(f_X,\\) then \\[\n0\\leq \\frac{f_X(|x|)}{g(|x|)} \\to\\infty,\\quad\\text{as}\\quad|x|\\to\\infty,\n\\] which makes the requirement that \\(\\frac{f_X(x)}{g(x)}\\leq M <\\infty\\) impossible to hold for extreme values of \\(x.\\)\n\n\n\n\n\n\nNote\n\n\n\nTherefore, it is, for instance, impossible to simulate from a Cauchy density \\(f_X\\) using a normal density \\(g\\). The reverse, however, works quite well."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "href": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "title": "1  Random Variable Generation",
    "section": "2.1 Classical Monte Carlo Integration",
    "text": "2.1 Classical Monte Carlo Integration\nThe generic problem here is the evaluation of integrals. (Be aware: Integrals are everywhere in statistics!). For instance, \\[\n\\mathbb{E}_f\\left(h(X)\\right)=\\mathbb{E}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f(x)\\,dx.\n\\]\nConvergence:\nGiven our previous developments, it is natural to propose using a realization \\(x_1,\\dots,x_m\\) from a (pseudo random) i.i.d. sample \\(X_1,\\dots,X_m\\) with each \\(X_j\\) distributed as \\(X\\sim f\\) to approximate the above integral by the empirical mean \\[\n\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h(x_j).\n\\] By the Strong Law of Large Numbers we know that the empirical mean \\(\\bar{h}_m\\) converges almost surely (a.s.) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\) as \\(m\\to\\infty\\). (The only prerequisits are that \\(f\\) has finite first moments, i.e., \\(\\mathbb{E}\\left(h(X)\\right)<\\infty\\), and that \\(\\bar{h}_m\\) is constructed from an i.i.d. sample \\(X_1,\\dots,X_m\\).)\nAs we can use the computer to produce realizations from the i.i.d. sample \\(X_1,\\dots,X_m\\), we can in principle choose an arbitrary large sample size \\(m\\) such that \\(\\bar{h}_m\\) can (in principle) be arbitrarily close to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\).\nThough, …\n\n… which sample size \\(m\\) is large enough?\nOr “equivalently”: How fast converges \\(\\bar{h}_m\\) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\)?\n\n\n\nSpeed of Convergence:\nOK, we know now that \\(\\bar{h}_m\\) reaches its limit (here in the “almost surely” sense) as \\(m\\to\\infty\\) under some rather loose conditions on the random sample \\(X_1,\\dots,X_m\\).\nIf we are willing to additionally assume that \\(f\\) has finite second moments, i.e., \\(\\mathbb{E}(h(X)^2)<\\infty\\), we can additionally say something about how fast \\(\\bar{h}_m\\) converges (a.s.) to \\(\\mathbb{E}(h(X))\\).\nThe speed of convergence of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) (i.e., now we think of \\(\\bar{h}_m\\) as the {RV} \\(\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h({\\color{red}{X_{j}}})\\)) to its limit \\(\\mathbb{E}(h(X))\\) can be assessed by answering the question how fast the standard deviation (which is a function of \\(m\\)) of the stochastic sequence converges to zero as \\(m\\to\\infty\\).\n\nThe variance of \\(\\bar{h}_m\\) is given by \\[\n\\mathbb{V}_f\\left(\\bar{h}_m\\right)=\n\\mathbb{V}\\left(\\frac{1}{m}\\sum_{j=1}^m h(X_j)\\right)=\n\\frac{1}{m}\\mathbb{V}\\left(h(X)\\right)\n\\]\nNote that assuming finite second moments \\(\\mathbb{E}(h(X)^2)<\\infty\\) is equivalent to assuming finite variance \\(\\mathbb{V}\\left(h(X)\\right)<\\infty\\). Consequently, we can set \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\) with \\(0<\\mathtt{const}<\\infty\\) such that \\[\n\\sqrt{\\mathbb{V}\\left(\\bar{h}_m\\right)}=m^{-1/2}\\mathtt{const}\\propto m^{-1/2}.\n\\]\n\nI.e., the speed of convergence (or rate) of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) is proportional to the deterministic sequence \\(\\{m^{-1/2}\\}\\).\n\n\nRemark: Even if we would not know the value of \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\), we know now that the improvement from \\(m=10\\) to \\(m=100\\) will be much higher than from \\(m=110\\) to \\(m=200\\). In practice, a typical choice is \\(m=10000\\); for moderate standard deviations this choice will guarantee a very good approximation.\n\n\nLimit Distribution:\nOf course, we can estimate the variance of the estimator \\(\\mathbb{V}\\left(\\bar{h}_m\\right)\\) by its empirical version \\[\nv_m=\\frac{1}{m}\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right),\n\\] where again by the Strong Law of Large Numbers (SLLN) \\[\n\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right)\\to_{\\text{a.s.}}\\mathbb{V}\\left(h(X)\\right).\n\\]  By the Central Limit Theorem (CLT) we have \\[\n\\sqrt{m}\\left(\\frac{\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)}{\\sqrt{\\mathbb{V}\\left(h(X)\\right)}}\\right)\\to_d Z,\n\\] where \\(Z\\sim N(0,1)\\). Note that the the above sequence \\(\\{\\sqrt{m}\\}\\) just hinders the convergence of the sequence \\(\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)\\to_{a.s.}0\\) such that the quotient converges to a “stable” distribution.\nThe above result can now be used for the construction of (asymptotically valid) convergence tests and confidence intervals with respect to \\(\\bar{h}_m\\), since for large \\(m\\) \\[\n\\bar{h}_m\\,\\overset{d}{\\approx} N\\left(\\mathbb{E}\\left(h(X)\\right),\\frac{\\mathbb{V}\\left(h(X)\\right)}{m}\\right).\n\\]\nAnd as we can use the computer to generate realizations of the i.i.d. sample \\(X_1,\\dots,X_m\\) from a generic \\(X\\sim f\\), we can easily approximate the mean \\(\\mathbb{E}\\left(h(X)\\right)\\) and the variance \\(\\mathbb{V}\\left(h(X)\\right)\\) with arbitrary accuracy as \\(m\\to\\infty\\); by the SLLN (or the WLLN).\n\n\nExample: A first Monte Carlo Integration\nLet’s say we want to integrate the function \\(h(x)=\\left(\\cos(50\\,x)+\\sin(20\\,x)\\right)^2\\). Although this function could be integrated analytically it is a good first test case. The left plot below shows the graph of the function \\(h(.)\\).\nTo approximate the integral \\[\n\\int_\\mathcal{X}h(x)dx\\quad\\text{with}\\quad\\mathcal{X}=[0,1]\n\\] we can use that \\[\n\\int_\\mathcal{X}h(x)dx=\\int_\\mathcal{[0,1]}1\\cdot h(x)dx =\\mathbb{E}_{f_\\text{Unif[0,1]}}(h(X)).\n\\]\nThus, we generate a realization \\((u_1,\\dots,u_n)\\) from the i.i.d. random sample \\(U_1,\\dots,U_n\\sim[0,1]\\) and approximate \\[\n\\int_\\mathcal{X}h(x)dx\\approx \\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(u_i).\n\\]\nIn order to assess how good this approximation is, we need to consider the stochastic propoerties of the RV \\[\n\\frac{1}{n}\\sum_{i=1}^n h(U_i).\n\\] This is done using the above (review of) results on the limit distribution of the sample mean which allows us to construct an approximative \\(95\\%\\) confidence interval, since for large \\(n\\) \\[\n\\left[\\bar{h}_n - 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}, \\bar{h}_n + 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}\\right]\\approx\n\\left[\\bar{h}_n - 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}, \\bar{h}_n + 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}\\right],\n\\] where \\(\\mathtt{std.error}_n^2=n^{-1}\\sum_{i=1}^n(h(u_i)-\\bar{h}_n)^2\\).\nThe right plot below shows one realization of the stochastic sequence \\(\\{\\bar{h}_1,\\dots,\\bar{h}_n\\}\\) with \\(n=10000\\), where the realized value of \\(\\bar{h}_n\\) is \\(0.966\\). This compares favorably with the with the exact value of \\(0.965\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks:\n\nThe approach followed in the above example can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency through numerical methods (e.g., Riemann Sum, Trapezoidal Rule, Simpson’s Rule, etc.) in dimensions 1 or 2.\nThe approach is particularly useful for approximating integrals over higher dimensional sets.\n\n\n\nExample: Approximation of Normal Distribution Tables\nA possible way to construct normal distribution tables is to use MC simulations.\nGenerate a realization \\((x_1,\\dots,x_n)\\) from an i.i.d. standard normal random sample, e.g., using the Box-Muller algorithm.\nThe approximation of the standard normal cdf \\[\n\\Phi(t)=\\int_{-\\infty}^t\\frac{1}{\\sqrt{2\\pi}}e^{-y^2/2}dy\n\\] by the Monte Carlo method is thus \\[\n\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n 1_{(x_i\\leq t)}.\n\\] The corresponding RV \\(\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\leq t)}\\) has (exact) variance \\[\n\\mathbb{V}(\\hat{\\Phi}_n(t))=\\frac{\\Phi(t)(1-\\Phi(t))}{n},\n\\] since the single RVs \\(1_{(X_i\\leq t)}\\) are independent Bernoulli with success probability \\(\\Phi(t)\\).\nFor values of \\(t\\) around \\(t=0\\), the variance is thus approximately \\(1/4n\\).\nTo achieve a precision of four decimals by means of a \\(99.9\\%\\) confidence interval, the approximation requires on average \\(n\\approx 10^8\\) simulations.\nThe table below gives the evolution of this approximation for several values of \\(t\\) and shows a very accurate evaluation for \\(n=10^8\\).\n\n\n\\[\n\\begin{array}{cccccccccc}\n\\hline\nn   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\\\\n\\hline\n10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\\\\n10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\\\\n10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\\\\n10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\\\\n10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\\\\n\\end{array}\n\\]\n\n\nRemarks:\n\nTo achieve a precision of two decimals by means of a \\(99.9\\%\\) confidence interval, already \\(n=10^4\\) leads to satisfactory results.\nNote that greater accuracy is achieved in the tails and that more efficient simulation methods could be used (e.g., Importance Sampling)."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "href": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "title": "1  Random Variable Generation",
    "section": "2.2 Importance Sampling",
    "text": "2.2 Importance Sampling\nImportance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it’s refereed to as a variance reduction technique. This variance reduction is achieved by weighting functions, so-called importance functions.\nAs in the case of Monte Carlo integration the focus lies on evaluating the integral \\[\n\\mathbb{E}_f(h(X))=\\int_\\mathcal{X}h(x)f(x)\\,dx.\n\\]\n\nThough, it turns out that the above approach, i.e., sampling from \\(f\\) is often suboptimal.\nObserve that the value of the above integral can be represented by infinitely many alternative choices of the triplet \\((\\mathcal{X}, h, f)\\). Therefore, the search for an optimal estimator should encompass all these possible representations.\n\nLet’s illustrate this with a simple example.\nExample: Cauchy Tail Probability (from Ripley 1987)\nSuppose that the quantity of interest is the probability, say \\(p\\), that a Cauchy \\(\\mathrm{C}(0,1)\\) RV is larger than \\(2\\), i.e.: \\[\np=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx.\n\\]\n1. Naive Approach: If \\(p\\) is approximated through the empirical mean \\[\n\\hat{p}_{1}=\\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\n\\] of an i.i.d. sample \\(X_1,\\dots,X_m\\sim\\mathrm{C}(0,1)\\), then the variance of this estimator, a binomial RV scaled by \\(1/m\\), is \\[\n\\mathbb{V}(\\hat{p}_{1})=\\frac{1}{m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(X_j>2)}\\right)=\\frac{p(1-p)}{m},\n\\] which is equal to \\(0.1275/m\\), since (we already know that) \\(p=0.15\\).\n\n\n2. Accounting for Symmetry (i.e., using the ‘Adjusting Screws’ \\(\\mathcal{X}\\) and \\(h\\)): We can achieve a more efficient estimator (i.e., an estimator with lower variance for a given same sample size \\(n\\)) if we take into account the symmetric nature of \\(\\mathrm{C}(0,1)\\). Obviously, our target integral can be equivalently written as \\[\np=\\frac{1}{2}\\left(\\int_{-\\infty}^{-2}\\frac{1}{\\pi(1+x^2)}\\,dx + \\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx \\right).\n\\] This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean: \\[\n\\hat{p}_{2}=\n\\frac{1}{2}\\left(\\frac{1}{m}\\sum_{j=1}^m1_{(X_j<-2)}+ \\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\\right)\\;=\\;\n\\frac{1}{2m}\\sum_{j=1}^m1_{(|X_i|>2)}.\n\\] The variance of this new estimator, \\[\n\\mathbb{V}(\\hat{p}_{2})=\\frac{1}{4m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(|X_i|>2)}\\right)=\\frac{2p(1-2p)}{4m},\n\\] is equal to \\(0.0525/m\\), i.e., lower than in the naive approach.\n\n\n3. Using all ‘Adjusting Screws’ \\(\\mathcal{X}\\), \\(h\\), and \\(f\\): The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, \\([2,+\\infty)\\), which are in some sense irrelevant for the approximation of \\(p\\). This motivates the following reformulation of \\(p\\):\nBy symmetry of \\(f\\): \\[\n\\frac{1}{2}=\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx + \\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}dx}_{=p}\n\\] \\[\n\\Leftrightarrow \\; p=\\frac{1}{2}-\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx.\n\\] Furthermore, we can re-arrange the last integral a bit such that \\[\n\\int_{0}^2\\;\\left(\\frac{1}{2}\\cdot 2\\right)\\;\\frac{1}{\\pi(1+x^2)}\\,dx =\n\\int_{0}^2\\;\\underbrace{\\frac{1}{2}}_{f_{\\mathrm{Unif}[0,2]}}\\;\\underbrace{\\frac{2}{\\pi(1+x^2)}}_{=h(x)}\\,dx =\n\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,2].\n\\]\nTherefore a new alternative method for evaluating \\(p\\) is: \\[\n\\hat{p}_{3}=\\frac{1}{2} - \\frac{1}{m}\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U_j\\sim\\mathrm{Unif}[0,2].\n\\] Using integration by parts, it can be shown that \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\). (Compare this to the former results: \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\).)\n\n\nA More General Point of View:\nThe idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx.\n\\]\nSome outcomes of \\(X\\sim f\\) may be more important than others in determining \\(\\theta\\) and we wish to select such values more frequently.\nFor instance, if \\(\\theta\\) denotes the probability of the occurrence of a very rare event, then the only way to estimate \\(\\theta\\) at all accurately may be to produce the rare events more frequently.\nTo achieve this, we can simulate a model which gives pdf \\(g\\) to \\(X\\) instead of the correct pdf \\(f\\), where both pdfs need to be known. This can be easily done, since \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)\\left(\\frac{g(x)}{g(x)}\\right)\\;f(x)dx=\n\\int \\underbrace{\\left(h(x)\\frac{f(x)}{g(x)}\\right)}_{=\\psi(x)}\\;g(x)dx=\n\\int \\psi(x)\\;g(x)dx=\n\\mathbb{E}_g(\\psi(X)).\n\\]\nThis leads to the following unbiased estimator for \\(\\theta\\) based on sampling from \\(g\\): \\[\n\\hat{\\theta}_g=\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i)\\quad\\text{with}\\quad X_i\\sim g,\n\\] which is a weighted mean of the \\(h(X_i)\\) with weights inversely proportional to the “selection factor” \\(\\frac{g(X_i)}{f(X_i)}\\). \nFor the variance of the estimator \\(\\hat{\\theta}_g\\) we have \\[\n\\mathbb{V}(\\hat{\\theta}_g)=\\frac{1}{n}\\mathbb{V}(\\psi(X_i))=\n\\frac{1}{n}\\int\\left(\\psi(x)-\\theta\\right)^2g(x)dx=\n\\frac{1}{n}\\int\\left(\\frac{h(x)\\,f(x)}{g(x)}-\\theta\\right)^2g(x)dx,\n\\] which, depending on the choice of \\(g(.)\\), can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean. \n\n\n\n\nMinimum Variance Theorem\n\nThe importance function \\(g(.)\\) which minimizes the variance \\(\\mathbb{V}(\\psi(X_i))\\), and therefore the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\), is given by \\[\ng^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\]\n\n\nProof: Done in the lecture.\n\n\nThough, this result is rather formal (in the sense of “impractical”), since, e.g., if \\(h(x)>0\\) then \\(g^\\ast\\) requires us to know \\(\\int h(z)f(z)dz\\), which is just the integral of interest!\nRemarks:\nThe above minimum variance result is still useful:\n\nIt tells us that a good choice of \\(g(x)\\) shall mimic the shape of \\(|h(x)|f(x)\\), since the optimal \\(g^\\ast(x)\\propto |h(x)|f(x)\\).\nFurthermore, \\(g(x)\\) should be chosen such that it has a thicker tail than \\(f(x)\\), since the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\) crucially depends on the quotient \\(f(x)/g(x)\\) which would “explode” for \\(g(x)\\approx 0\\).\n\n\n\nLet’s apply our new insights to the above example on the Cauchy tail probability \\(p\\).\nExample: Cauchy Tail Probability (cont.)\nAbove we had:\n\n\\(f(x)=\\frac{1}{\\pi(1+x^2)}\\), the pdf of \\(\\mathrm{C}(0,1)\\) and\n\\(h(x)=1_{(x>2)}\\), i.e., here \\(|h(x)|=h(x)\\).\n\nTherefore \\[\np=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx=\\int_{2}^{\\infty}f(x)dx=\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx=\\mathbb{E}_g(\\psi(X)),\n\\] where the \\(h\\) function is absorbed by the formulation of the definite integral.\nA possibly good (and simple) choice of \\(g\\) is, e.g., \\(g(x)=2/(x^2)\\), since this function:\n\n“closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: It is not straight forward to directly sample from \\(g\\), therefore we need some further steps:\n\n\nThe choice of \\(g\\) leads to \\[\np=\\mathbb{E}_g(\\psi(X))=\n\\int_{2}^{+\\infty}\\left(\\frac{x^2}{2\\,\\pi(1+x^2)}\\right)\\,\\frac{2}{x^2}\\,dx=\n\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\]\n\n\nNow we can apply some additional (rather case-specific) re-arrangements:\nIntegration by substitution (substituting \\(u=x^{-1}\\)) yields: \\[\np=\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] Again, we can re-arrange the last integral a bit such that \\[\np=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathrm{Unif}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du=\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2].\n\\] Therefore, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2]\n\\] and \\(h(u)=1/(2\\pi(1+u^2))\\).\nThe variance of \\(\\hat{p}_4\\) is \\((\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2)/m\\) and an integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.95\\cdot 10^{-4}/m\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox, George EP, and Mervin E Muller. 1958. “A Note on the Generation of Random Normal Deviates.” The Annals of Mathematical Statistics 29 (2): 610–11. https://projecteuclid.org/euclid.aoms/1177706645.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch_Bootstrap.html",
    "href": "Ch_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.1 The empirical distribution function",
    "text": "3.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 3.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 3.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#basic-idea",
    "href": "Ch_Bootstrap.html#basic-idea",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic idea",
    "text": "3.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 3.1)."
  },
  {
    "objectID": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The nonparametric (standard) bootstrap",
    "text": "3.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 3.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n3.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n3.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{3.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "3.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 3.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 3.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{3.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 3.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.5 Regression Analysis: Bootstrapping pairs",
    "text": "3.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 3.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n3.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Residual bootstrap",
    "text": "3.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 3.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html",
    "href": "Ch_MaximumLikelihood.html",
    "title": "4  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Properties of Maximum Likelihood Estimators",
    "text": "4.2 Properties of Maximum Likelihood Estimators\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\nExample: Coin Flipping (Bernoulli Trial)\nLet \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "href": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "title": "4  Maximum Likelihood",
    "section": "4.3 The (Log-)Likelihood Function",
    "text": "4.3 The (Log-)Likelihood Function\nHow do we combine information from \\(n\\) observations to estimate \\(\\theta\\)?\nIf we assume that all of the observations are drawn from same distribution and are independent, then joint probability of observing \\(h\\) heads and \\(n-h\\) tails in the \\(n\\) coin flips that we actually observed, given \\(\\theta\\), is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)&= \\theta^h(1-\\theta)^{n-h}  \\\\\n            &= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\texttt{HEAD}\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\texttt{TAIL}\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations are identically and independently distributed (i.i.d): \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\); the parameter \\(\\theta\\) denotes the density function parameter(s).\nOur goal is to choose a value for \\(\\theta\\) such that the value of the likelihood function is at a maximum, i.e. we choose the value of the parameter(s) that maximize the “probability” or better the likelihood of observing the data that we actually observed. That is: \\[\n\\hat\\theta=\\arg\\max_\\theta \\mathcal{L}(\\theta).\n\\] defines the maximum likelihood (ML) parameter estimator \\(\\hat\\theta\\).\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation (taking \\(\\ln\\)) to the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\]\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn this case, we can analytically solve for the value of \\(\\theta\\) that maximizes the log likelihood (and hence also the likelihood): \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\hat\\theta_{ML}=\\dfrac{h}{n}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Optimization: Non-Analytical Solutions",
    "text": "4.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 4.1).\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 4.1.\n\n\nTable 4.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "4  Maximum Likelihood",
    "section": "4.3 OLS-Estimation as ML-Estimation",
    "text": "4.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{4.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 3.4).\nFor the following, it is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 4.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.5}\\]\nUsing Equation 4.4 and Equation 4.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "href": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "title": "2  Random Variable Generation",
    "section": "2.4 Building on Exponential RVs",
    "text": "2.4 Building on Exponential RVs\nIn Example 2.5, we learned to generate an exponential random variable \\(X\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from an exponential distribution:\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\nSome Limitations:\n\nThere are more efficient algorithms to generate Gamma and Beta RVs.\nWe cannot use exponential RVs to generate Gamma RVs with a non-integer shape parameter \\(\\alpha\\). * This implies that we cannot generate a \\(\\chi^2_{1}\\) RV, which would, in turn, get us a \\(N(0,1)\\) RV. (Reminder: \\(\\chi^2_{1}\\) is identical to \\(\\Gamma(1/2, 2)\\).)\nFor that we look at the Box-Muller Theorem (1958) and the derived algorithm.\n\n\n\n\n\nExample: Normal Variable Generation\nThe well-known Box-Muller algorithm for generating (standard) normal RV is based on the following theorem:\n\nTheorem (Box and Muller, 1958)\n\nIf \\(U_1\\) and \\(U_2\\) are i.i.d. \\(U[0,1]\\), then \\[X_1 =\\sqrt{-2 \\log(U_1)}\\, \\cos(2\\pi U_2)\\quad\\text{and}\\quad X_2=\\sqrt{-2\\log(U_1)}\\,\\sin(2\\pi U_2)\\] are i.i.d. \\(N(0,1)\\).\n\n\nIdea & Proof: Done in the lecture.\n\n\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # 1. Step: Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # 2. Step: Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  return(c(X1, X2))\n}\n\n# Generation of Stand. Normal RVs through the Box-Muller Algo:\nset.seed(123)\nX_vec <- NULL\nfor(i in 1:500){\n  X_vec <- c(X_vec, BM_Algo())\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE)\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test (H0: Normality)\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99893, p-value = 0.8323\n\n\n\n\n\n\n\n\n2.4.1 Accept-Reject Methods\nFor many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the cdf \\(F(.)\\) is somehow unusable. For instance, surprisingly often there is no explicit form of \\(F(.)\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\n\nGeneral Idea and theoretical justification through the Fundamental Theorem of Simulation: Done in the lecture.\n\n\nThe case of pdfs with compact support:\nThe key-idea is easily explained using a bounded pdf \\(f\\) with compact support.\nNotions:\n\nBounded means that there exists a value \\(m\\) with \\(0<m<\\infty\\) s.t. \\(f(x)\\in[0,m]\\) for all \\(x\\).\nNote that only degenerated pdfs are not bounded.\nAn interval \\([a,b]\\) is called “compact” if it is closed and the boundaries are finite.\nFor instance, the Gaussian has not a compact support, since \\(\\mathrm{supp}(\\phi)=]-\\infty,\\infty[\\).\n\nFor instance, let’s say we want to simulate random numbers \\(X\\sim f\\) with \\[\nf(x)=\\frac{3}{4}\\left(1-\\left(x-1\\right)^2\\right)\\,1_{(|x-1|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[-1,1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f\\) is bounded from above by \\(3/4\\).\n\nThe idea is then to simulate the random pair \\((Y,U)\\sim\\mathrm{Unif}([a,b]\\times[0,m])\\) by simulating\n \\[Y\\sim\\mathrm{Unif}[a,b]\\quad\\text{and}\\quad U|Y=y \\sim \\mathrm{Unif}[0,m], \\] but to accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\leq x)=\\mathbb{P}(Y\\leq x|U\\leq f(Y))\n=\\frac{\\int_a^{\\color{red}x} \\int_0^{f(y)}\\,1\\,du\\,dy}{\\int_a^{\\color{red}b}\\int_0^{f(y)}\\,1\\,du\\,dy}\n=\\frac{\\int_a^x f(y)\\,dy}{\\int_a^b f(y)\\,dy}\n=\\int_a^x f(y)dy,\n\\]\nwhere we used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nThe Accept-Reject Algorithm (Simple Version):\n# Accept-Reject Algorithm:\nY <- runif(n, min = a, max = b) \nU <- runif(n, min = 0, max = m) \n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\nIn the following you see a graphical illustration of this procedure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe good thing is that we only need to evaluate the pdf \\(f(.)\\); nothing more.\n\n\nGeneralization: pdfs with non-compact support.\nThe larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set”, enclosing the pdf \\(f\\), as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of \\(f\\) is unbounded.\nLet the larger set denote by \\[\n\\mathscr{L}=\\{(y,u):\\, 0<u<m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathscr{L}\\) is feasible and\n\n\\(m(x)\\geq f(x)\\) for all \\(x\\).\n\n\n\nFrom the feasibility-requirement it follows that \\(m(.)\\) is necessarily integrable, i.e., that \\[\\int_{\\mathcal{X}}m(x)dx=M,\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathscr{L}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathscr{L}\\).\n\n\nIntegrability of \\(m(.)\\) is crucial here, since it allows us to relate \\(m(.)\\) with a corresponding (auxiliary) pdf \\(g(.)\\) as following: \\[m(x)=M\\,g(x),\\quad\\text{where}\\quad\\int_{\\mathcal{X}}m(x)\\,dx=\\int_{\\mathcal{X}}M\\,g(x)\\,dx=M.\\]\nTerminology:\n\nThe pdf \\(g(.)\\) is called the instrumental density. (Choose \\(g(.)\\) as a pdf from which it is easy to simulate!)\nThe pdf \\(f(.)\\) is called the target density.\n\n\n\nIn order to simulate the pair \\((Y,U)\\sim\\mathrm{Unif}(\\mathscr{L})\\) we can now simulate \\[Y\\sim g\\quad\\text{and}\\quad U|Y={\\color{red}y}\\sim\\mathrm{Unif}[0,M\\,g({\\color{red}y})],\\] but accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\in A)=\\mathbb{P}(Y\\in A|U\\leq f(Y))\n=\\frac{\\int_{\\color{red}A}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_\\mathcal{X}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\n=\\frac{\\int_A f(y)\\,dy}{\\int_\\mathcal{X} f(y)\\,dy}\n=\\int_A f(y)dy,\n\\] for every set \\(A\\),  where we again used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nNote that the above derivation implies that we only need to know the pdf \\(f(.)\\) up to an unkown multiplicative constant \\(c>0\\). I.e., it is enough to know \\(f(x)=c\\,\\tilde{f}_{\\textrm{true}}(x)\\), often written as \\(f(x)\\propto \\tilde{f}_{\\textrm{true}}(x)\\), since the unknown constant \\(c\\) cancels out in the above quotient anyways. This is not so much of importance for us, but useful in Bayesian Statistics.\n\n\nAll this leads to a more general version of the Fundamental Theorem of Simulation:\n\nFundamental Theorem of Simulation (General Version):\n\nLet \\(X\\sim f\\) and let \\(g(.)\\) be a pdf s.t. \\(f(x)\\leq M\\,g(x)\\) for some \\(M\\) with \\(1\\leq M<\\infty\\) and all \\(x\\). Then to simulate \\(X\\sim f\\) it is sufficient to generate \\[Y\\sim g\\quad\\text{and}\\quad U|Y=y\\sim\\mathrm{Unif}[0,M\\,g(y)]\\] if one accepts the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and rejects all others.\n\n\n\n\nThe Accept-Reject Algorithm (General Version):\n# Accept-Reject Algorithm:\nY   <- generate n random numbers from g(.)\n\n# Specify function m():\nm <- function(y){YOUR CODE}\n\nU   <- numeric(n)\nfor(i in 1:n){\n  U[i] <- runif(n=1, min = 0, max = m(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\n\n\nExample\nLet the target “density” be \\[f(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\] with upper bound (or, rather, dominating density) the standard normal density \\[g(x)=\\exp(-x^2/2)/\\sqrt{2\\pi},\\] which is obviously straightforward to generate.\nIn this example we can set \\(m(x)=M\\,g(x)\\) with \\(M=1\\), since we can simply scale the target “density” \\(f\\) such that \\(f(x)\\leq g(x)\\) for all \\(x\\). Specifically, we set \\(f(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\).\nIn the following you see the graphical illustration of this example:\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of the Accept-Reject algorithm:\nStatements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f\\) and \\(g\\) are normalized such that they are both pdfs. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. (The closer \\(M\\) is to \\(1\\) the better.)\n\\(M\\) is a function of how closely \\(g\\) can imitate \\(f\\).\n\nNote that, for such normalized \\(f\\) and \\(g\\) the inequality \\(f(x)\\leq M\\,g(x)\\) with \\(1\\leq M<\\infty\\) for all \\(x\\) is equivalent to saying that the quotient \\(f/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f(x)}{g(x)}\\leq M <\\infty\\quad\\text{for all}\\quad x.\n\\] That is, it is necessary for \\(g\\) to have, e.g., thicker tails than \\(f\\). This makes it, for instance, impossible to simulate a Cauchy distribution \\(f\\) using a normal distribution \\(g\\). The reverse, however, works quite well. \n\n\nExample: Normals from Double Exponentials\nConsider generating a \\(N(0,1)\\) by the Accept-Reject algorithm using a double-exponential distribution \\(\\mathcal{L}(\\alpha)\\), also called Laplace distribution, with density \\(g(x|b)=(1/(2b))\\exp(-\\,|x|/b)\\).  It is then straightforward to show that \\[\n\\frac{f(x)}{g(x|b)}\n%=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\n%=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\n\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf: \\[\n\\frac{f(x)}{g(x|1)}\n\\leq M=\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right).\n\\]\nThe probability of acceptance is then \\(\\sqrt{\\pi/(2e)}=0.76\\). I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average \\(1/0.76\\approx 1.3\\) uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1."
  },
  {
    "objectID": "Ch4_EMAlgorithmus.html",
    "href": "Ch4_EMAlgorithmus.html",
    "title": "4  The Expectation Maximization (EM) Algorithm",
    "section": "",
    "text": "Possible Applications of Gaussian mixture distributions:\n\nGeneral: Finding grouping structures (two or more) in data (Clusteranalyse). For instance:\n\nAutomatic video editing (e.g., separation of back- and foreground)\n\nBehavioral clustering\netc.\n\n\n\n\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22."
  },
  {
    "objectID": "Ch2_Bootstrap.html",
    "href": "Ch2_Bootstrap.html",
    "title": "2  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch2_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch2_Bootstrap.html#the-empirical-distribution-function",
    "title": "2  The Bootstrap",
    "section": "2.1 The empirical distribution function",
    "text": "2.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 2.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 2.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 2.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch2_Bootstrap.html#basic-idea",
    "href": "Ch2_Bootstrap.html#basic-idea",
    "title": "2  The Bootstrap",
    "section": "2.2 Basic idea",
    "text": "2.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 2.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 2.1)."
  },
  {
    "objectID": "Ch2_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch2_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "2  The Bootstrap",
    "section": "2.3 The nonparametric (standard) bootstrap",
    "text": "2.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 2.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n2.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n2.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n2.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{2.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{2.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 2.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch2_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch2_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "2  The Bootstrap",
    "section": "2.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "2.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 2.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n2.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 2.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{2.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 2.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n2.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch2_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch2_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "2  The Bootstrap",
    "section": "2.5 Regression Analysis: Bootstrapping pairs",
    "text": "2.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 2.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n2.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 2.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 2.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch2_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch2_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "2  The Bootstrap",
    "section": "2.6 Regression Analysis: Residual bootstrap",
    "text": "2.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 2.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n2.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 2.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 2.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html",
    "href": "Ch3_MaximumLikelihood.html",
    "title": "3  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch3_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "3  Maximum Likelihood",
    "section": "3.2 Optimization: Non-Analytical Solutions",
    "text": "3.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n3.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 3.1).\n\n\n\n\n\nFigure 3.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 3.1.\n\n\nTable 3.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch3_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "3  Maximum Likelihood",
    "section": "3.3 OLS-Estimation as ML-Estimation",
    "text": "3.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{3.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 2.4).\nFor the following, it is convenient to write Equation 3.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch3_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "3  Maximum Likelihood",
    "section": "3.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "3.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch3_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "3  Maximum Likelihood",
    "section": "3.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "3.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 2.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch3_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "3  Maximum Likelihood",
    "section": "3.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "3.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 3.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 3.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{3.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 3.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{3.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{3.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{3.5}\\]\nUsing Equation 3.4 and Equation 3.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 3.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 3.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{3.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 3.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 3.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 3.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch2_MonteCarlo.html",
    "href": "Ch2_MonteCarlo.html",
    "title": "2  Monte Carlo Integration",
    "section": "",
    "text": "In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:\n\nMonte Carlo Statistical Methods, Ch. 3, Robert and Casella (2004)\nIntroducing Monte Carlo Methods with R, Ch. 3, Robert and Casella (2010)\nNumerical Methods in Economics, Ch. 8.2 Monte Carlo Integration, Judd (1998)\n\nMonte Carlo methods take advantage of the availability of:\n\ncomputer generated random variables\nthe law of large numbers\nthe central limit theorem\n\nTerminology:\n\nMonte Carlo Method: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.\nMonte Carlo Integration: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a univariate and multivariate integral. (Integrals are everywhere in statistics!)\nStochastic Simulation (or Monte Carlo Simulation): The application of the Monte Carlo method.\n\nThis chapter is about Monte Carlo Integration which is a stochastic alternative to deterministic numerical integration methods such as numerical quadrature.\nFigure 2.1 shows a screenshot of a published example (see Bourreau, Sun, and Verboven (2021)). The authors use Monte Carlo integration to solve the shown integral.\n\n\n\nFigure 2.1: Market share function in Bourreau, Sun, and Verboven (2021). The authors use Monte Carlo integration to solve this integral."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#classical-monte-carlo-integration",
    "href": "Ch2_MonteCarlo.html#classical-monte-carlo-integration",
    "title": "2  Monte Carlo Integration",
    "section": "2.1 Classical Monte Carlo Integration",
    "text": "2.1 Classical Monte Carlo Integration\nThe generic problem here is the evaluation of integrals. For instance, \\[\n\\mathbb{E}_{f_{X}}\\left(h(X)\\right)=\\mathbb{E}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f_X(x)\\,dx,\n\\tag{2.1}\\] where \\(\\mathcal{X}\\) denotes the domain of the random variable \\(X\\in\\mathcal{X}\\subseteq\\mathbb{R}^d,\\) and where \\(h\\) is some transformation function, e.g., \\[\nh(x)=x^2,\\;\\;h(x)=\\ln(x),\\;\\;h(x)=x,\\;\\;\\text{etc.}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nComputing means means computing integrals. To stress that one computes the integral with respect to the distribution characterized by the density function \\(f_X,\\) one can write \\[\n\\mathbb{E}_{f_{X}}\\left(h(X)\\right)\n\\] instead of \\[\n\\mathbb{E}\\left(h(X)\\right).\n\\] We will use this notation more often below.\n\n\nOften, analytical solutions for integrals such as in Equation 2.1 are not readily available and one needs to use some numerical approaches/computational. Given our previous developments, it is kind of natural to propose using a realization \\[\nx_1,\\dots,x_n\n\\] from a (pseudo) random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}f_X\n\\] to approximate the integral in Equation 2.1 using the empirical mean \\[\n\\mathbb{E}\\left(h(X)\\right)\\approx\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(x_i).\n\\] By the Strong Law of Large Numbers (SLLN) we know that the empirical mean \\(\\bar{h}_n\\) converges almost surely (a.s.), and thus also “in probabiliuty” to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\) as the sample size \\(n\\) becomes large, i.e., as \\(n\\to\\infty\\). Prerequisites for the SLLN:\n\n\\(h(X)\\) has finite first moment, i.e., \\(\\mathbb{E}\\left(h(X)\\right)<\\infty\\) and\n\\(\\bar{h}_n\\) is constructed from a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}f_X.\\)\n\nAs we can use the computer to produce realizations from the i.i.d. sample \\(X_1,\\dots,X_n\\), we can in principle choose an arbitrary large sample size \\(n\\) such that \\(\\bar{h}_n\\) can, in principle, be arbitrarily close to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\).\nThough, …\n\n… which sample size \\(n\\) is large enough?\nor “equivalently”, how fast converges \\(\\bar{h}_n\\) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\)?\n\n\n2.1.1 Speed of Convergence\nOK, we know now that \\(\\bar{h}_n\\) reaches its limit (here in the “almost surely” sense, but likewise in the “in probability” case) as \\(n\\to\\infty\\) under some rather loose conditions on the random sample \\(X_1,\\dots,X_n\\).\nIf we are willing to additionally assume that \\(h(X)\\) has finite second moments, i.e. \\[\n\\mathbb{E}(h(X)^2)<\\infty,\n\\] then we can additionally say something about how fast \\[\n\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)\\to_{p} \\mathbb{E}(h(X)).\n\\]\nThe speed of convergence of the stochastic sequence \\[\n\\{\\bar{h}_n\\}_{n=1,2,\\dots}= \\bar{h}_1,\\;\\bar{h}_2,\\;\\;\\bar{h}_3,\\;\\dots\n\\] to its limit \\(\\mathbb{E}(h(X))\\) can be quantified by the rate at which the standard error \\[\n\\operatorname{SE}\\left(\\bar{h}_n\\right)=\\sqrt{\\mathbb{V}\\left(\\bar{h}_n\\right)}\n\\] converges to zero as \\(n\\to\\infty\\).\n\n\n\n\n\n\nTip\n\n\n\nWe think of \\(\\{\\bar{h}_n\\}_{n=1,2,\\dots}\\) as the sequence of random variables\n\\[\n\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h({\\color{red}{X_{i}}})\n\\] with \\({\\color{red}{X_1}},\\dots,{\\color{red}{X_n}}\\overset{\\text{i.i.d.}}{\\sim}f_X.\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that assuming finite second moments \\(\\mathbb{E}(h(X)^2)<\\infty\\) is equivalent to assuming finite variance \\(\\mathbb{V}\\left(h(X)\\right)<\\infty,\\) since \\[\n\\mathbb{V}\\left(h(X)\\right) = \\mathbb{E}(h(X)^2) - \\left(\\mathbb{E}(h(X))\\right)^2,\n\\] and since if higher moments, like \\(\\mathbb{E}(h(X)^2),\\) are finite, also the lower moments, like \\(\\mathbb{E}(h(X)),\\) are finite.\n\n\nThe standard error of \\(\\bar{h}_n\\) is just the square root of the variance of \\(\\bar{h}_n.\\) The variance of \\(\\bar{h}_n\\) is given by \\[\n\\begin{align*}\n\\mathbb{V}\\left(\\bar{h}_n\\right)\n&=\\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n h(X_i)\\right) \\\\[2ex]\n&=\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n h(X_i)\\right) \\\\[2ex]\n&=\\frac{n}{n^2}\\mathbb{V}\\left(h(X_1)\\right)\\quad \\text{(since i.i.d.)} \\\\[2ex]\n&=\\frac{1}{n}  \\mathbb{V}\\left(h(X_1)\\right)\n\\end{align*}\n\\] The square root of \\(\\mathbb{V}\\left(h(X_1)\\right)\\) equals some finite, positive constant \\(0<\\mathtt{const}<\\infty\\),\n\\[\n\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X_1)\\right)}\n\\] such that \\[\n\\sqrt{\\mathbb{V}\\left(\\bar{h}_n\\right)}=n^{-1/2}\\mathtt{const}%\\propto n^{-1/2}.\n\\] I.e., the speed of convergence (or rate) of the stochastic sequence \\(\\{\\bar{h}_n\\}\\) is proportional to the deterministic sequence \\(\\{n^{-1/2}\\}.\\)\n\n\n\n\n\n\nNote\n\n\n\nEven if we would not know the value of \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)},\\) we know now that the improvement from \\(n=10\\) to \\(n=100\\) will be much higher than from \\(n=110\\) to \\(n=200\\). In practice, a typical choice is \\(n=10,000;\\) for moderate standard errors this choice will guarantee a very good approximation.\n\n\n\n\n2.1.2 Distributional Properties\nBesides the speed of convergence of \\(\\{\\bar{h}_n\\}_{n=1,2,\\dots}\\) for \\(n\\to\\infty,\\) we can also say something about the distribution of the random variable \\(\\bar{h}_n\\) for large sample sizes \\(n.\\)\nWe can estimate the variance of the estimator \\(\\mathbb{V}\\left(\\bar{h}_n\\right)\\) by its empirical version \\[\nv_n^2=\\frac{1}{n}\\sum_{i=1}^n\\left(h(x_i)-\\bar{h}_n\\right)^2,\n\\] where by the SLLN, which also implies convergence in probability, \\[\nv_n^2\\to_{p}\\mathbb{V}\\left(h(X)\\right),\\quad n\\to\\infty.\n\\]  Then, by the Continuous Mapping Theorem (CMT), the Central Limit Theorem (CLT), and Slutsky’s theorem, we have that \\[\n\\sqrt{n}\\left(\\frac{\\bar{h}_n - \\mathbb{E}\\left(h(X)\\right)}{v_n}\\right)\\to_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] \nThe above result can now be used for the construction of (asymptotically valid) convergence tests and confidence intervals with respect to \\(\\bar{h}_n\\), since for large \\(n\\) \\[\n\\bar{h}_n\\,\\overset{d}{\\approx}\\mathcal{N}\\left(\\mathbb{E}\\left(h(X)\\right),\\frac{\\mathbb{V}\\left(h(X)\\right)}{n}\\right).\n\\]\nSince we can use the computer to generate realizations of the i.i.d. sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim f_X,\\) we can easily approximate the mean \\[\n\\mathbb{E}\\left(h(X)\\right)\\approx \\bar{h}_n\n\\] and the variance \\[\n\\mathbb{V}\\left(h(X)\\right)\\approx v_n^2\n\\] with arbitrary accuracy as \\(n\\to\\infty\\) (justification: strong/weak law of large numbers).\n\nExample 2.1 (A first Monte Carlo Integration) Let’s say we want to compute the integral \\[\n\\int_0^1h(x)dx\n\\] with \\[\nh(x)=\\left(\\cos(50\\,x)+\\sin(20\\,x)\\right)^2\n\\] over \\(x\\in[0,1].\\) Although this integral could also be computed analytically, it is a good first test case. The following code computes the analytic result that \\(\\int_0^1h(x)dx = 0.9652009.\\)\n\n## install.packages(\"mosaicCalc\")\nsuppressPackageStartupMessages(library(\"mosaicCalc\"))\n\n## Symbolic (= analytic) integration \nF <- antiD( (cos(50*x)+sin(20*x))^2 ~ x)\n\nF(1) - F(0)\n\n[1] 0.9652009\n\n\nFigure 2.2 shows the graph of the function \\(h\\).\n\nh_fun <- function(x){\n  result <- (cos(50*x)+sin(20*x))^2\n  return(result)\n}\n\nxx  <- seq(from=0, to=1, len=500)\nplot(x = xx, \n     y = h_fun(xx), \n     type=\"l\", \n     main=\"Function h\", \n     xlab=\"x\", ylab=\"h(x)\")\n\n\n\n\nFigure 2.2: Function \\(h\\) of Example 2.1.\n\n\n\n\nTo approximate the integral \\[\n\\int_0^1 h(x)dx\n\\] using Monte Carlo integration, we can use that \\[\n\\begin{align*}\n\\int_0^1 h(x)dx\n&=\\int_0^11\\cdot h(x)dx \\\\[2ex]\n&=\\int_0^1f_{\\mathcal{U}\\text{[0,1]}}(x)\\cdot h(x)dx \\\\[2ex]\n&= \\mathbb{E}_{f_{\\mathcal{U}\\text{[0,1]}}}(h(X)),\n\\end{align*}\n\\] where \\(f_{\\mathcal{U}\\text{[0,1]}}\\) denotes the density function of the standard uniform distribution \\(\\mathcal{U}\\text{[0,1]}.\\)\nThus, to compute \\(\\int_0^1 h(x)dx\\) we generate a realization \\((u_1,\\dots,u_n)\\) from the random sample \\(U_1,\\dots,U_n\\sim \\mathcal{U}[0,1]\\) and approximate \\[\n\\int_0^1 h(x)dx\\approx \\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(u_i).\n\\]\nIn order to assess how good this approximation is, we need to consider the stochastic properties of the random variable \\[\n\\bar{h}_n = \\frac{1}{n}\\sum_{i=1}^n h(U_i).\n\\] This is done using the above (review of) results on the limit distribution of the sample mean \\(\\bar{h}_n\\) which allows us to construct an approximate \\(95\\%\\) confidence interval, since for large \\(n\\) \\[\n\\begin{align*}\n\\operatorname{CI}^{95\\%}_n\n=&\\left[\\bar{h}_n - z_{1-\\alpha/2}\\sqrt{\\frac{v_n^2}{n}}, \\bar{h}_n + z_{1-\\alpha/2}\\sqrt{\\frac{v_n^2}{n}}\\right]\\\\[2ex]\n\\approx&\n\\left[\\bar{h}_n - z_{1-\\alpha/2} \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}, \\bar{h}_n + z_{1-\\alpha/2} \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}\\right],\n\\end{align*}\n\\] where \\(z_{1-\\alpha/2}\\approx 1.96\\) denotes the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1),\\) \\(v_n^2=n^{-1}\\sum_{i=1}^n(h(u_i)-\\bar{h}_n)^2,\\) and where \\[\nP\\left(\\int_0^1 h(x)dx  \\in \\operatorname{CI}^{95\\%}_n \\right) \\to 0.95,\\quad n \\to\\infty,\n\\] by the CLT.\nFigure 2.3 shows one realization of the stochastic sequence \\[\n\\bar{h}_1,\\dots,\\bar{h}_n\n\\] with \\(n=10000\\), where the realized value of \\(\\bar{h}_n\\) is \\(0.966\\). This compares favorably with the with the exact value of \\(\\int_0^1h(x)dx = 0.9652009.\\)\n\nlibrary(\"scales\")\n# h(x):\nh_fun <- function(x){\n  result <- (cos(50*x)+sin(20*x))^2\n  return(result)\n}\n\n# sample size\nn <- 10000\n\n# Generate sample of uniforms\nset.seed(321)\nu_vec <- runif(n=n)\n\n# Approximation of the integral \nh_bar_n <- cumsum(h_fun(u_vec))/c(1:n)\n\n# True value:\ntrue.value <- 0.9652009\n\n# 95% CI\n# Standard error of the estimator using the \"algebraic \n# formula\" for the variance (german: 'verschiebungssatz')\nst.error_n <-  sqrt((cumsum(h_fun(u_vec)^2)/(1:n) - \n                     cumsum(h_fun(u_vec))^2/(1:n)^2))\n\nCI_u       <-  h_bar_n + 1.96 * st.error_n / sqrt(1:n)\nCI_l       <-  h_bar_n - 1.96 * st.error_n / sqrt(1:n)\n\nplot(x = c(1:n), y = h_bar_n, type=\"n\", \n     ylim=c(0.7,1.2), \n     xlab = \"n\", \n     ylab = \"\")\npolygon(x = c(1:n, rev(1:n)), \n        y = c(CI_u, rev(CI_l)), \n        col    = alpha(\"blue\", 0.5), \n        border = alpha(\"blue\", 0.5))\nlines(x = c(1:n), \n      y = h_bar_n, type=\"l\")\nlines(x = c(1:n), \n      y = rep(true.value, n), type=\"l\", col=\"red\")\n##\nlegend(\"topright\", \n       legend = c(expression(bar(h)[n]), \n                  \"True Value\", \"95% CI\"), \n       lty   = c(1,1,0), pch=c(22,22,22), pt.cex=c(0,0,2),\n       pt.bg = c(\"black\", \"red\", alpha(\"blue\", 0.5)), \n       col   =   c(\"black\", \"red\", alpha(\"blue\", 0.5)))\n\n\n\n\nFigure 2.3: One realization of the stochastic sequence \\(\\bar{h}_1,\\dots,\\bar{h}_n\\) with \\(n=10000\\), where the realized value of \\(\\bar{h}_{n=10000}\\) is \\(0.966\\). The blue band shows the point-wise (i.e. for each given sample size \\(n\\)) confidence intervals \\(\\operatorname{CI}^{95\\%}_n.\\)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe approach of Example 2.1 can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency/accuracy through numerical methods (e.g., Riemann Sum, Trapezoidal Rule, Simpson’s Rule, etc.) in dimensions 1 or 2; see the following code example:\n\nnumericalIntegration <- integrate(f     = h_fun, \n                                  lower = 0, \n                                  upper = 1)\nnumericalIntegration\n\n0.9652009 with absolute error < 1.9e-10\n\n\nHowever, the Monte Carlo integration approach is particularly useful for approximating integrals over higher dimensional sets \\(\\mathcal{X}\\subseteq\\mathbb{R}^d.\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#importance-sampling",
    "href": "Ch2_MonteCarlo.html#importance-sampling",
    "title": "2  Monte Carlo Integration",
    "section": "2.3 Importance Sampling",
    "text": "2.3 Importance Sampling\nAs demonstrated in Example 2.2, the accuracy of the Monte Carlo integration method as a tool for approximating integral values depends on the variance of the estimate that approximates the integral value.\n“Importance sampling” aims to reduce the variance of the Monte Carlo integral estimator. Therefore, importance sampling is also refereed to as a variance reduction technique. This variance reduction is achieved by weighting functions, so-called importance functions.\nAs in the case of classical Monte Carlo integration (see Equation 2.1), the generic problem is the evaluation of the integral \\[\n\\mathbb{E}_{f}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f_X(x)\\,dx.\n\\]\nHowever, it turns out that the classical Monte Carlo integration approach, i.e., a direct sampling from \\(f\\) is often suboptimal.\nObserve that the value of the above integral can be represented by infinitely many alternative choices of the triplet \\[\n(\\mathcal{X}, h, f).\n\\] Therefore, the search for an optimal estimator should encompass all these possible representations.\nLet’s illustrate this with a simple example.\n\nExample 2.3 (Cauchy Tail Probability) This example is from Ripley (2009).\nSuppose that the quantity of interest is the probability, say \\(p\\), that a \\(\\mathcal{Cauchy}(0,1)\\)-distributed random variable is larger than \\(2\\), i.e. \\[\np=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx,\n\\tag{2.4}\\] where \\(1/(\\pi(1+x^2))\\) is the density function of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution.\nThis is a nice example. One the one hand, it allows us to showcase possibilities to improve efficiency of Monte Carlo integration. On the other hand, we know already the result of Equation 2.4; namely, \\(p=0.1476\\) \n\nround(1 - pcauchy(2), 4)\n\n[1] 0.1476\n\n\n1. Approach: The Naive Approach (Classical Monte Carlo Integration)\nThe most direct approach would be to use the following mean expression for the integral of interest: \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=P\\left(X \\geq 2\\right)\\\\[2ex]\n&=P\\left(1_{(X \\geq 2)}=1\\right)\\\\[2ex]\n&= P\\left(1_{(X \\geq 2)}=1\\right)\\cdot 1 + P\\left(1_{(X \\geq 2)}=0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(1_{(X \\geq 2)}\\right),\n\\end{align*}\n\\] where \\(X\\sim \\mathcal{Cauchy}(0,1),\\) \\(f\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution, and \\(h(x)=1_{(x\\geq 2)}\\).\nSo, we can approximate \\(p\\) using the empirical mean \\[\n\\hat{p}_{1}=\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\geq 2)}\n\\] of a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Cauchy}(0,1),\\) where \\[\n1_{(X_1\\geq 2)},\\dots,1_{(X_n\\geq 2)}\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bernoulli}(p).\n\\] The variance of \\(\\hat{p}_1\\) is thus \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_{1})\n&=\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n1_{(X_i\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\\mathbb{V}\\left(1_{(X_1\\geq 2)}\\right)\\quad\\text{(i.i.d.)}\\\\[2ex]\n&=\\frac{p(1-p)}{n},\n\\end{align*}\n\\] which is equal to \\(0.126/n,\\) since we know that \\(p=0.1476\\).\n2. Approach: Accounting for Symmetry\nIn this approach, we use the “adjusting Screws” \\(\\mathcal{X}\\) and \\(h.\\)\nWe can achieve a more efficient estimator (i.e., an estimator with lower variance for a given same sample size \\(n\\)) if we take into account the symmetric nature of \\(\\mathcal{Cauchy}(0,1).\\)\nObviously, due to the symmetry of our target integrand, can do the following rearrangement \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=\\frac{1}{2}\\left(\\int_{-\\infty}^{-2}\\frac{1}{\\pi(1+x^2)}\\,dx + \\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx \\right)\\\\[2ex]\n&=\\frac{1}{2}\\left(\\mathbb{E}\\left(1_{(X \\leq -2)}\\right) + \\mathbb{E}\\left(1_{(X \\geq 2)}\\right)\\right)\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(\\frac{1}{2} \\; 1_{(|X| \\geq 2)}\\right),\n\\end{align*}\n\\] i.e., here \\(f\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution and \\(h(x)=\\frac{1}{2} 1_{(|x|\\geq 2)}\\).\nThis representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean \\[\n\\begin{align*}\n\\hat{p}_{2}\n%&=\\frac{1}{2}\\left(\\frac{1}{n}\\sum_{i=1}^n1_{(X_i \\leq -2)}+ \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i \\geq 2)}\\right)\\\\[2ex]\n&=\\frac{1}{2n}\\sum_{i=1}^n 1_{(|X_i|\\geq 2)}\n\\end{align*}\n\\] again of a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Cauchy}(0,1),\\) where \\[\n1_{(|X_1|\\geq 2)},\\dots,1_{(|X_n|\\geq 2)}\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bernoulli}(2\\,p).\n\\] The variance of this new estimator, \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_{2})\n&=\\frac{1}{4n^2}\\mathbb{V}\\left(\\sum_{i=1}^n1_{(|X_i|\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{1}{4n}\\mathbb{V}\\left(1_{(|X_1|\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{2p(1-2p)}{4n},\n\\end{align*}\n\\] which is equal to \\(0.052/n,\\) since we know that \\(p=0.1476\\). This is clearly lower than in the naive approach, where we had \\(0.126/n.\\)\n3. Approach:\nIn this approach, we use all the “adjusting screws” \\(\\mathcal{X}\\), \\(h\\), and \\(f.\\)\nThe (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, \\([2,+\\infty)\\), which are in some sense irrelevant for the approximation of \\(p\\). This motivates the following reformulation of \\(p\\):\nBy symmetry of \\(f,\\) we have that \\[\n\\begin{align*}\n\\frac{1}{2}\n& =\\int_{0}^\\infty\\frac{1}{\\pi(1+x^2)}dx.\n\\end{align*}\n\\] This can be used to do the following: \\[\n\\begin{align*}\n\\frac{1}{2}\n& =\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx + \\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}dx}_{=p}\\\\[2ex]\n\\Leftrightarrow \\; p& =\\frac{1}{2}-\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx.\n\\end{align*}\n\\] Furthermore, we can rearrange the integral a bit such that \\[\n\\begin{align*}\np\n& =\\frac{1}{2} - \\int_{0}^2\\;\\left(\\frac{1}{2}\\cdot 2\\right)\\;\\frac{1}{\\pi(1+x^2)}\\,dx \\\\[2ex]\n& = \\frac{1}{2} - \\int_{0}^2\\;\\underbrace{\\frac{1}{2}}_{f_{\\mathcal{U}[0,2]}}\\;\\underbrace{\\frac{2}{\\pi(1+x^2)}}_{=h(x)}\\,dx \\\\[2ex]\n& = \\frac{1}{2} - \\mathbb{E}_{f_{\\mathcal{U}[0,2]}}(h(U)),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}[0,2].\\)\nTherefore, a new alternative method for evaluating \\(p\\) is: \\[\n\\hat{p}_{3}=\\frac{1}{2} - \\frac{1}{n}\\sum_{i=1}^n h(U_i),\n\\] where \\[\nU_1,\\dots,U_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{U}[0,2].\n\\] As in the previous approaches, the standard error converges with the classic parametric convergence rate: \\[\n\\begin{align*}\n\\mathbb{V}\\left(\\hat{p}_{3}\\right)\n& =\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n h(U_i)\\right)\\\\[2ex]\n& =\\frac{1}{n}\\mathbb{V}\\left(h(U_1)\\right)\\\\[2ex]\n\\Rightarrow\n\\operatorname{SE}\\left(\\hat{p}_{3}\\right)&=\\texttt{const}\\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\] Computing the variance \\(\\mathbb{V}\\left(\\hat{p}_{3}\\right)\\) is a bit cumbersome, but using integration by parts and that \\(p=0.1476\\), it can be shown that \\[\n\\mathbb{V}(\\hat p_3)=\\frac{0.0285}{n},\n\\] which is lower than both previous approaches, where we had that \\(\\mathbb{V}(\\hat{p}_{2})=0.052/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.126/n\\).\n\n\n2.3.1 A More General Point of View\nThe idea of importance sampling is related to weighted and stratified sampling ideas, when estimating \\[\n\\theta=\\mathbb{E}_{f}(h(X))=\\int h(x)f(x)dx,\n\\] as already illustrated in Example 2.3.\nSome outcomes of \\(X\\sim f\\) may be more important than others in determining \\(\\theta\\) and we wish to select such values more frequently.\nFor instance, if \\(\\theta\\) denotes the probability of the occurrence of a very rare event, then the only way to estimate \\(\\theta\\) at all accurately may be to produce the rare events more frequently.\nTo achieve this, we can simulate a model which gives a density \\(g\\) to \\(X\\) instead of the correct density \\(f,\\) where both density functions need to be known. This can be easily done, since \\[\n\\begin{align*}\n\\theta\n&=\\mathbb{E}_{f}(h(X))\\\\[2ex]\n&=\\int h(x)\\;f(x)dx\\\\[2ex]\n&=\\int h(x)\\left(\\frac{g(x)}{g(x)}\\right)\\;f(x)dx\\\\[2ex]\n&=\\int \\underbrace{\\left(h(x)\\frac{f(x)}{g(x)}\\right)}_{=:\\psi(x)}\\;g(x)dx\\\\[2ex]\n&=\\int \\psi(x)\\;g(x)dx\\\\[2ex]\n&=\\mathbb{E}_g(\\psi(X)).\n\\end{align*}\n\\]\nThis leads to the following unbiased estimator for \\(\\theta\\) based on sampling from \\(g\\): \\[\n\\begin{align*}\n\\hat{\\theta}_g\n&=\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\underbrace{\\left(\\frac{f(X_i)}{g(X_i)}\\right)}_{=:W_i} h(X_i)\n\\end{align*}\n\\tag{2.5}\\] with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} g.\n\\]\nNote that Equation 2.5 can be seen as a weighted mean of the transformed random variables \\(h(X_i)\\) with weights \\[\nW_i = \\frac{f(X_i)}{g(X_i)}.\n\\] These weights \\(W_i\\) are inversely proportional to the so-called selection factor \\[\n\\frac{g(X_i)}{f(X_i)}.\n\\] \nFor the variance of the estimator \\(\\hat{\\theta}_g\\) we have \\[\n\\begin{align*}\n\\mathbb{V}_g(\\hat{\\theta}_g)\n&=\\mathbb{V}_g\\left(\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i)\\right) \\\\[2ex]\n&=\\frac{1}{n}\\mathbb{V}_g(\\psi(X_i))\\quad\\text{(since i.i.d. random sample)}\\\\[2ex]\n&=\\frac{1}{n}\\int\\left(\\psi(x)-\\theta\\right)^2g(x)\\;dx\\\\[2ex]\n&=\\frac{1}{n}\\int\\left(\\frac{h(x)\\,f(x)}{g(x)}-\\theta\\right)^2g(x)\\;dx,\n\\end{align*}\n\\]\nDepending on the choice of the importance function \\(g,\\) the variance of the estimator \\(\\hat{\\theta}_g\\) can be much smaller than the variance of the more naive estimators from the classical Monte Carlo integrations which use unweighted empirical means.\nTheorem 2.1 gives us the best choice for the importance function \\(g.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Minimum Variance Theorem) The importance function \\(g\\) which minimizes the variance \\(\\mathbb{V}_g(\\psi(X_i))\\), and, therefore, minimizes also the variance of the integral estimator in Equation 2.5, \\(\\mathbb{V}_g(\\hat{\\theta}_g)\\), is given by \\[\ng^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\]\n\n\n\nProof of Theorem 2.1:\nNote that \\[\n\\begin{align*}\n\\mathbb{V}_g(\\psi(X_i))\n&=\\quad\\quad \\mathbb{E}_g(\\psi(X_i)^2)\\quad\\quad  - \\quad\\quad \\left(\\mathbb{E}_g(\\psi(X_i))\\right)^2\\\\[2ex]\n&=\\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx -\n  \\left(\\int\\frac{h(x)f(x)}{g(x)}g(x)dx\\right)^2\\\\[2ex]\n  &=\\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx -\n  \\left(\\int h(x)f(x)dx\\right)^2.\n\\end{align*}\n\\tag{2.6}\\] Thus, the second term, \\(\\left(\\mathbb{E}_g(\\psi(X_i))\\right)^2,\\) does not depend on \\(g.\\)\nConsequently, the density \\(g\\) that minimizes the variance \\(\\mathbb{V}_g(\\psi(X_i))\\) is also the density \\(g\\) that minimizes \\(\\mathbb{E}_g(\\psi(X_i)^2).\\) We use this in the following, since minimizing \\(\\mathbb{E}_g(\\psi(X_i)^2)\\) is simpler than minimizing \\(\\mathbb{V}_g(\\psi(X_i)).\\)\nWe begin with a small trick by including an absolute value function as following: \\[\n\\begin{align*}\n\\mathbb{E}_g(\\psi(X_i)^2)\n&=\\mathbb{E}_g({\\color{red}|}\\psi(X_i){\\color{red}|}^2).\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nJensen’s Inequality: Let \\(u\\) be a “convex function” such as, for instance, \\(u(x)=x^2,\\) and let \\(X\\) be some random variable. Then, \\[\n\\mathbb{E}\\left(u(X)\\right) \\geq u\\left(\\mathbb{E}\\left(X\\right)\\right),\n\\] where we assume that the moments exist.\n\n\nNow, using Jensen’s inequality, we can derive the lower bound for \\(\\mathbb{E}_g(\\psi(X_i)^2):\\) \\[\n\\begin{align*}\n\\mathbb{E}_g( \\; (\\psi(X_i) ) ^2 \\; )\n&=\\mathbb{E}_g( \\; |\\psi(X_i)|^2 \\; )\\quad\\text{(bringing in $|.|$)}\\\\[2ex]\n&\\geq \\left(\\mathbb{E}_g(|\\psi(X_i)|)\\right)^2\\quad\\text{(Jensen's inequality)}\\\\[2ex]\n&=\\left(\\int\\left|\\frac{h(x)f(x)}{g(x)}\\right|g(x)dx\\right)^2\\\\[2ex]\n&=\\left(\\int\\frac{\\left|h(x)\\right|f(x)}{g(x)}g(x)dx\\right)^2\\quad\\text{($f$ and $g$ are non-negative)}\\\\[2ex]\n&=\\underbrace{\\left(\\int\\left|h(x)\\right|f(x)dx\\right)^2}_{\\text{Lower bound of $\\mathbb{E}_g(\\psi(X_i)^2)$}} = \\left(\\mathbb{E}_{f}\\left(h(X)\\right)\\right)^2,\n\\end{align*}\n\\] where this lower bound is independent of \\(g.\\)\nIt is now straight forward to show that this lower bound is achieved if \\[\ng(x) = g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\] For this, observe that \\[\n\\begin{align*}\n\\mathbb{E}_g(\\psi(X_i)^2)\n& = \\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx\\\\[2ex]\n& = \\int\\frac{h^2(x)f^2(x)}{g(x)}dx.\n\\end{align*}\n\\] Now, setting \\(g(x)=g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}\\) yields \\[\n\\begin{align*}\n\\mathbb{E}_{g^\\ast}(\\psi(X_i)^2)\n& = \\int\\frac{h^2(x)f^2(x)}{|h(x)|f(x)} \\left(\\int |h(z)|f(z)dz\\right) dx\\\\[2ex]\n& = \\int\\frac{\\left|h(x)\\right|^2f(x)}{|h(x)|} \\; \\left(\\int |h(z)|f(z)dz\\right) dx\\\\\n& = \\left(\\int\\left|h(x)\\right|f(x) dx\\right) \\; \\left(\\int |h(z)|f(z)dz\\right) \\\\[2ex]\n& = \\underbrace{\\left(\\int\\left|h(x)\\right|f(x) dx\\right)^2}_{\\text{Lower bound of $\\mathbb{E}_g(\\psi(X_i)^2)$}}\n\\end{align*}\n\\] This shows that the lower bound of \\(\\mathbb{E}_g(\\psi(X_i)^2)\\) is achieved for \\(g(x)=g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz},\\) which implies the statement of Theorem 2.1.\n\n\n\n\n\n\nNote\n\n\n\nNote that Theorem 2.1 may appear as rather impractical. If, for instance, \\(h(x)\\geq 0\\) then \\(g^\\ast\\) requires us to know \\(\\theta=\\int h(z)f(z)dz,\\) which, however, is just the integral of interest!\nThe minimum variance result of Theorem 2.1 is, nevertheless, useful:\n\nIt tells us that a good choice of \\(g(x)\\) shall mimic the shape of \\(|h(x)|f(x)\\), since the optimal \\(g^\\ast(x)\\propto |h(x)|f(x)\\). (We know the integrand functions \\(h\\) and \\(f\\).)\nFurthermore, \\(g(x)\\) should be chosen such that it has a thicker tail than \\(f(x)\\), since the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\) crucially depends on the quotient \\(f(x)/g(x)\\) (see Equation 2.6) which would “explode” for \\(g(x)\\approx 0\\).\n\n\n\nLet’s apply our new insights to the above Example 2.3 on the Cauchy tail probability \\(p\\).\n\nExample 2.4 (Cauchy Tail Probability (continued)) Above, in Example 2.3, we had \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=P\\left(X \\geq 2\\right)\\\\[2ex]\n&=P\\left(1_{(X \\geq 2)}=1\\right)\\\\[2ex]\n&= P\\left(1_{(X \\geq 2)}=1\\right)\\cdot 1 + P\\left(1_{(X \\geq 2)}=0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(1_{(X \\geq 2)}\\right),\n\\end{align*}\n\\] where \\(f(x)=1/(\\pi(1+x^2))\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution and \\(h(x)=1_{(x\\geq 2)}\\).\nI.e., here \\(h\\) is non-negative, such that \\(|h(x)|=h(x)\\) for all \\(x.\\)\n\nTherefore, \\[\n\\begin{align*}\np\n& =\\mathbb{E}_f(h(X)) \\\\[2ex]\n& =\\int h(x)f(x)dx    \\\\[2ex]\n%& =\\int_{2}^{\\infty}f(x)dx \\\\[2ex]\n%& =\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\int\\underbrace{\\left(h(x) \\frac{f(x)}{g(x)}\\right)}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\mathbb{E}_g(\\psi(X)),\n\\end{align*}\n\\] with \\(h(x)=1_{(x\\geq 2)}\\) and \\(X\\sim g,\\) where \\(g\\) needs to be chosen by the statistician.\nA possibly good (and simple) choice for the importance function \\(g\\) is, for instance, \\[\ng(x)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{2}{x^2}& x\\geq 2\\\\\n  0&\\text{otherwise},\\\\\n  \\end{array}\\right.\n\\]\nThis choice takes into account our insights from Theorem 2.1, since\n\n\\(g\\) “closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f=1/(\\pi(1+x^2))\\).\n\n\n\n\n\n\nFigure 2.4: Density function \\(f(x)=1/(\\pi(1+x^2)),\\) and importance function \\(g(x)=2/x^2\\) for \\(x\\geq 2.\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function \\(g\\) is indeed a density function, since it is non-negative and integrates to one; i.e., \\(\\int_2^\\infty g(x)dx = 1.\\) (Double-check this.)\n\n\nCaution: It is not straight forward to directly sample from the importance function \\(g(x)=2/x^2,\\) but we can apply some further steps:\nThe choice of \\(g\\) leads to \\[\n\\begin{align*}\np\n& =\\mathbb{E}_g(\\psi(X)) \\\\[2ex]\n& =\\int \\left(h(x) \\frac{f(x)}{g(x)}\\right) \\;g(x)dx \\\\[2ex]\n& =\\int_{2}^{+\\infty} \\left(\\frac{f(x)}{g(x)}\\right) \\;g(x)dx\\quad\\text{(using $h(x)=1_{(x\\geq 2)}$)} \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{\\frac{1}{\\pi(1+x^2)}}{\\frac{2}{x^2}}\\right)\\,\\frac{2}{x^2}\\,dx \\quad\\text{(using $g(x)=2/x^2$)} \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^2) \\cdot \\frac{1}{x^2}} \\right)\\,\\frac{1}{x^2}\\,dx  \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(\\frac{1}{x^2}+ 1) } \\right)\\,\\frac{1}{x^2}\\,dx  \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\end{align*}\n\\]\nNow, integration by substitution\n\nsubstituting \\(u(x)=x^{-1}\\)\n\\(\\frac{du}{dx}=- x^{-2}\\)\n\nyields that \\[\n\\begin{align*}\np\n& =\\lim_{b\\to\\infty}\\int_{u(2)}^{u(b)} - \\frac{1}{\\pi(1+u^2)}du \\\\[2ex]\n& = - \\int_{1/2}^{0} \\frac{1}{\\pi(1+u^2)}du \\\\[2ex]\n& = \\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\end{align*}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nWe just showed that \\[\np =\\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx}_{\\text{Our original problem}}\n  =\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] That is, we shifted the integration problem from the tail to the “center” of the distribution. 🤓\n\n\nRearranging the integral a bit, allows us to write \\[\n\\begin{align*}\np\n&=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathcal{U}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du\\\\[2ex]\n&=\\mathbb{E}_{f_{\\mathcal{U}[0,1/2]}}(h(U)),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}[0,1/2].\\)\nThus, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\frac{1}{n}\\sum_{i=1}^n h(U_i),\n\\] where \\[\nU_1,\\dots,U_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{U}[0,1/2]\n\\] and \\[\nh(u)=\\frac{1}{2\\pi(1+u^2)}\n\\]\nThe variance and standard error of \\(\\hat{p}_4\\) are \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_4)\n& = \\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n h(U_i)\\right)\\\\[2ex]\n& = \\frac{1}{n}\\mathbb{V}\\left(h(U)\\right)\\\\[2ex]\n& = \\frac{\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2}{n}\\\\[2ex]\n\\Rightarrow\n\\operatorname{SE}(\\hat{p}_4)\n& = \\texttt{const}\\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\]\n\n\n\nAn integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.0955\\cdot 10^{-3}/n\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/n\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/n\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\)."
  },
  {
    "objectID": "Ch3_Bootstrap.html",
    "href": "Ch3_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch3_Bootstrap.html#the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.1 The empirical distribution function",
    "text": "3.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 3.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 3.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea",
    "href": "Ch3_Bootstrap.html#basic-idea",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic idea",
    "text": "3.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 3.1)."
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch3_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The nonparametric (standard) bootstrap",
    "text": "3.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 3.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n3.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n3.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{3.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch3_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "3.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 3.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 3.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{3.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 3.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.5 Regression Analysis: Bootstrapping pairs",
    "text": "3.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 3.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n3.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Residual bootstrap",
    "text": "3.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 3.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html",
    "href": "Ch4_MaximumLikelihood.html",
    "title": "4  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch4_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Optimization: Non-Analytical Solutions",
    "text": "4.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 4.1).\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 4.1.\n\n\nTable 4.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch4_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "4  Maximum Likelihood",
    "section": "4.3 OLS-Estimation as ML-Estimation",
    "text": "4.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{4.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 3.4).\nFor the following, it is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch4_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch4_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch4_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 4.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.5}\\]\nUsing Equation 4.4 and Equation 4.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html",
    "href": "Ch5_EMAlgorithmus.html",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "",
    "text": "Possible Applications of Gaussian mixture distributions:\n\nGeneral: Finding grouping structures (two or more) in data (Clusteranalyse). For instance:\n\nAutomatic video editing (e.g., separation of back- and foreground)\n\nBehavioral clustering\netc.\n\n\n\n\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#exercises",
    "href": "Ch1_Random_Variable_Generation.html#exercises",
    "title": "1  Random Variable Generation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider Example 1.9 (“Normals from Double Exponentials”). Let \\(f\\) be the density of the standard normal distribution \\(\\mathcal{N}(0,1),\\) \\[\n\\begin{align*}\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right),\n\\end{align*}\n\\] and \\(g\\) the density of the Laplace (double exponential) distribution \\[\n\\begin{align*}\ng(x|b) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{2}\\right),\\quad b>0.\n\\end{align*}\n\\]\n\nShow that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\]\nShow that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1.\\)\n\n\n\nExercise 2.\nLet \\(X \\sim f\\) and \\(Y\\sim g,\\) where \\(f\\) and \\(g\\) are density functions of the random variables \\(X\\) and \\(Y.\\) Show that\n\\[\n\\begin{align*}\nP\\left(X \\leq h (Y)\\right)\n&=\\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^{h (y)} f(x) dx \\right) g(y) dy.\n\\end{align*}\n\\] Note: This was used, for instance, in the proofs of Theorem 1.4 and Theorem 1.5.\nHint: Use that the probability of an event \\(A\\) (e.g. \\(A=X \\leq a\\)) can be written as \\[\n\\begin{align*}\nP(A)\n& = \\;\\;\\;\\;P(A) \\;\\;\\cdot \\;\\;1 \\;\\;+\\;\\; P(\\text{not}\\;A) \\;\\;\\cdot\\;\\; 0 \\\\[2ex]\n& = P(1_{(A)} = 1) \\cdot 1 + P(1_{(A)} = 0) \\;\\;\\cdot 0 \\\\[2ex]\n& = \\mathbb{E}\\left(1_{(A)}\\right),\n\\end{align*}\n\\] where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\) This allows you to use the iterated law of expectations.\n\n\nExercise 3.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) with compact support \\([a,b]\\) and \\(0\\leq f(x)\\leq m\\) for all \\(x\\in[a,b]\\). What is the probability of accepting \\(Y\\) from a simulation \\((Y,U)\\) as described in the lecture?\n\n\nExercise 4.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) and an instrumental density function \\(g\\) with \\[\nf_X(x) \\leq Mg(x)\n\\] for all \\(x\\in\\mathbb{R}\\) and \\(g(x)>0\\) for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\nWhat is the probability of accepting \\(Y\\) from a simulation \\[\n   (Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right)?\n   \\]\nShow that \\(M\\geq 1.\\)"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#references",
    "href": "Ch1_Random_Variable_Generation.html#references",
    "title": "1  Random Variable Generation",
    "section": "References",
    "text": "References\n\n\n\n\nBox, George EP, and Mervin E Muller. 1958. “A Note on the Generation of Random Normal Deviates.” The Annals of Mathematical Statistics 29 (2): 610–11. https://projecteuclid.org/euclid.aoms/1177706645.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#references",
    "href": "Ch2_MonteCarlo.html#references",
    "title": "2  Monte Carlo Integration",
    "section": "References",
    "text": "References\n\n\n\n\nBourreau, Marc, Yutec Sun, and Frank Verboven. 2021. “Market Entry, Fighting Brands, and Tacit Collusion: Evidence from the French Mobile Telecommunications Market.” American Economic Review 111 (11): 3459–99.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nJudd, Kenneth L. 1998. Numerical Methods in Economics. MIT press.\n\n\nRipley, Brian D. 2009. Stochastic Simulation. John Wiley & Sons. http://onlinelibrary.wiley.com/book/10.1002/9780470316726.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises-chapter-1-2",
    "href": "Ch2_MonteCarlo.html#exercises-chapter-1-2",
    "title": "2  Monte Carlo Method",
    "section": "Exercises (Chapter 1 & 2)",
    "text": "Exercises (Chapter 1 & 2)\n\n\nA possible algorithm for generating standard normal random variables from \\(n\\) standard uniformly distributed random variables \\(U_1,\\dots,U_n\\sim\\mathcal{U}[0,1],\\) would be to use the Central Limit Theorem (CLT), i.e. \\[\nY_n=\\frac{\\sqrt{n}\\left(\\bar{U}_n - \\mu\\right)}{\\sigma}\\to_d \\mathcal{N}(0,1).\n\\] as \\(n\\to\\infty,\\) where \\(\\mathbb{E}(U_1)=\\mu\\) and \\(\\mathbb{V}(U_1)=\\sigma^2.\\)\n\nAsses this “algorithm” critically by answering the following questions: (a) What is the maximal range of values of the generated random variables \\(Y_n\\)?\n(b) Compare the moments of \\(Y_n\\) with those of \\(\\mathcal{N}(0,1)\\) using the moment generating function \\[\nM_X(t) = \\mathbb{E}\\left(\\exp\\left(t X\\right)\\right)\n\\] Hint: For the case of \\(U\\sim\\mathcal{U}[0,1],\\) the moment generating function is \\[\nM_U(t) = \\mathbb{E}\\left(\\exp\\left(t U\\right)\\right)=\\frac{\\exp(t) - 1}{t}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThe distribution of a random variable \\(X\\) can be characterized (completely) using any of the following functions (provided they exist):\n\nDensity function \\(f_X\\)\nDistribution function \\(F_X(x)=\\int_{-\\infty}^x f_X(u)\\,du\\)\nMoment generating function \\(M_X(t)=\\mathbb{E}\\left(\\exp\\left(t X\\right)\\right)\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#solutions",
    "href": "Ch2_MonteCarlo.html#solutions",
    "title": "2  Monte Carlo Method",
    "section": "Solutions",
    "text": "Solutions\n\nDeriving the upper bound in Example 1.9 (“Normals from Double Exponentials”): \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n&= \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(-\\frac{x^2}{2}\\right) \\exp\\left(\\frac{|x|}{b}\\right)\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2|x| - x^2b}{2b}\\right)\\\\[2ex]\n&=\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{-2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\] Consider a given value of \\(b>0,\\) and the case of \\(x\\geq 0.\\) Maximizing\n\\[\n\\begin{align*}\n\\exp\\left(\\frac{2x - x^2b}{2b}\\right)\n& = \\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] maximal with respect to \\(x\\geq 0\\) yields \\[\n\\begin{align*}\n\\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=\\frac{1}{b}\n\\end{align*}\n\\] Correspondingly, for a given value of \\(b>0,\\) and the case of \\(x<0,\\) maximizing \\[\n\\begin{align*}\n\\exp\\left(\\frac{-2x - x^2b}{2b}\\right)\n& = \\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] yields \\[\n\\begin{align*}\n\\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(-\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=-\\frac{1}{b}\n\\end{align*}\n\\]\n\n\nb <- 2\nmyfun <- function(x, b){exp( (2*abs(x) - x^2 * b)/(2 * b) )}\nxx <- seq(-10, 10, len = 500)\nyy <- myfun(xx, b = b)\nplot(x = xx, y = yy, type = \"l\")\nabline(v=c(-1/b,1/b), lty=2)\n\n\n\n\nThus \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n& \\leq\n\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\\\\[2ex]\n& =\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b} }{2b}\\right) \\quad \\;\\text{for}\\; x\\in\\mathbb{R},\n\\end{align*}\n\\] which shows that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\] Now, to show that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1,\\) we minimize \\[\nb\\;\\exp\\left(\\frac{1}{2b^2}\\right)\n\\] with respect to \\(b>0,\\) which yields \\[\n\\begin{align*}\n1\\;\\exp\\left(\\frac{1}{2b^2}\\right) + b\\;\\exp\\left(\\frac{1}{2b^2}\\right)\\cdot \\left(-2\\frac{1}{2b^3}\\right) &\\overset{!}{=} 0\\\\[2ex]\n1\\; - 2\\frac{1}{2b} &= 0\\\\[2ex]\nb &= 1\n\\end{align*}\n\\] where we used that \\(\\exp\\left(\\frac{1}{2b^2}\\right)>0.\\)\n\nmyfun2 <- function(b){b * exp( 1/(2*b^2) )}\nbb <- seq(.5, 2, len = 500)\nyy <- myfun2(b = bb)\nplot(x = bb, y = yy, type = \"l\")\nabline(v = 1, lty=2)\n\n\n\n\n\n\nDeriving the probability of accepting \\(Y\\) from a simulation \\[\n(Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right).\n\\]\n\n\n\\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(U\\leq f_X(Y)\\right),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}\\left[0,Mg(Y)\\right].\\) Standardizing the uniform distribution yields \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(\\frac{U}{Mg(Y)}\\leq \\frac{f_X(Y)}{Mg(Y)}\\right),\\\\[2ex]\n\\end{align*}\n\\] where dividing by \\(Mg(Y)\\) is allowed, since \\(Mg(y)>0\\) for all \\(y\\in\\operatorname{supp}(f_X).\\) Using that \\[\n\\frac{U}{Mg(Y)} \\sim\\mathcal{U}\\left[0,1\\right],\n\\] we have \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=\\int_{-\\infty}^{\\infty}\\int_0^{\\frac{f_X(y)}{Mg(y)}} 1 \\; du\\; g(y) \\;dy\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty}\\;\\;\\;\\frac{f_X(y)}{Mg(y)}\\;\\; g(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\\int_{-\\infty}^{\\infty}\\;f_X(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\n\\end{align*}\n\\]  (b) \\[\n  \\begin{align*}\n  f_X(x) & \\leq M g(x)\\quad x\\in\\operatorname{f_X}\\\\[2ex]\n  \\int f_X(x) dx & \\leq \\int M g(x)dx\\\\[2ex]\n  1              & \\leq M \\cdot 1\\\\[2ex]\n  \\end{align*}\n  \\]"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises-of-chapters-1-2",
    "href": "Ch2_MonteCarlo.html#exercises-of-chapters-1-2",
    "title": "2  Monte Carlo Integration",
    "section": "Exercises of Chapters 1 & 2",
    "text": "Exercises of Chapters 1 & 2\n\nExercise 1.\nConsider Example 1.9 (“Normals from Double Exponentials”). Let \\(f\\) be the density of the standard normal distribution \\(\\mathcal{N}(0,1),\\) \\[\n\\begin{align*}\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right),\n\\end{align*}\n\\] and \\(g\\) the density of the Laplace (double exponential) distribution \\[\n\\begin{align*}\ng(x|b) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{2}\\right),\\quad b>0.\n\\end{align*}\n\\]\n\nShow that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\]\nShow that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1.\\)\n\n\n\nExercise 2.\nLet \\(X \\sim f\\) and \\(Y\\sim g,\\) where \\(f\\) and \\(g\\) are density functions of the random variables \\(X\\) and \\(Y.\\) Show that\n\\[\n\\begin{align*}\nP\\left(X \\leq h (Y)\\right)\n&=\\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^{h (y)} f(x) dx \\right) g(y) dy.\n\\end{align*}\n\\] Note: This was used, for instance, in the proofs of Theorem 1.4 and Theorem 1.5.\nHint: Use that the probability of an event \\(A\\) (e.g. \\(A=X \\leq a\\)) can be written as \\[\n\\begin{align*}\nP(A)\n& = \\;\\;\\;\\;P(A) \\;\\;\\cdot \\;\\;1 \\;\\;+\\;\\; P(\\text{not}\\;A) \\;\\;\\cdot\\;\\; 0 \\\\[2ex]\n& = P(1_{(A)} = 1) \\cdot 1 + P(1_{(A)} = 0) \\;\\;\\cdot 0 \\\\[2ex]\n& = \\mathbb{E}\\left(1_{(A)}\\right),\n\\end{align*}\n\\] where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\) This allows you to use the iterated law of expectations.\n\n\nExercise 3.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) with compact support \\([a,b]\\) and \\(0\\leq f(x)\\leq m\\) for all \\(x\\in[a,b]\\). What is the probability of accepting \\(Y\\) from a simulation \\((Y,U)\\) as described in the lecture?\n\n\nExercise 4.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) and an instrumental density function \\(g\\) with \\[\nf_X(x) \\leq Mg(x)\n\\] for all \\(x\\in\\mathbb{R}\\) and \\(g(x)>0\\) for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\nWhat is the probability of accepting \\(Y\\) from a simulation \\[\n   (Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right)?\n   \\]\nShow that \\(M\\geq 1.\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#solutions-of-exercises-of-chapters-1-2",
    "href": "Ch2_MonteCarlo.html#solutions-of-exercises-of-chapters-1-2",
    "title": "2  Monte Carlo Method",
    "section": "Solutions of Exercises of Chapters 1 & 2",
    "text": "Solutions of Exercises of Chapters 1 & 2\n\nDeriving the upper bound in Example 1.9 (“Normals from Double Exponentials”): \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n&= \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(-\\frac{x^2}{2}\\right) \\exp\\left(\\frac{|x|}{b}\\right)\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2|x| - x^2b}{2b}\\right)\\\\[2ex]\n&=\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{-2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\] Consider a given value of \\(b>0,\\) and the case of \\(x\\geq 0.\\) Maximizing\n\\[\n\\begin{align*}\n\\exp\\left(\\frac{2x - x^2b}{2b}\\right)\n& = \\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] maximal with respect to \\(x\\geq 0\\) yields \\[\n\\begin{align*}\n\\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=\\frac{1}{b}\n\\end{align*}\n\\] Correspondingly, for a given value of \\(b>0,\\) and the case of \\(x<0,\\) maximizing \\[\n\\begin{align*}\n\\exp\\left(\\frac{-2x - x^2b}{2b}\\right)\n& = \\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] yields \\[\n\\begin{align*}\n\\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(-\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=-\\frac{1}{b}\n\\end{align*}\n\\]\n\n\nb <- 2\nmyfun <- function(x, b){exp( (2*abs(x) - x^2 * b)/(2 * b) )}\nxx <- seq(-10, 10, len = 500)\nyy <- myfun(xx, b = b)\nplot(x = xx, y = yy, type = \"l\")\nabline(v=c(-1/b,1/b), lty=2)\n\n\n\n\nThus \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n& \\leq\n\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\\\\[2ex]\n& =\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b} }{2b}\\right) \\quad \\;\\text{for}\\; x\\in\\mathbb{R},\n\\end{align*}\n\\] which shows that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\] Now, to show that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1,\\) we minimize \\[\nb\\;\\exp\\left(\\frac{1}{2b^2}\\right)\n\\] with respect to \\(b>0,\\) which yields \\[\n\\begin{align*}\n1\\;\\exp\\left(\\frac{1}{2b^2}\\right) + b\\;\\exp\\left(\\frac{1}{2b^2}\\right)\\cdot \\left(-2\\frac{1}{2b^3}\\right) &\\overset{!}{=} 0\\\\[2ex]\n1\\; - 2\\frac{1}{2b} &= 0\\\\[2ex]\nb &= 1\n\\end{align*}\n\\] where we used that \\(\\exp\\left(\\frac{1}{2b^2}\\right)>0.\\)\n\nmyfun2 <- function(b){b * exp( 1/(2*b^2) )}\nbb <- seq(.5, 2, len = 500)\nyy <- myfun2(b = bb)\nplot(x = bb, y = yy, type = \"l\")\nabline(v = 1, lty=2)\n\n\n\n\n\n\nDeriving the probability of accepting \\(Y\\) from a simulation \\[\n(Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right).\n\\]\n\n\n\\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(U\\leq f_X(Y)\\right),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}\\left[0,Mg(Y)\\right].\\) Standardizing the uniform distribution yields \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(\\frac{U}{Mg(Y)}\\leq \\frac{f_X(Y)}{Mg(Y)}\\right),\\\\[2ex]\n\\end{align*}\n\\] where dividing by \\(Mg(Y)\\) is allowed, since \\(Mg(y)>0\\) for all \\(y\\in\\operatorname{supp}(f_X).\\) Using that \\[\n\\frac{U}{Mg(Y)} \\sim\\mathcal{U}\\left[0,1\\right],\n\\] we have \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=\\int_{-\\infty}^{\\infty}\\int_0^{\\frac{f_X(y)}{Mg(y)}} 1 \\; du\\; g(y) \\;dy\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty}\\;\\;\\;\\frac{f_X(y)}{Mg(y)}\\;\\; g(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\\int_{-\\infty}^{\\infty}\\;f_X(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\n\\end{align*}\n\\]  (b) \\[\n  \\begin{align*}\n  f_X(x) & \\leq M g(x)\\quad x\\in\\operatorname{f_X}\\\\[2ex]\n  \\int f_X(x) dx & \\leq \\int M g(x)dx\\\\[2ex]\n  1              & \\leq M \\cdot 1\\\\[2ex]\n  \\end{align*}\n  \\]"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises",
    "href": "Ch2_MonteCarlo.html#exercises",
    "title": "2  Monte Carlo Integration",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#cauchy-tail-probability-continued",
    "href": "Ch2_MonteCarlo.html#cauchy-tail-probability-continued",
    "title": "2  Monte Carlo Integration",
    "section": "2.3 Cauchy Tail Probability (continued)",
    "text": "2.3 Cauchy Tail Probability (continued)\nAbove we had:\n\n\\(f(x)=\\frac{1}{\\pi(1+x^2)}\\), the density of \\(\\mathcal{Cauchy}(0,1)\\) and\n\\(h(x)=1_{(x>2)}\\), i.e., here \\(|h(x)|=h(x)\\).\n\nTherefore \\[\n\\begin{align*}\np\n& =\\mathbb{E}_f(h(X)) \\\\[2ex]\n& =\\int h(x)f(x)dx    \\\\[2ex]\n& =\\int_{2}^{\\infty}f(x)dx \\\\[2ex]\n& =\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\mathbb{E}_g(\\psi(X)),\n\\end{align*}\n\\] where the \\(h\\) function is absorbed by the formulation of the definite integral.\nA possibly good (and simple) choice of \\(g\\) is, e.g., \\(g(x)=2/(x^2)\\), since this function:\n\n“closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: It is not straight forward to directly sample from \\(g\\), therefore we need some further steps:\n\n\nThe choice of \\(g\\) leads to \\[\np=\\mathbb{E}_g(\\psi(X))=\n\\int_{2}^{+\\infty}\\left(\\frac{x^2}{2\\,\\pi(1+x^2)}\\right)\\,\\frac{2}{x^2}\\,dx=\n\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\]\n\n\nNow we can apply some additional (rather case-specific) re-arrangements:\nIntegration by substitution (substituting \\(u=x^{-1}\\)) yields: \\[\np=\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] Again, we can re-arrange the last integral a bit such that \\[\np=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathrm{Unif}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du=\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2].\n\\] Therefore, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\sum_{i=1}n h(U_i),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2]\n\\] and \\(h(u)=1/(2\\pi(1+u^2))\\).\nThe variance of \\(\\hat{p}_4\\) is \\((\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2)/n\\) and an integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.95\\cdot 10^{-4}/n\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/n\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/n\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\)."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#classical-monte-carlo-integration-using-proportions",
    "href": "Ch2_MonteCarlo.html#classical-monte-carlo-integration-using-proportions",
    "title": "2  Monte Carlo Integration",
    "section": "2.2 Classical Monte Carlo Integration using Proportions",
    "text": "2.2 Classical Monte Carlo Integration using Proportions\nMany integration problems, such as the one in Example 2.2, can be written as a proportion \\(0\\leq p\\leq 1.\\) This allows us compute the integral using using empirical proportions \\[\n\\hat{p} =\\frac{\\text{Number of successes}}{\\text{Sample size}},\n\\] where we choose the sample size \\(n\\) to achieve a certain level of precision for our integral approximation.\n\nExample 2.2 (Approximation of Normal Distribution Tables) A possible way to construct normal distribution tables, i.e., table for the values of the distribution function \\(0\\leq \\Phi(x)\\leq 1,\\) is to use Monte Carlo integration.\nObserve that the distribution function \\(\\Phi(x)\\) can be written as the mean of a binary (taking values 0 and 1) random variable, \\[\n\\begin{align*}\n\\Phi(x)\n&=\\int_{-\\infty}^x\\frac{1}{\\sqrt{2\\pi}}e^{-y^2/2}dy\\\\[2ex]\n&=P\\left(X \\leq x\\right)\\\\[2ex]\n&=P\\left(1_{(X \\leq x)} = 1\\right)\\\\[2ex]\n&=P\\left(1_{(X \\leq x)} = 1\\right)\\cdot 1 + P\\left(1_{(X \\leq x)} = 0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}\\left(1_{(X \\leq x)}\\right),\n\\end{align*}\n\\] where \\(X\\sim\\mathcal{N}(0,1),\\) and where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\nThat is, we can write the integration problem as the mean of a Bernoulli random variable \\[\n1_{(X \\leq x)}\\in\\{0,1\\}\n\\] \\[\n1_{(X \\leq x)}\\sim\\mathcal{Bern}\\left(p=\\Phi(x)\\right),\n\\] with parameter \\[\np=P\\left(X \\leq x\\right) = \\Phi(x).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(1_{(X \\leq x)}\\) is a Bernoulli random variable no matter the distribution of \\(X.\\) In Example 2.2, \\(X\\) is a standard normal random variable; however, in other use cases \\(X\\) can have, of course, another distribution, but \\(1_{(X \\leq x)}\\) remains a Bernoulli random variable. We use this feature below when we think about the distributional properties of our estimator for approximating the integral value.\n\n\nMonte Carlo integration, allows us to approximate the integral \\(\\Phi(x)\\) using the empirical mean \\[\n\\hat{p}_n(x)=\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{N}(0,1),\n\\] and \\[\n\\hat{p}_n(x)\\in[0,1]\n\\] for all \\(x\\in\\mathbb{R},\\) and all “sample sizes” (chosen by us) \\(n=1,2,\\dots\\)\n\n2.2.1 Distributional Properties of Proportions\nNow, to assess the accuracy of our Monte Carlo integration for approximating \\(\\Phi_n(x),\\) we need to derive the distributional properties of our estimator \\(\\hat{p}_n(x).\\)\nFor this we consider, firstly, the transformed random variable \\[\nn \\hat{p}_n(x)=\\sum_{i=1}^n1_{(X_i\\leq x)},\n\\] which is just the sum of independent Bernoulli distributed random variables with success probability \\(p=\\Phi(x),\\) \\[\n1_{(X_1\\leq x)},\\dots,1_{(X_n\\leq x)}\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{Bern}\\left(p=\\Phi(x)\\right)\n\\] Thus, the transformed random variable \\[\nn\\hat{p}_n(x)=\\sum_{i=1}^n1_{(X_i\\leq x)}\n\\] is binomial distributed with parameters \\(n\\) and \\(p=\\Phi(x),\\) \\[\nn \\hat{p}_n(x)\\sim \\mathcal{Binom}\\left(n, p=\\Phi(x)\\right).\n\\] Thus, we know the exact variance which is \\[\n\\begin{align*}\n\\mathbb{V}\\left(n\\hat{p}_n(x)\\right)\n&=n p (1-p)\\\\[2ex]\n&=n\\Phi(x)(1-\\Phi(x)).\n%\\Leftrightarrow \\mathbb{V}\\left(\\hat{p}_n(t)\\right) &=\\frac{\\Phi(x)(1-\\Phi(x))}{n}\n\\end{align*}\n\\] The standard error for our estimator \\(\\hat{p}_n(x)\\) thus is equal to \\[\n\\begin{align*}\n\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)\n&=\\sqrt{\\frac{\\Phi(x)(1-\\Phi(x))}{n}}\\\\[2ex]\n&=\\texttt{const}\\cdot \\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\] That is, the Monte Carlo integration algorithm has the typical parametric convergence rate of \\[\n\\frac{1}{\\sqrt{n}}.\n\\]\n\nTable 2.1 gives the evolution of the Monte Carlo integration results for Example 2.2 \\[\n\\hat{p}_n(x)\\approx \\Phi(x)\n\\] for several values of \\(x\\) and \\(n.\\) Very accurate approximations are achieved for \\(n=10^8.\\)\n\n\nTable 2.1: Monte Carlo integration results \\(\\hat{p}_n(x)\\approx \\Phi(x)\\) for different values of \\(x\\) and sample sizes \\(n.\\)\n\n\n\\(n\\)\n\\(x=0.0\\)\n\\(x=0.84\\)\n\\(x=3.72\\)\n\n\n\n\n\\(10^2\\)\n\\(0.4850\\)\n\\(0.7700\\)\n\\(1.0000\\)\n\n\n\\(10^3\\)\n\\(0.4925\\)\n\\(0.8010\\)\n\\(1.0000\\)\n\n\n\\(10^4\\)\n\\(0.4962\\)\n\\(0.7941\\)\n\\(0.9999\\)\n\n\n\\(10^5\\)\n\\(0.4995\\)\n\\(0.7993\\)\n\\(0.9999\\)\n\n\n\\(10^6\\)\n\\(0.5001\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\\(10^7\\)\n\\({\\color{red}0.5002}\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\\(10^8\\)\n\\(0.5000\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that greater accuracy is achieved in the tails.\n\n\n\nConfidence Intervals for Proportions\nAs an additional tool for showing the accuracy of our Monte Carlo integration, we can report confidence intervals.\nFor large \\(n,\\) we have by the CLT that \\[\n\\begin{align*}\n\\frac{\\hat{p}_n(x) -  \\Phi(x)}{\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)} \\overset{d}{\\approx}\\mathcal{N}(0,1).\n\\end{align*}\n\\tag{2.2}\\] Thus, for large \\(n,\\) we can provide an approximate confidence interval \\[\n\\begin{align*}\n\\operatorname{CI}^{95\\%}_n\\left(\\Phi(x)\\right)\n=&\\left[\n  \\hat{p}_n(x) \\pm z_{1-\\alpha/2}\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)\n  \\right].\n\\end{align*}\n\\tag{2.3}\\] This confidence interval is an approximate one, since the quantile \\(z_{1-\\alpha/2}\\) uses the approximate asymptotic \\(\\mathcal{N}(0,1)\\) distribution in Equation 2.2. To make the confidence interval in Equation 2.3 usable in practice, we need to plug-in an estimate for the unknown \\(\\operatorname{SE}\\left(\\hat{p}_n(x)\\right),\\) \\[\n\\widehat{\\operatorname{SE}}\\left(\\hat{p}_n(x)\\right) = \\sqrt{\\frac{\\hat{p}_n(x)(1-\\hat{p}_n(x))}{n}}.\n\\]\nHowever, it turns out that going the indirect way via the central limit theorem to derive a confidence interval is not optimal here. A more efficient confidence interval for proportions is, for instance, the Clopper-Pearson confidence interval (Clopper and Pearson (1934)).\nThe following code snippet computes the 99% Clopper-Pearson confidence interval for the red marked Monte Carlo integration result, \\(\\hat{p}_n(0)=0.5002,\\) in Table 2.1.\n\n## install.packages(\"PropCIs\")\nlibrary(\"PropCIs\")\n\n## Clopper-Pearson confidence interval\n\nn <- 10e7       # \"sample\" size (chosen by use)\nx <- 0.5002 * n # observed value of n * \\hat{p}_n(x) \n\nCI <- exactci(x          = x, \n              n          = n, \n              conf.level = 0.99)\n\n## Lower and upper CI-border              \nc(CI$conf.int[1], CI$conf.int[2]) \n\n[1] 0.5001089 0.5002911\n\n\n\n\n\n\n\nThat is, we need a sample size of \\(n^7\\) to achieve a precision of three decimal places by means of a 99% Clopper-Pearson confidence interval."
  }
]