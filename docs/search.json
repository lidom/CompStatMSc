[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    12:15-13:45 \n    Jur / Hörsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/computational-statistics-script-MSc/\nWe’ll use an eWhiteboard for derivations and some extra explanations.\nBasic material from our econometrics course:\n\nIntroduction to R\nProbability\n\n\n\n\n\n\n\n\nYou can use the Zulip-Chat CompStat (M.Sc.) to post questions, share codes, etc. Happy sharing and discussing!\n\n\n\n\n\nConsider using git/github for your personal course notes.\n\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html",
    "href": "Ch1_Random_Variable_Generation.html",
    "title": "1  Random Variable Generation",
    "section": "",
    "text": "In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:\n\nMonte Carlo Statistical Methods, Ch. 2, Robert and Casella (2004)\nIntroducing Monte Carlo Methods with R, Ch. 2, Robert and Casella (2010)"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "href": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "title": "1  Random Variable Generation",
    "section": "1.1 Uniform Simulation",
    "text": "1.1 Uniform Simulation\nGeneral procedure:\n\nUsually, a random integer with values uniformly in \\([0,m]\\) with a large integer \\(m\\) is generated.\nTo achieve a random number in \\([0, 1]\\), we divide this number by \\(m\\).\nFrom this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.\n\nThere are many different Random Number Generators (RNGs), we consider the most simple class of RNGs:\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.1 (Linear Congruential Generators) Here the \\(i\\)th random integer \\(u_i\\) is generated by \\[\nu_i=(a u_{i-1}+c) \\,\\operatorname{mod}\\, m,\n\\] where the starting value \\(u_0\\) is a chosen and fixed value called seed.\nFurthermore:\n\n\\(m\\), with \\(0<m\\), is called the modulus\n\\(a\\), with \\(0<a<m\\), is called the multiplier\n\\(c\\), with \\(0\\leq c<m\\), is called the increment\n\n\n\n\n\n\n\n\n\n\nThe modulo operator: \\(\\operatorname{mod}\\)\n\n\n\n“\\(b\\,\\operatorname{mod}\\,c\\)” denotes the remainder of the division of \\(b\\) by \\(c\\).\nFor instance \\[\n\\begin{align*}\n4\\,&\\operatorname{mod}\\,2 = 0\\\\\n5\\,&\\operatorname{mod}\\,2 = 1\\\\\n1\\,&\\operatorname{mod}\\,2 = 1\\\\\n\\end{align*}\n\\]\n\n# Modulo computation using the modulo operator '%%'\n5 %% 4\n9 %% 4\n4 %% 5\n\n# own modulo-function:\nmy_mod <- function(x,m){\n  t1 <- floor(x/m)\n  return(x-t1*m)\n}\n\n\n\nSome Facts:\n\nThe above recursion generates a completely nonrandom sequence, therefore it is often called a pseudo random sequence.\nUnder appropriate choices of \\(u_0\\) , \\(a\\) and \\(m\\) the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on \\([0, m]\\).\nThe cycle length of linear congruential generators will never exceed modulus \\(m\\), but can maximized with the three following conditions (see Knuth (2002) for a proof):\n\nThe increment \\(c\\) is relatively prime to \\(m\\),\n\\(a - 1\\) is a multiple of every prime dividing \\(m\\),\n\\(a - 1\\) is a multiple of \\(4\\) when \\(m\\) is a multiple of \\(4\\).\n\n\n\nBad choice of parameters for the linear congruential random number generator:\n\nm <- 64    # modulus\na <- 33    # multiplier\nc <- 12    # increment\ns <- 57    # seed\nn <- 1000  # length of run (including seed)\n\nr_vec    <- numeric(n) # initialize vector\nr_vec[1] <- s # set seed\n\n## Recursive generation \nfor (i in 1:(n-1)){\n r_vec[i+1] <- (a * r_vec[i] + c) %% m\n}\n\n# scale result from [0,m] to [0,1]:\nmy_bad_runif_vec <- r_vec/m\n\n# BUT! Very short cycle-length (here: period=16)\nr_vec[ 1:16]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\nr_vec[17:32]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\n\n\nExample 1.1 (Good vs. Bad RNGs) Average heads ratios \\[\n\\bar{C}_n=\\frac{1}{n}\\sum_{i=1}^nC_i\n\\] for \\(n=1,2,\\dots\\) simulated independent tosses of a fair coin \\(C_i\\) with\n\\[\nC_{i}=\\left\\{\\begin{array}{ll}\n1&\\text{if Head}\\\\\n0&\\text{if Tail}\n\\end{array}\\right.\n\\] and \\[\nP(C_i=0)=P(C_i=1)=0.5.\n\\] By the strong (or weak) law of large numbers the average should converge stochastically (i.e., almost surely or in probability) to \\(0.5\\) as \\(n\\) becomes large \\((n\\to\\infty),\\) \\[\n\\bar{C}_n=\\frac{1}{n}\\sum_{i=1}^nC_i \\to_p 0.5,\\quad n \\to\\infty.\n\\]\n\n# using the above bad RNG:\nbar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)\n\n# using R's high-quality RNG:\nset.seed(223)\nbar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)\n\n# plotting the results:\nplot(bar_x_bad, type=\"l\", ylim=c(0.46,0.54), \n     xlab=\"\", ylab=\"\", main=\"Good vs. Bad RNG\")\nlines(bar_x_good, col=\"darkblue\")\n\n\n\n\nFigure 1.1: Two sample paths showing the pseudo random convergence of \\(\\bar{C}_n\\) to the limit 0.5—one based on a good RNG and the other based on a bad RNG.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIBM’s RANDU is a famous example of an miss-specified linear congruential RNG."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "title": "1  Random Variable Generation",
    "section": "1.2 Generation of Discrete Random Variables",
    "text": "1.2 Generation of Discrete Random Variables\n\nAssume the discrete random variable \\(X\\) of interest takes on the values \\[\nX\\in \\{x_1, \\dots , x_k \\}\n\\tag{1.1}\\] with \\[\np_i = \\mathbb{P}(X = x_i ), \\quad i = 1,\\dots , k,\n\\] and \\[\n\\sum_{i=1}^kp_i = 1.\n\\]\nAssume that you can generate pseudo-random realizations \\(u\\in[0,1]\\) from a uniformly distributed random variable \\(U\\sim\\mathcal{U}[0, 1]\\) using an RNG.\nGeneral principle:\n\nSubdivide \\([0, 1]\\) into \\(k\\) intervals with \\[\nI_i = (a_{i-1}, a_i],\n\\] where \\[\na_i = \\sum_{j=1}^ip_j\\quad\\text{and}\\quad a_0 = 0.\n\\]\nDefine the new discrete realizations \\[x=\\left\\{\n\\begin{array}{cc}\n       x_1&\\quad\\text{if}\\quad u\\in I_1\\\\\n       \\vdots& \\vdots\\\\\n       x_k&\\quad\\text{if}\\quad u\\in I_k\n       \\end{array}\\right.\n\\tag{1.2}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nLemma 1.1 Let \\(u\\) be a realization from \\(\\mathcal{U}[0, 1]\\) and if \\(u\\in I_i\\), set \\(x = x_i\\) (see Equation 1.2). Then \\(x\\) is a realizaton of the discrete random variable \\(X\\) (see Equation 1.1).\n\n\n\n\nProof of Lemma 1.1:\nFor any \\(i = 1, \\dots, k\\) we have that \\[\n\\begin{align*}\n\\mathbb{P}(X = x_i)\n& = \\mathbb{P}(U \\in I_i)  \\\\\n& = F_\\mathcal{U}(a_i) - F_\\mathcal{U}(a_{i-1})\\\\\n& = a_i - a_{i-1}\\\\\n& = \\sum_{j=1}^ip_j - \\sum_{j=1}^{i-1}p_j  = p_i,\n\\end{align*}\n\\] which shows the statement of Lemma 1.1.\n\nExample 1.2 (Bernoulli Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Bernoulli}(p),\n\\] where \\(p\\) is the probability of success, i.e., \\[\n\\mathbb{P}(X=1)=p\\quad\\text{and}\\quad\\mathbb{P}(X=0)=1-p.\n\\]\nAlgorithm: If \\(U\\sim\\mathcal{U}[0,1]\\) and \\(p\\) is specified, define \\[\nX=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\(X\\sim\\mathrm{Bernoulli}(p)\\).\n\nset.seed(321)\n# Generate one random number from Bernoulli(p) with p=0.5\np  <- 0.5\nU  <- runif(1)\n\nif(U<=p) X=1 else X=0\n\nX\n\n[1] 0\n\n\n\n\nExample 1.3 (Binomial Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Binomial}(n,p),\n\\] where \\(n\\) is the number of trials and \\(p\\) the probability of success such that \\[\n\\mathbb{P}(X=i)=\\binom{n}{i}p^i(1-p)^{n-i}\n\\] for \\(i=1,\\dots,n.\\)\n\n\n\n\n\n\nTip\n\n\n\nIf \\(X_1,\\dots,X_n\\overset{i.i.d}{\\sim}\\mathrm{Bernoulli}(p),\\) then \\[\nX=\\sum_{i=1}^nX_i \\sim\\mathrm{Binomial}(n,p).\n\\]\n\n\nAlgorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(n\\) and \\(p\\) are specified, define \\[\nX_i=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U_i\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Binomial}(n,p).\n\\]\n\nset.seed(321)\n\n# Generate one random number from B(n=10, p=0.5). \nn <- 10\nX <- numeric(n)\np <- 0.5\n\nfor(i in 1:n){\n  U <- runif(1)\n  if(U<=p) X[i]=1 else X[i]=0\n}\nY <- sum(X)\nY \n\n[1] 7\n\n\n\n\nExample 1.4 (Poisson Distribution) Algorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=\\min\\left\\{n=0,1,2\\dots,\\text{ such that }\\prod_{i=1}^{n+1} U_i \\leq e^{-\\lambda}\n\\right\\}.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Poisson}(\\lambda).\n\\]\n\nset.seed(321)\n\n# Generate one random number from Poisson(lambda) \nlambda <- 2\n\n## Initializations\nU <- 1\nn <- 0\n\nwhile(U > exp(-lambda)){\n  U <- U * runif(1)\n  n <- n + 1\n}\nn <- n-1\nn\n\n[1] 3"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "title": "1  Random Variable Generation",
    "section": "1.3 Generation of Continuous Random Variables",
    "text": "1.3 Generation of Continuous Random Variables\n\n1.3.1 The Inverse Method\nA rather general method to generate continuous random variables is the Inverse Method.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Inverse Method) Let \\(U\\sim\\mathcal{U}[0,1],\\) and let \\(F_X\\) be an invertible distribution function. The transformed random variable\n\\[\nX=F_X^{-1}(U)\n\\] has then the distribution function \\(F_X,\\) \\[\nP(X\\leq x) = F_X(x).\n\\]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTheorem 1.1 can only be used to generate random variables \\(X\\) with invertible distribution functions \\(F_X.\\)\n\n\nProof of Theorem 1.1:\nThe distribution function of the transformed random variable \\[\nX=F^{-1}(U)\n\\] can be derived as \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&= \\mathbb{P}(F_X^{-1}(U)\\leq x) \\\\\n&= \\mathbb{P}(U\\leq F_X(x)) \\\\\n&= F_U(F_X(x)) \\\\\n& = F_X(x),\n\\end{align*}\n\\] which shows the result of Theorem 1.1. The last (and important) equality follows since the distribution function of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\n\\mathbb{P}(U\\leq u) = F_U(u) = u, \\quad 0\\leq u \\leq 1\n\\] since the distribution function \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\nF_U(u) = \\left\\{\n  \\begin{array}{ll}\n  0 & \\text{for } u < 0\\\\\n  u & \\text{for } 0 \\leq  u \\leq 1\\\\\n  1 & \\text{for } 1 < u.\\\\\n  \\end{array}\n\\right.  \n\\tag{1.3}\\]\n\n\n\nExample 1.5 (Exponential Distribution) Since \\[\nF(x)= 1 - \\exp(-\\lambda x),\n\\] we have \\[\nF^{-1}(u) = - \\frac{\\ln(1-u)}{\\lambda}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that \\(1-U\\) has the same distribution as \\(U\\), if \\(U\\sim U[0,1]\\). Therefore also \\(-\\frac{\\ln(u)}{\\lambda}\\) leads to a value from \\(\\mathrm{Exp}(\\lambda).\\)\n\n\nAlgorithm: If \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=-\\frac{\\ln(U)}{\\lambda}.\n\\] Then \\[\nX\\sim \\mathrm{Exp}(\\lambda).\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods.\nThe inverse method often cannot be applied or is often inefficient, because the inverse of many important distribution functions cannot be derived in closed form:\n\nThe Gaussian distribution function \\(\\Phi\\) and therefore also its inverse \\(\\Phi^{-1}\\) is not available in closed form.\nFor discontinuous random variables we need efficient algorithms for computing the generalized inverse of their distribution function \\(F.\\)\n\n\n\n\n\n1.3.2 Transformation Methods\nIdea: Construct algorithms from theoretical links between distributions.\nPro: These methods can be advantageous if a distribution \\(f\\) is linked (in a relatively simple way) to another distribution that is easy to simulate.\nCon: Generally, these methods are rather case-specific, and difficult to generalize.\n\nExample 1.6 (Building on Exponential RVs) In Example 1.5, we learned to generate an exponential random variable \\(X\\sim\\operatorname{Exp}(\\lambda)\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from exponential random variables \\(X\\sim\\mathrm{Exp}(1):\\)\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\n\n\n\n\n\n\n\nNote\n\n\n\nThere are better algorithms to generate Gamma and Beta random variables.\nWe cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter \\(\\alpha\\).\n\nThis implies that we cannot generate a \\(\\chi^2_{1}\\)-distributed random variable, because the \\(\\chi^2_{1}\\) distribution is identical to the \\(\\Gamma(\\alpha, 2)\\) distribution with \\(\\alpha=\\frac{1}{2}.\\)\nThis then also implies that we cannot generate a \\(\\mathcal{N}(0,1)\\)-distrbuted random variable, since \\(X^2\\sim \\chi^2_{1}\\) for \\(X\\sim\\mathcal{N}(0,1)\\).\n\n\n\n\nThe well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of Box and Muller (1958):\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Normal Variable Generation (Box and Muller, 1958)) If \\(U_1\\) and \\(U_2\\) are i.i.d. as \\(U\\sim\\mathcal{U}[0,1]\\), then \\[\nX_1 =\\sqrt{-2 \\ln(U_1)}\\, \\cos(2\\pi U_2)\n\\] and \\[\nX_2=\\sqrt{-2\\ln(U_1)}\\,\\sin(2\\pi U_2)\n\\] are both i.i.d. as \\(X\\sim\\mathcal{N}(0,1).\\)\n\n\n\nProof of Theorem 1.2:\nDefine the random variables \\[\nR = \\sqrt{-2 \\ln(U_1)}\\quad\\text{and}\\quad Q = 2\\pi U_2,\n\\] where \\[\nR\\in(0,\\infty)\\quad\\text{and}\\quad Q\\in[0,2\\pi].\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIdea of the proof:\n\nDerive the bivariate density of \\((R, Q).\\)\nDetermine the functional connection \\(g\\) between \\((R, Q)\\) and \\((X_1, X_2)\\) and note that \\(g\\) is invertible.\nUse the transformation formula for densities to derive the bivariate density of \\((X_1,X_2)\\) using \\(g^{-1}\\) and the bivariate density of \\((R, Q).\\)\nThe result follows, if the bivariate density of \\((X_1,X_2)\\) equals the product of two standard normal densities.\n\nTransformation formula (bivariate case):\nAssume that the bivariate random variable \\[\n\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\n\\] has a bivariate density \\(f_{RQ}(r, q)\\) and that there is a mapping \\(g\\) between the bivariate random variables \\[\n\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\\text{ and }\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\n\\] such that \\[\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)=\ng(R, Q)=\n\\left(\\begin{matrix}g_1(R, Q)\\\\ g_2 (R, Q)\\end{matrix}\\right),\n\\] where \\(g:\\mathbb{R}^2\\to\\mathbb{R}^2\\) is a differentiable and invertible function with inverse \\(g^{-1}.\\)\nThen, the bivariate density of \\(\\left(\\begin{matrix}X_1\\\\X_2\\end{matrix}\\right)\\) is given by \\[\nf_{X_1X_2}(x_1,x_2)=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\,\\left|\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|,\n\\tag{1.4}\\] where \\[\n\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\n\\] denotes the determinant of the Jacobian matrix of \\(g^{-1}\\) evaluated at \\((x_1,x_2),\\) \\[\nJ_{g^{-1}}(x_1,x_2)=\\left(\\begin{matrix}\n\\frac{\\partial g_1^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_1^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\frac{\\partial g_2^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_2^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\end{matrix}\\right).\n\\] Note that the Jacobian of \\(g^{-1}\\) equals the inverse of the Jacobian of \\(g,\\) \\[\nJ_{g^{-1}}(x_1,x_2) = \\left(J_{g}(r,q)\\right)^{-1},\n\\] with points \\((x_1,x_2)\\) and \\((r,q)\\) such that \\[\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)=\ng(r, q)=\n\\left(\\begin{matrix}g_1(r, q)\\\\ g_2 (r, q)\\end{matrix}\\right),\n\\]\n\n\nWe can derive the distribution function of \\(R\\) as following \\[\n\\begin{align*}\nF_R(r)=\\mathbb{P}\\left(R\\leq r\\right)\n& = \\mathbb{P}\\left(\\sqrt{-2 \\ln(U_1)}\\leq r\\right) \\\\\n& = \\mathbb{P}\\left(\\ln(U_1)\\geq -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) < -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) \\leq -\\frac{r^2}{2}\\right) \\quad \\text{(continous)}\\\\\n& = 1 - \\mathbb{P}\\left(U_1 \\leq \\exp\\left(-\\frac{r^2}{2}\\right)\\right) \\\\\n& = 1 - F_U\\left(\\exp\\left(-\\frac{r^2}{2}\\right)\\right)\\\\\n& = 1 - \\exp\\left(-\\frac{r^2}{2}\\right),\n\\end{align*}\n\\] where the last step follows from applying the distribution \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1];\\) see Equation 1.3.\nFor the density function \\(f_R\\) of \\(R\\) we get \\[\nf_R(r)=F_R'(r)=\\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(-\\frac{r^2}{2}\\right)\\cdot r&\\text{for }r \\in(0,\\infty)\\\\\n  0&\\text{otherwise}\\\\\n  \\end{array}\\right.\n\\] Next, we derive the density function of \\[\nQ = 2\\pi U_2.\n\\] Since \\(U_2\\sim\\mathcal{U}[0,1],\\) \\[\nQ\\sim\\mathcal{U}[0,2\\pi].\n\\] with density function \\[\nf_Q(q)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{1}{2\\pi}&\\text{for } q\\in [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\] Since \\(U_1\\) and \\(U_2\\) are independent, \\(R=\\sqrt{-2 \\ln(U_1)}\\) and \\(Q = 2\\pi U_2\\) must also be independent, such that \\[\n\\begin{align*}\nf_{RQ}(r,q)\n& = f_R(r)\\cdot f_Q(q) \\\\\n& = \\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(-\\frac{r^2}{2}\\right) r\\cdot \\frac{1}{2\\pi} & \\text{for } (r,q) \\in (0,\\infty)\\times [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\end{align*}\n\\]\nNow, as we know the bivariate density of \\((R,Q)\\) we can use the functional connection \\[\n\\begin{align*}\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\n& = g(R,Q) \\\\\n& = \\left(\\begin{matrix}\n       g_1(R,Q)\\\\\n       g_2(R,Q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\n  R\\cos(Q)\\\\\n  R\\sin(Q)\n  \\end{matrix}\\right)\n% & = \\left(\\begin{matrix}\n%   \\sqrt{-2\\ln(U_1)}\\cos\\left(2\\pi U_2\\right)\\\\\n%   \\sqrt{-2\\ln(U_1)}\\sin\\left(2\\pi U_2\\right)\\\\\n%   \\end{matrix}\\right)\\\\\n\\end{align*}\n\\] with \\[\nR = \\sqrt{-\\ln(U_1)}\\in (0,\\infty)\n\\] and \\[\nQ=2\\pi U_2\\in[0, 2\\pi].\n\\] \n\n\n\n\n\n\nTip\n\n\n\nNote that, \\(g\\) is just the one-to-one transformation that maps points \\((r,q)\\) of the polar coordinate system\n\nradius \\(r\\in (0,\\infty)\\) and\nangle \\(q\\in[0, 2\\pi]\\)\n\nto points \\((x_1,x_2)\\) of the Cartesian coordinate system: \\[\n\\begin{align*}\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)\n& = g(r,q)\\\\\n& = \\left(\\begin{matrix}g_1(r,q) \\\\ g_2(r,q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}r\\cos(q)\\\\r\\sin(q)\\end{matrix}\\right)\n\\end{align*}\n\\]\nPlay around with this mapping here: https://mathinsight.org/applet/polar_coordinates_map_rectangle\nThe inverse mapping \\(g^{-1}\\) maps points \\((x_1,x_2)\\) from the Cartesian coordinate system to points \\((r,q)\\) in the polar coordinate system \\[\n\\begin{align*}\n\\left(\\begin{matrix}r\\\\ q\\end{matrix}\\right)\n& = g^{-1}(x_1,x_2)\\\\\n& = \\left(\\begin{matrix}g_1^{-1}(x_1,x_2) \\\\ g_2^{-1}(x_1,x_2)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\\sqrt{x_1^2 + x_2^2}\\\\ \\operatorname{atan2}(x_1,x_2)\n\\end{matrix}\\right),\n\\end{align*}\n\\] where the function \\(\\operatorname{atan2}\\) is the 2-argument arctangent. \n\n\n\\[\n\\begin{align*}\nJ_{g^{-1}}(x_1,x_2)\n&=\\left(J_{g}(r,q)\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\frac{\\partial g_1}{\\partial r}(r,q) & \\frac{\\partial g_1}{\\partial q}(r,q)\\\\\n\\frac{\\partial g_2}{\\partial r}(r,q) & \\frac{\\partial g_2}{\\partial q}(r,q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\cos(q) & -r\\sin(q)\\\\\n\\sin(q) & \\phantom{-}r\\cos(q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\n\\frac{1}{r\\cos^2(q) + r\\sin^2(q)}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\\\\\n&=\n\\frac{1}{r}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\n\\end{align*},\n\\] where the last step follows from Pythagorean’s identity \\(\\cos^2(q) + \\sin^2(q)=1.\\) So \\[\n\\begin{align*}\n\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\n&=\\left|\\operatorname{det}\\left(\\left(J_{g}(r,q)\\right)^{-1}\\right)\\right|\\\\\n&=\n\\left|\\operatorname{det}\\left(\n  \\begin{matrix}\n            \\cos(q) &            \\sin(q)\\\\\n-\\frac{1}{r}\\sin(q) & \\frac{1}{r}\\cos(q)\\\\\n\\end{matrix}\\right)\n\\right|\\\\\n&=\n\\left|\\frac{1}{r}\\cos^2(q) + \\frac{1}{r}\\sin^2(q)\\right| = \\frac{1}{r},\n\\end{align*}\n\\] again using Pythagorean’s identity \\(\\cos^2(x_2) + \\sin^2(x_2)=1\\) and using that \\(r\\in(0,\\infty).\\)\nThus, by the transformation formula for bivariate densities (Equation 1.4), we have \\[\n\\begin{align*}\nf_{X_1X_2}(x_1,x_2)\n&=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\\\\\n&=f_{RQ}\\Big(\\underbrace{\\sqrt{x_1^2+x_2^2}}_{=r},\\underbrace{\\operatorname{atan2}(x_1,x_2)}_{=q}\\Big)\\frac{1}{r}\\\\\n&=\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right) \\sqrt{x_1^2+x_2^2} \\cdot \\frac{1}{2\\pi} \\frac{1}{r}\\\\\n&=\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right)  \\frac{1}{2\\pi},\n\\end{align*}\n\\] where the last step uses that \\(r=\\sqrt{x_1^2+x_2^2}.\\)\nThis shows the result of Theorem 1.2, since the bivariate density of \\((X_1,X_2)\\) equals the product of two marginal standard normal density functions, \\[\n\\begin{align*}\nf_{X_1X_2}(x_1,x_2)\n& = \\frac{1}{2\\pi}\\exp\\left(-\\frac{x_1^2+x_2^2}{2}\\right) \\\\\n& = \\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x_1^2}{2}\\right)}_{=\\phi(x_1)} \\cdot\n    \\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x_2^2}{2}\\right)}_{=\\phi(x_2)},\n\\end{align*}\n\\] where \\(\\phi\\) denotes the density function of the standard normal distribution \\(\\mathcal{N}(0,1)\\).\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  # Return result\n  return(c(X1, X2))\n}\n\n# Generate n standard normal random variables:\nset.seed(321)\n\nn     <- 1000\nX_vec <- vector(length = n, mode = \"double\")\n\nfor(i in seq(1, n, by=2)){\n  X_vec[c(i, i+1)] <- BM_Algo()\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE, xlim=c(-4,4))\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test \n# H0: Normality\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99882, p-value = 0.7667\n\n\n\n\n\nThat is, the Shapiro Wilk is not able to reject its null-hypothesis that the data comes from a normal distribution. However, this was only a single test result and the results may differ in other samples. So, let’s check the test results across \\(B=5000\\) samples.\n\n## Monte Carlo (MC) replications\nB     <- 5000\n\n## Sample size\nn     <- 1000\n\n## Significance level\nalpha <- 0.05\n\n## Container vectors\nX_vec <- vector(length = n, mode = \"double\")\nT_vec <- vector(length = B, mode = \"logical\")\n\n## MC-Simulation\nfor(b in 1:B){ \n\n  for(i in seq(1, n, by=2)){\n    X_vec[c(i, i+1)] <- BM_Algo()\n  }\n\n  ## Collect test decisions\n  tmp      <- shapiro.test(X_vec)\n  T_vec[b] <- tmp$p.value < alpha\n}\n\nround(mean(T_vec), 2)\n\n[1] 0.05\n\n\nIt turns out that the false positive rate (empirical frequency of Type I errors) of the Shapiro Wilk test equals the chosen significance level \\(\\alpha.\\) That is, the test is not able to reject its null-hypothesis (“normal distribution”) beyond the expected frequency of a Type I error.\n\n\n1.3.3 Accept-Reject Methods\nFor many distributions \\(F\\) it is difficult (or impossible) to apply the Inverse Method or some Transformation Method, since the distribution function \\(F\\) is somehow “unsuitable”. For instance, surprisingly often there is no explicit form of \\(F\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest—actually, \\(f\\) needs to be known only up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.3 (Fundamental Theorem of Simulation) Let \\(X\\in\\mathbb{R}^d\\) be a random variable with density function \\(f_X,\\) \\[\nX\\sim f_X.\n\\] Then simulating \\(X\\) is equivalent to simulating \\((X,U)\\in\\mathbb{R}^{d+1},\\)\n\\[\n(X,U)\\sim\\mathcal{U}(\\mathcal{A}),\n\\] where \\(\\mathcal{U}(\\mathcal{A})\\) denotes the uniform distribution over the area \\[\n\\mathcal{A}=\\left\\{(x,u)\\;|\\; x\\in\\mathbb{R}^d \\text{ and } 0<u<f_X(x)\\right\\}.\n\\]\n\n\n\nFigure 1.2 visualizes the statement of Theorem 1.3 for the univariate \\((d=1)\\) case.\n\n\n\n\n\nFigure 1.2: To simulate a random variable \\(X,\\) one can simulate \\((X,U)\\) uniformely over \\(\\mathcal{A},\\) and then take those \\(X\\) realizations (tick-marks at \\(x\\)-axis) as simulation results.\n\n\n\n\nIt turns out that sampling \\((X, U)\\) uniformly over the set \\(\\mathcal{A}\\) is often challenging. However, one can consider some superset \\(\\mathcal{S},\\) \\[\n\\mathcal{A}\\subseteq \\mathcal{S},\n\\] such that simulating a random variable uniformly distributed over \\(\\mathcal{S}\\) is easy.\nA uniform distribution on \\(\\mathcal{A}\\) can then be obtained by\n\ndrawing from a uniform distribution on \\(\\mathcal{S}\\) and\nrejecting samples that are in \\(\\mathcal{S},\\) but not in \\(\\mathcal{A}.\\)\n\n\n1.3.3.1 The case of bounded densities with compact support\nThe general principle of the accept-reject method is easily explained using a bounded density function \\(f\\) with compact support.\n\n\n\n\n\n\nTip\n\n\n\n\nBounded (from above) means that there exists a constant \\(m\\) with \\(0<m<\\infty\\) such that \\[\n\\sup_xf(x)\\leq m\n\\]\nAn interval \\([a,b]\\) is called compact if it is closed and the boundaries are finite. For instance, the Gaussian density \\(\\phi\\) has not a compact support, but \\(\\mathrm{supp}(\\phi)=(-\\infty,\\infty)\\).\nExamples:\n\nA bounded density function with compact support: \\[\nf_c(x)=\\frac{3}{4}\\left(1-\\left(x-c\\right)^2\\right)\\,1_{(|x-c|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[c-1,c+1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f_c\\) is bounded from above by \\(3/4\\) for all \\(c\\in\\mathbb{R}.\\) \nAn unbounded density function with compact support: \\[\nf(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{x}{b^2}\\left(1-\\frac{x^2}{b^2}\\right)^{-1/2}&\\text{ for }x\\in(0,b)\\\\\n0&\\text{ otherwise}\n\\end{array}\n\\right.\n\\] \\(f(x)\\) is \\(0\\) for \\(x=0,\\) but diverges to positive infinity for \\(x\\to b.\\)\n\n\n\n\nThe Accept-Reject Algorithm for bounded densities with compact support:\n\nCompact support: Let \\[\n\\operatorname{supp}(f)=[a_1,b_1]\\times \\dots\\times[a_d,b_d],\n\\] where \\([a_j,b_j]\\subseteq\\mathbb{R}\\) are compact intervals for each \\(j=1,\\dots,d.\\)\nBounded (from above): Let \\[\n\\sup_{x\\in[a_1,b_1]\\times \\dots\\times[a_d,b_d]} f(x)\\leq m,\n\\] where \\(m\\) is a constant with \\(0<m<\\infty.\\)\n\nTo simulate \\[\nX\\sim f_X\n\\] simulate the random pair \\[\n(Y,U)\\sim\\mathcal{U}\\big(\\underbrace{[a_1,b_1]\\times \\dots\\times[a_d,b_d] \\times[0,m]}_{=\\mathcal{S}}\\big)\n\\] by simulating independently \\[\nY\\sim\\mathcal{U}([a_1,b_1]\\times \\dots\\times[a_d,b_d])\n\\] and \\[\nU \\sim \\mathcal{U}[0,m].\n\\]\nAccept-Reject Step:\nAccept a simulated \\(Y\\) as a simulation for \\(X\\), i.e. set \\[\nX=Y,\n\\] only if \\[\nU\\leq f_X(Y),\n\\] and reject all other \\(Y\\)’s.\n\n\nExample 1.7 Let the target density be \\[\nf_X(x)=\\frac{3}{4} \\left(1-x^2\\right)\\mathbf{1}_{(|x|\\leq 1)},\n\\] where \\[\n\\operatorname{supp}(f_X)=[-1,1]\n\\] and \\[\n\\sup_{x\\in[-1,1]}f_X(x)\\leq\\frac{3}{4}.\n\\] Thus the Accept-Reject algorithm for simulating \\(X\\sim f_X\\) can be implemented by generating\n\\[\n(Y,X)\\sim\\mathcal{U}([-1,1]\\times[0,3/4])\n\\] and accepting \\(Y\\) as realizations of \\(X\\) only if \\(U\\leq f_X(Y).\\)\n\nLook at the following code. It’s a very simple algorithm, where we effectively only need to evaluate the density function \\(f_X,\\) target_pdf:\n\n# Target pdf f_X\ntarget_pdf <- function(x){\n  pdf <- (3/4) * (1-(x)^2)\n  pdf[(x)^2 > 1] <- 0\n  ##\n  return(pdf)\n}\n\n# Accept-Reject Algo:\nY <- runif(n=1000, min = -1, max = 1) \nU <- runif(n=1000, min =  0, max = 3/4) \n# A-R Step:\naccept <- U <= target_pdf(Y)\nX      <- Y[accept]\n\nFigure 1.3 illustrates Example 1.7.\n\n\n\n\n\nFigure 1.3: Accept-Reject algorithm for univariate \\((d=1)\\) bounded densities with compact support.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.4 (Accept-Reject Algorithm bounded density functions with compact support.) Let \\(f_X\\) be a \\(d\\)-dimensional density function with\n\ncompact support \\(\\operatorname{supp}(f)=[a_1,b_1]\\times \\dots\\times[a_d,b_d]\\subseteq\\mathbb{R}^d,\\) and\nbounded from above \\(\\sup_{x\\in[a_1,b_1]\\times \\dots\\times[a_d,b_d]}f_X(x)\\leq m\\) with \\(0<m<\\infty.\\)\n\nThen the Accept-Reject algorithm as introduced in Section 1.3.3.1 allows to simulate \\[\nX\\sim f_X.\n\\]\n\n\n\nProof of Theorem 1.4 for the case \\(d=1\\):\nThe following derivation shows that the Accept-Reject algorithm as introduced in Section 1.3.3.1 allows to simulate random variables \\(X\\) with the correct target distribution \\(F_X(x)=\\int_a^xf_X(x)dx.\\)\n\\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&=\\mathbb{P}(Y\\leq x|U\\leq f_X(Y))\\\\[2ex]\n&= \\frac{\\mathbb{P}(Y\\leq x, U\\leq f_X(Y))}{\\mathbb{P}(U\\leq f_X(Y))}\\\\[2ex]\n&= \\frac{\\mathbb{P}(a\\leq Y\\leq x, \\; 0\\leq U\\leq f_X(Y))}{\\mathbb{P}(0\\leq U\\leq f_X(Y))}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,c\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,c\\;du\\,dy},\n\\end{align*}\n\\] where the constant \\(c\\) is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation \\[\n\\int_a^{b} \\int_0^{m}\\,c\\,du\\,dy = 1,\n\\] but which is irrelevant here since \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{c\\;\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{c\\;\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}.\\\\\n\\end{align*}\n\\] Now, using that \\(\\int_{0}^{f_X(y)}1du=\\big[x\\big]^{f_X(y)}_0=f_X(y)\\) yields \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{\\int_a^x f_X(y)\\,dy}{\\int_a^b f_X(y)\\,dy}\\\\[2ex]\n& =\\int_a^x f_X(y)dy = F_X(x),\n\\end{align*}\n\\] which completes the proof.\n\n\n1.3.3.2 The case of densities with non-compact support\nIf \\(f_X\\) has no compact support, the larger set \\(\\mathcal{S}\\) does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set” \\(\\mathcal{S},\\) enclosing the density function \\(f_X\\), as long as simulating uniformly over this larger set \\(\\mathcal{S}\\) is feasible. This generalization allows for cases, where the support of \\(f_X\\) is not compact (i.e. unbounded).\nLet the larger set denote by \\[\n\\mathcal{S}=\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathcal{S}\\) is feasible and\n\\(f_X(x)\\leq m(x)\\) for all \\(x\\in\\mathbb{R}^d.\\)\n\nFrom the feasibility-requirement it follows that the function \\(m\\) is necessarily integrable, i.e., that \\[\n\\int\\dots \\int m(x_1,\\dots,x_d)dx_1\\dots dx_d=M,\n\\] or short with \\(x=(x_1,\\dots,x_d)\\) and \\(X\\in\\mathbb{R}^d\\) \\[\n\\int_{\\mathbb{R}^d} m(x)dx=M,\n\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathcal{S}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathcal{S}\\).\nIntegrability of \\(m\\) is crucial here, since it allows us to relate the function \\(m\\) with a corresponding (auxiliary) density function \\(g\\) as following: \\[\nm(x)=M\\,g(x),\n\\] where \\[\n\\int_{\\mathbb{R}^d} m(x)\\,dx=\\int_{\\mathbb{R}^d} M\\,g(x)\\,dx=M.\n\\] Therefore, a uniform distribution over \\(\\mathcal{S},\\) \\[\n\\mathcal{U}\\big(\\underbrace{\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\}}_{\\mathcal{S}}\\big)\n\\] has a density function that is equal to \\[\n\\frac{1}{M}\n\\] for all points \\((y,u)\\in\\mathcal{S}\\) and zero else—otherwise the density would not integrate to one.\nTerminology:\n\nThe density function \\(g\\) is called the instrumental density. (Choose \\(g\\) as a density from which it is easy to simulate!)\nThe density function \\(f_X\\) is called the target density.\n\nThe Accept-Reject Algorithm for densities with non-compact support:\nTo simulate \\[\nX\\sim f_X\n\\] simulate the \\((d+1)\\)-dimensional random variable\n\\[\n(Y,U)\\sim\\mathcal{U}\\big(\\underbrace{\\{(y,u)\\;|\\;y\\in\\mathbb{R}^d\\text{ and } 0\\leq u\\leq m(y)\\}}_{\\mathcal{S}}\\big)\n\\] by simulating\n\na realization of \\[\nY\\sim g\n\\] and\nfor a given realization \\(Y={\\color{red}y},\\) a realization of\n\\[\nU|Y={\\color{red}y}\\sim\\mathcal{U}[0,M\\,g({\\color{red}y})].\n\\]\n\nAccept-Reject Step:\nAccept a simulated \\(Y\\) as a simulation for \\(X\\), i.e. set \\[\nX=Y\n\\] only if \\[\nU\\leq f_X(Y)\n\\] and reject all other \\(Y\\)’s.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.5 (Accept-Reject Algorithm bounded density functions with non-compact support.) Let \\(f_X\\) be a \\(d\\)-dimensional density function and let \\(m(x)=Mg(x)\\) such that \\(g\\) is a \\(d\\)-dimensional auxiliary density function with\n\n\\(\\int m(x)dx=\\int Mg(x)dx = M\\) with constant \\(0<M<\\infty,\\) and\n\\(f_X(x)\\leq m(x)\\) for all \\(x\\in\\mathbb{R^d}.\\)\n\nThen the Accept-Reject algorithm as introduced in Section 1.3.3.2 allows to simulate \\[\nX\\sim f_X.\n\\]\n\n\n\nProof of Theorem 1.5:\nThe following derivation shows that the Appcept-Reject algorithm as introduced in Section 1.3.3.2 allows to simulate random variables \\(X\\) with the correct target distribution \\(f_X.\\)\nFirst, note that \\(X\\sim f_X\\) if and only if \\[\nP(X\\in A) = \\int_A f_X(y)dy\n\\] for every \\(A\\subseteq \\mathbb{R}^d.\\) I.e., this is what we need to show.\nIn a first step, we use that \\(X=Y\\) only if \\(U\\leq f_X(Y),\\) which implies that\n\\[\n\\begin{align*}\n\\mathbb{P}(X\\in A) & =\\mathbb{P}(Y\\in A|U\\leq f_X(Y)).\n\\end{align*}\n\\] The definition of conditional probabilities yields that \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& = \\frac{\\mathbb{P}(Y\\in A, \\; U\\leq f_X(Y))}{\\mathbb{P}(U\\leq f_X(Y))}.\n\\end{align*}\n\\] Using that the density function of \\((Y,U)\\sim\\mathcal{U}(\\mathcal{S})\\) equals \\(\\frac{1}{M}\\) for all \\((y,u)\\in\\mathcal{S}\\) and zero else we have that\n\n\\(\\mathbb{P}(Y\\in{\\color{red}A}, \\; U\\leq f_X(Y)) = \\int_{\\color{red}A}\\int_0^{f(y)}\\frac{1}{M}\\,du\\,dy\\quad\\)\n\nand that\n\n\\(\\mathbb{P}(U\\leq f_X(Y)) = \\int_{\\mathbb{R}^d}\\int_0^{f(y)}\\frac{1}{M}\\,du\\,dy\\quad\\) by integrating out \\(y\\) to get the marginal density.\n\nThus, \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& =\\frac{\\int_A \\int_0^{f_X(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_{\\mathbb{R}^d}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\\\\[2ex]\n& =\\frac{\\frac{1}{M}\\,\\int_A \\,\\int_0^{f_X(y)}\\,1\\,du\\,dy}{\\frac{1}{M}\\,\\int_{\\mathbb{R}^d}\\,\\int_0^{f(y)}\\,1\\,du\\,dy}\\\\[2ex]\n& =\\frac{\\int_A \\,\\int_0^{f_X(y)}\\,1\\,du\\,dy}{\\int_{\\mathbb{R}^d}\\,\\int_0^{f(y)}\\,1\\,du\\,dy}\\\\[2ex]\n\\end{align*}\n\\] Using that \\(\\int_{0}^{f_X(y)}\\, 1\\, du = f_X(y),\\) we get that \\[\n\\begin{align*}\n\\mathbb{P}(X\\in A)\n& =\\frac{\\int_A f_X(y)\\,dy}{\\int_{\\mathbb{R}^d} f_X(y)\\,dy}\\\\[2ex]\n& =\\int_A f_X(y)dy,\n\\end{align*}\n\\] which applies to every set \\(A\\) and thus shows the result.\n\n\n\n\n\n\nNote\n\n\n\nNote that the above derivation implies that we only need to know the density function \\(f_X\\) up to a multiplicative constant \\(c>0,\\) since constant factors cancel—just as the normalizing constant \\(1/M.\\)\nThat is, instead of working with \\(f_X,\\) we can also work with a scaled version \\(\\tilde{f}_X\\) that is proportional to \\(f_X,\\) \\[\nf_{X}(x) = c \\tilde{f}_X(x)\n\\] often written as \\[\nf_{X}(x)\\propto \\tilde{f}_X(x),\n\\] since the constant \\(c\\) cancels out anyways in the above quotient expression. (The sign “\\(\\propto\\)” means “proportional to”.)\nThis is a very useful property for implementing the Accept-Reject algorithm, since it implies that it is irrelevant whether we scale the instrumental density \\(g\\) by some \\(1\\leq M <\\infty,\\) or the target density \\(f_X\\) by some constant \\(c>0.\\) All we need is that \\[\n\\frac{1}{c}f_X(x)=\\tilde{f}_X(x) \\leq M g(x) \\text{ for all } x\\in\\mathbb{R}^d.\n\\] for some some \\(c>0\\) and some \\(1\\leq M < \\infty.\\) This allows us to choose \\(c\\) and \\(M\\) such that simulating from \\(M g\\) is as simple as possible.\n\n\n\n\n\n\nExample 1.8 Let the target “density” be \\[\nf_X(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1).\n\\]\nGiven our considerations above, we can try to scaling such that the above “density” is dominated (i.e. \\(\\tilde{f}_X(x) \\leq g(x)\\) for all \\(x\\)) by the standard normal density \\[\ng(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2),\n\\] which is obviously straightforward to simulate from.\nThus, in this example we can set \\[\nm(x)=M\\,g(x)\\quad\\text{with}\\quad M=1,\n\\] since we can simply scale the target “density” \\(f_X\\) such that \\[\n\\tilde{f}_X(x)\\leq g(x)\\quad\\text{for all}\\quad x.\n\\] Specifically, we can set \\[\n\\tilde{f}_X(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1).\n\\]\nThe following code provides a possible implementation of the Accept-Reject algorithm for this example. Figure 1.4 shows the results.\n\n# Target density\ntarget_pdf <- function(x, c=.075){\n  pdf <- c * exp(-x^2 / 2) * \n            (sin(6*x)^2 + 3*cos(x)^2 * sin(4*x)^2 + 1)\n  return(pdf)\n}\n\n# Upper bound\nm_fun <- function(x){\n  m <- exp(-x^2 / 2)/sqrt(2*pi)\n  return(m)  \n}\n\n# Accept-Reject Algo:\nset.seed(32280)\n\n## 1. Generate Y\nY   <- rnorm(n=1000)\n\n## 2. Generate U|Y=y \nU   <- vector(length = length(Y), mode = \"double\")\n\nfor(i in 1:length(Y)){\n  U[i] <- runif(n=1, min = 0, max = m_fun(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= target_pdf(Y)\nX      <- Y[accept]\n\n\n\n\n\n\nFigure 1.4: Accept-Reject algorithm for univariate \\((d=1)\\) densities with non-compact support—visualizing Example 1.8.\n\n\n\n\n\n\n\n\n\n\n1.3.3.3 Efficiency of the Accept-Reject algorithm\nSimple statements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f_X\\) and \\(g\\) are both normalized such that they are both density functions. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. The closer \\(M\\) is to \\(1\\) the better.\n\\(M\\) quantifies how closely \\(g\\) can imitate \\(f_X.\\)\n\n\nExample 1.9 (Normals from Double Exponentials) Consider simulating from \\(\\mathcal{N}(0,1)\\) using the Accept-Reject algorithm with the a double-exponential distribution \\(\\mathcal{L}(\\alpha),\\) also called Laplace distribution, as the instrumental density, \\[\ng(x|b)=\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right).\n\\]  It is then straightforward to show (see Exercises) that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n%&=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n%&=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\\\\[2ex]\n&\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\end{align*}\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental density: \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|1)} \\leq M\n& =\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right)\\\\[2ex]\n& =\\sqrt{\\frac{2}{\\pi} \\,\\exp\\left(1\\right)}.\n\\end{align*}\n\\]\nThe probability of acceptance is then \\[\n\\frac{1}{M}=\\sqrt{\\frac{\\pi}{2\\exp(1)}}\\approx 0.76.\n\\] I.e., to produce one normal random variable, this Accept-Reject algorithm requires on average \\[\n\\frac{1}{0.76}\\approx 1.3\n\\] uniform variables.\nThis is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is \\(1.\\)\n\n\n\n1.3.3.4 The instrumental density \\(g\\) needs thicker tails than the target density \\(f_X\\)\nLet \\(f_X\\) and \\(g\\) be both density functions, and let the instrumental density \\(g\\) be such that \\[\ng(x)>0  \n\\] for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\n\n\n\n\n\nTip\n\n\n\nThe defining properties of a density functions \\(f\\) are:\n\nNon-negative: \\(f(x)\\geq 0\\) for all \\(x\\)\nNormalized: $f(x)dx = 1 $\n\n\n\nThen, the inequality \\[\nf_X(x)\\leq M\\,g(x),\n\\] with \\(1<M<\\infty\\) for all \\(x\\in\\mathbb{R}^d,\\) implies that the quotient \\(f_X/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f_X(x)}{g(x)}\\leq M <\\infty\n\\] for all \\(x\\in\\operatorname{supp}(f_X).\\)\nThis implies that \\(g\\) has to have thicker tails than \\(f_X.\\)\nTo see this, observe that both \\[\nf_X(|x|)\\to 0 \\quad\\text{and}\\quad g(|x|)\\to 0\n\\] for “extreme” values \\(|x|\\to\\infty,\\) simply because both \\(f_X\\) and \\(g\\) are densities.\nIf \\(g\\) has thicker tails than \\(f_X,\\) then \\[\n0 \\leq \\frac{f_X(|x|)}{g(|x|)} \\leq 1 \\leq M\n\\] for all sufficiently large values \\(|x|.\\) Thus with \\(g\\) having thicker tails than \\(f_X,\\) we can be sure that the requirement \\(\\frac{f_X(x)}{g(x)}\\leq M <\\infty\\) holds for all sufficiently extreme values of \\(x.\\)\nBy constrast, if \\(g\\) has strictly thinner tails than \\(f_X,\\) then \\[\n0\\leq \\frac{f_X(|x|)}{g(|x|)} \\to\\infty,\\quad\\text{as}\\quad|x|\\to\\infty,\n\\] which makes the requirement that \\(\\frac{f_X(x)}{g(x)}\\leq M <\\infty\\) impossible to hold for extreme values of \\(x.\\)\n\n\n\n\n\n\nNote\n\n\n\nTherefore, it is, for instance, impossible to simulate from a Cauchy density \\(f_X\\) using a normal density \\(g\\). The reverse, however, works quite well."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "href": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "title": "1  Random Variable Generation",
    "section": "2.1 Classical Monte Carlo Integration",
    "text": "2.1 Classical Monte Carlo Integration\nThe generic problem here is the evaluation of integrals. (Be aware: Integrals are everywhere in statistics!). For instance, \\[\n\\mathbb{E}_f\\left(h(X)\\right)=\\mathbb{E}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f(x)\\,dx.\n\\]\nConvergence:\nGiven our previous developments, it is natural to propose using a realization \\(x_1,\\dots,x_m\\) from a (pseudo random) i.i.d. sample \\(X_1,\\dots,X_m\\) with each \\(X_j\\) distributed as \\(X\\sim f\\) to approximate the above integral by the empirical mean \\[\n\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h(x_j).\n\\] By the Strong Law of Large Numbers we know that the empirical mean \\(\\bar{h}_m\\) converges almost surely (a.s.) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\) as \\(m\\to\\infty\\). (The only prerequisits are that \\(f\\) has finite first moments, i.e., \\(\\mathbb{E}\\left(h(X)\\right)<\\infty\\), and that \\(\\bar{h}_m\\) is constructed from an i.i.d. sample \\(X_1,\\dots,X_m\\).)\nAs we can use the computer to produce realizations from the i.i.d. sample \\(X_1,\\dots,X_m\\), we can in principle choose an arbitrary large sample size \\(m\\) such that \\(\\bar{h}_m\\) can (in principle) be arbitrarily close to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\).\nThough, …\n\n… which sample size \\(m\\) is large enough?\nOr “equivalently”: How fast converges \\(\\bar{h}_m\\) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\)?\n\n\n\nSpeed of Convergence:\nOK, we know now that \\(\\bar{h}_m\\) reaches its limit (here in the “almost surely” sense) as \\(m\\to\\infty\\) under some rather loose conditions on the random sample \\(X_1,\\dots,X_m\\).\nIf we are willing to additionally assume that \\(f\\) has finite second moments, i.e., \\(\\mathbb{E}(h(X)^2)<\\infty\\), we can additionally say something about how fast \\(\\bar{h}_m\\) converges (a.s.) to \\(\\mathbb{E}(h(X))\\).\nThe speed of convergence of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) (i.e., now we think of \\(\\bar{h}_m\\) as the {RV} \\(\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h({\\color{red}{X_{j}}})\\)) to its limit \\(\\mathbb{E}(h(X))\\) can be assessed by answering the question how fast the standard deviation (which is a function of \\(m\\)) of the stochastic sequence converges to zero as \\(m\\to\\infty\\).\n\nThe variance of \\(\\bar{h}_m\\) is given by \\[\n\\mathbb{V}_f\\left(\\bar{h}_m\\right)=\n\\mathbb{V}\\left(\\frac{1}{m}\\sum_{j=1}^m h(X_j)\\right)=\n\\frac{1}{m}\\mathbb{V}\\left(h(X)\\right)\n\\]\nNote that assuming finite second moments \\(\\mathbb{E}(h(X)^2)<\\infty\\) is equivalent to assuming finite variance \\(\\mathbb{V}\\left(h(X)\\right)<\\infty\\). Consequently, we can set \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\) with \\(0<\\mathtt{const}<\\infty\\) such that \\[\n\\sqrt{\\mathbb{V}\\left(\\bar{h}_m\\right)}=m^{-1/2}\\mathtt{const}\\propto m^{-1/2}.\n\\]\n\nI.e., the speed of convergence (or rate) of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) is proportional to the deterministic sequence \\(\\{m^{-1/2}\\}\\).\n\n\nRemark: Even if we would not know the value of \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\), we know now that the improvement from \\(m=10\\) to \\(m=100\\) will be much higher than from \\(m=110\\) to \\(m=200\\). In practice, a typical choice is \\(m=10000\\); for moderate standard deviations this choice will guarantee a very good approximation.\n\n\nLimit Distribution:\nOf course, we can estimate the variance of the estimator \\(\\mathbb{V}\\left(\\bar{h}_m\\right)\\) by its empirical version \\[\nv_m=\\frac{1}{m}\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right),\n\\] where again by the Strong Law of Large Numbers (SLLN) \\[\n\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right)\\to_{\\text{a.s.}}\\mathbb{V}\\left(h(X)\\right).\n\\]  By the Central Limit Theorem (CLT) we have \\[\n\\sqrt{m}\\left(\\frac{\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)}{\\sqrt{\\mathbb{V}\\left(h(X)\\right)}}\\right)\\to_d Z,\n\\] where \\(Z\\sim N(0,1)\\). Note that the the above sequence \\(\\{\\sqrt{m}\\}\\) just hinders the convergence of the sequence \\(\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)\\to_{a.s.}0\\) such that the quotient converges to a “stable” distribution.\nThe above result can now be used for the construction of (asymptotically valid) convergence tests and confidence intervals with respect to \\(\\bar{h}_m\\), since for large \\(m\\) \\[\n\\bar{h}_m\\,\\overset{d}{\\approx} N\\left(\\mathbb{E}\\left(h(X)\\right),\\frac{\\mathbb{V}\\left(h(X)\\right)}{m}\\right).\n\\]\nAnd as we can use the computer to generate realizations of the i.i.d. sample \\(X_1,\\dots,X_m\\) from a generic \\(X\\sim f\\), we can easily approximate the mean \\(\\mathbb{E}\\left(h(X)\\right)\\) and the variance \\(\\mathbb{V}\\left(h(X)\\right)\\) with arbitrary accuracy as \\(m\\to\\infty\\); by the SLLN (or the WLLN).\n\n\nExample: A first Monte Carlo Integration\nLet’s say we want to integrate the function \\(h(x)=\\left(\\cos(50\\,x)+\\sin(20\\,x)\\right)^2\\). Although this function could be integrated analytically it is a good first test case. The left plot below shows the graph of the function \\(h(.)\\).\nTo approximate the integral \\[\n\\int_\\mathcal{X}h(x)dx\\quad\\text{with}\\quad\\mathcal{X}=[0,1]\n\\] we can use that \\[\n\\int_\\mathcal{X}h(x)dx=\\int_\\mathcal{[0,1]}1\\cdot h(x)dx =\\mathbb{E}_{f_\\text{Unif[0,1]}}(h(X)).\n\\]\nThus, we generate a realization \\((u_1,\\dots,u_n)\\) from the i.i.d. random sample \\(U_1,\\dots,U_n\\sim[0,1]\\) and approximate \\[\n\\int_\\mathcal{X}h(x)dx\\approx \\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(u_i).\n\\]\nIn order to assess how good this approximation is, we need to consider the stochastic propoerties of the RV \\[\n\\frac{1}{n}\\sum_{i=1}^n h(U_i).\n\\] This is done using the above (review of) results on the limit distribution of the sample mean which allows us to construct an approximative \\(95\\%\\) confidence interval, since for large \\(n\\) \\[\n\\left[\\bar{h}_n - 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}, \\bar{h}_n + 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}\\right]\\approx\n\\left[\\bar{h}_n - 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}, \\bar{h}_n + 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}\\right],\n\\] where \\(\\mathtt{std.error}_n^2=n^{-1}\\sum_{i=1}^n(h(u_i)-\\bar{h}_n)^2\\).\nThe right plot below shows one realization of the stochastic sequence \\(\\{\\bar{h}_1,\\dots,\\bar{h}_n\\}\\) with \\(n=10000\\), where the realized value of \\(\\bar{h}_n\\) is \\(0.966\\). This compares favorably with the with the exact value of \\(0.965\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks:\n\nThe approach followed in the above example can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency through numerical methods (e.g., Riemann Sum, Trapezoidal Rule, Simpson’s Rule, etc.) in dimensions 1 or 2.\nThe approach is particularly useful for approximating integrals over higher dimensional sets.\n\n\n\nExample: Approximation of Normal Distribution Tables\nA possible way to construct normal distribution tables is to use MC simulations.\nGenerate a realization \\((x_1,\\dots,x_n)\\) from an i.i.d. standard normal random sample, e.g., using the Box-Muller algorithm.\nThe approximation of the standard normal cdf \\[\n\\Phi(t)=\\int_{-\\infty}^t\\frac{1}{\\sqrt{2\\pi}}e^{-y^2/2}dy\n\\] by the Monte Carlo method is thus \\[\n\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n 1_{(x_i\\leq t)}.\n\\] The corresponding RV \\(\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\leq t)}\\) has (exact) variance \\[\n\\mathbb{V}(\\hat{\\Phi}_n(t))=\\frac{\\Phi(t)(1-\\Phi(t))}{n},\n\\] since the single RVs \\(1_{(X_i\\leq t)}\\) are independent Bernoulli with success probability \\(\\Phi(t)\\).\nFor values of \\(t\\) around \\(t=0\\), the variance is thus approximately \\(1/4n\\).\nTo achieve a precision of four decimals by means of a \\(99.9\\%\\) confidence interval, the approximation requires on average \\(n\\approx 10^8\\) simulations.\nThe table below gives the evolution of this approximation for several values of \\(t\\) and shows a very accurate evaluation for \\(n=10^8\\).\n\n\n\\[\n\\begin{array}{cccccccccc}\n\\hline\nn   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\\\\n\\hline\n10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\\\\n10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\\\\n10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\\\\n10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\\\\n10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\\\\n\\end{array}\n\\]\n\n\nRemarks:\n\nTo achieve a precision of two decimals by means of a \\(99.9\\%\\) confidence interval, already \\(n=10^4\\) leads to satisfactory results.\nNote that greater accuracy is achieved in the tails and that more efficient simulation methods could be used (e.g., Importance Sampling)."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "href": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "title": "1  Random Variable Generation",
    "section": "2.2 Importance Sampling",
    "text": "2.2 Importance Sampling\nImportance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it’s refereed to as a variance reduction technique. This variance reduction is achieved by weighting functions, so-called importance functions.\nAs in the case of Monte Carlo integration the focus lies on evaluating the integral \\[\n\\mathbb{E}_f(h(X))=\\int_\\mathcal{X}h(x)f(x)\\,dx.\n\\]\n\nThough, it turns out that the above approach, i.e., sampling from \\(f\\) is often suboptimal.\nObserve that the value of the above integral can be represented by infinitely many alternative choices of the triplet \\((\\mathcal{X}, h, f)\\). Therefore, the search for an optimal estimator should encompass all these possible representations.\n\nLet’s illustrate this with a simple example.\nExample: Cauchy Tail Probability (from Ripley 1987)\nSuppose that the quantity of interest is the probability, say \\(p\\), that a Cauchy \\(\\mathrm{C}(0,1)\\) RV is larger than \\(2\\), i.e.: \\[\np=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx.\n\\]\n1. Naive Approach: If \\(p\\) is approximated through the empirical mean \\[\n\\hat{p}_{1}=\\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\n\\] of an i.i.d. sample \\(X_1,\\dots,X_m\\sim\\mathrm{C}(0,1)\\), then the variance of this estimator, a binomial RV scaled by \\(1/m\\), is \\[\n\\mathbb{V}(\\hat{p}_{1})=\\frac{1}{m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(X_j>2)}\\right)=\\frac{p(1-p)}{m},\n\\] which is equal to \\(0.1275/m\\), since (we already know that) \\(p=0.15\\).\n\n\n2. Accounting for Symmetry (i.e., using the ‘Adjusting Screws’ \\(\\mathcal{X}\\) and \\(h\\)): We can achieve a more efficient estimator (i.e., an estimator with lower variance for a given same sample size \\(n\\)) if we take into account the symmetric nature of \\(\\mathrm{C}(0,1)\\). Obviously, our target integral can be equivalently written as \\[\np=\\frac{1}{2}\\left(\\int_{-\\infty}^{-2}\\frac{1}{\\pi(1+x^2)}\\,dx + \\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx \\right).\n\\] This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean: \\[\n\\hat{p}_{2}=\n\\frac{1}{2}\\left(\\frac{1}{m}\\sum_{j=1}^m1_{(X_j<-2)}+ \\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\\right)\\;=\\;\n\\frac{1}{2m}\\sum_{j=1}^m1_{(|X_i|>2)}.\n\\] The variance of this new estimator, \\[\n\\mathbb{V}(\\hat{p}_{2})=\\frac{1}{4m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(|X_i|>2)}\\right)=\\frac{2p(1-2p)}{4m},\n\\] is equal to \\(0.0525/m\\), i.e., lower than in the naive approach.\n\n\n3. Using all ‘Adjusting Screws’ \\(\\mathcal{X}\\), \\(h\\), and \\(f\\): The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, \\([2,+\\infty)\\), which are in some sense irrelevant for the approximation of \\(p\\). This motivates the following reformulation of \\(p\\):\nBy symmetry of \\(f\\): \\[\n\\frac{1}{2}=\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx + \\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}dx}_{=p}\n\\] \\[\n\\Leftrightarrow \\; p=\\frac{1}{2}-\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx.\n\\] Furthermore, we can re-arrange the last integral a bit such that \\[\n\\int_{0}^2\\;\\left(\\frac{1}{2}\\cdot 2\\right)\\;\\frac{1}{\\pi(1+x^2)}\\,dx =\n\\int_{0}^2\\;\\underbrace{\\frac{1}{2}}_{f_{\\mathrm{Unif}[0,2]}}\\;\\underbrace{\\frac{2}{\\pi(1+x^2)}}_{=h(x)}\\,dx =\n\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,2].\n\\]\nTherefore a new alternative method for evaluating \\(p\\) is: \\[\n\\hat{p}_{3}=\\frac{1}{2} - \\frac{1}{m}\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U_j\\sim\\mathrm{Unif}[0,2].\n\\] Using integration by parts, it can be shown that \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\). (Compare this to the former results: \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\).)\n\n\nA More General Point of View:\nThe idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx.\n\\]\nSome outcomes of \\(X\\sim f\\) may be more important than others in determining \\(\\theta\\) and we wish to select such values more frequently.\nFor instance, if \\(\\theta\\) denotes the probability of the occurrence of a very rare event, then the only way to estimate \\(\\theta\\) at all accurately may be to produce the rare events more frequently.\nTo achieve this, we can simulate a model which gives pdf \\(g\\) to \\(X\\) instead of the correct pdf \\(f\\), where both pdfs need to be known. This can be easily done, since \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)\\left(\\frac{g(x)}{g(x)}\\right)\\;f(x)dx=\n\\int \\underbrace{\\left(h(x)\\frac{f(x)}{g(x)}\\right)}_{=\\psi(x)}\\;g(x)dx=\n\\int \\psi(x)\\;g(x)dx=\n\\mathbb{E}_g(\\psi(X)).\n\\]\nThis leads to the following unbiased estimator for \\(\\theta\\) based on sampling from \\(g\\): \\[\n\\hat{\\theta}_g=\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i)\\quad\\text{with}\\quad X_i\\sim g,\n\\] which is a weighted mean of the \\(h(X_i)\\) with weights inversely proportional to the “selection factor” \\(\\frac{g(X_i)}{f(X_i)}\\). \nFor the variance of the estimator \\(\\hat{\\theta}_g\\) we have \\[\n\\mathbb{V}(\\hat{\\theta}_g)=\\frac{1}{n}\\mathbb{V}(\\psi(X_i))=\n\\frac{1}{n}\\int\\left(\\psi(x)-\\theta\\right)^2g(x)dx=\n\\frac{1}{n}\\int\\left(\\frac{h(x)\\,f(x)}{g(x)}-\\theta\\right)^2g(x)dx,\n\\] which, depending on the choice of \\(g(.)\\), can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean. \n\n\n\n\nMinimum Variance Theorem\n\nThe importance function \\(g(.)\\) which minimizes the variance \\(\\mathbb{V}(\\psi(X_i))\\), and therefore the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\), is given by \\[\ng^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\]\n\n\nProof: Done in the lecture.\n\n\nThough, this result is rather formal (in the sense of “impractical”), since, e.g., if \\(h(x)>0\\) then \\(g^\\ast\\) requires us to know \\(\\int h(z)f(z)dz\\), which is just the integral of interest!\nRemarks:\nThe above minimum variance result is still useful:\n\nIt tells us that a good choice of \\(g(x)\\) shall mimic the shape of \\(|h(x)|f(x)\\), since the optimal \\(g^\\ast(x)\\propto |h(x)|f(x)\\).\nFurthermore, \\(g(x)\\) should be chosen such that it has a thicker tail than \\(f(x)\\), since the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\) crucially depends on the quotient \\(f(x)/g(x)\\) which would “explode” for \\(g(x)\\approx 0\\).\n\n\n\nLet’s apply our new insights to the above example on the Cauchy tail probability \\(p\\).\nExample: Cauchy Tail Probability (cont.)\nAbove we had:\n\n\\(f(x)=\\frac{1}{\\pi(1+x^2)}\\), the pdf of \\(\\mathrm{C}(0,1)\\) and\n\\(h(x)=1_{(x>2)}\\), i.e., here \\(|h(x)|=h(x)\\).\n\nTherefore \\[\np=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx=\\int_{2}^{\\infty}f(x)dx=\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx=\\mathbb{E}_g(\\psi(X)),\n\\] where the \\(h\\) function is absorbed by the formulation of the definite integral.\nA possibly good (and simple) choice of \\(g\\) is, e.g., \\(g(x)=2/(x^2)\\), since this function:\n\n“closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: It is not straight forward to directly sample from \\(g\\), therefore we need some further steps:\n\n\nThe choice of \\(g\\) leads to \\[\np=\\mathbb{E}_g(\\psi(X))=\n\\int_{2}^{+\\infty}\\left(\\frac{x^2}{2\\,\\pi(1+x^2)}\\right)\\,\\frac{2}{x^2}\\,dx=\n\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\]\n\n\nNow we can apply some additional (rather case-specific) re-arrangements:\nIntegration by substitution (substituting \\(u=x^{-1}\\)) yields: \\[\np=\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] Again, we can re-arrange the last integral a bit such that \\[\np=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathrm{Unif}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du=\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2].\n\\] Therefore, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2]\n\\] and \\(h(u)=1/(2\\pi(1+u^2))\\).\nThe variance of \\(\\hat{p}_4\\) is \\((\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2)/m\\) and an integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.95\\cdot 10^{-4}/m\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox, George EP, and Mervin E Muller. 1958. “A Note on the Generation of Random Normal Deviates.” The Annals of Mathematical Statistics 29 (2): 610–11. https://projecteuclid.org/euclid.aoms/1177706645.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch_Bootstrap.html",
    "href": "Ch_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.1 The empirical distribution function",
    "text": "3.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 3.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 3.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#basic-idea",
    "href": "Ch_Bootstrap.html#basic-idea",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic idea",
    "text": "3.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 3.1)."
  },
  {
    "objectID": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The nonparametric (standard) bootstrap",
    "text": "3.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 3.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n3.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n3.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{3.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "3.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 3.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 3.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{3.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 3.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.5 Regression Analysis: Bootstrapping pairs",
    "text": "3.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 3.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n3.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Residual bootstrap",
    "text": "3.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 3.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html",
    "href": "Ch_MaximumLikelihood.html",
    "title": "4  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Properties of Maximum Likelihood Estimators",
    "text": "4.2 Properties of Maximum Likelihood Estimators\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\nExample: Coin Flipping (Bernoulli Trial)\nLet \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "href": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "title": "4  Maximum Likelihood",
    "section": "4.3 The (Log-)Likelihood Function",
    "text": "4.3 The (Log-)Likelihood Function\nHow do we combine information from \\(n\\) observations to estimate \\(\\theta\\)?\nIf we assume that all of the observations are drawn from same distribution and are independent, then joint probability of observing \\(h\\) heads and \\(n-h\\) tails in the \\(n\\) coin flips that we actually observed, given \\(\\theta\\), is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)&= \\theta^h(1-\\theta)^{n-h}  \\\\\n            &= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\texttt{HEAD}\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\texttt{TAIL}\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations are identically and independently distributed (i.i.d): \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\); the parameter \\(\\theta\\) denotes the density function parameter(s).\nOur goal is to choose a value for \\(\\theta\\) such that the value of the likelihood function is at a maximum, i.e. we choose the value of the parameter(s) that maximize the “probability” or better the likelihood of observing the data that we actually observed. That is: \\[\n\\hat\\theta=\\arg\\max_\\theta \\mathcal{L}(\\theta).\n\\] defines the maximum likelihood (ML) parameter estimator \\(\\hat\\theta\\).\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation (taking \\(\\ln\\)) to the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\]\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn this case, we can analytically solve for the value of \\(\\theta\\) that maximizes the log likelihood (and hence also the likelihood): \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\hat\\theta_{ML}=\\dfrac{h}{n}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Optimization: Non-Analytical Solutions",
    "text": "4.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 4.1).\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 4.1.\n\n\nTable 4.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "4  Maximum Likelihood",
    "section": "4.3 OLS-Estimation as ML-Estimation",
    "text": "4.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{4.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 3.4).\nFor the following, it is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 4.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.5}\\]\nUsing Equation 4.4 and Equation 4.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "href": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "title": "2  Random Variable Generation",
    "section": "2.4 Building on Exponential RVs",
    "text": "2.4 Building on Exponential RVs\nIn Example 2.5, we learned to generate an exponential random variable \\(X\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from an exponential distribution:\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\nSome Limitations:\n\nThere are more efficient algorithms to generate Gamma and Beta RVs.\nWe cannot use exponential RVs to generate Gamma RVs with a non-integer shape parameter \\(\\alpha\\). * This implies that we cannot generate a \\(\\chi^2_{1}\\) RV, which would, in turn, get us a \\(N(0,1)\\) RV. (Reminder: \\(\\chi^2_{1}\\) is identical to \\(\\Gamma(1/2, 2)\\).)\nFor that we look at the Box-Muller Theorem (1958) and the derived algorithm.\n\n\n\n\n\nExample: Normal Variable Generation\nThe well-known Box-Muller algorithm for generating (standard) normal RV is based on the following theorem:\n\nTheorem (Box and Muller, 1958)\n\nIf \\(U_1\\) and \\(U_2\\) are i.i.d. \\(U[0,1]\\), then \\[X_1 =\\sqrt{-2 \\log(U_1)}\\, \\cos(2\\pi U_2)\\quad\\text{and}\\quad X_2=\\sqrt{-2\\log(U_1)}\\,\\sin(2\\pi U_2)\\] are i.i.d. \\(N(0,1)\\).\n\n\nIdea & Proof: Done in the lecture.\n\n\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # 1. Step: Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # 2. Step: Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  return(c(X1, X2))\n}\n\n# Generation of Stand. Normal RVs through the Box-Muller Algo:\nset.seed(123)\nX_vec <- NULL\nfor(i in 1:500){\n  X_vec <- c(X_vec, BM_Algo())\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE)\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test (H0: Normality)\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99893, p-value = 0.8323\n\n\n\n\n\n\n\n\n2.4.1 Accept-Reject Methods\nFor many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the cdf \\(F(.)\\) is somehow unusable. For instance, surprisingly often there is no explicit form of \\(F(.)\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\n\nGeneral Idea and theoretical justification through the Fundamental Theorem of Simulation: Done in the lecture.\n\n\nThe case of pdfs with compact support:\nThe key-idea is easily explained using a bounded pdf \\(f\\) with compact support.\nNotions:\n\nBounded means that there exists a value \\(m\\) with \\(0<m<\\infty\\) s.t. \\(f(x)\\in[0,m]\\) for all \\(x\\).\nNote that only degenerated pdfs are not bounded.\nAn interval \\([a,b]\\) is called “compact” if it is closed and the boundaries are finite.\nFor instance, the Gaussian has not a compact support, since \\(\\mathrm{supp}(\\phi)=]-\\infty,\\infty[\\).\n\nFor instance, let’s say we want to simulate random numbers \\(X\\sim f\\) with \\[\nf(x)=\\frac{3}{4}\\left(1-\\left(x-1\\right)^2\\right)\\,1_{(|x-1|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[-1,1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f\\) is bounded from above by \\(3/4\\).\n\nThe idea is then to simulate the random pair \\((Y,U)\\sim\\mathrm{Unif}([a,b]\\times[0,m])\\) by simulating\n \\[Y\\sim\\mathrm{Unif}[a,b]\\quad\\text{and}\\quad U|Y=y \\sim \\mathrm{Unif}[0,m], \\] but to accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\leq x)=\\mathbb{P}(Y\\leq x|U\\leq f(Y))\n=\\frac{\\int_a^{\\color{red}x} \\int_0^{f(y)}\\,1\\,du\\,dy}{\\int_a^{\\color{red}b}\\int_0^{f(y)}\\,1\\,du\\,dy}\n=\\frac{\\int_a^x f(y)\\,dy}{\\int_a^b f(y)\\,dy}\n=\\int_a^x f(y)dy,\n\\]\nwhere we used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nThe Accept-Reject Algorithm (Simple Version):\n# Accept-Reject Algorithm:\nY <- runif(n, min = a, max = b) \nU <- runif(n, min = 0, max = m) \n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\nIn the following you see a graphical illustration of this procedure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe good thing is that we only need to evaluate the pdf \\(f(.)\\); nothing more.\n\n\nGeneralization: pdfs with non-compact support.\nThe larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set”, enclosing the pdf \\(f\\), as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of \\(f\\) is unbounded.\nLet the larger set denote by \\[\n\\mathscr{L}=\\{(y,u):\\, 0<u<m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathscr{L}\\) is feasible and\n\n\\(m(x)\\geq f(x)\\) for all \\(x\\).\n\n\n\nFrom the feasibility-requirement it follows that \\(m(.)\\) is necessarily integrable, i.e., that \\[\\int_{\\mathcal{X}}m(x)dx=M,\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathscr{L}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathscr{L}\\).\n\n\nIntegrability of \\(m(.)\\) is crucial here, since it allows us to relate \\(m(.)\\) with a corresponding (auxiliary) pdf \\(g(.)\\) as following: \\[m(x)=M\\,g(x),\\quad\\text{where}\\quad\\int_{\\mathcal{X}}m(x)\\,dx=\\int_{\\mathcal{X}}M\\,g(x)\\,dx=M.\\]\nTerminology:\n\nThe pdf \\(g(.)\\) is called the instrumental density. (Choose \\(g(.)\\) as a pdf from which it is easy to simulate!)\nThe pdf \\(f(.)\\) is called the target density.\n\n\n\nIn order to simulate the pair \\((Y,U)\\sim\\mathrm{Unif}(\\mathscr{L})\\) we can now simulate \\[Y\\sim g\\quad\\text{and}\\quad U|Y={\\color{red}y}\\sim\\mathrm{Unif}[0,M\\,g({\\color{red}y})],\\] but accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\in A)=\\mathbb{P}(Y\\in A|U\\leq f(Y))\n=\\frac{\\int_{\\color{red}A}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_\\mathcal{X}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\n=\\frac{\\int_A f(y)\\,dy}{\\int_\\mathcal{X} f(y)\\,dy}\n=\\int_A f(y)dy,\n\\] for every set \\(A\\),  where we again used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nNote that the above derivation implies that we only need to know the pdf \\(f(.)\\) up to an unkown multiplicative constant \\(c>0\\). I.e., it is enough to know \\(f(x)=c\\,\\tilde{f}_{\\textrm{true}}(x)\\), often written as \\(f(x)\\propto \\tilde{f}_{\\textrm{true}}(x)\\), since the unknown constant \\(c\\) cancels out in the above quotient anyways. This is not so much of importance for us, but useful in Bayesian Statistics.\n\n\nAll this leads to a more general version of the Fundamental Theorem of Simulation:\n\nFundamental Theorem of Simulation (General Version):\n\nLet \\(X\\sim f\\) and let \\(g(.)\\) be a pdf s.t. \\(f(x)\\leq M\\,g(x)\\) for some \\(M\\) with \\(1\\leq M<\\infty\\) and all \\(x\\). Then to simulate \\(X\\sim f\\) it is sufficient to generate \\[Y\\sim g\\quad\\text{and}\\quad U|Y=y\\sim\\mathrm{Unif}[0,M\\,g(y)]\\] if one accepts the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and rejects all others.\n\n\n\n\nThe Accept-Reject Algorithm (General Version):\n# Accept-Reject Algorithm:\nY   <- generate n random numbers from g(.)\n\n# Specify function m():\nm <- function(y){YOUR CODE}\n\nU   <- numeric(n)\nfor(i in 1:n){\n  U[i] <- runif(n=1, min = 0, max = m(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\n\n\nExample\nLet the target “density” be \\[f(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\] with upper bound (or, rather, dominating density) the standard normal density \\[g(x)=\\exp(-x^2/2)/\\sqrt{2\\pi},\\] which is obviously straightforward to generate.\nIn this example we can set \\(m(x)=M\\,g(x)\\) with \\(M=1\\), since we can simply scale the target “density” \\(f\\) such that \\(f(x)\\leq g(x)\\) for all \\(x\\). Specifically, we set \\(f(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\).\nIn the following you see the graphical illustration of this example:\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of the Accept-Reject algorithm:\nStatements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f\\) and \\(g\\) are normalized such that they are both pdfs. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. (The closer \\(M\\) is to \\(1\\) the better.)\n\\(M\\) is a function of how closely \\(g\\) can imitate \\(f\\).\n\nNote that, for such normalized \\(f\\) and \\(g\\) the inequality \\(f(x)\\leq M\\,g(x)\\) with \\(1\\leq M<\\infty\\) for all \\(x\\) is equivalent to saying that the quotient \\(f/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f(x)}{g(x)}\\leq M <\\infty\\quad\\text{for all}\\quad x.\n\\] That is, it is necessary for \\(g\\) to have, e.g., thicker tails than \\(f\\). This makes it, for instance, impossible to simulate a Cauchy distribution \\(f\\) using a normal distribution \\(g\\). The reverse, however, works quite well. \n\n\nExample: Normals from Double Exponentials\nConsider generating a \\(N(0,1)\\) by the Accept-Reject algorithm using a double-exponential distribution \\(\\mathcal{L}(\\alpha)\\), also called Laplace distribution, with density \\(g(x|b)=(1/(2b))\\exp(-\\,|x|/b)\\).  It is then straightforward to show that \\[\n\\frac{f(x)}{g(x|b)}\n%=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\n%=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\n\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf: \\[\n\\frac{f(x)}{g(x|1)}\n\\leq M=\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right).\n\\]\nThe probability of acceptance is then \\(\\sqrt{\\pi/(2e)}=0.76\\). I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average \\(1/0.76\\approx 1.3\\) uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1."
  },
  {
    "objectID": "Ch4_EMAlgorithmus.html",
    "href": "Ch4_EMAlgorithmus.html",
    "title": "4  The Expectation Maximization (EM) Algorithm",
    "section": "",
    "text": "Possible Applications of Gaussian mixture distributions:\n\nGeneral: Finding grouping structures (two or more) in data (Clusteranalyse). For instance:\n\nAutomatic video editing (e.g., separation of back- and foreground)\n\nBehavioral clustering\netc.\n\n\n\n\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22."
  },
  {
    "objectID": "Ch2_Bootstrap.html",
    "href": "Ch2_Bootstrap.html",
    "title": "2  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch2_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch2_Bootstrap.html#the-empirical-distribution-function",
    "title": "2  The Bootstrap",
    "section": "2.1 The empirical distribution function",
    "text": "2.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 2.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 2.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 2.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch2_Bootstrap.html#basic-idea",
    "href": "Ch2_Bootstrap.html#basic-idea",
    "title": "2  The Bootstrap",
    "section": "2.2 Basic idea",
    "text": "2.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 2.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 2.1)."
  },
  {
    "objectID": "Ch2_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch2_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "2  The Bootstrap",
    "section": "2.3 The nonparametric (standard) bootstrap",
    "text": "2.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 2.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n2.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n2.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n2.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{2.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{2.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 2.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch2_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch2_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "2  The Bootstrap",
    "section": "2.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "2.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 2.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n2.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 2.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{2.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 2.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n2.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch2_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch2_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "2  The Bootstrap",
    "section": "2.5 Regression Analysis: Bootstrapping pairs",
    "text": "2.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 2.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n2.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 2.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 2.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch2_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch2_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "2  The Bootstrap",
    "section": "2.6 Regression Analysis: Residual bootstrap",
    "text": "2.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 2.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n2.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 2.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 2.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html",
    "href": "Ch3_MaximumLikelihood.html",
    "title": "3  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch3_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "3  Maximum Likelihood",
    "section": "3.2 Optimization: Non-Analytical Solutions",
    "text": "3.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n3.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 3.1).\n\n\n\n\n\nFigure 3.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 3.1.\n\n\nTable 3.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch3_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "3  Maximum Likelihood",
    "section": "3.3 OLS-Estimation as ML-Estimation",
    "text": "3.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{3.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 2.4).\nFor the following, it is convenient to write Equation 3.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch3_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "3  Maximum Likelihood",
    "section": "3.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "3.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch3_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "3  Maximum Likelihood",
    "section": "3.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "3.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 2.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch3_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch3_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "3  Maximum Likelihood",
    "section": "3.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "3.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 3.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 3.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{3.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 3.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{3.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{3.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{3.5}\\]\nUsing Equation 3.4 and Equation 3.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 3.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 3.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{3.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 3.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 3.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 3.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch2_MonteCarlo.html",
    "href": "Ch2_MonteCarlo.html",
    "title": "2  Monte Carlo Integration",
    "section": "",
    "text": "In principle, it is sufficient to read this script, but I can recommend the following books for further self-studies:\n\nMonte Carlo Statistical Methods, Ch. 3, Robert and Casella (2004)\nIntroducing Monte Carlo Methods with R, Ch. 3, Robert and Casella (2010)\nNumerical Methods in Economics, Ch. 8.2 Monte Carlo Integration, Judd (1998)\n\nMonte Carlo methods take advantage of the availability of:\n\ncomputer generated random variables\nthe law of large numbers\nthe central limit theorem\n\nTerminology:\n\nMonte Carlo Method: The use of (pseudo) random sampling as a tool to produce observations on which statistical inference can be performed to extract information about a system.\nMonte Carlo Integration: A specific Monte Carlo method, where we randomly sample uniformly over some domain and use the produced sample to construct an estimator for a univariate and multivariate integral. (Integrals are everywhere in statistics!)\nStochastic Simulation (or Monte Carlo Simulation): The application of the Monte Carlo method.\n\nThis chapter is about Monte Carlo Integration which is a stochastic alternative to deterministic numerical integration methods such as numerical quadrature.\nFigure 2.1 shows a screenshot of a published example (see Bourreau, Sun, and Verboven (2021)). The authors use Monte Carlo integration to solve the shown integral.\n\n\n\nFigure 2.1: Market share function in Bourreau, Sun, and Verboven (2021). The authors use Monte Carlo integration to solve this integral."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#classical-monte-carlo-integration",
    "href": "Ch2_MonteCarlo.html#classical-monte-carlo-integration",
    "title": "2  Monte Carlo Integration",
    "section": "2.1 Classical Monte Carlo Integration",
    "text": "2.1 Classical Monte Carlo Integration\nThe generic problem here is the evaluation of integrals. For instance, \\[\n\\mathbb{E}_{f_{X}}\\left(h(X)\\right)=\\mathbb{E}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f_X(x)\\,dx,\n\\tag{2.1}\\] where \\(\\mathcal{X}\\) denotes the domain of the random variable \\(X\\in\\mathcal{X}\\subseteq\\mathbb{R}^d,\\) and where \\(h\\) is some transformation function, e.g., \\[\nh(x)=x^2,\\;\\;h(x)=\\ln(x),\\;\\;h(x)=x,\\;\\;\\text{etc.}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nComputing means means computing integrals. To stress that one computes the integral with respect to the distribution characterized by the density function \\(f_X,\\) one can write \\[\n\\mathbb{E}_{f_{X}}\\left(h(X)\\right)\n\\] instead of \\[\n\\mathbb{E}\\left(h(X)\\right).\n\\] We will use this notation more often below.\n\n\nOften, analytical solutions for integrals such as in Equation 2.1 are not readily available and one needs to use some numerical approaches/computational. Given our previous developments, it is kind of natural to propose using a realization \\[\nx_1,\\dots,x_n\n\\] from a (pseudo) random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}f_X\n\\] to approximate the integral in Equation 2.1 using the empirical mean \\[\n\\mathbb{E}\\left(h(X)\\right)\\approx\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(x_i).\n\\] By the Strong Law of Large Numbers (SLLN) we know that the empirical mean \\(\\bar{h}_n\\) converges almost surely (a.s.), and thus also “in probabiliuty” to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\) as the sample size \\(n\\) becomes large, i.e., as \\(n\\to\\infty\\). Prerequisites for the SLLN:\n\n\\(h(X)\\) has finite first moment, i.e., \\(\\mathbb{E}\\left(h(X)\\right)<\\infty\\) and\n\\(\\bar{h}_n\\) is constructed from a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}f_X.\\)\n\nAs we can use the computer to produce realizations from the i.i.d. sample \\(X_1,\\dots,X_n\\), we can in principle choose an arbitrary large sample size \\(n\\) such that \\(\\bar{h}_n\\) can, in principle, be arbitrarily close to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\).\nThough, …\n\n… which sample size \\(n\\) is large enough?\nor “equivalently”, how fast converges \\(\\bar{h}_n\\) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\)?\n\n\n2.1.1 Speed of Convergence\nOK, we know now that \\(\\bar{h}_n\\) reaches its limit (here in the “almost surely” sense, but likewise in the “in probability” case) as \\(n\\to\\infty\\) under some rather loose conditions on the random sample \\(X_1,\\dots,X_n\\).\nIf we are willing to additionally assume that \\(h(X)\\) has finite second moments, i.e. \\[\n\\mathbb{E}(h(X)^2)<\\infty,\n\\] then we can additionally say something about how fast \\[\n\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)\\to_{p} \\mathbb{E}(h(X)).\n\\]\nThe speed of convergence of the stochastic sequence \\[\n\\{\\bar{h}_n\\}_{n=1,2,\\dots}= \\bar{h}_1,\\;\\bar{h}_2,\\;\\;\\bar{h}_3,\\;\\dots\n\\] to its limit \\(\\mathbb{E}(h(X))\\) can be quantified by the rate at which the standard error \\[\n\\operatorname{SE}\\left(\\bar{h}_n\\right)=\\sqrt{\\mathbb{V}\\left(\\bar{h}_n\\right)}\n\\] converges to zero as \\(n\\to\\infty\\).\n\n\n\n\n\n\nTip\n\n\n\nWe think of \\(\\{\\bar{h}_n\\}_{n=1,2,\\dots}\\) as the sequence of random variables\n\\[\n\\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h({\\color{red}{X_{i}}})\n\\] with \\({\\color{red}{X_1}},\\dots,{\\color{red}{X_n}}\\overset{\\text{i.i.d.}}{\\sim}f_X.\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that assuming finite second moments \\(\\mathbb{E}(h(X)^2)<\\infty\\) is equivalent to assuming finite variance \\(\\mathbb{V}\\left(h(X)\\right)<\\infty,\\) since \\[\n\\mathbb{V}\\left(h(X)\\right) = \\mathbb{E}(h(X)^2) - \\left(\\mathbb{E}(h(X))\\right)^2,\n\\] and since if higher moments, like \\(\\mathbb{E}(h(X)^2),\\) are finite, also the lower moments, like \\(\\mathbb{E}(h(X)),\\) are finite.\n\n\nThe standard error of \\(\\bar{h}_n\\) is just the square root of the variance of \\(\\bar{h}_n.\\) The variance of \\(\\bar{h}_n\\) is given by \\[\n\\begin{align*}\n\\mathbb{V}\\left(\\bar{h}_n\\right)\n&=\\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n h(X_i)\\right) \\\\[2ex]\n&=\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n h(X_i)\\right) \\\\[2ex]\n&=\\frac{n}{n^2}\\mathbb{V}\\left(h(X_1)\\right)\\quad \\text{(since i.i.d.)} \\\\[2ex]\n&=\\frac{1}{n}  \\mathbb{V}\\left(h(X_1)\\right)\n\\end{align*}\n\\] The square root of \\(\\mathbb{V}\\left(h(X_1)\\right)\\) equals some finite, positive constant \\(0<\\mathtt{const}<\\infty\\),\n\\[\n\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X_1)\\right)}\n\\] such that \\[\n\\sqrt{\\mathbb{V}\\left(\\bar{h}_n\\right)}=n^{-1/2}\\mathtt{const}%\\propto n^{-1/2}.\n\\] I.e., the speed of convergence (or rate) of the stochastic sequence \\(\\{\\bar{h}_n\\}\\) is proportional to the deterministic sequence \\(\\{n^{-1/2}\\}.\\)\n\n\n\n\n\n\nNote\n\n\n\nEven if we would not know the value of \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)},\\) we know now that the improvement from \\(n=10\\) to \\(n=100\\) will be much higher than from \\(n=110\\) to \\(n=200\\). In practice, a typical choice is \\(n=10,000;\\) for moderate standard errors this choice will guarantee a very good approximation.\n\n\n\n\n2.1.2 Distributional Properties\nBesides the speed of convergence of \\(\\{\\bar{h}_n\\}_{n=1,2,\\dots}\\) for \\(n\\to\\infty,\\) we can also say something about the distribution of the random variable \\(\\bar{h}_n\\) for large sample sizes \\(n.\\)\nWe can estimate the variance of the estimator \\(\\mathbb{V}\\left(\\bar{h}_n\\right)\\) by its empirical version \\[\nv_n^2=\\frac{1}{n}\\sum_{i=1}^n\\left(h(x_i)-\\bar{h}_n\\right)^2,\n\\] where by the SLLN, which also implies convergence in probability, \\[\nv_n^2\\to_{p}\\mathbb{V}\\left(h(X)\\right),\\quad n\\to\\infty.\n\\]  Then, by the Continuous Mapping Theorem (CMT), the Central Limit Theorem (CLT), and Slutsky’s theorem, we have that \\[\n\\sqrt{n}\\left(\\frac{\\bar{h}_n - \\mathbb{E}\\left(h(X)\\right)}{v_n}\\right)\\to_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] \nThe above result can now be used for the construction of (asymptotically valid) convergence tests and confidence intervals with respect to \\(\\bar{h}_n\\), since for large \\(n\\) \\[\n\\bar{h}_n\\,\\overset{d}{\\approx}\\mathcal{N}\\left(\\mathbb{E}\\left(h(X)\\right),\\frac{\\mathbb{V}\\left(h(X)\\right)}{n}\\right).\n\\]\nSince we can use the computer to generate realizations of the i.i.d. sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim f_X,\\) we can easily approximate the mean \\[\n\\mathbb{E}\\left(h(X)\\right)\\approx \\bar{h}_n\n\\] and the variance \\[\n\\mathbb{V}\\left(h(X)\\right)\\approx v_n^2\n\\] with arbitrary accuracy as \\(n\\to\\infty\\) (justification: strong/weak law of large numbers).\n\nExample 2.1 (A first Monte Carlo Integration) Let’s say we want to compute the integral \\[\n\\int_0^1h(x)dx\n\\] with \\[\nh(x)=\\left(\\cos(50\\,x)+\\sin(20\\,x)\\right)^2\n\\] over \\(x\\in[0,1].\\) Although this integral could also be computed analytically, it is a good first test case. The following code computes the analytic result that \\(\\int_0^1h(x)dx = 0.9652009.\\)\n\n## install.packages(\"mosaicCalc\")\nsuppressPackageStartupMessages(library(\"mosaicCalc\"))\n\n## Symbolic (= analytic) integration \nF <- antiD( (cos(50*x)+sin(20*x))^2 ~ x)\n\nF(1) - F(0)\n\n[1] 0.9652009\n\n\nFigure 2.2 shows the graph of the function \\(h\\).\n\nh_fun <- function(x){\n  result <- (cos(50*x)+sin(20*x))^2\n  return(result)\n}\n\nxx  <- seq(from=0, to=1, len=500)\nplot(x = xx, \n     y = h_fun(xx), \n     type=\"l\", \n     main=\"Function h\", \n     xlab=\"x\", ylab=\"h(x)\")\n\n\n\n\nFigure 2.2: Function \\(h\\) of Example 2.1.\n\n\n\n\nTo approximate the integral \\[\n\\int_0^1 h(x)dx\n\\] using Monte Carlo integration, we can use that \\[\n\\begin{align*}\n\\int_0^1 h(x)dx\n&=\\int_0^11\\cdot h(x)dx \\\\[2ex]\n&=\\int_0^1f_{\\mathcal{U}\\text{[0,1]}}(x)\\cdot h(x)dx \\\\[2ex]\n&= \\mathbb{E}_{f_{\\mathcal{U}\\text{[0,1]}}}(h(X)),\n\\end{align*}\n\\] where \\(f_{\\mathcal{U}\\text{[0,1]}}\\) denotes the density function of the standard uniform distribution \\(\\mathcal{U}\\text{[0,1]}.\\)\nThus, to compute \\(\\int_0^1 h(x)dx\\) we generate a realization \\((u_1,\\dots,u_n)\\) from the random sample \\(U_1,\\dots,U_n\\sim \\mathcal{U}[0,1]\\) and approximate \\[\n\\int_0^1 h(x)dx\\approx \\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(u_i).\n\\]\nIn order to assess how good this approximation is, we need to consider the stochastic properties of the random variable \\[\n\\bar{h}_n = \\frac{1}{n}\\sum_{i=1}^n h(U_i).\n\\] This is done using the above (review of) results on the limit distribution of the sample mean \\(\\bar{h}_n\\) which allows us to construct an approximate \\(95\\%\\) confidence interval, since for large \\(n\\) \\[\n\\begin{align*}\n\\operatorname{CI}^{95\\%}_n\n=&\\left[\\bar{h}_n - z_{1-\\alpha/2}\\sqrt{\\frac{v_n^2}{n}}, \\bar{h}_n + z_{1-\\alpha/2}\\sqrt{\\frac{v_n^2}{n}}\\right]\\\\[2ex]\n\\approx&\n\\left[\\bar{h}_n - z_{1-\\alpha/2} \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}, \\bar{h}_n + z_{1-\\alpha/2} \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}\\right],\n\\end{align*}\n\\] where \\(z_{1-\\alpha/2}\\approx 1.96\\) denotes the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1),\\) \\(v_n^2=n^{-1}\\sum_{i=1}^n(h(u_i)-\\bar{h}_n)^2,\\) and where \\[\nP\\left(\\int_0^1 h(x)dx  \\in \\operatorname{CI}^{95\\%}_n \\right) \\to 0.95,\\quad n \\to\\infty,\n\\] by the CLT.\nFigure 2.3 shows one realization of the stochastic sequence \\[\n\\bar{h}_1,\\dots,\\bar{h}_n\n\\] with \\(n=10000\\), where the realized value of \\(\\bar{h}_n\\) is \\(0.966\\). This compares favorably with the with the exact value of \\(\\int_0^1h(x)dx = 0.9652009.\\)\n\nlibrary(\"scales\")\n# h(x):\nh_fun <- function(x){\n  result <- (cos(50*x)+sin(20*x))^2\n  return(result)\n}\n\n# sample size\nn <- 10000\n\n# Generate sample of uniforms\nset.seed(321)\nu_vec <- runif(n=n)\n\n# Approximation of the integral \nh_bar_n <- cumsum(h_fun(u_vec))/c(1:n)\n\n# True value:\ntrue.value <- 0.9652009\n\n# 95% CI\n# Standard error of the estimator using the \"algebraic \n# formula\" for the variance (german: 'verschiebungssatz')\nst.error_n <-  sqrt((cumsum(h_fun(u_vec)^2)/(1:n) - \n                     cumsum(h_fun(u_vec))^2/(1:n)^2))\n\nCI_u       <-  h_bar_n + 1.96 * st.error_n / sqrt(1:n)\nCI_l       <-  h_bar_n - 1.96 * st.error_n / sqrt(1:n)\n\nplot(x = c(1:n), y = h_bar_n, type=\"n\", \n     ylim=c(0.7,1.2), \n     xlab = \"n\", \n     ylab = \"\")\npolygon(x = c(1:n, rev(1:n)), \n        y = c(CI_u, rev(CI_l)), \n        col    = alpha(\"blue\", 0.5), \n        border = alpha(\"blue\", 0.5))\nlines(x = c(1:n), \n      y = h_bar_n, type=\"l\")\nlines(x = c(1:n), \n      y = rep(true.value, n), type=\"l\", col=\"red\")\n##\nlegend(\"topright\", \n       legend = c(expression(bar(h)[n]), \n                  \"True Value\", \"95% CI\"), \n       lty   = c(1,1,0), pch=c(22,22,22), pt.cex=c(0,0,2),\n       pt.bg = c(\"black\", \"red\", alpha(\"blue\", 0.5)), \n       col   =   c(\"black\", \"red\", alpha(\"blue\", 0.5)))\n\n\n\n\nFigure 2.3: One realization of the stochastic sequence \\(\\bar{h}_1,\\dots,\\bar{h}_n\\) with \\(n=10000\\), where the realized value of \\(\\bar{h}_{n=10000}\\) is \\(0.966\\). The blue band shows the point-wise (i.e. for each given sample size \\(n\\)) confidence intervals \\(\\operatorname{CI}^{95\\%}_n.\\)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe approach of Example 2.1 can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency/accuracy through numerical methods (e.g., Riemann Sum, Trapezoidal Rule, Simpson’s Rule, etc.) in dimensions 1 or 2; see the following code example:\n\nnumericalIntegration <- integrate(f     = h_fun, \n                                  lower = 0, \n                                  upper = 1)\nnumericalIntegration\n\n0.9652009 with absolute error < 1.9e-10\n\n\nHowever, the Monte Carlo integration approach is particularly useful for approximating integrals over higher dimensional sets \\(\\mathcal{X}\\subseteq\\mathbb{R}^d.\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#importance-sampling",
    "href": "Ch2_MonteCarlo.html#importance-sampling",
    "title": "2  Monte Carlo Integration",
    "section": "2.3 Importance Sampling",
    "text": "2.3 Importance Sampling\nAs demonstrated in Example 2.2, the accuracy of the Monte Carlo integration method as a tool for approximating integral values depends on the variance of the estimate that approximates the integral value.\n“Importance sampling” aims to reduce the variance of the Monte Carlo integral estimator. Therefore, importance sampling is also refereed to as a variance reduction technique. This variance reduction is achieved by weighting functions, so-called importance functions.\nAs in the case of classical Monte Carlo integration (see Equation 2.1), the generic problem is the evaluation of the integral \\[\n\\mathbb{E}_{f}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f_X(x)\\,dx.\n\\]\nHowever, it turns out that the classical Monte Carlo integration approach, i.e., a direct sampling from \\(f\\) is often suboptimal.\nObserve that the value of the above integral can be represented by infinitely many alternative choices of the triplet \\[\n(\\mathcal{X}, h, f).\n\\] Therefore, the search for an optimal estimator should encompass all these possible representations.\nLet’s illustrate this with a simple example.\n\nExample 2.3 (Cauchy Tail Probability) This example is from Ripley (2009).\nSuppose that the quantity of interest is the probability, say \\(p\\), that a \\(\\mathcal{Cauchy}(0,1)\\)-distributed random variable is larger than \\(2\\), i.e. \\[\np=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx,\n\\tag{2.4}\\] where \\(1/(\\pi(1+x^2))\\) is the density function of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution.\nThis is a nice example. One the one hand, it allows us to showcase possibilities to improve efficiency of Monte Carlo integration. On the other hand, we know already the result of Equation 2.4; namely, \\(p=0.1476\\) \n\nround(1 - pcauchy(2), 4)\n\n[1] 0.1476\n\n\n1. Approach: The Naive Approach (Classical Monte Carlo Integration)\nThe most direct approach would be to use the following mean expression for the integral of interest: \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=P\\left(X \\geq 2\\right)\\\\[2ex]\n&=P\\left(1_{(X \\geq 2)}=1\\right)\\\\[2ex]\n&= P\\left(1_{(X \\geq 2)}=1\\right)\\cdot 1 + P\\left(1_{(X \\geq 2)}=0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(1_{(X \\geq 2)}\\right),\n\\end{align*}\n\\] where \\(X\\sim \\mathcal{Cauchy}(0,1),\\) \\(f\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution, and \\(h(x)=1_{(x\\geq 2)}\\).\nSo, we can approximate \\(p\\) using the empirical mean \\[\n\\hat{p}_{1}=\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\geq 2)}\n\\] of a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Cauchy}(0,1),\\) where \\[\n1_{(X_1\\geq 2)},\\dots,1_{(X_n\\geq 2)}\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bernoulli}(p).\n\\] The variance of \\(\\hat{p}_1\\) is thus \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_{1})\n&=\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n1_{(X_i\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\\mathbb{V}\\left(1_{(X_1\\geq 2)}\\right)\\quad\\text{(i.i.d.)}\\\\[2ex]\n&=\\frac{p(1-p)}{n},\n\\end{align*}\n\\] which is equal to \\(0.126/n,\\) since we know that \\(p=0.1476\\).\n2. Approach: Accounting for Symmetry\nIn this approach, we use the “adjusting Screws” \\(\\mathcal{X}\\) and \\(h.\\)\nWe can achieve a more efficient estimator (i.e., an estimator with lower variance for a given same sample size \\(n\\)) if we take into account the symmetric nature of \\(\\mathcal{Cauchy}(0,1).\\)\nObviously, due to the symmetry of our target integrand, can do the following rearrangement \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=\\frac{1}{2}\\left(\\int_{-\\infty}^{-2}\\frac{1}{\\pi(1+x^2)}\\,dx + \\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx \\right)\\\\[2ex]\n&=\\frac{1}{2}\\left(\\mathbb{E}\\left(1_{(X \\leq -2)}\\right) + \\mathbb{E}\\left(1_{(X \\geq 2)}\\right)\\right)\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(\\frac{1}{2} \\; 1_{(|X| \\geq 2)}\\right),\n\\end{align*}\n\\] i.e., here \\(f\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution and \\(h(x)=\\frac{1}{2} 1_{(|x|\\geq 2)}\\).\nThis representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean \\[\n\\begin{align*}\n\\hat{p}_{2}\n%&=\\frac{1}{2}\\left(\\frac{1}{n}\\sum_{i=1}^n1_{(X_i \\leq -2)}+ \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i \\geq 2)}\\right)\\\\[2ex]\n&=\\frac{1}{2n}\\sum_{i=1}^n 1_{(|X_i|\\geq 2)}\n\\end{align*}\n\\] again of a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Cauchy}(0,1),\\) where \\[\n1_{(|X_1|\\geq 2)},\\dots,1_{(|X_n|\\geq 2)}\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bernoulli}(2\\,p).\n\\] The variance of this new estimator, \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_{2})\n&=\\frac{1}{4n^2}\\mathbb{V}\\left(\\sum_{i=1}^n1_{(|X_i|\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{1}{4n}\\mathbb{V}\\left(1_{(|X_1|\\geq 2)}\\right)\\\\[2ex]\n&=\\frac{2p(1-2p)}{4n},\n\\end{align*}\n\\] which is equal to \\(0.052/n,\\) since we know that \\(p=0.1476\\). This is clearly lower than in the naive approach, where we had \\(0.126/n.\\)\n3. Approach:\nIn this approach, we use all the “adjusting screws” \\(\\mathcal{X}\\), \\(h\\), and \\(f.\\)\nThe (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, \\([2,+\\infty)\\), which are in some sense irrelevant for the approximation of \\(p\\). This motivates the following reformulation of \\(p\\):\nBy symmetry of \\(f,\\) we have that \\[\n\\begin{align*}\n\\frac{1}{2}\n& =\\int_{0}^\\infty\\frac{1}{\\pi(1+x^2)}dx.\n\\end{align*}\n\\] This can be used to do the following: \\[\n\\begin{align*}\n\\frac{1}{2}\n& =\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx + \\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}dx}_{=p}\\\\[2ex]\n\\Leftrightarrow \\; p& =\\frac{1}{2}-\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx.\n\\end{align*}\n\\] Furthermore, we can rearrange the integral a bit such that \\[\n\\begin{align*}\np\n& =\\frac{1}{2} - \\int_{0}^2\\;\\left(\\frac{1}{2}\\cdot 2\\right)\\;\\frac{1}{\\pi(1+x^2)}\\,dx \\\\[2ex]\n& = \\frac{1}{2} - \\int_{0}^2\\;\\underbrace{\\frac{1}{2}}_{f_{\\mathcal{U}[0,2]}}\\;\\underbrace{\\frac{2}{\\pi(1+x^2)}}_{=h(x)}\\,dx \\\\[2ex]\n& = \\frac{1}{2} - \\mathbb{E}_{f_{\\mathcal{U}[0,2]}}(h(U)),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}[0,2].\\)\nTherefore, a new alternative method for evaluating \\(p\\) is: \\[\n\\hat{p}_{3}=\\frac{1}{2} - \\frac{1}{n}\\sum_{i=1}^n h(U_i),\n\\] where \\[\nU_1,\\dots,U_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{U}[0,2].\n\\] As in the previous approaches, the standard error converges with the classic parametric convergence rate: \\[\n\\begin{align*}\n\\mathbb{V}\\left(\\hat{p}_{3}\\right)\n& =\\frac{1}{n^2}\\mathbb{V}\\left(\\sum_{i=1}^n h(U_i)\\right)\\\\[2ex]\n& =\\frac{1}{n}\\mathbb{V}\\left(h(U_1)\\right)\\\\[2ex]\n\\Rightarrow\n\\operatorname{SE}\\left(\\hat{p}_{3}\\right)&=\\texttt{const}\\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\] Computing the variance \\(\\mathbb{V}\\left(\\hat{p}_{3}\\right)\\) is a bit cumbersome, but using integration by parts and that \\(p=0.1476\\), it can be shown that \\[\n\\mathbb{V}(\\hat p_3)=\\frac{0.0285}{n},\n\\] which is lower than both previous approaches, where we had that \\(\\mathbb{V}(\\hat{p}_{2})=0.052/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.126/n\\).\n\n\n2.3.1 A More General Point of View\nThe idea of importance sampling is related to weighted and stratified sampling ideas, when estimating \\[\n\\theta=\\mathbb{E}_{f}(h(X))=\\int h(x)f(x)dx,\n\\] as already illustrated in Example 2.3.\nSome outcomes of \\(X\\sim f\\) may be more important than others in determining \\(\\theta\\) and we wish to select such values more frequently.\nFor instance, if \\(\\theta\\) denotes the probability of the occurrence of a very rare event, then the only way to estimate \\(\\theta\\) at all accurately may be to produce the rare events more frequently.\nTo achieve this, we can simulate a model which gives a density \\(g\\) to \\(X\\) instead of the correct density \\(f,\\) where both density functions need to be known. This can be easily done, since \\[\n\\begin{align*}\n\\theta\n&=\\mathbb{E}_{f}(h(X))\\\\[2ex]\n&=\\int h(x)\\;f(x)dx\\\\[2ex]\n&=\\int h(x)\\left(\\frac{g(x)}{g(x)}\\right)\\;f(x)dx\\\\[2ex]\n&=\\int \\underbrace{\\left(h(x)\\frac{f(x)}{g(x)}\\right)}_{=:\\psi(x)}\\;g(x)dx\\\\[2ex]\n&=\\int \\psi(x)\\;g(x)dx\\\\[2ex]\n&=\\mathbb{E}_g(\\psi(X)).\n\\end{align*}\n\\]\nThis leads to the following unbiased estimator for \\(\\theta\\) based on sampling from \\(g\\): \\[\n\\begin{align*}\n\\hat{\\theta}_g\n&=\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\underbrace{\\left(\\frac{f(X_i)}{g(X_i)}\\right)}_{=:W_i} h(X_i)\n\\end{align*}\n\\tag{2.5}\\] with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} g.\n\\]\nNote that Equation 2.5 can be seen as a weighted mean of the transformed random variables \\(h(X_i)\\) with weights \\[\nW_i = \\frac{f(X_i)}{g(X_i)}.\n\\] These weights \\(W_i\\) are inversely proportional to the so-called selection factor \\[\n\\frac{g(X_i)}{f(X_i)}.\n\\] \nFor the variance of the estimator \\(\\hat{\\theta}_g\\) we have \\[\n\\begin{align*}\n\\mathbb{V}_g(\\hat{\\theta}_g)\n&=\\mathbb{V}_g\\left(\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i)\\right) \\\\[2ex]\n&=\\frac{1}{n}\\mathbb{V}_g(\\psi(X_i))\\quad\\text{(since i.i.d. random sample)}\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_g\\Big(\\psi(X_i)-\\underbrace{\\mathbb{E}_g(\\psi(X_i))}_{=\\theta}\\Big)^2\\;dx\\\\[2ex]\n&=\\frac{1}{n}\\int\\left(\\psi(x)-\\theta\\right)^2g(x)\\;dx\\\\[2ex]\n&=\\frac{1}{n}\\int\\left(\\frac{h(x)\\,f(x)}{g(x)}-\\theta\\right)^2g(x)\\;dx,\n\\end{align*}\n\\]\nDepending on the choice of the importance function \\(g,\\) the variance of the estimator \\(\\hat{\\theta}_g\\) can be much smaller than the variance of the more naive estimators from the classical Monte Carlo integrations which use unweighted empirical means.\nTheorem 2.1 gives us the best choice for the importance function \\(g.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Minimum Variance Theorem) The importance function \\(g\\) which minimizes the variance \\(\\mathbb{V}_g(\\psi(X_i))\\), and, therefore, minimizes also the variance of the integral estimator in Equation 2.5, \\(\\mathbb{V}_g(\\hat{\\theta}_g)\\), is given by \\[\ng^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\]\n\n\n\nProof of Theorem 2.1:\nNote that \\[\n\\begin{align*}\n\\mathbb{V}_g(\\psi(X_i))\n&=\\quad\\quad \\mathbb{E}_g(\\psi(X_i)^2)\\quad\\quad  - \\quad\\quad \\left(\\mathbb{E}_g(\\psi(X_i))\\right)^2\\\\[2ex]\n&=\\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx -\n  \\left(\\int\\frac{h(x)f(x)}{g(x)}g(x)dx\\right)^2\\\\[2ex]\n  &=\\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx -\n  \\left(\\int h(x)f(x)dx\\right)^2.\n\\end{align*}\n\\tag{2.6}\\] Thus, the second term, \\(\\left(\\mathbb{E}_g(\\psi(X_i))\\right)^2,\\) does not depend on \\(g.\\)\nConsequently, the density \\(g\\) that minimizes the variance \\(\\mathbb{V}_g(\\psi(X_i))\\) is also the density \\(g\\) that minimizes \\(\\mathbb{E}_g(\\psi(X_i)^2).\\) We use this in the following, since minimizing \\(\\mathbb{E}_g(\\psi(X_i)^2)\\) is simpler than minimizing \\(\\mathbb{V}_g(\\psi(X_i)).\\)\nWe begin with a small trick by including an absolute value function as following: \\[\n\\begin{align*}\n\\mathbb{E}_g(\\psi(X_i)^2)\n&=\\mathbb{E}_g({\\color{red}|}\\psi(X_i){\\color{red}|}^2).\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nJensen’s Inequality: Let \\(u\\) be a “convex function” such as, for instance, \\(u(x)=x^2,\\) and let \\(X\\) be some random variable. Then, \\[\n\\mathbb{E}\\left(u(X)\\right) \\geq u\\left(\\mathbb{E}\\left(X\\right)\\right),\n\\] where we assume that the moments exist.\n\n\nNow, using Jensen’s inequality, we can derive the lower bound for \\(\\mathbb{E}_g(\\psi(X_i)^2):\\) \\[\n\\begin{align*}\n\\mathbb{E}_g( \\; (\\psi(X_i) ) ^2 \\; )\n&=\\mathbb{E}_g( \\; |\\psi(X_i)|^2 \\; )\\quad\\text{(bringing in $|.|$)}\\\\[2ex]\n&\\geq \\left(\\mathbb{E}_g(|\\psi(X_i)|)\\right)^2\\quad\\text{(Jensen's inequality)}\\\\[2ex]\n&=\\left(\\int\\left|\\frac{h(x)f(x)}{g(x)}\\right|g(x)dx\\right)^2\\\\[2ex]\n&=\\left(\\int\\frac{\\left|h(x)\\right|f(x)}{g(x)}g(x)dx\\right)^2\\quad\\text{($f$ and $g$ are non-negative)}\\\\[2ex]\n&=\\underbrace{\\left(\\int\\left|h(x)\\right|f(x)dx\\right)^2}_{\\text{Lower bound of $\\mathbb{E}_g(\\psi(X_i)^2)$}} = \\left(\\mathbb{E}_{f}\\left(h(X)\\right)\\right)^2,\n\\end{align*}\n\\] where this lower bound is independent of \\(g.\\)\nIt is now straight forward to show that this lower bound is achieved if \\[\ng(x) = g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\] For this, observe that \\[\n\\begin{align*}\n\\mathbb{E}_g(\\psi(X_i)^2)\n& = \\int\\left(\\frac{h(x)f(x)}{g(x)}\\right)^2g(x)dx\\\\[2ex]\n& = \\int\\frac{h^2(x)f^2(x)}{g(x)}dx.\n\\end{align*}\n\\] Now, setting \\(g(x)=g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}\\) yields \\[\n\\begin{align*}\n\\mathbb{E}_{g^\\ast}(\\psi(X_i)^2)\n& = \\int\\frac{h^2(x)f^2(x)}{|h(x)|f(x)} \\left(\\int |h(z)|f(z)dz\\right) dx\\\\[2ex]\n& = \\int\\frac{\\left|h(x)\\right|^2f(x)}{|h(x)|} \\; \\left(\\int |h(z)|f(z)dz\\right) dx\\\\\n& = \\left(\\int\\left|h(x)\\right|f(x) dx\\right) \\; \\left(\\int |h(z)|f(z)dz\\right) \\\\[2ex]\n& = \\underbrace{\\left(\\int\\left|h(x)\\right|f(x) dx\\right)^2}_{\\text{Lower bound of $\\mathbb{E}_g(\\psi(X_i)^2)$}}\n\\end{align*}\n\\] This shows that the lower bound of \\(\\mathbb{E}_g(\\psi(X_i)^2)\\) is achieved for \\(g(x)=g^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz},\\) which implies the statement of Theorem 2.1.\n\n\n\n\n\n\nNote\n\n\n\nNote that Theorem 2.1 may appear as rather impractical. If, for instance, \\(h(x)\\geq 0\\) then \\(g^\\ast\\) requires us to know \\(\\theta=\\int h(z)f(z)dz,\\) which, however, is just the integral of interest!\nThe minimum variance result of Theorem 2.1 is, nevertheless, useful:\n\nIt tells us that a good choice of \\(g(x)\\) shall mimic the shape of \\(|h(x)|f(x)\\), since the optimal \\(g^\\ast(x)\\propto |h(x)|f(x)\\). (We know the integrand functions \\(h\\) and \\(f\\).)\nFurthermore, \\(g(x)\\) should be chosen such that it has a thicker tail than \\(f(x)\\), since the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\) crucially depends on the quotient \\(f(x)/g(x)\\) (see Equation 2.6) which would “explode” for \\(g(x)\\approx 0\\).\n\n\n\nLet’s apply our new insights to the above Example 2.3 on the Cauchy tail probability \\(p\\).\n\nExample 2.4 (Cauchy Tail Probability (continued)) Above, in Example 2.3, we had \\[\n\\begin{align*}\np\n&=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx\\\\[2ex]\n&=P\\left(X \\geq 2\\right)\\\\[2ex]\n&=P\\left(1_{(X \\geq 2)}=1\\right)\\\\[2ex]\n&= P\\left(1_{(X \\geq 2)}=1\\right)\\cdot 1 + P\\left(1_{(X \\geq 2)}=0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}_{f}\\left(1_{(X \\geq 2)}\\right),\n\\end{align*}\n\\] where \\(f(x)=1/(\\pi(1+x^2))\\) is the density of the \\(\\mathcal{Cauchy}(0,1)\\)-distribution and \\(h(x)=1_{(x\\geq 2)}\\).\nI.e., here \\(h\\) is non-negative, such that \\(|h(x)|=h(x)\\) for all \\(x.\\)\n\nTherefore, \\[\n\\begin{align*}\np\n& =\\mathbb{E}_f(h(X)) \\\\[2ex]\n& =\\int h(x)f(x)dx    \\\\[2ex]\n%& =\\int_{2}^{\\infty}f(x)dx \\\\[2ex]\n%& =\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\int\\underbrace{\\left(h(x) \\frac{f(x)}{g(x)}\\right)}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\mathbb{E}_g(\\psi(X)),\n\\end{align*}\n\\] with \\(h(x)=1_{(x\\geq 2)}\\) and \\(X\\sim g,\\) where \\(g\\) needs to be chosen by the statistician.\nA possibly good (and simple) choice for the importance function \\(g\\) is, for instance, \\[\ng(x)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{2}{x^2}& x\\geq 2\\\\\n  0&\\text{otherwise},\\\\\n  \\end{array}\\right.\n\\]\nThis choice takes into account our insights from Theorem 2.1, since\n\n\\(g\\) “closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f=1/(\\pi(1+x^2))\\).\n\n\n\n\n\n\nFigure 2.4: Density function \\(f(x)=1/(\\pi(1+x^2)),\\) and importance function \\(g(x)=2/x^2\\) for \\(x\\geq 2.\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function \\(g\\) is indeed a density function, since it is non-negative and integrates to one; i.e., \\(\\int_2^\\infty g(x)dx = 1.\\) (Double-check this.)\n\n\nCaution: It is not straight forward to directly sample from the importance function \\(g(x)=2/x^2,\\) but we can apply some further steps:\nThe choice of \\(g\\) leads to \\[\n\\begin{align*}\np\n& =\\mathbb{E}_g(\\psi(X)) \\\\[2ex]\n& =\\int \\left(h(x) \\frac{f(x)}{g(x)}\\right) \\;g(x)dx \\\\[2ex]\n& =\\int_{2}^{+\\infty} \\left(\\frac{f(x)}{g(x)}\\right) \\;g(x)dx\\quad\\text{(using $h(x)=1_{(x\\geq 2)}$)} \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{\\frac{1}{\\pi(1+x^2)}}{\\frac{2}{x^2}}\\right)\\,\\frac{2}{x^2}\\,dx \\quad\\text{(using $g(x)=2/x^2$)} \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^2) \\cdot \\frac{1}{x^2}} \\right)\\,\\frac{1}{x^2}\\,dx  \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(\\frac{1}{x^2}+ 1) } \\right)\\,\\frac{1}{x^2}\\,dx  \\\\[2ex]\n& =\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\end{align*}\n\\]\nNow, integration by substitution\n\nsubstituting \\(u(x)=x^{-1}\\)\n\\(\\frac{du}{dx}=- x^{-2}\\)\n\nyields that \\[\n\\begin{align*}\np\n& =\\lim_{b\\to\\infty}\\int_{u(2)}^{u(b)} - \\frac{1}{\\pi(1+u^2)}du \\\\[2ex]\n& = - \\int_{1/2}^{0} \\frac{1}{\\pi(1+u^2)}du \\\\[2ex]\n& = \\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\end{align*}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nWe just showed that \\[\np =\\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx}_{\\text{Our original problem}}\n  =\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] That is, we shifted the integration problem from the tail to the “center” of the distribution. 🤓\n\n\nRearranging the integral a bit, allows us to write \\[\n\\begin{align*}\np\n&=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathcal{U}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du\\\\[2ex]\n&=\\mathbb{E}_{f_{\\mathcal{U}[0,1/2]}}(h(U)),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}[0,1/2].\\)\nThus, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\frac{1}{n}\\sum_{i=1}^n h(U_i),\n\\] where \\[\nU_1,\\dots,U_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{U}[0,1/2]\n\\] and \\[\nh(u)=\\frac{1}{2\\pi(1+u^2)}\n\\]\nThe variance and standard error of \\(\\hat{p}_4\\) are \\[\n\\begin{align*}\n\\mathbb{V}(\\hat{p}_4)\n& = \\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n h(U_i)\\right)\\\\[2ex]\n& = \\frac{1}{n}\\mathbb{V}\\left(h(U)\\right)\\\\[2ex]\n& = \\frac{\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2}{n}\\\\[2ex]\n\\Rightarrow\n\\operatorname{SE}(\\hat{p}_4)\n& = \\texttt{const}\\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\]\n\n\n\nAn integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.0955\\cdot 10^{-3}/n\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/n\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/n\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\)."
  },
  {
    "objectID": "Ch3_Bootstrap.html",
    "href": "Ch3_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch3_Bootstrap.html#the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.1 The Empirical Distribution Function",
    "text": "3.1 The Empirical Distribution Function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\) from \\(X\\sim F\\)\nLet \\(1_{(\\cdot)}\\) denote the indicator function, i.e., \\(1_{(x\\leq t)}=1\\) if \\(x\\leq t\\), and \\(1_{(x\\leq t)}=0\\) if \\(x>t.\\)\n\nDefinition 3.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] I.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0,\\) if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic.\n\\(F_n(x)=1,\\) if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) is a monotonically increasing step function\n\n\n\n\n\n\n\nNote\n\n\n\nStructurally, \\(F_n\\) itself is a distribution function. \\(F_n\\) is the distribution function of a discrete random variable \\(X^*\\)\n\nwith possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and\nwith \\(P(X^*=X_i)=\\frac{1}{n}\\) for each \\(i=1,\\dots,n.\\)\n\n\n\n\nExample 3.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.30, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the i.i.d. random sample \\(X_1,\\dots,X_n\\) and thus is itself a random function.\nWe obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) (“number of trials”) and \\(p=F(x)\\) (“probability of success on a single trial”).\n\\(\\mathbb{E}(F_n(x))=F(x)\\)\n\\(\\mathbb{V}(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea",
    "href": "Ch3_Bootstrap.html#basic-idea",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic idea",
    "text": "3.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 3.1)."
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch3_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The Nonparametric (Standard) Bootstrap",
    "text": "3.3 The Nonparametric (Standard) Bootstrap\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\[\n\\hat\\theta-\\theta\n\\] in order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis.\n\nThe Bootstrap Algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\n\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well.\n\n\n\n\n\n\nTip\n\n\n\nThe bootstrap distribution of \\[\n\\hat\\theta^*-\\hat\\theta\n\\] is used to approximate the unknown distribution of \\[\n\\hat\\theta-\\theta.\n\\]\nNote: For the bootstrap distribution \\(\\hat\\theta\\) is a “population parameter”.\n\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\nBootstrap Consistency\n\n\n\n\n\n\nCaution: The bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 3.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference About the Population Mean\nSetup:\n\ni.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\sim F.\\)\nContinuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\[\n\\mu = \\int x f(x) dx = \\int x d F(x),\\] where \\(f=F'.\\)\nEstimator: empirical mean \\[\n\\begin{align*}\n\\bar{X}\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the distribution of \\[\n\\bar{X} -\\mu?\n\\]\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0,  1.4, -0.8, \n                     1.6, 1.9, -0.1,  0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical “population” in order to generate the bootstrap sample \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\[\n\\bar X -\\mu\n\\] is approximated by the conditional distribution of \\[\n\\bar X^* -\\bar X,\n\\] given the original sample \\({\\cal S}_n,\\) i.e. more formally \\[\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\approx\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}.\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data \\(X_1^*,\\dots,X_n^*\\) from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate the bootstrap distribution \\[\n%\\overbrace{\n  \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}}%}^{=P^*\\left(\\bar X^*-\\bar X\\leq \\delta\\right)}\n\\approx\n\\frac{1}{m}\\sum_{k=1}^m 1_{( \\bar X^*_k-\\bar X\\leq \\delta)},  \n\\] where this approximation will be arbitrarily precise as \\(m\\to\\infty.\\) (So, we can effectively ignore this type of approximation error.)\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( Xbar_boot - Xbar ), \n     main=\"Approximate Bootstrap Distribution\")\n\n\n\n\nTo approximate, for instance, the standard error of \\(\\bar{X},\\) we can now simply use the empirical standard deviation of \\(\\bar{X}^*_k,\\) \\(k=1,\\dots,m.\\)\n\nround(sd(Xbar_boot), 2)\n\n[1] 0.34\n\n\n\n\nTheory: The Bootstrap Distribution of \\(\\bar X^*- \\bar{X}\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(\\mathbb{V}^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;\\mathbb{V}^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\n\\mathbb{V}^*(\\cdot)=\\mathbb{V}(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n.\n\\] These conditional distributions are usually called bootstrap distributions.\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know 🤟 the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\n\\mathbb{V}^*(X_i^*)\n&=\\mathbb{V}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\[2ex]\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\nThat is, in the bootstrap sample \\(X_1^*,\\dots,X_n^*\\) the “population” mean and the “population” variance are equal to the empirical mean, \\(\\bar{X},\\) and the empirical variance, \\(\\hat{\\sigma}^2,\\) of the original sample \\(X_1,\\dots,X_n.\\)\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq \\delta)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F\\) of \\(X_i.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= P(1_{(X_i^*\\leq \\delta)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq \\delta)}=1) \\cdot 1 + P(1_{(X_i^*\\leq \\delta)}=0) \\cdot 0\\\\[2ex]\n&= E\\left(1_{\\left(X_i^*\\leq \\delta\\right)}\\right)\\\\[2ex]\n&= E\\left[E\\left(1_{\\left(X_i^*\\leq \\delta\\right)}|\\mathcal{S}_n\\right)\\right]\\\\[2ex]\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq \\delta\\right)}\\right]\\\\[2ex]\n&= \\frac{n}{n}E\\left[1_{\\left(X_i\\leq \\delta\\right)}\\right]\\\\[2ex]\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\]\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\n\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*.\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(\\bar X^*)\n&=\\mathbb{E}(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\n\\mathbb{V}^*(\\bar X^*)\n&=\\mathbb{V}(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\mathbb{V}(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2,\n\\end{align*}\n\\] where \\(\\hat{\\sigma}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(X_i - \\bar{X}\\right)^2}.\\)\n\n\nAn appropriate central limit theorem argument implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\begin{align*}\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\left(\\left.\\sqrt{n}(\\bar X^* -\\bar X)\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,\\sigma^2),\\quad n\\to\\infty.\n\\end{align*}\n\\]\nOn the other hand, by the CLT, we also have that \\[\n\\begin{align*}\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\left(\\left.\\sqrt{n}(\\bar X - \\mu)\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,\\sigma^2),\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThis means that the bootstrap is consistent, since the bootstrap distribution of \\[\n\\sqrt{n}(\\bar X^* -\\bar X)|{\\cal S}_n\n\\] asymptotically \\((n\\rightarrow\\infty)\\) coincides with the distribution of \\[\n\\sqrt{n}(\\bar X-\\mu).\n\\] In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n) \\approx\n\\frac{1}{m}\\sum_{k=1}^m 1_{( \\bar X^*_k-\\bar X\\leq \\delta)},  \n\\] which we can approximate (arbitrary precise as \\(m\\to\\infty\\)) using the bootstrap realizations \\[\n\\bar{X}^*_1,\\;\\bar{X}^*_2, \\dots, \\bar{X}^*_m.\n\\]\n\n\n\n3.3.2 Example: Inference about a Population Proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\[\nS=\\sum_{i=1}^n 1_{(X_i = 1)}\n\\] denote the number of \\(X_i\\) which are equal to \\(1.\\) Then, the maximum likelihood estimate of \\(p\\) is \\[\n\\hat p=\\frac{1}{n}S.\n\\]\nInference Problem: What is the distribution of \\[\n(\\hat{p} - p)?\n\\]\n\n\n\n\n\n\n\nRecall Asymptotics:\n\n\n\n\n\\(n\\hat p=S\\sim \\mathcal{Binom}(n,p)\\)\nAs \\(n\\rightarrow\\infty,\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\] Thus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\n\n\nBootstrap Approach:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\[\nS^*=\\sum_{i=1}^n 1_{(X_i^* = 1)}\n\\]\ndenote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\[\n\\hat p^*=\\frac{1}{n}S^*\n\\]\n\n\nThe bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\((\\hat p^*-\\hat p)|\\mathcal{S}_n\\) given the observed sample \\({\\cal S}_n,\\) where the latter can be approximated arbitrarily well \\((m\\to\\infty)\\) using the bootstrap estimators \\[\np^*_1,p^*_2,\\dots,p^*_m;\n\\] namely by \\[\nP\\left(\\hat{p}^* - \\hat{p} \\leq \\delta|\\mathcal{S}_n\\right)\\approx \\frac{1}{m}\\sum_{k=1}^m 1_{(\\hat{p}^*_k - \\hat{p} \\leq\\delta )}.\n\\]\nThe bootstrap is called consistent if asymptotically \\((n\\rightarrow \\infty)\\) the conditional distribution of \\((\\hat p^*-\\hat p)|{\\cal S}_n\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nThe distribution of \\(X_i^*|\\mathcal{S}_n\\)\nThe conditional random variable \\(X_i^*|\\mathcal{S}_n\\) is a binary random variable \\[\nX_i^*|\\mathcal{S}_n\\in\\{0,1\\}.\n\\] Since \\(X_i^*\\) is drawn independently and with replacement from \\(\\mathcal{S}_n,\\) we obtain for each \\(i=1,\\dots,n,\\) \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|{\\cal S}_n)=\\hat p, \\\\[2ex]  \n& P^*(X_i^*=0)=P(X_i^*=0|{\\cal S}_n)=1-\\hat p.\n\\end{align*}\n\\] Thus, \\(X_i^*|{\\cal S}_n\\) is a Bernoulli distributed random variable with parameter \\(p=\\hat{p}\\) \\[\nX_i^*|{\\cal S}_n \\sim\\mathcal{Bern}(p=\\hat p), \\quad i=1,\\dots,n.\\\\[5ex]\n\\]\nThe distribution of \\(\\hat{p}^*|\\mathcal{S}_n\\)\nThe above implies that \\(n \\hat{p}^*|{\\cal S}_n\\) has a Binomial distribution with parameters \\(n\\) and \\(p=\\hat{p},\\)\n\\[\n\\underbrace{n \\hat{p}_i^*}_{=S^*}|{\\cal S}_n \\sim\\mathcal{Binom}(n, p=\\hat p), \\quad i=1,\\dots,n.\n\\]\nTherefore, \\[\n\\begin{align*}\n\\mathbb{E}^*(n \\hat p^*)\n&=\\mathbb{E}(n \\hat p^*|\\ {\\cal S}_n)\\\\[2ex]\n& = n \\hat{p}\\\\[2ex]\n\\Rightarrow \\mathbb{E}^*(\\hat p^*) & = \\hat{p}\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n\\mathbb{V}^*(n \\hat p^*)\n&=\\mathbb{V}(n \\hat p^*|\\ {\\cal S}_n)\\\\[2ex]\n& = n \\hat{p} (1- \\hat{p})\\\\[2ex]\n\\Rightarrow \\mathbb{V}^*(\\hat p^*) & = \\frac{\\hat{p}(1-\\hat{p})}{n}\n\\end{align*}\n\\]\nAn appropriate central limit theorem argument implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p,\\) and thus \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] Therefore, \\(\\hat p(1-\\hat p)\\) can be replaced asymptotically by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] So, we can conclude that, \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0\n\\] as \\(n\\rightarrow\\infty,\\) where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] and therefore also \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p).%\\approx N(0,p(1-p)/n)\n\\]\n\n\n3.3.3 Confidence Intervals\n\n\n\n\n\n\nRecall: The traditional (non-bootstrap) approach\n\n\n\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\n\n\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\nThe Bootstrap Approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g. \\(\\lfloor 4.9\\rfloor = 4\\)).\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha,\n\\end{align*}\n\\] where the approximation becomes arbitrarily precise for \\(m\\to\\infty.\\) Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{3.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence Intervals for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu\\) and variance \\(\\sigma^2.\\) \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\n\n\n\n\nRecall: The traditional (non-bootstrap) approach\n\n\n\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/s)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{s}{\\sqrt{n}}\\right],\n\\] where \\(t_{n-1,1-\\frac{\\alpha}{2}}\\) denotes the \\(1-\\frac{\\alpha}{2}\\)-quantile of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\(\\bar X\\) is exactly normal distributed, also for small \\(n,\\) which requires that the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed.\nIf the underlying distribution is not normal, then this normal distribution holds approximately if the sample size \\(n\\) is sufficiently large (central limit theorem), i.e., \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\n\n\nThe Nonparametric (Standard) Bootstrap Approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10,000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch3_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.4 Pivot Statistics and the Bootstrap-\\(t\\) Method",
    "text": "3.4 Pivot Statistics and the Bootstrap-\\(t\\) Method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of the “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is Asymptotically Pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu\\), variance \\(\\mathbb{V}(X)=\\sigma^2>0\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{s}\\sim t_{n-1}\n\\] with \\(s^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{s}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\(v_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.4.1 Bootstrap-t Confidence Interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter (vector) \\(\\theta.\\) Assume that the bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard error, \\(v,\\) of \\(\\hat{\\theta}\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\nv^*\\equiv v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT^*&=T^*(X_1^*,\\dots,X_n^*)\\\\\n&=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g. \\(m=100,000\\)) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*},\\) conditionally on \\(\\mathcal{S}_n,\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\[\n\\left.\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\right|\\mathcal{S}_n\n\\] using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*\\) (see Equation 3.1).\n\nThis implies, for large \\(m,\\) \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}.\n\\] Therefore, for lage \\(n,\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\\;\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{3.3}\\]\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2}\\) to generate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\frac{\\bar X^*_1-\\bar X}{s^*_1},\\dots,\\frac{\\bar X^*_m-\\bar X}{s^*_m}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) from \\(\\frac{\\bar X^*_1-\\bar X}{s^*_1},\\dots,\\frac{\\bar X^*_m-\\bar X}{s^*_m}\\) using Equation 3.1.\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 3.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}s,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}s\\right],\n\\] where \\(s\\) is computed from the original sample, i.e., \\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}.\n\\]\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}{v^*_n}\\right|\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(v^*_n\\) depends on the bootstrap sample — not the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})\\right|\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.5 Regression Analysis: Bootstrapping Pairs",
    "text": "3.5 Regression Analysis: Bootstrapping Pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Random and fixed design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(\\mathbb{E}(\\varepsilon_i)=0\\) and homoscedastic errors \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\n\n\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.5.1 Bootstrapping Pairs: Bootstrap under Random Design\nUnder the random design, we additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)=\\mathbb{E}(\\sigma^2(X_i)X_iX_i^T)\n\\]\n\n\n\n\n\n\nTip\n\n\n\nFor homoscedastic errors we have \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2(X_i)X_iX_i^T)\\\\\n&=\\sigma^2\\mathbb{E}(X_iX_i^T)\\, =\\sigma^2 M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.4).\nUnder a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of the estimation errors \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\nAlgorithm:\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\n\nConfidence Intervals\nThis allows to construct basic bootstrap confidence intervals for the \\(j\\)th regression coefficient \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nGenerate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j1}^*,\\dots,\\hat\\beta_{jm}^*\n\\]\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nfrom the bootstrap realizations \\(\\hat{\\beta}_{j1}^*,\\dots,\\hat\\beta_{jm}^*\\) using Equation 3.1.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Residual bootstrap",
    "text": "3.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under fixed design (Definition 3.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(\\mathbb{E}(\\varepsilon_i)=0\\) and homoscedastic errors \\[\n\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\mathbb{E}(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe Residual Bootstrap Algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g. \\(m=100,000\\)) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\n\nMotivating the Residual Bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\n\\mathbb{E}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\n\\mathbb{V}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2.\n\\] An appropriate central limit theorem argument implies that \\[\n\\left.\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\right|\\mathcal{S}_n\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_p \\sigma^2\n\\] as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\(\\hat\\beta_j^*\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles (see Equation 3.1) based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_{j1}^*,\\hat\\beta_{j2}^*, \\dots, \\hat\\beta_{jm}^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), i.e., \\[\n\\gamma_{jj}:=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat{\\sigma}=\\sqrt{\\frac{1}{n-p}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\n\\] is an asymptotically pivotal statistic, \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles (see Equation 3.1) based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\\;\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right],\n\\] where \\(\\hat{\\sigma}=\\sqrt{\\frac{1}{n-p}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nThere are many more bootstrap procedures. In case of heteroscedastic errors, for instance, there’s also the “Wild Bootstrap.”\nFor high-dimensional problems (\\(p\\) as large as \\(n\\) or larger), one can use (under certain regularity assumptions) the “Multiplier Bootstrap”."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html",
    "href": "Ch4_MaximumLikelihood.html",
    "title": "4  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g. the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed.\nIn (classical) maximum likelihood estimation we must be rather specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: This means that no consistent estimator has lower asymptotic mean squared error than the maximum likelihood estimator.\n\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{Bern}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To address this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if $i$th Coinflip}=T\\\\[2ex]\n    1 & \\text{if $i$th Coinflip}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bern}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{Bern}(\\theta)\\) denotes the Bernoulli distribution with unknown probability of success parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\nh=\\sum_{i=1}^n1_{(x_i=1)}\n\\] many heads and \\(n-h\\) many tails, where\n\\[\n0\\leq h\\leq n\\quad\\text{and}\\quad 0\\leq n-h\\leq n.\n\\]\n\n\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\[2ex]\n&= \\theta^h(1-\\theta)^{n-h}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\[\nx_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if $i$th Coinflip}=T\\\\[2ex]\n    1 & \\text{if $i$th Coinflip}=H\n    \\end{matrix}\n    \\right.\n\\] is the observed realization of \\(X_i.\\)\nThe function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\nWe estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\[2ex]\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\[2ex]\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} \\\\[2ex]\n\\Rightarrow\\quad \\ell(\\theta)&=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\end{align*}\n\\]\nThe coin flip example is actually so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\ell'(\\theta)&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta}\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our maximum likelihood estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\ell'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nUsually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch4_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Optimization: Non-Analytical Solutions",
    "text": "4.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\n\n\n\n\n\n\nNote\n\n\n\n\nBy strange convention Newton usually shares credit for this algorithm when it is applied to root-finding, but not when it is used for optimization. However, root-finding can be used for finding the root of the first-derivative function and thus can be used for optimization.\nNote that minimization and maximization are essentially the same problems since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally, i.e. for \\(h\\approx 0,\\) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure 4.1.\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell\\) be a log-likelihood function with continuous first, \\(\\ell',\\) and second, \\(\\ell''',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell,\\) we try to find the root of \\(\\ell',\\) i.e. the value of \\(\\theta\\) such that \\[\n\\ell'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order conidtion of the optimaization problem. We do so using a step-wise (\\(h\\) steps) optimization approach.\nInitialization: Let \\(\\theta_0\\) be our first guess of the root \\(\\theta.\\)\nTypically, \\(\\theta_0\\neq\\theta\\) and thus \\(\\ell'(\\theta_0)\\neq 0.\\) Therefore, we want to move from \\(\\theta_0\\) to a new root-candidate \\(\\theta_1\\) by doing an \\(h\\)-step update \\[\n\\theta_1 = \\theta_0 + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell'\\) around our first guess \\(\\theta_0\\) gives \\[\n\\begin{align*}\n\\ell'(\\theta_0 + h) & \\approx \\ell'(\\theta_0) + \\ell''(\\theta_0)h\n\\end{align*}\n\\] Thus, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell'(\\theta_0) + \\ell''(\\theta_0)h = 0\\\\[2ex]\n\\Rightarrow h = \\frac{\\ell'(\\theta_0)}{\\ell''(\\theta_0)}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\theta_1 = \\theta_0 - \\frac{\\ell'(\\theta_0)}{\\ell''(\\theta_0)}.\n\\] Likewise, the \\(n\\)th root-candidate is \\[\n\\theta_n = \\theta_{n-1} + \\frac{\\ell'(\\theta_{n-1})}{\\ell''(\\theta_{n-1})};\n\\] see also Figure 4.2.\n\n\n\n\n\nFigure 4.2: A step in the Newton-Raphson root-finding method.\n\n\n\n\nOne can shown that if \\(\\ell'\\) is “well behaved” at its root \\(\\theta\\) (i.e. if \\(\\ell''(\\theta)\\neq 0\\) and if \\(\\ell'''(\\theta)\\) is finite and continuous at \\(\\theta\\)) and you start with \\(\\theta_0\\) “close enough” to the root \\(\\theta,\\) then \\(\\theta_n\\) will fastly converge to \\(\\theta.\\) Unfortunately, we don’t know if \\(\\ell'\\) is well behaved at \\(\\theta\\) until we know \\(\\theta,\\) and we don’t know beforehand how close is “close enough”. So, we cannot guarantee convergence of the Newton-Raphson algorithm.\nHowever, if \\(\\theta_n\\to\\theta\\) then, since \\(\\ell'\\) and \\(\\ell''\\) are continuous, we have \\[\n\\begin{align*}\n\\theta = \\lim_{n\\to\\infty}\\theta_{n+1}\n&=\\lim_{n\\to\\infty}\\left(\\theta_{n} + \\frac{\\ell'(\\theta_{n})}{\\ell''(\\theta_{n})}\\right)\\\\[2ex]\n&=\\lim_{n\\to\\infty}\\theta_{n} + \\frac{\\ell'(\\lim_{n\\to\\infty}\\theta_{n})}{\\ell''(\\lim_{n\\to\\infty}\\theta_{n})}\\\\[2ex]\n&=\\theta + \\frac{\\ell'(\\theta)}{\\ell''(\\theta)}.\n\\end{align*}\n\\]\nTherefore, provided that \\(\\ell''(\\theta)\\neq \\pm\\infty,\\) we must have that \\(\\ell'(\\theta) = 0.\\)\nSince we are expecting that \\(\\ell'(\\theta_n)\\to 0,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell'(\\theta_n)|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon>0.\\)\n\n\n\n\n\n\nPseudo-Code\n\n\n\nThe Newton-Raphson algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} n=0                &  \\\\\n\\texttt{\\textbf{while }}  | \\ell'(\\theta_i) | >\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} n = n+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_n = \\theta_{n-1} - \\frac{\\ell'(\\theta_{i-1})}{\\ell''(\\theta_{n-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_n & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\n\n\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\n\n\nNewton-Raphson Algorithm: Coin-Flipping Example\nLet’s return to our earlier coin-flipping example.\nIf we observe, for instance, only one head \\(h=1\\) for a sample size of \\(n=5,\\) we already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2,\\) but let’s apply the Newton-Raphson Algorithm.\nRecall that \\[\n\\begin{align*}\n\\ell'(\\theta)&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\[2ex]\n\\ell''(\\theta) &= -\\dfrac{h}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{h}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\]\nLet’s consider a sample size of \\(n=5,\\) where one coin-flip resulted in H, i.e. \\(h=1,\\) and four coin-flips resulted in T, i.e. \\(n-h=4.\\) Setting the tolerance level \\(\\varepsilon=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table Table 4.1. The numeric optimization solution is \\(\\hat\\theta_{ML} = 0.2\\) which equals the analytic solution.\n\n\nTable 4.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\n\\(n\\)\n\\(\\hat\\theta_n\\)\n\\(\\ell'(\\hat\\theta_n)\\)\n\\(\\ell'(\\hat\\theta_n)/\\ell''(\\hat\\theta_n)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch4_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "4  Maximum Likelihood",
    "section": "4.3 OLS-Estimation as ML-Estimation",
    "text": "4.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{4.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 3.4).\nFor the following, it is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply classical ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classical ML estimation theory requires us to assumed the correct error distribution. Luckily, the quasi maximum likelihood theory of White (1982) shows that false distributional assumptions are typically not problematic.\n\n\n\n\n\n\nLet \\[\n\\varphi_{\\mu,\\sigma^2}(x) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\] denote the univariate density function of the normal distribution with mean \\(\\mu\\in\\mathbb{R}\\) and variance \\(\\sigma^2\\in\\mathbb{R}.\\)\nLikelihood function: \\[\n\\begin{align*}\n\\mathcal{L}_n(\\beta,\\sigma^2)\n& =\\prod_{i=1}^n \\varphi_{\\mu=0,\\sigma^2}(\\varepsilon_i)\\\\\n& =\\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{\\varepsilon_i^2}{2\\sigma^2}\\right)\\\\[2ex]\n& =\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)\\\\[2ex]\n& =(2\\pi)^{-n/2} \\cdot (\\sigma^2)^{-n/2}\\cdot  \\exp\\left(-\\frac{(Y-X\\beta)'(Y-X\\beta)}{2\\sigma^2}\\right)\\\\[2ex]\n\\end{align*}\n\\]   Log-likelihood function: \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K+1\\) unknown parameters:\n\n\\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\in\\mathbb{R}^K\\) and\n\\(\\sigma^2\\in\\mathbb{R}_{>0}.\\)\n\nTaking derivatives gives \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell}{\\partial \\beta}(\\beta,\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell}{\\partial \\sigma^2}(\\beta,\\sigma^2)}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}}\\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n\nScore Function\n\n\n\nMore generally, let \\(\\ell(\\theta)\\) denote the log-likelihood function of a \\(p\\)-dimensional parameter vector \\(\\theta=(\\theta_1,\\dots,\\theta_p).\\)\nThen the gradient vector \\[\\left(\n  \\dfrac{\\partial \\ell}{\\partial \\theta_1}(\\theta), \\dots,\n  \\dfrac{\\partial \\ell}{\\partial \\theta_p}(\\theta)\n  \\right),\n\\] is called the score-function. The score function is random, since it depends on the random sample. At the true parameter vector \\(\\theta,\\) the score function satisfies \\[\n\\mathbb{E}\\left(\\dfrac{\\partial \\ell}{\\partial \\theta_j}(\\theta)\\right)=0\n\\] for all \\(j=1,\\dots,p.\\) We show this below in Section 4.4.\n\n\nSo, we have a system of \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives: \\[\n\\begin{align*}\n& - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\hat\\beta_{ML})  \\overset{!}{=}0\\\\[2ex]\n\\Rightarrow\\quad & \\hat\\beta_{ML}=(X'X)^{-1}X'Y\\\\[2ex]\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}  \\overset{!}{=}0\\ \\\\[2ex]\n\\Rightarrow\\quad  &\ns_{ML}^2 =\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\\\[2ex]\n& =\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2.\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\n\n4.3.1 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nTo derive the variance of the ML-estimators, we need the second derivatives of the log-likelihood function \\(\\ell\\) as well as their expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\underset{(K\\times K)}{\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}(\\beta,\\sigma^2)}\n&= - \\dfrac{1}{\\sigma^2}(X'X)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}(\\beta,\\sigma^2)\\right)\\\\[2ex]\n&=   \\dfrac{1}{\\sigma^2} \\mathbb{E}(X'X)\\\\[2ex]\n&=  \\dfrac{n}{\\sigma^2} \\Sigma_{X'X},\n\\end{align*}\n\\] since\n\\[\n\\mathbb{E}\\left(X'X\\right)\n=\\mathbb{E}\\left(\\sum_{i=1}^nX_iX_i'\\right)\n=n\\underbrace{\\mathbb{E}\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}} = n\\Sigma_{X'X}.\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(1\\times 1)}{\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta,\\sigma^2)}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{3}} \\\\[2ex]\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\[2ex]\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta,\\sigma^2)\\right)\\\\[2ex]\n&=\\left(-\\frac{n}{2\\sigma^{4}}+\\frac{\\mathbb{E}\\left(\\sum_{i=1}^n\\varepsilon_i^2\\right)}{\\sigma^{6}} \\right)\\\\[2ex]\n&=\\left(-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\\right)\\\\[2ex]\n&=\\frac{n}{2\\sigma^{4}}\\\\[2ex]\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}(\\beta,\\sigma^2)}\n=   \\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\beta}(\\beta,\\sigma^2)\\right)^T\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\[2ex]\n& = \\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&(-1)\\cdot  \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\sigma^2}(\\beta,\\sigma^2)\\right)\\\\[2ex]\n&\\left((-1)\\cdot  \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\beta}(\\beta,\\sigma^2)\\right)\\right)^T\\\\[2ex]\n&=\\frac{\\mathbb{E}(X'\\varepsilon)}{\\sigma^4}\\\\[2ex]\n&=\\frac{\\mathbb{E}(\\mathbb{E}(X'\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=\\frac{\\mathbb{E}(X'\\mathbb{E}(\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=0,\n\\end{align*}\n\\] since \\(\\mathbb{E}(\\varepsilon|X)=0\\) is an \\((n\\times 1)\\) zero vector.\n\n\nFisher Information Matrix\nThe variance of an MLE is given by the inverse of the Fisher information matrix.\nThe Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function.\nFor the case of the multiple linear regression model (Equation 4.1), the Fisher information matrix is \\[\n\\begin{align*}\n&\\mathcal{I}\\left(\\beta, \\sigma^2\\right)\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}(\\beta,\\sigma^2)\\right) &\n(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\beta}(\\beta,\\sigma^2)\\right)\\\\\n(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\beta}(\\beta,\\sigma^2)\\right) &\n(-1)\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta,\\sigma^2) \\right)\n\\end{array}\\right]\\\\[2ex]\n% &=\n% \\left[\\begin{array}{cc}\n% \\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n% 0 & \\frac{n}{2\\sigma^4}\n% \\end{array}\\right]\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\dfrac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\[2ex]\n0 & \\ \\dfrac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\]\n\nTaking the inverse of the Fisher information matrix \\(\\mathcal{I}\\left(\\beta, \\sigma^2\\right)\\) gives the variance-covariance matrix of the vector of estimators \\((\\hat\\beta_{ML}, s_{ML}^2)\\) \\[\n\\begin{align*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n& \\approx \\left(\\mathcal{I}\\left(\\beta, \\sigma^2\\right)\\right)^{-1}\\\\[2ex]\n& =\n\\left[\\begin{array}{cc}\n\\dfrac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\dfrac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{align*}\n\\tag{4.2}\\] where the approximation error becomes small as \\(n\\to\\infty.\\) That is, \\[\nn Var\\left(\\hat\\beta_{ML}\\right) \\to \\sigma^2\\Sigma_{X'X}^{-1}\\quad\\text{as}\\quad n\\to\\infty,\n\\] and \\[\nn Var\\left(s_{ML}^2\\right) \\to 2\\sigma^4\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nGiven this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\).\nOf course, the variance expressions in Equation 4.2 contain unknown quantities and thus are not directly usable in practice. However, we can plug in estimates of the unknown quantities: \\[\n\\begin{align}\ns_{ML}^2                         &\\approx \\sigma^2\\\\\n\\frac{1}{n}\\sum_{i=1}^nX_i X_i^T &\\approx \\Sigma_{X'X}.\n\\end{align}\n\\]\n\n\n\n4.3.2 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch4_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix.\nThe Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function.\nFor the case of the multiple linear regression model (Equation 4.1), the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n(-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right) &\n(-1)\\cdot E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\\\\\n(-1)\\cdot E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right) &\n(-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n\\end{array}\\right]\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\[2ex]\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\) \\[\n\\begin{align*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n& = \\left(\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\\right)^{-1}\\\\[2ex]\n& =\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{align*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch4_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch4_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x|\\theta),\n\\] where the true (unknown, univariate) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta\\) is an “interior point” of \\(\\Theta\\) if \\(\\theta_l<\\theta<\\theta_u.\\)\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on a Taylor expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta\\) (see Equation 4.2). To derive this expression, we use the mean value theorem (Theorem 5.1).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\nBy the Mean Value Theorem (Theorem 5.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.2, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1 = 0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we have thus \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we have thus \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.5}\\]\nUsing Equation 4.4 and Equation 4.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{above mean zero result}]\\\\\n&=:\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-\\mathbb{E}(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.6}\\]\nFurther analysis requires us to study the statistic \\[\n\\frac{1}{n}\\ell_n''(\\psi_n).\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nFirst, the mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=0 - \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\[2ex]\n&=-\\mathcal{J}(\\theta),\n\\end{align*}\n\\tag{4.7}\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\[2ex]\n&=0.\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWe have now gathered different equivalent expressions for \\(\\mathcal{J}(\\theta).\\) The last one (Equation 4.7) shows that \\(\\mathcal{J}(\\theta)\\) is nothing but the Fischer information scaled by \\(\\frac{1}{n}\\): \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n%& = Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\[2ex]\n%& = Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\\\[2ex]\n%&=\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)\\\\[2ex]\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n} (-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathcal{I}(\\theta)\n\\end{align*}\n\\]\nNote: For multivariate (\\(p\\)-dimensional) parameters \\(\\theta,\\) the Fisher information \\((-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section 4.3.1).\n\n\nSecond, the variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic figure}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\[2ex]\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator, i.e. \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator, i.e.  \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 Remember, we wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.6 not \\(\\frac{1}{n}\\ell_n''(\\theta).\\)\nLuckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n\\Leftrightarrow\\quad -\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\phantom{-}\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right),\n\\end{align*}\n\\tag{4.8}\\] where \\[\n\\mathcal{J}(\\theta)=\\frac{1}{n}(-1)\\cdot E(\\ell_n''(\\theta))=\\frac{1}{n}\\mathcal{I}(\\theta).\n\\] Equation 4.8 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta),\\) with \\(\\mathcal{I}(\\theta)\\) being the \\((p\\times p)\\) Fisher information matrix.\n\n\n\n\n\n\n\n\nMachine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020))."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html",
    "href": "Ch5_EMAlgorithmus.html",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "",
    "text": "The EM algorithm is often used to simplify, or make possible, complex maximum likelihood estimation problems. In this chapter, we present the EM algorithm for estimating Gaussian mixture distributions, as this is probably its most well-known application. Even the original work on the EM algorithm (Dempster, Laird, and Rubin 1977) already dealt with the estimation of Gaussian mixture distributions."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#exercises",
    "href": "Ch1_Random_Variable_Generation.html#exercises",
    "title": "1  Random Variable Generation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider Example 1.9 (“Normals from Double Exponentials”). Let \\(f\\) be the density of the standard normal distribution \\(\\mathcal{N}(0,1),\\) \\[\n\\begin{align*}\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right),\n\\end{align*}\n\\] and \\(g\\) the density of the Laplace (double exponential) distribution \\[\n\\begin{align*}\ng(x|b) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right),\\quad b>0.\n\\end{align*}\n\\]\n\nShow that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\]\nShow that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1.\\)\n\n\n\nExercise 2.\nLet \\(X \\sim f\\) and \\(Y\\sim g,\\) where \\(f\\) and \\(g\\) are density functions of the random variables \\(X\\) and \\(Y.\\) Show that\n\\[\n\\begin{align*}\nP\\left(X \\leq h (Y)\\right)\n&=\\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^{h (y)} f(x) dx \\right) g(y) dy.\n\\end{align*}\n\\] Note: This was used, for instance, in the proofs of Theorem 1.4 and Theorem 1.5.\nHint: Use that the probability of an event \\(A\\) (e.g. \\(A=X \\leq a\\)) can be written as \\[\n\\begin{align*}\nP(A)\n& = \\;\\;\\;\\;P(A) \\;\\;\\cdot \\;\\;1 \\;\\;+\\;\\; P(\\text{not}\\;A) \\;\\;\\cdot\\;\\; 0 \\\\[2ex]\n& = P(1_{(A)} = 1) \\cdot 1 + P(1_{(A)} = 0) \\;\\;\\cdot 0 \\\\[2ex]\n& = \\mathbb{E}\\left(1_{(A)}\\right),\n\\end{align*}\n\\] where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\) This allows you to use the iterated law of expectations.\n\n\nExercise 3.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) with compact support \\([a,b]\\) and \\(0\\leq f(x)\\leq m\\) for all \\(x\\in[a,b]\\). What is the probability of accepting \\(Y\\) from a simulation \\((Y,U)\\) as described in the lecture?\n\n\nExercise 4.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) and an instrumental density function \\(g\\) with \\[\nf_X(x) \\leq Mg(x)\n\\] with \\(M>0\\) for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\nWhat is the probability of accepting \\(Y\\) from a simulation \\[\n   (Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(y)\\right\\}\\right)?\n   \\]\nShow that \\(M\\geq 1.\\)"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#references",
    "href": "Ch1_Random_Variable_Generation.html#references",
    "title": "1  Random Variable Generation",
    "section": "References",
    "text": "References\n\n\n\n\nBox, George EP, and Mervin E Muller. 1958. “A Note on the Generation of Random Normal Deviates.” The Annals of Mathematical Statistics 29 (2): 610–11. https://projecteuclid.org/euclid.aoms/1177706645.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#references",
    "href": "Ch2_MonteCarlo.html#references",
    "title": "2  Monte Carlo Integration",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBourreau, Marc, Yutec Sun, and Frank Verboven. 2021. “Market Entry, Fighting Brands, and Tacit Collusion: Evidence from the French Mobile Telecommunications Market.” American Economic Review 111 (11): 3459–99.\n\n\nClopper, Charles J, and Egon S Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nJudd, Kenneth L. 1998. Numerical Methods in Economics. MIT press.\n\n\nRipley, Brian D. 2009. Stochastic Simulation. John Wiley & Sons. http://onlinelibrary.wiley.com/book/10.1002/9780470316726.\n\n\nRobert, Christian P, and George Casella. 2004. Monte Carlo Statistical Methods. 2nd ed. Springer Texts in Statistics. Springer.\n\n\n———. 2010. Introducing Monte Carlo Methods with r. 1st ed. Use r! Springer."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises-chapter-1-2",
    "href": "Ch2_MonteCarlo.html#exercises-chapter-1-2",
    "title": "2  Monte Carlo Method",
    "section": "Exercises (Chapter 1 & 2)",
    "text": "Exercises (Chapter 1 & 2)\n\n\nA possible algorithm for generating standard normal random variables from \\(n\\) standard uniformly distributed random variables \\(U_1,\\dots,U_n\\sim\\mathcal{U}[0,1],\\) would be to use the Central Limit Theorem (CLT), i.e. \\[\nY_n=\\frac{\\sqrt{n}\\left(\\bar{U}_n - \\mu\\right)}{\\sigma}\\to_d \\mathcal{N}(0,1).\n\\] as \\(n\\to\\infty,\\) where \\(\\mathbb{E}(U_1)=\\mu\\) and \\(\\mathbb{V}(U_1)=\\sigma^2.\\)\n\nAsses this “algorithm” critically by answering the following questions: (a) What is the maximal range of values of the generated random variables \\(Y_n\\)?\n(b) Compare the moments of \\(Y_n\\) with those of \\(\\mathcal{N}(0,1)\\) using the moment generating function \\[\nM_X(t) = \\mathbb{E}\\left(\\exp\\left(t X\\right)\\right)\n\\] Hint: For the case of \\(U\\sim\\mathcal{U}[0,1],\\) the moment generating function is \\[\nM_U(t) = \\mathbb{E}\\left(\\exp\\left(t U\\right)\\right)=\\frac{\\exp(t) - 1}{t}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThe distribution of a random variable \\(X\\) can be characterized (completely) using any of the following functions (provided they exist):\n\nDensity function \\(f_X\\)\nDistribution function \\(F_X(x)=\\int_{-\\infty}^x f_X(u)\\,du\\)\nMoment generating function \\(M_X(t)=\\mathbb{E}\\left(\\exp\\left(t X\\right)\\right)\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#solutions",
    "href": "Ch2_MonteCarlo.html#solutions",
    "title": "2  Monte Carlo Integration",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\nWe can write \\[\n\\begin{align*}\n&\\int_3^{10} \\int_1^7 \\sin(x-y) dxdy\\\\[2ex]\n=& \\int_3^{10} \\int_1^7 \\sin(x-y) \\cdot  42\\cdot f_{(X,Y)}(x,y) dx dy\\\\[2ex]\n=& \\mathbb{E}_f\\left(h( (X,Y) )\\right)\n\\end{align*}\n\\] where \\(h(x,y) = \\sin(x-y),\\) and where \\[\nf_{(X,Y)}(x,y) = \\left\\{\n    \\begin{array}{ll}\n    \\frac{1}{(10-3)}\\cdot\\frac{1}{(7-1)} = \\frac{1}{42}&\\text{for all }(x,y)\\in[3,10]\\times[1,7]\\\\[2ex]\n    0&\\text{else}\n    \\end{array}\n    \\right.\n\\] is the bi-variate density function of \\((X,Y)\\) with \\(X\\) and \\(Y\\) being two independent uniformly distributed random variables \\[\nX\\sim\\mathcal{U}[1,7]\\quad\\text{and}\\quad Y\\sim\\mathcal{U}[3,10],\n\\]\nThus, we can approximate the two-dimensional integral \\[\n\\int_3^{10} \\int_1^7 \\sin(x-y) dxdy\n\\] using sample averages of \\(\\sin(X-Y),\\) \\[\n\\frac{1}{n}\\sum_{i=1}^n \\sin(X_i - Y_i),\n\\] where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim\\mathcal{U}[1,7]\\) and \\(Y_1,\\dots,Y_n\\overset{\\text{i.i.d.}}\\sim\\mathcal{U}[3,10].\\)\n\nset.seed(321)\n\nX   <-  runif(1000000, min = 1, max =  7)\nY   <-  runif(1000000, min = 3, max = 10) \n\nmean(sin(X -Y)) * 42\n\n[1] 0.09818893\n\n\n\n\nSolutions of Exercise 2.\n\nTo approximate \\[\n\\int_1^\\infty \\exp\\left(-x^2\\right) dx\n\\] we can use that \\[\n\\begin{align*}\n\\int_1^\\infty \\exp\\left(-x^2\\right) dx\n&=\\int_0^\\infty \\exp\\left(-(x+1)^2\\right) dx\\\\[2ex]\n&=\\int_0^\\infty \\underbrace{\\frac{\\exp\\left(-(x+1)^2\\right)}{\\exp\\left(- x\\right)}}_{=h(x)} \\underbrace{\\exp\\left(-x\\right)}_{=f(x)} dx\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\exp\\left(-(X+1)^2\\right)}{\\exp\\left(- X \\right)}\\right) = \\mathbb{E}\\left(h(X)\\right)\n\\end{align*}\n\\] with \\(X\\sim f,\\) where \\(f\\) is the density function of the Exponential distribution \\(\\mathcal{Exp}(\\lambda = 1).\\)\nThus, we can approximate an integral \\(\\int_1^\\infty \\exp\\left(-x^2\\right) dx\\) using the sample mean of \\[\n\\frac{\\exp\\left(-(X+1)^2\\right)}{\\exp\\left(- X \\right)},\n\\] where \\(X\\sim f.\\)\n\nset.seed(321)\n\nX <- rexp(100000, rate = 1)\nmean( exp(-(X + 1)^2) / dexp(X))\n\n[1] 0.1398062\n\n\n\n\nSolutions of Exercise 3.\n\n(a) Function \\(h\\)\n\\[\n\\begin{align}\nP(X\\geq 3)\n& = \\theta \\\\[2ex]\n& = \\int h(x) f(x) dx\\\\[2ex]\n& = \\int 1_{(x\\geq 3)} f(x) dx,\n\\end{align}\n\\] where \\(X\\sim f\\) with \\(f\\) denoting the density function of the standard normal distribution. That is, \\[\nh(x) = 1_{(x\\geq 3)}.\n\\]\n\n\n(b) Classical Monte Carlo integration estimator\nThe classical Monte Carlo integration estimator is given by \\[\n\\begin{align}\n\\hat{\\theta} & = \\frac{1}{n}\\sum_{i=1}^n h(X_i),\n\\end{align}\n\\] where \\(X_1,\\dots,X_n\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,1)\\) and \\(h(x) = 1_{(x\\geq 3)}.\\)\n\nset.seed(321)\n\nn <- 100 \nX <- rnorm(n)\nh <- function(x){\n    x[x < 3]  <- 0\n    x[x >= 3] <- 1\n    return(x)\n}\n\nmean(h(X))\n\n[1] 0\n\n\nObviously, not a good estimation result. We know that \\(\\theta = P(X\\geq 3)\\) is some small number, but certainly larger than zero.\n\n\n(c) Approximate the mean and the variance of \\(\\hat{\\theta}\\) using a simulation.\nApproximating the mean, \\(\\mathbb{E}(\\hat{\\theta}),\\) and the variance, \\(\\mathbb{V}(\\hat{\\theta}),\\) of \\(\\hat{\\theta}\\) via a Monte Carlo simulation:\n\nset.seed(321)\n\nn                   <- 100 \nB                   <- 10000\ntheta_hat_estim_vec <- vector(mode   = \"double\", \n                              length = B)\n\nh <- function(x){\n    x[x <  3] <- 0\n    x[x >= 3] <- 1\n    return(x)\n}\n\nfor(b in 1:B){\n    X                      <- rnorm(n)    \n    theta_hat_estim_vec[b] <- mean(h(X))\n}\n\nmean(theta_hat_estim_vec)\n\n[1] 0.001363\n\nvar(theta_hat_estim_vec) \n\n[1] 1.357359e-05\n\n\n\n\n(d) Why is \\(\\hat{\\theta}\\) not efficient?\nMost observations are wasted in the sense that most are not near the right tail (\\(X\\geq 3\\)).\n\n\n(e) Importance sampling approach\n\\[\n\\begin{align}\nP(X\\geq 3)\n& = \\theta \\\\[2ex]\n& = \\int h(x) f(x) dx\\\\[2ex]\n& = \\int \\frac{h(x) f(x)}{g(x)} g(x) dx\n\\end{align}\n\\] where \\(h(x) = 1_{(x\\geq 3)},\\) \\(f\\) is the density function of \\(\\mathcal{N}(0,1),\\) and \\(g\\) is the density function of \\(\\mathcal{N}(4,1).\\)\n\nThe importance function \\(g\\sim\\mathcal{N}(4,1)\\) is potentially a good choice since\n\n\\(g\\) has a more or less similar shape as \\(|h|f\\)\n\\(g\\) has thicker tails than \\(|h|f\\)\nit is very simple to sample from \\(g\\)\n\n\nx_vec  <- seq(3, 8, len = 50)\nhf_vec <- dnorm(x_vec, mean = 0, sd = 1)\ng_vec  <- dnorm(x_vec, mean = 4, sd = 1)\n\nplot(x = x_vec, y = hf_vec, type = \"l\", \nylim = range(hf_vec, g_vec), xlab = \"x\", ylab = \"\")\nlines(x = x_vec, y = g_vec, col = \"red\")\nlegend(\"topright\", legend = c(\"|h|f\", \"g\"), \ncol=c(\"black\", \"red\"), lty=c(1,1))\n\n\n\n\n\n\n(f) Importance sampling estimator\nThe importance sampling integration estimator is given by\n\\[\n\\begin{align}\n\\hat{\\theta}_{IS} & = \\frac{1}{n}\\sum_{i=1}^n \\frac{h(X_i)f(X_i)}{g(X_i)},\n\\end{align}\n\\] where\n\n\\(X_1,\\dots,X_n\\overset{i.i.d.}{\\sim}\\mathcal{N}(4,1)\\)\n\\(f\\) is the density function of \\(\\mathcal{N}(0,1)\\)\n\\(h(x) = 1_{(x\\geq 3)}\\)\n\n\nset.seed(321)\n\nn   <- 100 \nX   <- rnorm(n, mean = 4, sd = 1)\n\nhfg <- function(x){\n    tmp    <- dnorm(x, mean = 0, sd = 1)/\n              dnorm(x, mean = 4, sd = 1) \n    tmp[x < 3] <- 0\n    result <- tmp\n    return(result)\n}\n\nmean(hfg(X))\n\n[1] 0.001567801\n\n\n\n\n(g) Approximate the mean and the variance of \\(\\hat{\\theta}_{IS}\\) using a simulation.\nApproximating the mean, \\(\\mathbb{E}(\\hat{\\theta}_{IS}),\\) and the variance, \\(\\mathbb{V}(\\hat{\\theta}_{IS}),\\) of \\(\\hat{\\theta}_{IS}\\) via a Monte Carlo simulation:\n\nset.seed(321)\n\nn                      <- 100 \nB                      <- 10000\ntheta_hat_IS_estim_vec <- vector(mode   = \"double\", \n                                 length = B)\n\nhfg <- function(x){\n    tmp    <- dnorm(x, mean = 0, sd = 1)/\n              dnorm(x, mean = 4, sd = 1) \n    tmp[x < 3] <- 0\n    result <- tmp\n    return(result)\n}\n\nfor(b in 1:B){\n    X                         <- rnorm(n    = n, \n                                       mean = 4, \n                                       sd   = 1)    \n    theta_hat_IS_estim_vec[b] <- mean(hfg(X))\n}\n\nmean(theta_hat_IS_estim_vec)\n\n[1] 0.00135138\n\nvar(theta_hat_IS_estim_vec) \n\n[1] 9.617291e-08\n\n\nThe importance sampling estimator, \\(\\hat{\\theta}_{IS},\\) is a much lower variance than the classical Monte Carlo integration estimator, \\(\\hat{\\theta}.\\)\n\n\n\nSolutions of Exercise 4.\n\n(a) Classical Monte Carlo integration estimator\nClassical Monte Carlo integration estimates \\[\n\\begin{align}\np=P(X\\geq 4.5)\n& = \\int \\underbrace{1_{(x\\geq 4.5)}}_{=h(x)} f(x) dx = \\int_{4.5}^\\infty f(x)dx\n\\end{align}\n\\tag{2.7}\\] by \\[\n\\begin{align}\n\\hat{p}\n& = \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\geq 4.5)},\n\\end{align}\n\\] where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim} f.\\)\n\n\n(b) Unbiasedness of \\(\\hat{p}\\)\n\\[\n\\begin{align}\n\\mathbb{E}\\left(\\hat{p}\\right)\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\geq 4.5)}\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(1_{(X_1\\geq 4.5)}\\right)\\qquad [\\text{i.i.d.}]\\\\[2ex]\n& = \\int 1_{(x\\geq 4.5)} f(x) dx\\\\[2ex]\n& = \\int_{4.5}^\\infty f(x) dx\\\\[2ex]\n& = p,\n\\end{align}\n\\] where the latter follows from Equation 2.7. Thus, the classical Monte Carlo integration estimator, \\(\\hat{p},\\) is an unbiased estimator.\n\n\n(c) Variance of \\(\\hat{p}\\)\n\\[\n\\begin{align}\n\\mathbb{V}\\left(\\hat{p}\\right)\n& = \\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\geq 4.5)}\\right)\\\\[2ex]\n& = \\frac{1}{n} \\mathbb{V}\\left(1_{(X_1\\geq 4.5)}\\right)\\qquad [\\text{i.i.d.}]\\\\[2ex]\n& = \\frac{p(1-p)}{n} \\qquad [1_{(X_1\\geq 4.5)}\\sim\\mathcal{Bern}(p)]\n\\end{align}\n\\] Thus \\[\n\\mathbb{V}\\left(\\hat{p}\\right) = \\frac{1}{n}(p-p^2)\n\\tag{2.8}\\]\n\n\n(d) Deriving the importance sampling estimator \\(\\hat{p}_{IS}\\)\nImportance sampling makes use of the importance function \\(g\\) \\[\n\\begin{align}\np=P(X\\geq 4.5)\n& = \\int \\frac{h(x) f(x)}{g(x)} g(x) dx\n\\end{align}\n\\] where here \\[\ng(x) = \\left\\{\\begin{array}{ll}f_{\\mathcal{Exp}\\text{(1)}}(x-4.5),& x \\geq 4.5\\\\0,&\\text{else.}\\end{array}\\right.\n\\] Since \\(g(x)=0\\) for all \\(x<4.5,\\) we can simplify the integral by removing \\(h,\\) \\[\n\\begin{align}\np=P(X\\geq 4.5)\n& = \\int \\frac{h(x) f(x)}{g(x)} g(x) dx \\\\[2ex]\n& = \\int \\frac{f(x)}{g(x)} g(x) dx.\n\\end{align}\n\\] Thus, \\(p\\) can be estimated by \\[\n\\begin{align}\n\\hat{p}_{IS}\n& = \\frac{1}{n}\\sum_{i=1}^n \\frac{f(X_i)}{g(X_i)},\n\\end{align}\n\\] where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}g.\\)\n\nNote that \\(g\\) equals the density function of \\(\\mathcal{Exp}(\\lambda=1),\\) but with a location-shift of \\(+4.5.\\) Therefore,\n\\[\nX_i \\overset{d}{=} Y_i + 4.5,\n\\] where \\(Y_i\\sim \\mathcal{Exp}(\\lambda=1).\\)\nThus, an alternative, equivalent way to define \\(\\hat{p}_{IS}\\) is \\[\n\\begin{align}\n\\hat{p}_{IS}\n& = \\frac{1}{n}\\sum_{i=1}^n \\frac{f(Y_i + 4.5)}{g(Y_i + 4.5)}\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )},\n\\end{align}\n\\] where \\(Y_1,\\dots,Y_n\\overset{\\text{i.i.d.}}{\\sim}f_{\\mathcal{Exp}\\text{(1)}}.\\) This version is very convenient since it is very simple to sample from \\(f_{\\mathcal{Exp}\\text{(1)}}.\\)\n\n\n(e) Unbiasedness of \\(\\hat{p}_{IS}\\)\nThe following shows that \\(\\hat{p}_{IS}\\) is an unbiased estimator of \\(p:\\)\n\\[\n\\begin{align}\n\\mathbb{E}\\left(\\hat{p}_{IS}\\right)\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n \\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\frac{f(Y_1 + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_1)}\\right)\\qquad [\\text{i.i.d.}]\\\\[2ex]\n& = \\int \\frac{f(y + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(y)} f_{\\mathcal{Exp}\\text{(1)}}(y) dy\\\\[2ex]\n& = \\int_0^\\infty \\frac{f(y + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(y)} f_{\\mathcal{Exp}\\text{(1)}}(y) dy\\qquad [\\text{support of }f_{\\mathcal{Exp}\\text{(1)}}]\\\\[2ex]\n& = \\int_0^\\infty f(y + 4.5)  dy\\\\[2ex]\n& = \\int_{4.5}^\\infty f(z)  dz\\qquad[\\text{subst.~}z=y+4.5]\\\\[2ex]\n& = p,\n\\end{align}\n\\] where the latter follows from Equation 2.7. Thus \\(\\hat{p}_{IS}\\) is an unbiased estimator.\n\n\n(f) Comparing the variance of \\(\\hat{p}\\) with that of \\(\\hat{p}_{IS}\\)\n\\[\n\\begin{align}\n\\mathbb{V}\\left(\\hat{p}_{IS}\\right)\n& = \\mathbb{V}\\left(\\frac{1}{n}\\sum_{i=1}^n \\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)\\\\[2ex]\n& = \\frac{1}{n}\\mathbb{V}\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)\\qquad [\\text{i.i.d.}]\\\\[2ex]\n& = \\frac{1}{n}\\left[\\mathbb{E}\\left(\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)^2\\right)- \\left(\\mathbb{E}\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)\\right)^2\\right]\\qquad [\\text{variance formula}]\\\\[2ex]\n& = \\frac{1}{n}\\left[\\mathbb{E}\\left(\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)^2\\right)- p^2\\right]\\qquad [\\text{using unbiasedness}]\n\\end{align}\n\\]\nNext, we focus on \\(\\mathbb{E}\\left(\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)^2\\right)\\) \\[\n\\begin{align}\n\\mathbb{E}\\left(\\left(\\frac{f(Y_i + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(Y_i )}\\right)^2\\right)\n& = \\int_0^\\infty \\frac{f^2(y + 4.5)}{f^2_{\\mathcal{Exp}\\text{(1)}}(y)} f_{\\mathcal{Exp}\\text{(1)}}(y) dy\\\\[2ex]\n& = \\int_0^\\infty \\frac{f^2(y + 4.5)}{f_{\\mathcal{Exp}\\text{(1)}}(y)}  dy\\\\[2ex]\n& = \\int_{4.5}^\\infty \\frac{f^2(x)}{f_{\\mathcal{Exp}\\text{(1)}}(x-4.5)}  dx\\qquad [\\text{subst.~}x=y+4.5]\\\\[2ex]\n& = \\int_{4.5}^\\infty \\frac{f^2(x)}{g(x)} dx\\qquad [\\text{by definition of }g]\\\\[2ex]\n& = \\int_{4.5}^\\infty \\left(\\frac{f(x)}{g(x)}\\right) f(x) dx\\\\[2ex]\n& < c_{\\max}\\; \\int_{4.5}^\\infty   f(x)\\;  dx\n  = c_{\\max}\\; p < p\n\\end{align}\n\\] where \\[\nc_{\\max}=\\max_{y\\geq 4.5}\\left(\\frac{f(y)}{g(y)}\\right) < 1\n\\] since \\(g(x) > f(x)\\) for all \\(x\\geq 4.5.\\)\nThus, we have that \\[\n\\begin{align}\n\\mathbb{V}\\left(\\hat{p}_{IS}\\right)\n& < \\frac{1}{n}\\left[c_{\\max}\\; p - p^2\\right] < \\frac{1}{n}\\left[p - p^2\\right] = \\mathbb{V}\\left(\\hat{p}\\right)\n\\end{align}\n\\] where the latter follows from the result in Equation 2.8.\n\n\n\n\n\n\n\n\n\n\nNote: Using importance sampling there’s no need to sample directly from \\(f,\\) which may be difficult. This is a further advantage of importance sampling over classical Monte Carlo integration."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises-of-chapters-1-2",
    "href": "Ch2_MonteCarlo.html#exercises-of-chapters-1-2",
    "title": "2  Monte Carlo Integration",
    "section": "Exercises of Chapters 1 & 2",
    "text": "Exercises of Chapters 1 & 2\n\nExercise 1.\nConsider Example 1.9 (“Normals from Double Exponentials”). Let \\(f\\) be the density of the standard normal distribution \\(\\mathcal{N}(0,1),\\) \\[\n\\begin{align*}\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right),\n\\end{align*}\n\\] and \\(g\\) the density of the Laplace (double exponential) distribution \\[\n\\begin{align*}\ng(x|b) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{2}\\right),\\quad b>0.\n\\end{align*}\n\\]\n\nShow that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\]\nShow that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1.\\)\n\n\n\nExercise 2.\nLet \\(X \\sim f\\) and \\(Y\\sim g,\\) where \\(f\\) and \\(g\\) are density functions of the random variables \\(X\\) and \\(Y.\\) Show that\n\\[\n\\begin{align*}\nP\\left(X \\leq h (Y)\\right)\n&=\\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^{h (y)} f(x) dx \\right) g(y) dy.\n\\end{align*}\n\\] Note: This was used, for instance, in the proofs of Theorem 1.4 and Theorem 1.5.\nHint: Use that the probability of an event \\(A\\) (e.g. \\(A=X \\leq a\\)) can be written as \\[\n\\begin{align*}\nP(A)\n& = \\;\\;\\;\\;P(A) \\;\\;\\cdot \\;\\;1 \\;\\;+\\;\\; P(\\text{not}\\;A) \\;\\;\\cdot\\;\\; 0 \\\\[2ex]\n& = P(1_{(A)} = 1) \\cdot 1 + P(1_{(A)} = 0) \\;\\;\\cdot 0 \\\\[2ex]\n& = \\mathbb{E}\\left(1_{(A)}\\right),\n\\end{align*}\n\\] where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\) This allows you to use the iterated law of expectations.\n\n\nExercise 3.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) with compact support \\([a,b]\\) and \\(0\\leq f(x)\\leq m\\) for all \\(x\\in[a,b]\\). What is the probability of accepting \\(Y\\) from a simulation \\((Y,U)\\) as described in the lecture?\n\n\nExercise 4.\nConsider the Accept-Reject Algorithm for a target density function \\(f_X\\) and an instrumental density function \\(g\\) with \\[\nf_X(x) \\leq Mg(x)\n\\] for all \\(x\\in\\mathbb{R}\\) and \\(g(x)>0\\) for all \\(x\\in\\operatorname{supp}(f_X).\\)\n\nWhat is the probability of accepting \\(Y\\) from a simulation \\[\n   (Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right)?\n   \\]\nShow that \\(M\\geq 1.\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#solutions-of-exercises-of-chapters-1-2",
    "href": "Ch2_MonteCarlo.html#solutions-of-exercises-of-chapters-1-2",
    "title": "2  Monte Carlo Method",
    "section": "Solutions of Exercises of Chapters 1 & 2",
    "text": "Solutions of Exercises of Chapters 1 & 2\n\nDeriving the upper bound in Example 1.9 (“Normals from Double Exponentials”): \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n&= \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(-\\frac{x^2}{2}\\right) \\exp\\left(\\frac{|x|}{b}\\right)\\\\[2ex]\n&= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2|x| - x^2b}{2b}\\right)\\\\[2ex]\n&=\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{-2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\] Consider a given value of \\(b>0,\\) and the case of \\(x\\geq 0.\\) Maximizing\n\\[\n\\begin{align*}\n\\exp\\left(\\frac{2x - x^2b}{2b}\\right)\n& = \\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] maximal with respect to \\(x\\geq 0\\) yields \\[\n\\begin{align*}\n\\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=\\frac{1}{b}\n\\end{align*}\n\\] Correspondingly, for a given value of \\(b>0,\\) and the case of \\(x<0,\\) maximizing \\[\n\\begin{align*}\n\\exp\\left(\\frac{-2x - x^2b}{2b}\\right)\n& = \\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n\\end{align*}\n\\] yields \\[\n\\begin{align*}\n\\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(-\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n\\Leftrightarrow\nx=-\\frac{1}{b}\n\\end{align*}\n\\]\n\n\nb <- 2\nmyfun <- function(x, b){exp( (2*abs(x) - x^2 * b)/(2 * b) )}\nxx <- seq(-10, 10, len = 500)\nyy <- myfun(xx, b = b)\nplot(x = xx, y = yy, type = \"l\")\nabline(v=c(-1/b,1/b), lty=2)\n\n\n\n\nThus \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\n& \\leq\n\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n\\end{array}\n\\right.\\\\[2ex]\n& =\n\\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b} }{2b}\\right) \\quad \\;\\text{for}\\; x\\in\\mathbb{R},\n\\end{align*}\n\\] which shows that \\[\n\\begin{align*}\n\\frac{f(x)}{g(x|b)}\\leq \\sqrt{\\frac{2}{\\pi}} \\; b\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\end{align*}\n\\] Now, to show that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1,\\) we minimize \\[\nb\\;\\exp\\left(\\frac{1}{2b^2}\\right)\n\\] with respect to \\(b>0,\\) which yields \\[\n\\begin{align*}\n1\\;\\exp\\left(\\frac{1}{2b^2}\\right) + b\\;\\exp\\left(\\frac{1}{2b^2}\\right)\\cdot \\left(-2\\frac{1}{2b^3}\\right) &\\overset{!}{=} 0\\\\[2ex]\n1\\; - 2\\frac{1}{2b} &= 0\\\\[2ex]\nb &= 1\n\\end{align*}\n\\] where we used that \\(\\exp\\left(\\frac{1}{2b^2}\\right)>0.\\)\n\nmyfun2 <- function(b){b * exp( 1/(2*b^2) )}\nbb <- seq(.5, 2, len = 500)\nyy <- myfun2(b = bb)\nplot(x = bb, y = yy, type = \"l\")\nabline(v = 1, lty=2)\n\n\n\n\n\n\nDeriving the probability of accepting \\(Y\\) from a simulation \\[\n(Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(x)\\right\\}\\right).\n\\]\n\n\n\\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(U\\leq f_X(Y)\\right),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}\\left[0,Mg(Y)\\right].\\) Standardizing the uniform distribution yields \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(\\frac{U}{Mg(Y)}\\leq \\frac{f_X(Y)}{Mg(Y)}\\right),\\\\[2ex]\n\\end{align*}\n\\] where dividing by \\(Mg(Y)\\) is allowed, since \\(Mg(y)>0\\) for all \\(y\\in\\operatorname{supp}(f_X).\\) Using that \\[\n\\frac{U}{Mg(Y)} \\sim\\mathcal{U}\\left[0,1\\right],\n\\] we have \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=\\int_{-\\infty}^{\\infty}\\int_0^{\\frac{f_X(y)}{Mg(y)}} 1 \\; du\\; g(y) \\;dy\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty}\\;\\;\\;\\frac{f_X(y)}{Mg(y)}\\;\\; g(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\\int_{-\\infty}^{\\infty}\\;f_X(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\n\\end{align*}\n\\]  (b) \\[\n  \\begin{align*}\n  f_X(x) & \\leq M g(x)\\quad x\\in\\operatorname{f_X}\\\\[2ex]\n  \\int f_X(x) dx & \\leq \\int M g(x)dx\\\\[2ex]\n  1              & \\leq M \\cdot 1\\\\[2ex]\n  \\end{align*}\n  \\]"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#exercises",
    "href": "Ch2_MonteCarlo.html#exercises",
    "title": "2  Monte Carlo Integration",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\n\nApproximate the bi-variate integral \\[\n\\int_3^{10} \\int_1^7 \\sin(x-y) dxdy\n\\] using classical Monte Carlo integration.\n\n\nExercise 2.\n\nApproximate the integral \\[\n\\int_1^\\infty \\exp\\left(-x^2\\right) dx\n\\] using Monte Carlo integration. Hint: Use of the density of the exponential distribution with rate parameter \\(\\lambda=1\\).\n\n\nExercise 3.\n\nLet us estimate \\[\n\\theta = P(X\\geq 3),\n\\] where \\(X\\sim\\mathcal{N}(0,1),\\) using Monte Carlo integration.\nFor a reference, this is the integral value computed in R:\n\nround(pnorm(3, mean = 0, sd = 1, lower.tail = FALSE), 5)\n\n[1] 0.00135\n\n\n\nWrite the parameter \\(\\theta\\) as \\[\n\\theta = \\int h(x) f(x) dx = \\mathbb{E}_f(h(X))\n\\] where \\(X\\sim f\\) with \\(f\\) denoting the density function of the standard normal distribution. What is the function \\(h\\)? \nWhat is the classical Monte Carlo integration estimator, \\(\\hat{\\theta},\\) of \\(\\theta\\)? Compute one estimate of \\(\\theta\\) using a sample size of \\(n=100.\\)\nRepeat the simulation many times (e.g. \\(10,000\\) times) to get an estimate of the mean, \\(\\mathbb{E}(\\hat{\\theta}),\\) and the variance, \\(\\mathbb{V}(\\hat{\\theta}),\\) of \\(\\hat{\\theta}.\\)\nWhy is the classic Monte Carlo integration approach here not very efficient?\nTake now an importance sampling approach and write the parameter \\(\\theta\\) as \\[\n\\theta = \\int \\frac{h(x)f(x)}{g(x)} g(x) dx = \\mathbb{E}_g(Y),\n\\] where \\(Y=\\frac{h(X)f(X)}{g(X)}\\) and \\(X\\sim g.\\)  Why is the density function \\(g\\sim\\mathcal{N}(4,1)\\) potentially a good choice?\nWhat is the importance sampling estimator, \\(\\hat{\\theta}_{IS},\\) of \\(\\theta\\)? Compute one estimate of \\(\\theta\\) using a sample size of \\(n=100.\\)\nRepeat the simulation many times (e.g. \\(10,000\\) times) to get an estimate of the mean, \\(\\mathbb{E}(\\hat{\\theta}),\\) and the variance, \\(\\mathbb{V}(\\hat{\\theta}),\\) of \\(\\hat{\\theta}.\\)\n\n\n\nExercise 4.\nThe Bavarian city Passau needs to know tomorrows probability of facing a flood catastrophy. They received a density function \\(f\\) from some experts which describes tomorrows distribution of standardized water levels \\(X\\). The city is particularly interested in the probability of observing water levels above of 11 meters, which in standardized water levels is equivalent to the event \\(X\\geq 4.5\\), where \\(X\\sim f\\).\nUnfortunately, \\(f\\) is a very complicated function such that they cannot compute the probability analytically. Therefore, they tried to approximate the probability \\(P(X\\geq 4.5)\\) using classical Monte Carlo integration.\n\nDerive the classical Monte Carlo integration estimator. Hint: You can assume that it is possible to sample from \\(f.\\) (This is not a restrictive assumption, since one could use, for instance, the very general, but quite inefficient, Accept-Reject-Algorithm.)\nShow that the classical Monte Carlo integration estimator is unbiased.\nDerive the variance of the classical Monte Carlo integration estimator.\nIt turns out that the tail probabilities of \\(f\\) are very low and when simulating, e.g., \\(B=10,000\\) realizations from \\(f\\) usually no single realization is greater or equal to \\(4.5\\). Derive the importance sampling estimator using an appropriately shifted version of the exponential density function with \\(\\lambda=1\\) as the importance function \\(g\\) such that all of the \\(B=10,000\\) realizations are used for the estimation of the probability \\(P(X\\geq 4.5).\\)  Hint: You can assume that \\(g(x) > f(x)\\) for all \\(x\\geq 4.5.\\)\nShow that the importance sampling estimator is unbiased.\nShow (mathematically) that the variance of the importance sampling estimator of the probability \\(P(X\\geq 4.5)\\) is strictly smaller than the variance of the classical Monte Carlo integration estimator. Hint: You can assume and should use that \\(g(x) > f(x)\\) for all \\(x\\geq 4.5.\\)"
  },
  {
    "objectID": "Ch2_MonteCarlo.html#cauchy-tail-probability-continued",
    "href": "Ch2_MonteCarlo.html#cauchy-tail-probability-continued",
    "title": "2  Monte Carlo Integration",
    "section": "2.3 Cauchy Tail Probability (continued)",
    "text": "2.3 Cauchy Tail Probability (continued)\nAbove we had:\n\n\\(f(x)=\\frac{1}{\\pi(1+x^2)}\\), the density of \\(\\mathcal{Cauchy}(0,1)\\) and\n\\(h(x)=1_{(x>2)}\\), i.e., here \\(|h(x)|=h(x)\\).\n\nTherefore \\[\n\\begin{align*}\np\n& =\\mathbb{E}_f(h(X)) \\\\[2ex]\n& =\\int h(x)f(x)dx    \\\\[2ex]\n& =\\int_{2}^{\\infty}f(x)dx \\\\[2ex]\n& =\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx \\\\[2ex]\n& =\\mathbb{E}_g(\\psi(X)),\n\\end{align*}\n\\] where the \\(h\\) function is absorbed by the formulation of the definite integral.\nA possibly good (and simple) choice of \\(g\\) is, e.g., \\(g(x)=2/(x^2)\\), since this function:\n\n“closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: It is not straight forward to directly sample from \\(g\\), therefore we need some further steps:\n\n\nThe choice of \\(g\\) leads to \\[\np=\\mathbb{E}_g(\\psi(X))=\n\\int_{2}^{+\\infty}\\left(\\frac{x^2}{2\\,\\pi(1+x^2)}\\right)\\,\\frac{2}{x^2}\\,dx=\n\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\]\n\n\nNow we can apply some additional (rather case-specific) re-arrangements:\nIntegration by substitution (substituting \\(u=x^{-1}\\)) yields: \\[\np=\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] Again, we can re-arrange the last integral a bit such that \\[\np=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathrm{Unif}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du=\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2].\n\\] Therefore, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\sum_{i=1}n h(U_i),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2]\n\\] and \\(h(u)=1/(2\\pi(1+u^2))\\).\nThe variance of \\(\\hat{p}_4\\) is \\((\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2)/n\\) and an integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.95\\cdot 10^{-4}/n\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/n\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/n\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/n\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\)."
  },
  {
    "objectID": "Ch2_MonteCarlo.html#classical-monte-carlo-integration-using-proportions",
    "href": "Ch2_MonteCarlo.html#classical-monte-carlo-integration-using-proportions",
    "title": "2  Monte Carlo Integration",
    "section": "2.2 Classical Monte Carlo Integration using Proportions",
    "text": "2.2 Classical Monte Carlo Integration using Proportions\nMany integration problems, such as the one in Example 2.2, can be written as a proportion \\(0\\leq p\\leq 1.\\) This allows us compute the integral using using empirical proportions \\[\n\\hat{p} =\\frac{\\text{Number of successes}}{\\text{Sample size}},\n\\] where we choose the sample size \\(n\\) to achieve a certain level of precision for our integral approximation.\n\nExample 2.2 (Approximation of Normal Distribution Tables) A possible way to construct normal distribution tables, i.e., table for the values of the distribution function \\(0\\leq \\Phi(x)\\leq 1,\\) is to use Monte Carlo integration.\nObserve that the distribution function \\(\\Phi(x)\\) can be written as the mean of a binary (taking values 0 and 1) random variable, \\[\n\\begin{align*}\n\\Phi(x)\n&=\\int_{-\\infty}^x\\frac{1}{\\sqrt{2\\pi}}e^{-y^2/2}dy\\\\[2ex]\n&=P\\left(X \\leq x\\right)\\\\[2ex]\n&=P\\left(1_{(X \\leq x)} = 1\\right)\\\\[2ex]\n&=P\\left(1_{(X \\leq x)} = 1\\right)\\cdot 1 + P\\left(1_{(X \\leq x)} = 0\\right)\\cdot 0\\\\[2ex]\n&=\\mathbb{E}\\left(1_{(X \\leq x)}\\right),\n\\end{align*}\n\\] where \\(X\\sim\\mathcal{N}(0,1),\\) and where \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\nThat is, we can write the integration problem as the mean of a Bernoulli random variable \\[\n1_{(X \\leq x)}\\in\\{0,1\\}\n\\] \\[\n1_{(X \\leq x)}\\sim\\mathcal{Bern}\\left(p=\\Phi(x)\\right),\n\\] with parameter \\[\np=P\\left(X \\leq x\\right) = \\Phi(x).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(1_{(X \\leq x)}\\) is a Bernoulli random variable no matter the distribution of \\(X.\\) In Example 2.2, \\(X\\) is a standard normal random variable; however, in other use cases \\(X\\) can have, of course, another distribution, but \\(1_{(X \\leq x)}\\) remains a Bernoulli random variable. We use this feature below when we think about the distributional properties of our estimator for approximating the integral value.\n\n\nMonte Carlo integration, allows us to approximate the integral \\(\\Phi(x)\\) using the empirical mean \\[\n\\hat{p}_n(x)=\\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{N}(0,1),\n\\] and \\[\n\\hat{p}_n(x)\\in[0,1]\n\\] for all \\(x\\in\\mathbb{R},\\) and all “sample sizes” (chosen by us) \\(n=1,2,\\dots\\)\n\n2.2.1 Distributional Properties of Proportions\nNow, to assess the accuracy of our Monte Carlo integration for approximating \\(\\Phi_n(x),\\) we need to derive the distributional properties of our estimator \\(\\hat{p}_n(x).\\)\nFor this we consider, firstly, the transformed random variable \\[\nn \\hat{p}_n(x)=\\sum_{i=1}^n1_{(X_i\\leq x)},\n\\] which is just the sum of independent Bernoulli distributed random variables with success probability \\(p=\\Phi(x),\\) \\[\n1_{(X_1\\leq x)},\\dots,1_{(X_n\\leq x)}\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{Bern}\\left(p=\\Phi(x)\\right)\n\\] Thus, the transformed random variable \\[\nn\\hat{p}_n(x)=\\sum_{i=1}^n1_{(X_i\\leq x)}\n\\] is binomial distributed with parameters \\(n\\) and \\(p=\\Phi(x),\\) \\[\nn \\hat{p}_n(x)\\sim \\mathcal{Binom}\\left(n, p=\\Phi(x)\\right).\n\\] Thus, we know the exact variance which is \\[\n\\begin{align*}\n\\mathbb{V}\\left(n\\hat{p}_n(x)\\right)\n&=n p (1-p)\\\\[2ex]\n&=n\\Phi(x)(1-\\Phi(x)).\n%\\Leftrightarrow \\mathbb{V}\\left(\\hat{p}_n(t)\\right) &=\\frac{\\Phi(x)(1-\\Phi(x))}{n}\n\\end{align*}\n\\] The standard error for our estimator \\(\\hat{p}_n(x)\\) thus is equal to \\[\n\\begin{align*}\n\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)\n&=\\sqrt{\\frac{\\Phi(x)(1-\\Phi(x))}{n}}\\\\[2ex]\n&=\\texttt{const}\\cdot \\frac{1}{\\sqrt{n}}\n\\end{align*}\n\\] That is, the Monte Carlo integration algorithm has the typical parametric convergence rate of \\[\n\\frac{1}{\\sqrt{n}}.\n\\]\n\nTable 2.1 gives the evolution of the Monte Carlo integration results for Example 2.2 \\[\n\\hat{p}_n(x)\\approx \\Phi(x)\n\\] for several values of \\(x\\) and \\(n.\\) Very accurate approximations are achieved for \\(n=10^8.\\)\n\n\nTable 2.1: Monte Carlo integration results \\(\\hat{p}_n(x)\\approx \\Phi(x)\\) for different values of \\(x\\) and sample sizes \\(n.\\)\n\n\n\\(n\\)\n\\(x=0.0\\)\n\\(x=0.84\\)\n\\(x=3.72\\)\n\n\n\n\n\\(10^2\\)\n\\(0.4850\\)\n\\(0.7700\\)\n\\(1.0000\\)\n\n\n\\(10^3\\)\n\\(0.4925\\)\n\\(0.8010\\)\n\\(1.0000\\)\n\n\n\\(10^4\\)\n\\(0.4962\\)\n\\(0.7941\\)\n\\(0.9999\\)\n\n\n\\(10^5\\)\n\\(0.4995\\)\n\\(0.7993\\)\n\\(0.9999\\)\n\n\n\\(10^6\\)\n\\(0.5001\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\\(10^7\\)\n\\({\\color{red}0.5002}\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\\(10^8\\)\n\\(0.5000\\)\n\\(0.8000\\)\n\\(0.9999\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that greater accuracy is achieved in the tails.\n\n\n\nConfidence Intervals for Proportions\nAs an additional tool for showing the accuracy of our Monte Carlo integration, we can report confidence intervals.\nFor large \\(n,\\) we have by the CLT that \\[\n\\begin{align*}\n\\frac{\\hat{p}_n(x) -  \\Phi(x)}{\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)} \\overset{d}{\\approx}\\mathcal{N}(0,1).\n\\end{align*}\n\\tag{2.2}\\] Thus, for large \\(n,\\) we can provide an approximate confidence interval \\[\n\\begin{align*}\n\\operatorname{CI}^{95\\%}_n\\left(\\Phi(x)\\right)\n=&\\left[\n  \\hat{p}_n(x) \\pm z_{1-\\alpha/2}\\operatorname{SE}\\left(\\hat{p}_n(x)\\right)\n  \\right].\n\\end{align*}\n\\tag{2.3}\\] This confidence interval is an approximate one, since the quantile \\(z_{1-\\alpha/2}\\) uses the approximate asymptotic \\(\\mathcal{N}(0,1)\\) distribution in Equation 2.2. To make the confidence interval in Equation 2.3 usable in practice, we need to plug-in an estimate for the unknown \\(\\operatorname{SE}\\left(\\hat{p}_n(x)\\right),\\) \\[\n\\widehat{\\operatorname{SE}}\\left(\\hat{p}_n(x)\\right) = \\sqrt{\\frac{\\hat{p}_n(x)(1-\\hat{p}_n(x))}{n}}.\n\\]\nHowever, it turns out that going the indirect way via the central limit theorem to derive a confidence interval is not optimal here. A more efficient confidence interval for proportions is, for instance, the Clopper-Pearson confidence interval (Clopper and Pearson (1934)).\nThe following code snippet computes the 99% Clopper-Pearson confidence interval for the red marked Monte Carlo integration result, \\(\\hat{p}_n(0)=0.5002,\\) in Table 2.1.\n\n## install.packages(\"PropCIs\")\nlibrary(\"PropCIs\")\n\n## Clopper-Pearson confidence interval\n\nn <- 10e7       # \"sample\" size (chosen by use)\nx <- 0.5002 * n # observed value of n * \\hat{p}_n(x) \n\nCI <- exactci(x          = x, \n              n          = n, \n              conf.level = 0.99)\n\n## Lower and upper CI-border              \nc(CI$conf.int[1], CI$conf.int[2]) \n\n[1] 0.5001089 0.5002911\n\n\n\n\n\n\n\nThat is, we need a sample size of \\(n^7\\) to achieve a precision of three decimal places by means of a 99% Clopper-Pearson confidence interval."
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "href": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic Idea of the Bootstrap",
    "text": "3.2 Basic Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\nPopulation distribution \\(F\\): The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nEmpirical distribution \\(F_n\\): For large \\(n,\\) the empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F,\\) the bootstrap uses random sampling from the known \\(F_n.\\) This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Glivenko-Cantelli Theorem 3.1).\n\n\n\n\n\n\nBradley Efron\n\n\n\nThe bootstrap method is attributed to Bradley Efron, who received the International Prize in Statistics (the Nobel price of statistics) for his seminal works on the bootstrap method."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#solutions",
    "href": "Ch1_Random_Variable_Generation.html#solutions",
    "title": "1  Random Variable Generation",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\n\nDeriving the upper bound: \\[\n  \\begin{align*}\n  \\frac{f(x)}{g(x|b)}\n  &= \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\\\\[2ex]\n  &= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(-\\frac{x^2}{2}\\right) \\exp\\left(\\frac{|x|}{b}\\right)\\\\[2ex]\n  &= \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2|x| - x^2b}{2b}\\right)\\\\[2ex]\n  &=\\left\\{\n  \\begin{array}{ll}\n  \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n  \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{-2 x  - x^2b}{2b}\\right) &\\;\\text{for}\\; x <0,\\\\\n  \\end{array}\n  \\right.\n  \\end{align*}\n  \\] where \\(b>0.\\)  First, consider the case of \\(x\\geq 0.\\) Maximizing\n\\[\n  \\begin{align*}\n  \\exp\\left(\\frac{2x - x^2b}{2b}\\right)\n  & = \\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n  \\end{align*}\n  \\] with respect to \\(x\\geq 0\\) yields \\[\n  \\begin{align*}\n  \\exp\\left(\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n  \\Leftrightarrow\n  x=\\frac{1}{b}\n  \\end{align*}\n  \\] Second, consider the case of \\(x<0.\\) Maximizing \\[\n  \\begin{align*}\n  \\exp\\left(\\frac{-2x - x^2b}{2b}\\right)\n  & = \\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right)\n  \\end{align*}\n  \\] with respect to \\(x < 0\\) yields \\[\n  \\begin{align*}\n  \\exp\\left(-\\frac{1}{b}x - \\frac{1}{2}x^2 \\right) \\cdot \\left(-\\frac{1}{b} - x \\right) &\\overset{!}{=} 0\n  \\Leftrightarrow\n  x=-\\frac{1}{b}\n  \\end{align*}\n  \\] Thus \\[\n  \\begin{align*}\n  \\frac{f(x)}{g(x|b)}\n  & \\leq\n  %\\left\\{\n  % \\begin{array}{ll}\n  % \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x\\geq 0\\\\\n  % \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b^2} b}{2b}\\right) &\\;\\text{for}\\; x <0\\\\\n  % \\end{array}\n  %\\right.\\\\[2ex]\n  \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2 \\frac{1}{b}  - \\frac{1}{b} }{2b}\\right) \\\\[2ex]\n  &=\n  \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{1}{2b^2}\\right)\n  \\end{align*}\n  \\] for \\(x\\in\\mathbb{R}.\\) The following code produces a plot of the graph of \\[\n  \\frac{f(x)}{g(x|b)} = \\sqrt{\\frac{2}{\\pi}}\\;b\\; \\exp\\left(\\frac{2|x| - x^2b}{2b}\\right)\n  \\] and marks the maximizing arguments \\(x=\\pm 1/b\\) for a given value of \\(b>0:\\)\n\n\n  b <- 1\n  myfun <- function(x, b){sqrt(2/pi) * exp( (2*abs(x) - x^2 * b)/(2 * b) )}\n  xx <- seq(-(1/b)*3, (1/b)*3, len = 500)\n  yy <- myfun(xx, b = b)\n  plot(x = xx, y = yy, type = \"l\")\n  abline(v=c(-1/b, 1/b), lty=2)\n\n\n\n\n\nTo show that the minimum of the bound (in \\(b>0\\)) is attained at \\(b=1,\\) we minimize \\[\nb\\;\\exp\\left(\\frac{1}{2b^2}\\right)\n\\] with respect to \\(b>0,\\) which yields \\[\n\\begin{align*}\n1\\;\\exp\\left(\\frac{1}{2b^2}\\right) + b\\;\\exp\\left(\\frac{1}{2b^2}\\right)\\cdot \\left(-2\\frac{1}{2b^3}\\right) &\\overset{!}{=} 0\\\\[2ex]\n1\\; - 2\\frac{1}{2b^2} &= 0\\\\[2ex]\nb &= 1,\n\\end{align*}\n\\] where we used that \\(\\exp\\left(\\frac{1}{2b^2}\\right)>0,\\) which allows us to multiply both sides by \\(1/\\exp\\left(\\frac{1}{2b^2}\\right),\\) and where the second solution \\(b=-1\\) does not fulfill the requirement that \\(b>0.\\) The following code produces a plot of the graph of \\[\nb\\;\\exp\\left(\\frac{1}{2b^2}\\right).\n\\]\n\n\nmyfun2 <- function(b){b * exp( 1/(2*b^2) )}\nbb <- seq(.5, 2, len = 500)\nyy <- myfun2(b = bb)\nplot(x = bb, y = yy, type = \"l\")\nabline(v = 1, lty=2)\n\n\n\n\n\n\nSolutions of Exercise 2.\n\\[\n\\begin{align*}\nP\\left(X \\leq h (Y)\\right)\n&=\\mathbb{E}\\left(1_{(X \\leq h (Y))}\\right)\\\\[2ex]\n&=\\mathbb{E}\\left[\\mathbb{E}\\left(1_{(X \\leq h (Y))}|Y\\right)    \\right]\\\\[2ex]\n&=\\mathbb{E}\\left[P\\left(1_{(X \\leq h (Y))}=1|Y\\right)\\cdot 1 + 0\\right]\\\\[2ex]\n&=\\mathbb{E}\\left[\\;\\;P\\left(X \\leq h (Y)|Y\\right)\\;\\;\\cdot\\; 1\\; + 0\\right]\\\\[2ex]\n&=\\mathbb{E}\\left[\\;\\;\\int_{-\\infty}^{h(Y)} f(x)dx \\right]\\\\[2ex]\n&=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\;\\;\\int_{-\\infty}^{h(Y)} f(x)dx\\right| Y \\right]\\right]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\;\\;\\left(\\int_{-\\infty}^{h (y)} f(x) dx \\right) \\quad g(y) dy\n\\end{align*}\n\\]\n\n\nSolutions of Exercise 3.\nOne can provide a graphical solution as following:\n\nAlternatively, one can do the following derivations:\n\\[\n\\begin{align*}\nP\\left(U\\leq f_X(Y)\\right)\n&=\\int_a^b\\left(\\int_0^{f_X(y)}\\frac{1}{m}du\\right)\\frac{1}{b-a}dy\\\\[2ex]\n&=\\frac{1}{m}\\cdot\\frac{1}{b-a}\\int_a^b\\left(\\int_0^{f_X(y)}\\;1\\;du\\right)\\;dy\\\\[2ex]\n&=\\frac{1}{m}\\cdot\\frac{1}{b-a}\\int_a^b f_X(y) dy\\\\[2ex]\n&=\\frac{1}{m}\\cdot\\frac{1}{b-a}\\cdot 1\n\\end{align*}\n\\]\n\n\nSolutions of Exercise 4.\n\nDeriving the probability of accepting \\(Y\\) from a simulation \\[\n(Y,U)\\sim\\mathcal{U}\\left(\\left\\{(y,u)|y\\in \\operatorname{supp}(f_X)\\;\\text{and}\\;0\\leq u\\leq Mg(y)\\right\\}\\right).\n\\] Since we accept \\(Y\\) only if \\(U\\leq f_X(Y),\\) \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(U\\leq f_X(Y)\\right),\n\\end{align*}\n\\] where \\(U\\sim\\mathcal{U}\\left[0,Mg(Y)\\right].\\)  Standardizing the uniform distribution yields \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=P\\left(\\frac{U}{Mg(Y)}\\leq \\frac{f_X(Y)}{Mg(Y)}\\right),\\\\[2ex]\n\\end{align*}\n\\] where dividing by \\(Mg(Y)\\) is allowed, since \\(Mg(y)>0\\) for all \\(y\\in\\operatorname{supp}(f_X).\\)  Now, using that \\[\n\\frac{U}{Mg(Y)} \\sim\\mathcal{U}\\left[0,1\\right],\n\\] we have \\[\n\\begin{align*}\nP\\left(\\text{Accepting }Y\\right)\n&=\\int_{-\\infty}^{\\infty}\\int_0^{\\frac{f_X(y)}{Mg(y)}} 1 \\; du\\; g(y) \\;dy\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty}\\;\\;\\;\\frac{f_X(y)}{Mg(y)}\\;\\; g(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\\int_{-\\infty}^{\\infty}\\;f_X(y) \\;\\;dy\\\\[2ex]\n&=\\frac{1}{M}\n\\end{align*}\n\\] \nShowing that \\(M\\geq 1:\\) \\[\n  \\begin{align*}\n  f_X(x) & \\leq M g(x)\\quad \\text{for all }\\quad x\\in\\operatorname{supp}(f_X)\\\\[2ex]\n  \\int f_X(x) dx & \\leq \\int M g(x)dx = M \\int  g(x)dx\\\\[2ex]\n  1              & \\leq M \\cdot 1\\\\[2ex]\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#exercises",
    "href": "Ch3_Bootstrap.html#exercises",
    "title": "3  The Bootstrap",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider the empirical distribution function \\[\nF_n(x) = \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] for a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} F.\n\\]\n\nDerive the exact distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nDerive the asymptotic distribution of \\(F_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nShow that \\(F_n(x)\\) is a point-wise (weakly) consistent estimator of \\(F(x)\\) for each given \\(x\\in\\mathbb{R}\\).\n\n\n\nExercise 2.\n\n\n\n\n\n\nTip\n\n\n\nExercise 1 shows that the empirical distribution function is a point-wise consistent estimator for each given \\(x\\in\\mathbb{R}.\\) However, point-wise consistency generally does not imply uniformly consistency for all \\(x\\in\\mathbb{R},\\) and therefore the Clivenko-Cantelli (Theorem 3.1) is so famous.\nThis exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.\n\n\nPoint-wise convergence of a function \\(g_n(x),\\) i.e., \\[\n|g_n(x) - g(x)|\\to 0\n\\] for each \\(x\\in\\mathcal{X}\\subset\\mathbb{R}\\) as \\(n\\to\\infty\\) generally does not imply uniform convergence, i.e., \\[\n\\sup_{x\\in\\mathcal{X}}|g_n(x) - g(x)|\\to 0\n\\] as \\(n\\to\\infty.\\)\nShow this by providing an example for \\(g_n\\) which converges point-wise, but not uniformly for \\(x\\in\\mathcal{X}\\).\n\n\n\nExercise 3.\nConsider the following setup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(\\mathbb{V}(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\n\nDerive the classic confidence interval for \\(\\mu\\) using the asymptotic normality of the estimator \\(\\bar{X}.\\) Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of \\(n=20\\) and,\n\n\nPart 1: For \\(F\\) being the normal distribution with \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\), and\nPart 2: For \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom.\n\n\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-\\(t\\) confidence interval.\n\n\n\nExercise 4.\n\nLet \\(\\mathcal{S}_n = \\{Y_1 , \\dots, Y_n\\}\\) be a random sample from a population with mean \\(\\mu\\), variance \\(\\sigma^2,\\) and distribution function \\(F.\\) Let \\(F_n\\) be the empirical distribution function. Let \\(\\bar{Y}\\) be the sample mean for \\(\\mathcal{S}_n.\\) Let \\(\\mathcal{S}^*_n = \\{Y_1^∗,\\dots, Y_n^∗\\}\\) be a random sample taken independently and with replacement from \\(\\mathcal{S}_n.\\) Let \\(\\bar{Y}^*\\) be the sample mean for \\(\\mathcal{S}^*_n.\\)\n\nShow that \\[\n\\mathbb{E}^*(\\bar{Y}^*) = \\bar{Y}\n\\]\nShow that \\[\n\\mathbb{E}(\\bar{Y}^*) = \\mu\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#references",
    "href": "Ch3_Bootstrap.html#references",
    "title": "3  The Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\nDavison, Anthony Christopher, and David Victor Hinkley. 2013. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\n\n\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Science.\n\n\nHorowitz, Joel L. 2001. “The Bootstrap.” In Handbook of Econometrics, 5:3159–3228.\n\n\nShao, Jun, and Dongsheng Tu. 1996. The Jackknife and Bootstrap. Springer Science."
  },
  {
    "objectID": "Ch3_Bootstrap.html#solutions",
    "href": "Ch3_Bootstrap.html#solutions",
    "title": "3  The Bootstrap",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\n\n(a)\nThe exact point-wise distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\n\\[\n\\begin{align*}\nF_n(x)\n& = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\\n\\Rightarrow nF_n(x)\n& = \\sum_{i=1}^n 1_{(X_i\\leq x)} \\sim \\mathcal{Binom}\\left(n,p=F(x)\\right),\n\\end{align*}\n\\] since \\(1_{(X_i\\leq x)}\\) is a Bernoulli random variable with parameter \\[\n\\begin{align*}\np\n& = P(1_{(X_i\\leq x)} = 1)\n& = P(X_i \\leq x)\n& = F(x).\n\\end{align*}\n\\]\n\n\n(b)\n\nFrom (a), we have that \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x)) &= nF(x)\\\\\n\\Leftrightarrow\\quad  \\mathbb{E}(F_n(x)) &= F(x)\n\\end{align*}\n\\] and that \\[\n\\begin{align*}\n\\mathbb{V}(nF_n(x)) &= nF(x)(1-F(x))\\\\\n\\Leftrightarrow \\quad \\mathbb{V}(F_n(x)) &= \\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\]\nMoreover, since \\(F_n(x) = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\) is an average over i.i.d. random variables \\(1_{(X_1\\leq x)},\\dots,1_{(X_n\\leq x)},\\) the standard CLT implies \\[\n\\frac{F_n(x)-F(x)}{\\sqrt{\\frac{F(x)(1-F(x))}{n}}}\\to_d\\mathcal{N}(0,1).\n\\] Or with a slight abuse of notation: \\[\nF_n(x)\\overset{a}{\\sim}\\mathcal{N}\\left(F(x),\\frac{F(x)(1-F(x))}{n}\\right).\n\\]\n\n\n(c)\nThe mean squared error between \\(F_n(x)\\) and \\(F(x)\\) is given by \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n&= \\mathbb{E}\\left((F_n(x)-F(x))^2\\right)\\\\[2ex]\n&= \\mathbb{V}(F_n(x)) + \\left(\\mathbb{E}(F_n(x))-F(x)\\right)^2.\n\\end{align*}\n\\] It follows from our previous results that for each \\(x\\in\\mathbb{R}\\) \\[\n\\mathbb{V}(F_n(x)) = \\frac{F(x)(1-F(x))}{n} \\to 0\n\\] as \\(n\\to\\infty,\\) and that \\[\n\\mathbb{E}(F_n(x)) -F(x) = 0\n\\] for all \\(n.\\) Therefore, \\[\n\\operatorname{MSE}(F_n(x)) = \\mathbb{V}(F_n(x)) \\to 0\n\\] as \\(n\\to\\infty.\\) Thus we can conclude that \\(F_n(x)\\) converges in the mean-square sense to \\(F(x)\\) for each \\(x\\in\\mathbb{R},\\) \\[\nF_n(x)\\to_{ms} F(x)\n\\] as \\(n\\to\\infty.\\)\nSince convergence in the mean square sense implies convergence in probability, we also have that for each \\(x\\in\\mathbb{R}\\) \\[\nF_n(x)\\to_{p} F(x)\n\\] as \\(n\\to\\infty\\) which shows that \\(F_n(x)\\) is weakly consistent for \\(F(x)\\) for each \\(x\\in\\mathbb{R}.\\)\n\n\n\nSolutions of Exercise 2.\n\n\n\n\n\n\nTip\n\n\n\nAnother, equivalent way to define uniform convergence:\n\\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if for every \\(\\varepsilon>0,\\) there exists an \\(N\\) such that \\[\n|g_n(x) - g(x)| < \\varepsilon\n\\] for all \\(n\\geq N\\) and for all \\(x\\in\\mathcal{X}.\\)\nI.e., \\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if it is possible to draw an \\(\\varepsilon\\)-band around the graph of \\(g(x)\\) that contains all of the graphs of \\(g_n(x)\\) for large enough \\(n.\\)\n\n\nExample 1: \\(\\mathcal{X}=\\mathbb{R}\\) The function \\[\ng_n(x) = x\\left(1+\\frac{1}{n}\\right)\n\\] converges point-wise to \\[\ng(x)=x,\n\\] since \\[\n|g_n(x)-g(x)|=\\frac{|x|}{n}%\\to 0\\quad \\text{as}\\quad n\\to\\infty.\n\\] converges to zero as \\(n\\to\\infty\\) for each given \\(x\\in\\mathcal{X}.\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in\\mathbb{R}}|g_n(x)-g(x)|=\\sup_{x\\in\\mathbb{R}}\\frac{|x|}{n}=\\infty\\neq 0\n\\] for each \\(n.\\)\nNote that for a small \\(\\varepsilon> 0,\\) an \\(\\varepsilon\\)-band around \\(g(x) = x\\) fails to capture the graphs of \\(g_n(x)=x(1+1/n).\\)\nExample 2: \\(\\mathcal{X}=(0,1)\\) The function \\[\ng_n(x) = x^n\n\\] converges point-wise to \\[\ng(x)=0,\n\\] since \\[\n|g_n(x)-g(x)|=x^n\n\\] converges to zero as \\(n\\to\\infty\\) for each given \\(x\\in(0,1).\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in(0,1)}|g_n(x)-g(x)|=\\sup_{x\\in(0,1)}x^n=1\\neq 0\n\\] for each \\(n.\\)\nNote that for a small \\(\\varepsilon> 0,\\) an \\(\\varepsilon\\)-band around \\(g(x) = 0\\) fails to capture the graphs of \\(g_n(x)=x^n.\\)\n\n\nSolutions of Exercise 3.\nLink to the video: HERE\n\n(a) Part 1:\nSetup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(\\mathbb{V}(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\nIf \\(F\\) is a normal distribution:\n\\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\sim \\mathcal{N}(0,1)\\quad\\text{for all}\\;n.\n\\end{array}\n\\]\nFor non-normal distributions \\(F\\) we have by the classic CLT: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nUsually, we do not know \\(\\sigma\\) and have to estimate this parameter using a consistent estimator such as \\(s^2=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), where \\(s\\to_p\\sigma\\) as \\(n\\to\\infty\\).\nThen by Slusky’s Theorem (allows to combine “\\(\\to_d\\)” and “\\(\\to_p\\)” statements) we have that: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{s}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nThe classic confidence interval is then based on the above (asymptotic) normality result: \\[\n\\operatorname{CI}_{\\operatorname{classic},n}=\\left[\\bar{X}_n\\,-\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}},\\bar{X}_n\\,+\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}\\right],\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of the standard normal distribution. Alternatively, one can apply a “small-sample correction” by using the \\((1-\\alpha/2)\\)-quantile \\(t_{n-1, 1-\\alpha/2}\\) of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nFrom the above arguments it follows that: \\[\nP\\left(\\mu\\in \\operatorname{CI}_{\\operatorname{classic},n}\\right)\\to 1-\\alpha\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nLet us consider the finite-\\(n\\) (with \\(n=20\\)) performance of the classic confidence interval for the case where \\(F\\) is a normal distribution with mean \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\):\n\n##  Setup:\nn     <-   20 # Sample Size\nmean  <-    1 # Mean\nsdev  <-    2 # Standard Deviation\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rnorm(n=n, mean = mean, sd = sdev) \n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n\n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(a) Part 2: Classic Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nNow, we consider the finite-\\(n\\) performance of the classic confidence interval under the same setup as above, but for the case where \\(F\\) is a non-normal distribution, namely, a \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  \n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(b) Standard bootstrap confidence interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nLet’s generate an iid random sample \\(S_n\\) with \\(X_i\\sim\\chi^2_1\\) and the corresponding estimate \\(\\bar X_n\\):\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean:\n(X.bar <- mean(S_n))\n\n[1] 0.6737282\n\n\nThe standard bootstrap confidence interval is given by (see lecture script): \\[\n\\left[2\\bar{X}_n - \\hat{t}_{1-\\alpha/2}, 2\\bar{X}_n - \\hat{t}_{\\alpha/2}\\right],\n\\] where \\(\\hat{t}_{\\alpha/2}\\) and \\(\\hat{t}_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and \\((1-\\alpha/2)\\)-quantiles of the conditional distribution of \\(\\bar{X}_n^\\ast\\) given \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\}\\), i.e., of the bootrap distribution of \\(\\bar{X}_n^\\ast\\).\nIn the following we approximate the bootstrap distribution of \\(\\bar{X}_n^\\ast\\) using \\(m=1500\\) boostrap resamplings, compute the quantiles \\(\\hat{t}_{\\alpha/2}\\) and \\(\\hat{t}_{1-\\alpha/2}\\), and plot all of this:\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Boostrap draws of \\bar{X}_n^*:\nX.bar.bootstr.vec <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\n\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nt.1 <- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)\nt.2 <- quantile(X.bar.bootstr.vec, probs = alpha/2)\n## plot\nplot(ecdf(X.bar.bootstr.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-Distr. of \",bar(X)[n]^{\" *\"})))\nabline(v=c(t.1,t.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the standard bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Basic.Bootstr.lo <- 2*X.bar - t.1\nCI.Basic.Bootstr.up <- 2*X.bar - t.2\n\n## Re-labeling of otherwise false names:\nattr(CI.Basic.Bootstr.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Basic.Bootstr.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)\n\n     2.5%     97.5% \n0.1545224 1.0425228 \n\n\nNow, we can investigate the finite-\\(n\\) performance of the standard bootstrap confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Basic.Bstr.lo.vec <- rep(NA, B)\nCI.Basic.Bstr.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimate:\n  X.bar.MC      <- mean(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  ## (1-alpha/2)-quantile:\n  t.1.MC <- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)\n  t.2.MC <- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Basic.Bstr.lo.vec[b] <- 2*X.bar.MC - t.1.MC\n  CI.Basic.Bstr.up.vec[b] <- 2*X.bar.MC - t.2.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Basic.Bstr.lo.vec<=mean & mean<=CI.Basic.Bstr.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Basic Bootrap 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(c) Bootstrap-\\(t\\) confidence interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nThe bootstrap-t confidence interval is given by (see lecture script): \\[\n\\left[\\bar{X}_n-\\hat{\\tau}_{1-\\alpha/2}\\hat\\sigma,  \\bar{X}_n-\\hat{\\tau}_{\\alpha/2}\\hat\\sigma\\right],\n\\] where \\(\\hat\\sigma=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), and where \\(\\hat{\\tau}_{\\alpha/2}\\) and \\(\\hat{\\tau}_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and the \\((1-\\alpha/2)\\)-quantiles of the bootstrap distribution of: \\[\n\\frac{\\bar{X}_n^\\ast-\\bar{X}_n}{\\hat\\sigma^\\ast}.\n\\]\nIn the following we approximate the bootstrap distribution of \\((\\bar{X}_n^\\ast-\\bar{X}_n)/\\hat\\sigma^\\ast\\), compute the quantiles \\(\\hat{\\tau}_{\\alpha/2}\\) and \\(\\hat{\\tau}_{1-\\alpha/2}\\), and plot all of this:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean and sd:\nX.bar   <- mean(S_n)\nsd.hat  <- sd(S_n)\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Compute boostrap draws of (\\bar{X}_n^*-\\bar{X}_n)/\\hat{\\sigma}^\\ast:\nX.bar.bootstr.vec    <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\nsd.bootstr.vec       <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)\n##\nBootstr.t.sample.vec <- (X.bar.bootstr.vec - X.bar)/sd.bootstr.vec\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\ntau.1 <- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)\ntau.2 <- quantile(Bootstr.t.sample.vec, probs = alpha/2)\n## plot\nplot(ecdf(Bootstr.t.sample.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-t-Distr. of \",\n          (bar(X)[n]^{\" *\"}-bar(X)[n])/hat(sigma)^{\"*\"})))\nabline(v=c(tau.1,tau.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the basic bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Bstr.t.lo <- X.bar - tau.1 * sd.hat\nCI.Bstr.t.up <- X.bar - tau.2 * sd.hat\n\n## Re-labeling of otherwise false names:\nattr(CI.Bstr.t.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Bstr.t.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Bstr.t.lo, CI.Bstr.t.up)\n\n     2.5%     97.5% \n0.3052027 2.0241321 \n\n\nLet us investigate the finite-\\(n\\) performance of the bootstrap-t confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Bstr.t.lo.vec <- rep(NA, B)\nCI.Bstr.t.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC      <- mean(S_n.MC)\n  sd.MC         <- sd(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  sd.bootstr.MC.vec    <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)\n  ## Make it a \"Bootstrap-t\" sample:\n  Bootstr.t.MC.vec <- (X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec\n  ## (1-alpha/2)-quantile:\n  tau.1.MC <- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)\n  tau.2.MC <- quantile(Bootstr.t.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Bstr.t.lo.vec[b] <- X.bar.MC - tau.1.MC * sd.MC\n  CI.Bstr.t.up.vec[b] <- X.bar.MC - tau.2.MC * sd.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Bstr.t.lo.vec<=mean & mean<=CI.Bstr.t.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Bootrap-t 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], \n       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], \n       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\nSolutions of Exercise 4.\nLink to the video: HERE\n\n(a)\n\\[\n\\begin{align*}\n\\mathbb{E}^*(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\left.\\bar{Y}^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(\\left.Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\frac{1}{n} Y_i\n= \\bar{Y}\n\\end{align*}\n\\] since \\((Y_i^*|\\mathcal{S}_n)\\in\\{Y_1,\\dots,Y_n\\}\\) and \\(P(Y_j^*=Y_i|\\mathcal{S}_n)=\\frac{1}{n}\\) for each \\(i,j\\in 1,\\dots,n.\\)\n\n\n(b)\n\\[\n\\begin{align*}\n\\mathbb{E}(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mu\n\\end{align*}\n\\] since \\(Y_i^*\\sim Y_i\\sim F.\\)"
  },
  {
    "objectID": "Ch3_Bootstrap.html#exercise-3.-1",
    "href": "Ch3_Bootstrap.html#exercise-3.-1",
    "title": "3  The Bootstrap",
    "section": "Exercise 3.",
    "text": "Exercise 3.\n\n(a) Part 1:\nSetup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(\\mathbb{V}(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\nIf \\(F\\) is a normal distribution:\n\\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\sim \\mathcal{N}(0,1)\\quad\\text{for all}\\;n.\n\\end{array}\n\\]\nFor non-normal distributions \\(F\\) we have by the classic CLT: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nUsually, we do not know \\(\\sigma\\) and have to estimate this parameter using a consistent estimator such as \\(s^2=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), where \\(s\\to_p\\sigma\\) as \\(n\\to\\infty\\).\nThen by Slusky’s Theorem (allows to combine “\\(\\to_d\\)” and “\\(\\to_p\\)” statements) we have that: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{s}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nThe classic confidence interval is then based on the above (asymptotic) normality result: \\[\n\\operatorname{CI}_{\\operatorname{classic},n}=\\left[\\bar{X}_n\\,-\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}},\\bar{X}_n\\,+\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}\\right],\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of the standard normal distribution. Alternatively, one can apply a “small-sample correction” by using the \\((1-\\alpha/2)\\)-quantile \\(t_{n-1, 1-\\alpha/2}\\) of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nFrom the above arguments it follows that: \\[\nP\\left(\\mu\\in \\operatorname{CI}_{\\operatorname{classic},n}\\right)\\to 1-\\alpha\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nLet us consider the finite-\\(n\\) (with \\(n=20\\)) performance of the classic confidence interval for the case where \\(F\\) is a normal distribution with mean \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\):\n\n##  Setup:\nn     <-   20 # Sample Size\nmean  <-    1 # Mean\nsdev  <-    2 # Standard Deviation\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rnorm(n=n, mean = mean, sd = sdev) \n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n\n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(a) Part 2: Classic Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nNow, we consider the finite-\\(n\\) performance of the classic confidence interval under the same setup as above, but for the case where \\(F\\) is a non-normal distribution, namely, a \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  \n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(b) Standard bootstrap confidence interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nLet’s generate an iid random sample \\(S_n\\) with \\(X_i\\sim\\chi^2_1\\) and the corresponding estimate \\(\\bar X_n\\):\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean:\n(X.bar <- mean(S_n))\n\n[1] 0.6737282\n\n\nThe standard bootstrap confidence interval is given by (see lecture script): \\[\n\\left[2\\bar{X}_n - \\hat{t}_{1-\\alpha/2}, 2\\bar{X}_n - \\hat{t}_{\\alpha/2}\\right],\n\\] where \\(\\hat{t}_{\\alpha/2}\\) and \\(\\hat{t}_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and \\((1-\\alpha/2)\\)-quantiles of the conditional distribution of \\(\\bar{X}_n^\\ast\\) given \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\}\\), i.e., of the bootrap distribution of \\(\\bar{X}_n^\\ast\\).\nIn the following we approximate the bootstrap distribution of \\(\\bar{X}_n^\\ast\\) using \\(m=1500\\) boostrap resamplings, compute the quantiles \\(\\hat{t}_{\\alpha/2}\\) and \\(\\hat{t}_{1-\\alpha/2}\\), and plot all of this:\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Boostrap draws of \\bar{X}_n^*:\nX.bar.bootstr.vec <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\n\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nt.1 <- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)\nt.2 <- quantile(X.bar.bootstr.vec, probs = alpha/2)\n## plot\nplot(ecdf(X.bar.bootstr.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-Distr. of \",bar(X)[n]^{\" *\"})))\nabline(v=c(t.1,t.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the standard bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Basic.Bootstr.lo <- 2*X.bar - t.1\nCI.Basic.Bootstr.up <- 2*X.bar - t.2\n\n## Re-labeling of otherwise false names:\nattr(CI.Basic.Bootstr.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Basic.Bootstr.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)\n\n     2.5%     97.5% \n0.1545224 1.0425228 \n\n\nNow, we can investigate the finite-\\(n\\) performance of the standard bootstrap confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Basic.Bstr.lo.vec <- rep(NA, B)\nCI.Basic.Bstr.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimate:\n  X.bar.MC      <- mean(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  ## (1-alpha/2)-quantile:\n  t.1.MC <- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)\n  t.2.MC <- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Basic.Bstr.lo.vec[b] <- 2*X.bar.MC - t.1.MC\n  CI.Basic.Bstr.up.vec[b] <- 2*X.bar.MC - t.2.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Basic.Bstr.lo.vec<=mean & mean<=CI.Basic.Bstr.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Basic Bootrap 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(c) Bootstrap-\\(t\\) confidence interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nThe bootstrap-t confidence interval is given by (see lecture script): \\[\n\\left[\\bar{X}_n-\\hat{\\tau}_{1-\\alpha/2}\\hat\\sigma,  \\bar{X}_n-\\hat{\\tau}_{\\alpha/2}\\hat\\sigma\\right],\n\\] where \\(\\hat\\sigma=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), and where \\(\\hat{\\tau}_{\\alpha/2}\\) and \\(\\hat{\\tau}_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and the \\((1-\\alpha/2)\\)-quantiles of the bootstrap distribution of: \\[\n\\frac{\\bar{X}_n^\\ast-\\bar{X}_n}{\\hat\\sigma^\\ast}.\n\\]\nIn the following we approximate the bootstrap distribution of \\((\\bar{X}_n^\\ast-\\bar{X}_n)/\\hat\\sigma^\\ast\\), compute the quantiles \\(\\hat{\\tau}_{\\alpha/2}\\) and \\(\\hat{\\tau}_{1-\\alpha/2}\\), and plot all of this:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean and sd:\nX.bar   <- mean(S_n)\nsd.hat  <- sd(S_n)\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Compute boostrap draws of (\\bar{X}_n^*-\\bar{X}_n)/\\hat{\\sigma}^\\ast:\nX.bar.bootstr.vec    <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\nsd.bootstr.vec       <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)\n##\nBootstr.t.sample.vec <- (X.bar.bootstr.vec - X.bar)/sd.bootstr.vec\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\ntau.1 <- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)\ntau.2 <- quantile(Bootstr.t.sample.vec, probs = alpha/2)\n## plot\nplot(ecdf(Bootstr.t.sample.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-t-Distr. of \",\n          (bar(X)[n]^{\" *\"}-bar(X)[n])/hat(sigma)^{\"*\"})))\nabline(v=c(tau.1,tau.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the basic bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Bstr.t.lo <- X.bar - tau.1 * sd.hat\nCI.Bstr.t.up <- X.bar - tau.2 * sd.hat\n\n## Re-labeling of otherwise false names:\nattr(CI.Bstr.t.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Bstr.t.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Bstr.t.lo, CI.Bstr.t.up)\n\n     2.5%     97.5% \n0.3052027 2.0241321 \n\n\nLet us investigate the finite-\\(n\\) performance of the bootstrap-t confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Bstr.t.lo.vec <- rep(NA, B)\nCI.Bstr.t.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC      <- mean(S_n.MC)\n  sd.MC         <- sd(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  sd.bootstr.MC.vec    <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)\n  ## Make it a \"Bootstrap-t\" sample:\n  Bootstr.t.MC.vec <- (X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec\n  ## (1-alpha/2)-quantile:\n  tau.1.MC <- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)\n  tau.2.MC <- quantile(Bootstr.t.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Bstr.t.lo.vec[b] <- X.bar.MC - tau.1.MC * sd.MC\n  CI.Bstr.t.up.vec[b] <- X.bar.MC - tau.2.MC * sd.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Bstr.t.lo.vec<=mean & mean<=CI.Bstr.t.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Bootrap-t 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], \n       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], \n       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\nExercise 4.\n\n(a)\n\\[\n\\begin{align*}\n\\mathbb{E}^*(\\bar{Y}^*)\n& = \\mathbb{E}(\\bar{Y}^*|\\mathcal{S}_n)\\\\[2ex]\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^*|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(Y_i^*|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(Y_i^*|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\frac{1}{n} Y_i\n& = \\bar{Y}\n\\end{align*}\n\\] since \\(P(Y_j^*=Y_i)=\\frac{1}{n}\\) for each \\(i,j\\in 1,\\dots,n.\\)\n\n\n(b)\n\\[\n\\begin{align*}\n\\mathbb{E}(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mu\n\\end{align*}\n\\] since \\(Y_i^*\\sim Y_i\\sim F.\\)"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#exercises",
    "href": "Ch4_MaximumLikelihood.html#exercises",
    "title": "4  Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nProgram the Newton-Raphson algorithm for a numerical computation of the ML estimate \\(\\hat\\theta\\) of the parameter \\(\\theta=P(\\text{Coin}=\\texttt{HEAD})\\) in our coin toss example of this chapter. Replicate the results shown in Table 4.1.\n\n\nExercise 2.\nAssume an i.i.d. random sample \\(X_1,\\dots,X_n\\) from an exponential distribution, i.e. the underlying density of \\(X_i\\) is given by \\[\nf(x|\\theta)=\\theta\\exp(-\\theta x).\n\\] We then have \\[\n\\mu:=\\mathbb{E}(X_i)=\\frac{1}{\\theta}\n\\] as well as \\[\nVar(X_i)=\\frac{1}{\\theta^2}.\n\\]\n\nWhat is the log-likelihood function for the i.i.d. random sample \\(X_1,\\dots,X_n\\)?\nDerive the maximum likelihood (ML) estimator \\(\\hat\\theta_n\\) of \\(\\theta\\).\nFrom maximum likelihood theory we know that \\[\n(\\hat\\theta_n-\\theta)\\to_d \\mathcal{N}\\left(0,\\frac{1}{n \\mathcal{J}(\\theta)}\\right)\n\\] Derive the expression for the Fischer information \\(\\mathcal{I}(\\theta)=n\\mathcal{J}(\\theta)\\). Use the Fisher information to give the explicit formula for the asymptotic distribution of \\(\\hat\\theta_n\\).\n\n\n\nExercise 3.\n\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Unif}(0,\\theta).\\)\n\nWhat is the likelihood function?\nWhat is the maximum likelihood estimator of \\(\\theta\\)?\n\n\n\nExercise 4.\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Poisson}(\\lambda).\\) That is \\(X\\sim f\\) with density function \\[\nf(x|\\lambda) = \\frac{\\lambda^x \\exp(-\\lambda)}{x!}.\n\\]\n\nFind the maximum likelihood estimator, \\(\\hat{\\lambda},\\) of \\(\\lambda.\\)\nLet \\(0<\\lambda\\leq 4.\\) Find the maximum likelihood estimator, \\(\\hat{P}(X=4),\\) of \\(P(X=4).\\)"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#references",
    "href": "Ch4_MaximumLikelihood.html#references",
    "title": "4  Maximum Likelihood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017. “Overcoming Catastrophic Forgetting in Neural Networks.” Proceedings of the National Academy of Sciences 114 (13): 3521–26.\n\n\nMartens, James. 2020. “New Insights and Perspectives on the Natural Gradient Method.” The Journal of Machine Learning Research 21 (1): 5776–5851.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 1–25."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#references",
    "href": "Ch5_EMAlgorithmus.html#references",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "References",
    "text": "References\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "href": "Ch5_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.1 Motivation: Cluster Analysis using Gaussian Mixture Models",
    "text": "5.1 Motivation: Cluster Analysis using Gaussian Mixture Models\nAs a data example we use the palmerpenguins data (Horst, Hill, and Gorman (2020)).\nThese data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (Figure 5.1). We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.\n\n\n\nFigure 5.1: Cheeky penguin in action.\n\n\nThe following code chunk prepares the data\n\n\n\n\n\n\nCaution\n\n\n\nWe have the information about the different penguin species (penguin_species) but in the following we pretend not to know this information.\nWe want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (penguin_flipper) alone.\nAfterwards we can use the data in penguin_species to check how good our cluster analysis is.\n\n\n\nlibrary(\"tidyverse\", quietly = TRUE)\nlibrary(\"palmerpenguins\") # Penguin data \nlibrary(\"RColorBrewer\")   # nice colors\nlibrary(\"scales\")         # transparent colors: alpha()\n\ncol_v <- RColorBrewer::brewer.pal(n = 3, name = \"Set2\")\n\n## Vorbereitung der Daten:\npenguins <- palmerpenguins::penguins %>%  # penguin data\n  tidyr::as_tibble() %>%                  # 'tibble'-dataframe\n  dplyr::filter(species!=\"Adelie\") %>%    # remove penguin species 'Adelie' \n  droplevels() %>%                        # remove the non-used factor level\n  tidyr::drop_na() %>%                    # remove NAs\n  dplyr::mutate(species = species,        # rename variables \n                flipper = flipper_length_mm) %>% \n  dplyr::select(species, flipper)         # select variables \n\n##  \nn      <- nrow(penguins)                  # sample size\n\n## Variable 'Penguine_Art' aus penguins-Daten \"herausziehen\"\npenguin_species    <- dplyr::pull(penguins, species)\n\n## Variable 'penguin_flipper' aus penguins-Daten \"herausziehen\"\npenguin_flipper <- dplyr::pull(penguins, flipper)\n\n## Plot\n## Histogramm:\nhist(x = penguin_flipper, freq = FALSE, \n     xlab=\"Flipper-Length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))\n## Stipchart hinzufügen:\nstripchart(x = penguin_flipper, method = \"jitter\", \n           jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[3],.5), \n           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)\n\n\n\n\n\n\n\n\n\nClustering using Gaussian Mixture Distributions\n\nEstimate the Gaussian mixture distribution using the EM algorithm\nAssign the data points \\(x_i\\) to the group that maximizes the “posterior probability” (see Figure 5.2 and Section 5.3.2)\n\n\n\n\n\n\nFigure 5.2: Cluster analysis based on a mixture distribution with two weighted normal distributions.\n\n\n\n\nFigure Figure 5.2 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. Cluster result: 95% of the penguins could be correctly assigned - based only on their flipper lengths.\nThe following R codes can be used to reproduce the above cluster analysis (using the R package mclust) and Figure 5.2. We’ll learn everything about it in this chapter:\n\n## mclust R package:\n## Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(\"mclust\"))\n\n## Number of Groups\nG <- 2 \n\n## Schätzung des Gaußschen Mischmodells (per EM Algorithmus)\n## und Clusteranalyse\nmclust_obj <- mclust::Mclust(data       = penguin_flipper, \n                             G          = G, \n                             modelNames = \"V\", \n                             verbose    = FALSE)\n\n# summary(mclust_obj)\n# str(mclust_obj)\n\n## estimated group assignment \nclass <- mclust_obj$classification\n\n## Fraction of correct group assignments:\n# cbind(class, penguin_species)\nround(sum(class == as.numeric(penguin_species))/n, 2)\n\n## estimated means of the two Gaussian distributions\nmean_m <- t(mclust_obj$parameters$mean)\n\n## estimated variances (and possibly covariances) \ncov_l  <- list(\"Cov1\" = mclust_obj$parameters$variance$sigmasq[1], \n               \"Cov2\" = mclust_obj$parameters$variance$sigmasq[2])\n\n## estimated mixture weights (prior-probabilities) \nprop_v <- mclust_obj$parameters$pro\n\n## evaluating the Gaussian mixture density function \nnp      <- 100 # number of evaluation points\nxxd     <- seq(min(penguin_flipper)-3, \n               max(penguin_flipper)+5, \n               length.out = np)\n## mixture density\nyyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +\n           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n## single densities\nyyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]\nyyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n\n## Plot\nhist(x = penguin_flipper, xlab=\"Flipper length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))\nlines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))\nlines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)\nlines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)\nabline(v=203.1, lty=3)\nstripchart(penguin_flipper[class==1], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)\nstripchart(penguin_flipper[class==2], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#mclust-r-package",
    "href": "Ch5_EMAlgorithmus.html#mclust-r-package",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.2 mclust R package:",
    "text": "5.2 mclust R package:"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#cluster-analysis-using-gaussian-mixture-distributions",
    "href": "Ch5_EMAlgorithmus.html#cluster-analysis-using-gaussian-mixture-distributions",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.3 Cluster analysis using Gaussian mixture distributions",
    "text": "5.3 Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(“mclust”))"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#number-of-groups",
    "href": "Ch5_EMAlgorithmus.html#number-of-groups",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.4 Number of Groups",
    "text": "5.4 Number of Groups\nG <- 2"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#schätzung-des-gaußschen-mischmodells-per-em-algorithmus",
    "href": "Ch5_EMAlgorithmus.html#schätzung-des-gaußschen-mischmodells-per-em-algorithmus",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.5 Schätzung des Gaußschen Mischmodells (per EM Algorithmus)",
    "text": "5.5 Schätzung des Gaußschen Mischmodells (per EM Algorithmus)"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#und-clusteranalyse",
    "href": "Ch5_EMAlgorithmus.html#und-clusteranalyse",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.6 und Clusteranalyse",
    "text": "5.6 und Clusteranalyse\nmclust_obj <- mclust::Mclust(data = penguin_flipper, G = G, modelNames = “V”, verbose = FALSE)"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#estimated-group-assignment",
    "href": "Ch5_EMAlgorithmus.html#estimated-group-assignment",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "7.1 estimated group assignment",
    "text": "7.1 estimated group assignment\nclass <- mclust_obj$classification"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#fraction-of-correct-group-assignments",
    "href": "Ch5_EMAlgorithmus.html#fraction-of-correct-group-assignments",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "7.2 Fraction of correct group assignments:",
    "text": "7.2 Fraction of correct group assignments:"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#estimated-means-of-the-two-gaussian-distributions",
    "href": "Ch5_EMAlgorithmus.html#estimated-means-of-the-two-gaussian-distributions",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "8.1 estimated means of the two Gaussian distributions",
    "text": "8.1 estimated means of the two Gaussian distributions\nmean_m <- t(mclust_obj\\(parameters\\)mean)"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#estimated-variances-and-possibly-covariances",
    "href": "Ch5_EMAlgorithmus.html#estimated-variances-and-possibly-covariances",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "8.2 estimated variances (and possibly covariances)",
    "text": "8.2 estimated variances (and possibly covariances)\ncov_l <- list(“Cov1” = mclust_obj\\(parameters\\)variance\\(sigmasq[1],  \"Cov2\" = mclust_obj\\)parameters\\(variance\\)sigmasq[2])"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#estimated-mixture-weights-prior-probabilities",
    "href": "Ch5_EMAlgorithmus.html#estimated-mixture-weights-prior-probabilities",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "8.3 estimated mixture weights (prior-probabilities)",
    "text": "8.3 estimated mixture weights (prior-probabilities)\nprop_v <- mclust_obj\\(parameters\\)pro"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#evaluating-the-gaussian-mixture-density-function",
    "href": "Ch5_EMAlgorithmus.html#evaluating-the-gaussian-mixture-density-function",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "8.4 evaluating the Gaussian mixture density function",
    "text": "8.4 evaluating the Gaussian mixture density function\nnp <- 100 # number of evaluation points xxd <- seq(min(penguin_flipper)-3, max(penguin_flipper)+5, length.out = np) ## mixture density yyd <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))prop_v[1] + dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))prop_v[2] ## single densities yyd1 <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))prop_v[1] yyd2 <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))prop_v[2]"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#plot",
    "href": "Ch5_EMAlgorithmus.html#plot",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "8.5 Plot",
    "text": "8.5 Plot\nhist(x = penguin_flipper, xlab=“Flipper length (mm)”, main=“Penguins(Two Groups)”, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) abline(v=203.1, lty=3) stripchart(penguin_flipper[class==1], method = “jitter”, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(penguin_flipper[class==2], method = “jitter”, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)```"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "href": "Ch5_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions",
    "text": "5.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions\n\n5.2.1 Gaussian Mixture Models (GMM)\nWe denote a random variable \\(X\\) that follows a Gaussian mixed distribution as \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\]\nThe corresponding density function of a Gaussian mixture distribution is defined as follows: \\[\nf_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g,\\sigma_g)\n\\tag{5.1}\\]\n\nWeights: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) with \\(\\pi_g>0\\) and \\(\\sum_{g=1}^G\\pi_g=1\\)\nMeans: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) with \\(\\mu_g\\in\\mathbb{R}\\)\nStandard deviations: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) with \\(\\sigma_g>0\\)\nNormal density of group \\(g=1,\\dots,G\\): \\[\nf(x|\\mu_g,\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right)\n\\]\nUnknown parameters: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)\n\n\n\n5.2.2 Maximum Likelihood (ML) Estimation\nWe could try to estimate the unknown parameters \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) using the maximum likelihood method.\n\nI’ll say it right away: The attempt will fail.\n\n\nBasic Idea of ML Estimation\n\nAssumption: The data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) is a realization of a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] with \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}).\n\\]\n\n\nThat is, in a certain sense, the observed data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) “know” the unknown parameters \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) and we “only” have to elicit this information from them.\n\n\nEstimation Idea: Choose \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) such that \\(f_G(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) “optimally” fits the observed data \\(\\mathbf{x}\\).\nImplementation of the Estimation Idea: Maximize (with respect to \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\)) the likelihood function \\[\n\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\] Or maximize the log-likelihood function (simpler maximization) \\[\n\\begin{align*}\n%\\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)=\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n=&\\sum_{i=1}^n\\ln\\left(f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\\n=&\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe maximization must take into account the parameter constraints in Equation 5.1; namely, \\(\\sigma_g>0\\) and \\(\\pi_g>0\\) for all \\(g=1,\\dots,G\\) and \\(\\sum_{g=1}^G\\pi_g=1\\).\n\n\nThe maximizing parameter values \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\hat{\\boldsymbol{\\sigma}}\\) are the ML-Estimators:\n\\[\n(\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n\\]\n😒 Problems with singularities in numerical solutions: If one tries to solve the above maximization problem numerically with the help of the computer, one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities.\nFor real GMMs (i.e. \\(G>1\\)), problems with singularities occur very easily during a numerical maximization. This happens whenever one or more of the normal distribution component(s) try to describe only single data points. A Gaussian density function centered around a single data point \\(x_i,\\) i.e.  \\[\n\\mu_g=x_i\\quad\\text{and}\\quad  \\sigma_g\\to 0\n\\] will thereby assume very large density function values, \\[\nf(x_i|\\mu_g=x_i,\\sigma_g)\\to\\infty\\quad\\text{for}\\quad \\sigma_g\\to 0,\n\\] and thus maximize the log-likelihood in an undesirable way (see Figure 5.3). Such undesirable, trivial maximization solutions usually lead to implausible estimation results.\n\n\n\n\n\nFigure 5.3: Gaussian density with \\(\\mu_g=x_i\\) for \\(\\sigma_g\\to 0\\).\n\n\n\n\n🤓 Analytic solution: It is a bit tedious, but you can try to maximize the log-likelihood analytically. If you do this, you will get the following expressions: \\[\n\\begin{align*}\n\\hat\\pi_g&=\\frac{1}{n}\\sum_{i=1}^np_{ig},\\quad\n\\hat\\mu_g=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\[2ex]\n\\hat\\sigma_g&=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2}\n\\end{align*}\n\\tag{5.2}\\] for \\(g=1,\\dots,G.\\)\n\n\n\n\n\n\nNote\n\n\n\nDeriving the above expressions for \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) and \\(\\hat{\\pi}_g\\) is really a bit tedious (multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints) but in principle doable.\n\n\n\n🙈 However: The above expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) and \\(\\hat\\sigma_g\\) depend themselves on the unknown parameters \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\), because: \\[\np_{ig}=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\tag{5.3}\\] for \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G\\).\n\nThe probability \\(p_{ig}\\) in Equation 5.3 is called the posterior probability. The posterior probability \\(p_{ig}\\) is the probability that penguine \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).\nThe probability \\(\\pi_g\\) in Equation 5.3 is called the prior probability. The prior probability \\(\\pi_g\\) is the probability that a penguine \\(i\\), from which we know nothing about its flipper length, belongs to group \\(g\\).\n\nWe’ll discuss the prior and the posterior probability in more detail in Section 5.3.2.\nThus, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) do not allow direct estimation of the unknown parameters \\(\\pi_g\\), \\(\\mu_g\\), and \\(\\sigma_g\\).\n🥳 Solution: The EM Algorithm\n\n\n\n5.2.3 The EM Algorithm for GMMs\nHowever, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) suggest a simple iterative ML estimation procedure: An alternating estimation of \\[\np_{ig}\n\\] and \\[\n(\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g).\n\\]\n\n\n\n\n\n\nTip\n\n\n\nOnce you know \\(p_{ig},\\) you can compute \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) using Equation 5.2.\nOnce you know \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) you can compute \\(p_{ig}\\) using Equation 5.3.\n\n\nThe EM Algorithm:\n\nSet starting values \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\)\nFor \\(r=1,2,\\dots\\)\n\n(Expectation) Compute: \\[p_{ig}^{(r)}=\\frac{\\pi_g^{(r-1)}f(x_i|\\mu^{(r-1)}_g,\\sigma_g^{(r-1)})}{f_G(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\]\n(Maximization) Compute:\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence:  Stop if the value of the maximized log-likelihood function, \\(\\ell(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)}|\\mathbf{x})\\), does not change anymore substantially.\n\nThe above pseudo code is implemented in the following code chunk:\n\nlibrary(\"MASS\")\nlibrary(\"mclust\")\n\n## data:\nx <- cbind(penguin_flipper) # data [n x d]-dimensional. \nd <- ncol(x)                # dimension (d=1: univariat)\nn <- nrow(x)                # sample size\nG <- 2                      # number of groups\n\n## further stuff \nllk       <- matrix(NA, n, G)\np         <- matrix(NA, n, G)  \nloglikOld <- 1e07\ntol       <- 1e-05\nit        <- 0\ncheck     <- TRUE \n\n## EM Algorithm\n\n## 1. Starting values for pi, mu and sigma:\npi    <- rep(1/G, G)              # naive pi \nsigma <- array(diag(d), c(d,d,G)) # varianz = 1\nmu    <- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )\n\nwhile(check){\n  \n  ## 2.a Expectation step\n  for(g in 1:G){\n    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  p <- sweep(p, 1, STATS = rowSums(p), FUN = \"/\")\n  \n  ## 2.b Maximization step \n  par   <- mclust::covw(x, p, normalize = FALSE)\n  mu    <- par$mean\n  sigma <- par$S\n  pi    <- colMeans(p)\n  \n  ## 3. Check convergence \n  for(g in 1:G) {\n    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  loglik <- sum(log(rowSums(llk))) # current max. log-likelihood value\n  ##\n  diff      <- abs(loglik - loglikOld)/abs(loglik) # rate of change\n  loglikOld <- loglik\n  it        <- it + 1\n  ## Check whether rate of change is still large enough (> tol)?\n  check     <- diff > tol\n}\n\n## Estimation results:\nresults <- matrix(c(pi, mu, sqrt(sigma)), \n                  nrow = 3, \n                  ncol = 2, \n                  byrow = TRUE,\n                  dimnames = list(c(\"weights\", \n                                    \"means\", \n                                    \"standard-deviations\"),\n                                  c(\"group 1\", \n                                    \"group 2\"))) \n##\nresults %>% round(., 2)"
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#the-alternative-true-view-on-the-em-algorithm",
    "href": "Ch5_EMAlgorithmus.html#the-alternative-true-view-on-the-em-algorithm",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.3 The Alternative / True View on the EM Algorithm",
    "text": "5.3 The Alternative / True View on the EM Algorithm\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This principle is the real true contribution of the EM Algorithm. It allows the solution of various maximum likelihood problems - but we stay here with the estimation of GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n=\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)\n\\] directly.\nThe \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n5.3.1 Completion of the Data\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle there are \\(G=2\\) dimensional assignment vectors \\((z_{i1},z_{i2})\\) with\n\\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nCase of \\(G>2\\) groups:\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe assignments \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) where, however, it must hold true that \\[\n\\sum_{g=1}^Gz_{ig}=1.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor each data point \\(x_i\\) (each penguin \\(i\\)) there is only one group (hence \\(\\sum_{g=1}^Gz_{ig}=1\\)). This is an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures.\n\n\nUnfortunately, the assignments \\(z_{ig}\\) are unknown (latent). However, we nevertheless know something about these assignments. The weights \\(\\pi_1,\\dots,\\pi_G\\) of the Gaussian mixture distribution \\[\nf_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(f(\\cdot|\\mu_g,\\sigma_g)\\) in the total distribution \\(f_G\\).\nOn average, \\(\\pi_g\\cdot 100\\%\\) of the data points come from group \\(g.\\) Thus, we can consider the (latent) assignment \\(z_{ig}\\) as a realization of a discrete (binary) random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probability mass function \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] for each \\(i=1,\\dots,n.\\)\nThe condition \\[\n\\sum_{g=1}^Gz_{ig}=1\n\\] implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n5.3.2 Prior and Posterior Probabilities\nPrior Probability \\(\\pi_g\\) If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\(p_{ig}\\) If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig=1}|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 5.1 (Bayes’ Theorem) \\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}f(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\] Thus, the computation of \\(p_{ig}\\) in the Expectation-step of the EM algorithm (Section 5.2.3) is indeed a computation of an expectation."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#the-big-picture",
    "href": "Ch5_EMAlgorithmus.html#the-big-picture",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.4 The Big Picture",
    "text": "5.4 The Big Picture\nIf, in addition to the data points, \\(\\mathbf{x}=(x_1,\\dots,x_n),\\) we had also observed the group assignments, \\(\\mathbf{z}=(z_{11},\\dots,z_{nG}),\\) then we could establish the following likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (\\(\\ell\\)), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize. Since there is no sum inside the logarithm function, we can directly calculate the logarithm of the normal distribution. This simplifies the maximization problem considerably, since the normal distribution belongs to the exponential family.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, however, can calculate the conditional expected value: \\[\n\\mathbb{E}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)=\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\]\nThe following EM algorithm differs only in notation from the version already discussed in Section 5.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step. Moreover, the chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems.\nIn the following, the parameter vector \\((\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) will be denoted as \\(\\boldsymbol{\\theta}\\) for simplicity. \n\nSet starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nFor \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function, \\(\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\\), does not change anymore substantially.\n\n\nSynopsis\nThe average penguin doesn’t care about the EM Algorithm.\n\n\n\nFigure 5.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "href": "Ch5_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "5.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This principle is the real true contribution of the EM Algorithm. It allows the solution of various maximum likelihood problems - but we stay here with the estimation of GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n=\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)\n\\] directly.\nThe \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n5.3.1 Completion of the Data\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle there are \\(G=2\\) dimensional assignment vectors \\((z_{i1},z_{i2})\\) with\n\\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nCase of \\(G>2\\) groups:\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe assignments \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) where, however, it must hold true that \\[\n\\sum_{g=1}^Gz_{ig}=1.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor each data point \\(x_i\\) (each penguin \\(i\\)) there is only one group (hence \\(\\sum_{g=1}^Gz_{ig}=1\\)). This is an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures.\n\n\nUnfortunately, the assignments \\(z_{ig}\\) are unknown (latent). However, we nevertheless know something about these assignments. The weights \\(\\pi_1,\\dots,\\pi_G\\) of the Gaussian mixture distribution \\[\nf_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(f(\\cdot|\\mu_g,\\sigma_g)\\) in the total distribution \\(f_G\\).\nOn average, \\(\\pi_g\\cdot 100\\%\\) of the data points come from group \\(g.\\) Thus, we can consider the (latent) assignment \\(z_{ig}\\) as a realization of a discrete (binary) random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probability mass function \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] for each \\(i=1,\\dots,n.\\)\nThe condition \\[\n\\sum_{g=1}^Gz_{ig}=1\n\\] implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n5.3.2 Prior and Posterior Probabilities\nPrior Probability \\(\\pi_g\\) If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\(p_{ig}\\) If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 5.1 (Bayes’ Theorem) \\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}f(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\] Thus, the computation of \\(p_{ig}\\) in the Expectation-step of the EM algorithm (Section 5.2.3) is indeed a computation of an expectation.\n\n\n\n\n5.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points, \\(\\mathbf{x}=(x_1,\\dots,x_n),\\) we had also observed the group assignments, \\(\\mathbf{z}=(z_{11},\\dots,z_{nG}),\\) then we could establish the following likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (\\(\\ell\\)), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize. Since there is no sum inside the logarithm function, we can directly calculate the logarithm of the normal distribution. This simplifies the maximization problem considerably, since the normal distribution belongs to the exponential family.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, however, can calculate the conditional expected value: \\[\n\\mathbb{E}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)=\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\]\nThe following EM algorithm differs only in notation from the version already discussed in Section 5.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step. Moreover, the chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems.\nIn the following, the parameter vector \\((\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) will be denoted as \\(\\boldsymbol{\\theta}\\) for simplicity. \n\nSet starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nFor \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function, \\(\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\\), does not change anymore substantially."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#numeric-optimization",
    "href": "Ch4_MaximumLikelihood.html#numeric-optimization",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Numeric Optimization",
    "text": "4.2 Numeric Optimization\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\n\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or effectively zero, smaller than some convergence criterion.).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that minimization and maximization are essentially the same problems since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally, i.e. for \\(h\\approx 0,\\) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure 4.1.\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell\\) be a log-likelihood function with continuous first, \\(\\ell',\\) and second, \\(\\ell'',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell,\\) we try to find the root of \\(\\ell',\\) i.e. the value of \\(\\theta\\) such that \\[\n\\ell'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order condition of the optimization problem. We do so using a step-wise (\\(h\\) steps) optimization approach.\nInitialization: Let \\(\\theta_0\\) be our first guess of the root \\(\\theta.\\)\n\\(h\\)-Steps: Typically, \\(\\theta_0\\neq\\theta\\) and thus \\(\\ell'(\\theta_0)\\neq 0.\\) Therefore, we want to move from \\(\\theta_0\\) to a new root-candidate \\(\\theta_1\\) by doing an \\(h\\)-step update \\[\n\\theta_1 = \\theta_0 + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell'\\) around our first guess \\(\\theta_0\\) gives \\[\n\\begin{align*}\n\\ell'(\\theta_0 + h) & \\approx \\ell'(\\theta_0) + \\ell''(\\theta_0)h\n\\end{align*}\n\\] Thus, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell'(\\theta_0) + \\ell''(\\theta_0)h = 0\\\\[2ex]\n\\Rightarrow h = -\\frac{\\ell'(\\theta_0)}{\\ell''(\\theta_0)}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\theta_1 = \\theta_0 - \\frac{\\ell'(\\theta_0)}{\\ell''(\\theta_0)}.\n\\] Likewise, the \\(n\\)th root-candidate is \\[\n\\theta_n = \\theta_{n-1} - \\frac{\\ell'(\\theta_{n-1})}{\\ell''(\\theta_{n-1})};\n\\] see also Figure 4.2.\n\n\n\n\n\nFigure 4.2: A step in the Newton-Raphson root-finding method.\n\n\n\n\nOne can shown that if \\(\\ell'\\) is “well behaved” at its root \\(\\theta\\) (i.e. if \\(\\ell''(\\theta)\\neq 0\\) and if \\(\\ell'''(\\theta)\\) is finite and continuous at \\(\\theta\\)) and you start with \\(\\theta_0\\) “close enough” to the root \\(\\theta,\\) then \\(\\theta_n\\) will fastly converge to \\(\\theta.\\) Unfortunately, we don’t know if \\(\\ell'\\) is well behaved at \\(\\theta\\) until we know \\(\\theta,\\) and we don’t know beforehand how close is “close enough”. So, we cannot guarantee convergence of the Newton-Raphson algorithm.\n\nChecking convergence: Since we are expecting that \\(\\ell'(\\theta_n)\\to 0,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell'(\\theta_n)|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon>0.\\)\n\n\n\n\n\n\nPseudo-Code: Newton-Raphson Algorithm\n\n\n\n\\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} n=0                &  \\\\\n\\texttt{\\textbf{while }}  | \\ell'(\\theta_n) | >\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} n = n+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_n = \\theta_{n-1} - \\frac{\\ell'(\\theta_{i-1})}{\\ell''(\\theta_{n-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_n & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nIn actual practice, implementation of the Newton-Raphson algorithm can be tricky. We may have \\(\\ell''(\\theta_n)=0,\\) in which case the function looks locally like a straight line, with no solution to the Taylor series approximation \\[\n\\begin{align*}\n\\ell'(\\theta_n + h) & \\approx \\ell'(\\theta_n) + \\ell''(\\theta_n)h.\n\\end{align*}\n\\] In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on \\(\\ell'(\\theta_n).\\)\nIn other cases where \\(\\theta_n\\) is too far from the true maximizer \\(\\theta\\), the Taylor approximation may be so inaccurate that \\(\\ell(\\theta_{n+1})\\) is actually smaller than \\(\\ell(\\theta_{n}).\\) When this happens one may replace \\(\\theta_{n+1}\\) with \\((\\theta_{n+1}+\\theta_{n})/2\\) (or some other value between \\(\\theta_{n}\\) and \\(\\theta_{n+1}\\)) in the hope that a smaller step will produce better results.\n\n\n\n\n\nNewton-Raphson Algorithm: Coin-Flipping Example\nLet’s return to our earlier coin-flipping example.\nIf we observe, for instance, only one head \\(h=1\\) for a sample size of \\(n=5,\\) we already know that \\[\n\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2,\n\\] but let us, nevertheless, apply the Newton-Raphson algorithm.\nFirst and second derivatives of \\(\\ell\\) are \\[\n\\begin{align*}\n\\ell'(\\theta)&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\[2ex]\n\\ell''(\\theta) &= -\\dfrac{h}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{h}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}.\n\\end{align*}\n\\]\nWe consider a sample size of \\(n=5,\\) where one coin-flip resulted in H and the remaining four coin-flips resulted in T, i.e. \\(h=1\\) and \\(n-h=4.\\)\nSetting the tolerance level \\(\\varepsilon=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table Table 4.1. The numeric optimization solution is \\(\\hat\\theta_{ML} = 0.2\\) which equals the analytic solution.\n\n\nTable 4.1: Result of applying the Newton Raphson optimization algorithm to our coin flipping example for given data (\\(h=1,\\) sample size \\(n=5\\)).\n\n\n\n\n\n\n\n\n\\(n\\)\n\\(\\hat\\theta_n\\)\n\\(\\ell'(\\hat\\theta_n)\\)\n\\(\\ell'(\\hat\\theta_n)/\\ell''(\\hat\\theta_n)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#sec-MLAsymp",
    "href": "Ch4_MaximumLikelihood.html#sec-MLAsymp",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x|\\theta),\n\\] where the true (unknown, univariate) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta\\) is an “interior point” of \\(\\Theta\\) if \\(\\theta_l<\\theta<\\theta_u.\\)\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on a Taylor expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta\\) (see Equation 4.3). To derive this expression, we use the mean value theorem (Theorem 5.1).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\nBy the Mean Value Theorem (Theorem 5.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.3}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.3, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.4}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1 = 0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we have thus \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.5}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we have thus \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.6}\\]\nUsing Equation 4.5 and Equation 4.6, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.5.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{above mean zero result}]\\\\\n&=:\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-\\mathbb{E}(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.4), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.7}\\]\nFurther analysis requires us to study the statistic \\[\n\\frac{1}{n}\\ell_n''(\\psi_n).\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nFirst, the mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=0 - \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\[2ex]\n&=-\\mathcal{J}(\\theta),\n\\end{align*}\n\\tag{4.8}\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\left(-\\mathcal{J}(\\theta)\\right)\\\\[2ex]\n&=0.\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWe have now gathered different equivalent expressions for \\(\\mathcal{J}(\\theta).\\) The last one (Equation 4.8) shows that \\(\\mathcal{J}(\\theta)\\) is nothing but the Fisher information scaled by \\(\\frac{1}{n}\\): \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n%& = Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\[2ex]\n%& = Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\\\[2ex]\n%&=\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)\\\\[2ex]\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n} (-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathcal{I}(\\theta)\n\\end{align*}\n\\]\nNote: For multivariate (\\(p\\)-dimensional) parameters \\(\\theta,\\) the Fisher information \\((-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section 4.3.1); i.e. a special version of a Hesse matrix.\n\n\nSecond, the variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic figure}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\[2ex]\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\[2ex]\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator, i.e. \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator, i.e.  \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 Remember, we wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.7 not \\(\\frac{1}{n}\\ell_n''(\\theta).\\)\nLuckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty;\n\\] see, for instance, Section 4.3.2 for the case of ML estimation in the linear regression model.\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.3), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuous mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n\\Leftrightarrow\\quad -\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\phantom{-}\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.7 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right),\n\\end{align*}\n\\tag{4.9}\\] where \\[\n\\mathcal{J}(\\theta)=\\frac{1}{n}(-1)\\cdot E(\\ell_n''(\\theta))=\\frac{1}{n}\\mathcal{I}(\\theta).\n\\] Equation 4.9 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) Hesse matrix, and \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d \\mathcal{N}_p\\left(0, \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta),\\) with \\(\\mathcal{I}(\\theta)\\) being the \\((p\\times p)\\) Fisher information matrix.\n\n\n\n\n\n\n\n\nMachine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020))."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#bayes-classifier-clustering-using-gaussian-mixture-distributions",
    "href": "Ch5_EMAlgorithmus.html#bayes-classifier-clustering-using-gaussian-mixture-distributions",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "5.4 Bayes Classifier: Clustering using Gaussian Mixture Distributions",
    "text": "5.4 Bayes Classifier: Clustering using Gaussian Mixture Distributions\n\n5.4.1 Cluster Analysis: Unsupervised Classification\n\n\n5.4.2 Bayes Classifier\n\nBayes decision boundary\n\nBayes Classifier The Bayes classifier assigns each observation \\(i\\) to the group with the largest posterior probability \\(p_{ig}\\) \\[\n\\hat{g}^{Bayes}_i = \\arg\\max_{g=1,\\dots,G} p_{ig}.\n\\]\nIf\nHere \\[\n\\hat{g}^{Bayes}_i = \\arg\\max_{g=1,\\dots,G} \\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}.\n\\]\n\nEstimate the Gaussian mixture distribution using the EM algorithm\nAssign the data points \\(x_i\\) to the group that maximizes the “posterior probability”\n\nFigure 5.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm.\n\n\n\n\n\nFigure 5.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure 5.2."
  },
  {
    "objectID": "Ch5_EMAlgorithmus.html#synopsis",
    "href": "Ch5_EMAlgorithmus.html#synopsis",
    "title": "5  The Expectation Maximization (EM) Algorithm",
    "section": "Synopsis",
    "text": "Synopsis\nThe average penguin doesn’t care about the EM Algorithm.\n\n\n\nFigure 5.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#equivariance-or-invariance-property-of-the-ml-estimator",
    "href": "Ch4_MaximumLikelihood.html#equivariance-or-invariance-property-of-the-ml-estimator",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Equivariance (or Invariance) Property of the ML-Estimator",
    "text": "4.5 Equivariance (or Invariance) Property of the ML-Estimator\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.2 (Equivariance (or Invariance) Property of the ML-Estimator) Let \\[\n\\tau=g(\\theta),\n\\] where \\(g\\) is a one-to-one function of \\(\\theta,\\) i.e. \\(g\\) maps every element of its range to exactly one element of its domain — that is, the outputs never repeat.\nLet \\(\\hat{\\theta}_{n}\\) be the maximum likelihood estimator of \\(\\theta.\\)\nThen \\[\n\\hat{\\tau}_{n} = g\\left(\\hat{\\theta}_{n}\\right)\n\\] is the maximum likelihood estimator of \\(\\tau.\\)\n\n\n\nProof of Theorem 4.2:\nSince \\(g\\) is a one-to-one function, it posses an inverse \\(h=g^{-1}.\\)\nThus \\[\n\\hat{\\theta}_{n} = h(\\hat{\\tau}_{n})\n\\] and \\[\n\\theta = h(\\tau).\n\\] This allows us to express the likelihood problem for \\(\\tau\\) in terms of the likelihood problem for \\(\\theta = h(\\tau).\\) Thus, the likelihood with respect to \\(\\tau\\) can be expressed the likelihood with respect to \\(\\theta = h(\\tau),\\) i.e.  \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\tau) := \\mathcal{L}(h(\\tau))\n& = \\prod_{i=1}^n f(x_i|h(\\tau))\\\\[2ex]\n& = \\prod_{i=1}^n f(x_i|\\theta)  = \\mathcal{L}(\\theta).\n\\end{align*}\n\\] Thus \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\hat{\\tau}_{n})\n%&= \\mathcal{L}(h(\\hat{\\tau}_{n}))\\\\[2ex]\n&= \\mathcal{L}(\\hat{\\theta}_n)\n\\end{align*}\n\\] and for any \\(\\tau\\) \\[\n\\tilde{\\mathcal{L}}(\\tau)   = \\mathcal{L}(\\theta)\n\\leq\n\\mathcal{L}(\\hat{\\theta}_n) = \\tilde{\\mathcal{L}}(\\hat{\\tau}_{n}),\n\\] which shows the statement of Theorem 4.2: \\[\n\\tilde{\\mathcal{L}}(\\hat{\\tau}_n)=\\arg\\max_\\tau\\tilde{\\mathcal{L}}(\\tau).\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#equivariance-property-of-the-ml-estimator",
    "href": "Ch4_MaximumLikelihood.html#equivariance-property-of-the-ml-estimator",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Equivariance Property of the ML-Estimator",
    "text": "4.5 Equivariance Property of the ML-Estimator\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.2 (Equivariance (or Invariance) Property of the ML-Estimator) Let \\[\n\\tau=g(\\theta),\n\\] where \\(g\\) is a one-to-one (or injective) function of \\(\\theta,\\) i.e. \\(g\\) maps distinct elements of its domain to distinct elements—that is, the outputs never repeat. (One-to-one functions are invertible.)\nLet \\(\\hat{\\theta}_{n}\\) be the maximum likelihood estimator of \\(\\theta.\\)\nThen \\[\n\\hat{\\tau}_{n} = g\\left(\\hat{\\theta}_{n}\\right)\n\\] is the maximum likelihood estimator of \\(\\tau.\\)\n\n\n\nProof of Theorem 4.2:\nSince \\(g\\) is a one-to-one function, it posses an inverse \\(h=g^{-1}.\\)\nThus \\[\n\\hat{\\theta}_{n} = h(\\hat{\\tau}_{n})\n\\] and \\[\n\\theta = h(\\tau).\n\\] This allows us to express the likelihood problem for \\(\\tau\\) in terms of the likelihood problem for \\(\\theta = h(\\tau).\\) Thus, the likelihood with respect to \\(\\tau\\) can be expressed the likelihood with respect to \\(\\theta = h(\\tau),\\) i.e.  \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\tau) := \\mathcal{L}(h(\\tau))\n& = \\prod_{i=1}^n f(x_i|h(\\tau))\\\\[2ex]\n& = \\prod_{i=1}^n f(x_i|\\theta)  = \\mathcal{L}(\\theta).\n\\end{align*}\n\\] Thus \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\hat{\\tau}_{n})\n%&= \\mathcal{L}(h(\\hat{\\tau}_{n}))\\\\[2ex]\n&= \\mathcal{L}(\\hat{\\theta}_n)\n\\end{align*}\n\\] and for any \\(\\tau\\) \\[\n\\tilde{\\mathcal{L}}(\\tau)   = \\mathcal{L}(\\theta)\n\\leq\n\\mathcal{L}(\\hat{\\theta}_n) = \\tilde{\\mathcal{L}}(\\hat{\\tau}_{n}),\n\\] which shows the statement of Theorem 4.2: \\[\n\\tilde{\\mathcal{L}}(\\hat{\\tau}_n)=\\arg\\max_\\tau\\tilde{\\mathcal{L}}(\\tau).\n\\]"
  },
  {
    "objectID": "Ch4_MaximumLikelihood.html#solutions",
    "href": "Ch4_MaximumLikelihood.html#solutions",
    "title": "4  Maximum Likelihood",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\nBelow I use the same data (one H, four T) that was used to produce the results in Table 4.1 of our script. However, you can produce new data by setting another seed-value\n\ntheta_true <- 0.2    # unknown true theta value\nn          <-  5     # sample size\n\nset.seed(1)\n\n# simulate data: n many (unfair) coin tosses\nx <- sample(x          = c(0,1), \n               size    = n, \n               replace = TRUE, \n               prob    = c(1-theta_true, theta_true)) \n\n## number of heads (i.e., the number of \"1\"s in x)\nh <- sum(x)\n\n## First derivative of the log-likelihood function\nLp_fct   <- function(theta, h = h, n = n){\n    (h/theta) - (n - h)/(1 - theta)    \n}\n## Second derivative of the log-likelihood function\nLpp_fct   <- function(theta, h = h, n = n){\n    - (h/theta^2) - (n - h)/(1 - theta)^2    \n}\n\n\nt     <- 1e-10   # convergence criterion\ncheck <- TRUE    # for stopping the while-loop\ni     <- 0       # count iterations\n\n\ntheta <- 0.4     # starting value \nLp    <- Lp_fct( theta, h=h, n=n)\nLpp   <- Lpp_fct(theta, h=h, n=n)\n\n\n\nwhile(check){\n    i         <- i + 1\n    ##\n    theta_new <- theta[i] - (Lp_fct(theta[i], h=h, n=n) / Lpp_fct(theta[i], h=h, n=n))    \n    Lp_new    <- Lp_fct( theta_new, h = h, n = n)\n    Lpp_new   <- Lpp_fct(theta_new, h = h, n = n)\n    ##\n    theta     <- c(theta, theta_new) \n    Lp        <- c(Lp,    Lp_new) \n    Lpp       <- c(Lpp,   Lpp_new) \n    ##\n    if( abs(Lp_fct(theta_new, h=h, n=n)) < t ){check <- FALSE}\n}\n\ncbind(theta, Lp, Lp/Lpp)\n\n         theta            Lp              \n[1,] 0.4000000 -4.166667e+00  2.400000e-01\n[2,] 0.1600000  1.488095e+00 -3.326733e-02\n[3,] 0.1932673  2.159084e-01 -6.558924e-03\n[4,] 0.1998263  5.433195e-03 -1.736356e-04\n[5,] 0.1999999  3.539786e-06 -1.132731e-07\n[6,] 0.2000000  1.504574e-12 -4.814638e-14\n\n\n\n\nSolutions of Exercise 2.\n\n(a) Log-likelihood function\nThe log-likelihood function is given by \\[\n\\begin{align*}\n\\ell(\\theta)\n&=\\sum_{i=1}^n \\ln (\\theta\\exp(-\\theta X_i))\\\\\n&=\\sum_{i=1}^n (\\ln \\theta -\\theta X_i)\\\\\n&=n \\ln \\theta -\\sum_{i=1}^n \\theta X_i\n\\end{align*}\n\\]\n\n\n(b) ML-estimator\nThe ML estimator is defined as \\(\\hat{\\theta}_{n}=\\arg\\max\\ell(\\theta)\\). Deriving the ML estimator \\(\\hat\\theta_n\\): \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=n\\frac{1}{\\theta} - \\sum_{i=1}^n X_i\\\\\n\\ell_n'(\\hat\\theta_n)=0\\quad \\Leftrightarrow &\\quad 0=n\\frac{1}{\\hat\\theta_n} - \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad n\\frac{1}{\\hat\\theta_n} = \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad \\hat\\theta_n = \\frac{1}{\\frac{1}{n}\\sum_{i=1}^n X_i}= \\frac{1}{\\bar{X}}\n\\end{align*}\n\\]\n\n\n(b) Fisher information\nThe Fisher information is given by \\[\n\\mathcal{I}(\\theta)=n\\mathcal{J}(\\theta),\n\\] where \\(\\mathcal{J}(\\theta)=-\\frac{1}{n}E(\\ell''(\\theta))\\). The second derivative of \\(\\ell(\\theta)\\) is given by \\[\n\\ell''(\\theta)=-n\\frac{1}{\\theta^2}\n\\] So, the expression for \\(\\ell''(\\theta)\\) is here deterministic as it doesn’t depend on the random variables \\(X_i\\). \\[\n\\mathcal{J}(\\theta)=-\\frac{1}{n}E(\\ell''(\\theta))=-\\frac{1}{n}\\left(-n\\frac{1}{\\theta^2}\\right)=\\frac{1}{\\theta^2}\n\\] That is, the Fisher information is \\(\\mathcal{I}(\\theta)=n\\mathcal{J}(\\theta)=n/\\theta^2\\). Therefore, the asymptotic distribution of \\(\\hat\\theta_n\\) is \\[\n\\begin{align*}\n(\\hat\\theta_n-\\theta)&\\to_d \\mathcal{N}\\left(0,\\frac{\\theta^2}{n}\\right)\\\\\n\\Leftrightarrow\\quad \\sqrt{n}(\\hat\\theta_n-\\theta)&\\to_d \\mathcal{N}\\left(0,\\theta^2\\right)\n\\end{align*}\n\\]\n\n\n\nSolutions of Exercise 3.\n\n(a) Likelihood function\nRecall that the density function of \\(\\mathcal{Unif}(0,\\theta)\\) is \\[\nf(x|\\theta)\n=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{\\theta} & 0\\leq x\\leq \\theta\\\\\n0                & \\text{otherwise}\\\\\n\\end{array}\n\\right.\n\\]\nThus the likelihood function is \\[\n\\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i|\\theta)\n\\] If any \\(X_i>\\theta,\\) we have that \\(\\mathcal{L}_n(\\theta)=0.\\) Putting it differently, let \\(X_{(n)}=\\max\\{X_1,\\dots,X_n\\}\\) (\\(n\\)th order-statistic), then \\(\\mathcal{L}_n(\\theta)=0\\) for all \\(\\theta<X_{(n)}.\\)\nHowever, for all \\(\\theta \\geq X_{(n)}\\) we have that \\(f(X_i|\\theta)=1/\\theta\\) such that \\(\\mathcal{L}_n(\\theta)=(1/\\theta)^n\\). Summing up, \\[\n\\mathcal{L}_n(\\theta)\n=\\left\\{\n\\begin{array}{ll}\n\\left(\\frac{1}{\\theta}\\right)^n & \\theta \\geq  X_{(n)}\\\\\n0                               & \\theta < X_{(n)}\\\\\n\\end{array}\n\\right.\n\\tag{4.10}\\]\n\n\n\n(b) Maximum likelihood estimator of \\(\\theta\\)\n\\(\\mathcal{L}_n(\\theta)\\) is strictly decreasing over the interval \\([X_{(n)},\\infty);\\) see Figure 4.3.\n\nn          <- 20   # sample size\nX_max      <- 0.25\n\ntheta_vec  <- seq(from = 0, \n                  to   = X_max * 1.5, \n                  len  = 100) \nlikelihood_fun <- function(theta, X_max, n){ \n    likelihood              <- 1/(theta^n)\n    likelihood[theta < X_max] <- 0 \n    return(likelihood) \n}\n\nlikelihood_vec <- likelihood_fun(theta = theta_vec,\n                                 X_max = X_max, \n                                 n     = n)\n\nplot(y = likelihood_vec, \n     x = theta_vec, \n     type = \"l\", \n     xlab = expression(theta),\n     ylab = \"Likelihood\", \n     main = \"\")            \naxis(1, at = X_max, labels = expression(X[(n)]))                  \n\n\n\n\nFigure 4.3: Graph of the likelihood function \\(\\mathcal{L}_n(\\theta)\\) given in Equation 4.10.\n\n\n\n\nThus, the maximum likelihood estimator is \\[\n\\begin{align}\n\\hat{\\theta}_{ML}\n& =\\arg\\max_{\\theta>0}\\mathcal{L}_n(\\theta)\\\\\n& = X_{(n)}.\n\\end{align}\n\\]\n\n\nSolutions of Exercise 4.\n\n(a) Maximum likelihood estimator \\(\\hat{\\lambda}\\)\n\\[\n\\begin{align}\n\\mathcal{L}(\\lambda)\n& = \\prod_{i=1}^n f(x_i|\\lambda)\\\\[2ex]\n& = \\prod_{i=1}^n \\frac{\\lambda^{x_i} \\exp(-\\lambda)}{x_i!} \\\\[2ex]\n& = \\frac{\\lambda^{\\sum_{i=1}^n x_i}  \\exp(-n \\lambda)}{\\prod_{i=1}^n (x_i!)} \\\\[4ex]\n\\ell(\\lambda)\n&= \\left(\\sum_{i=1}^n x_i\\right) \\ln(\\lambda) -n\\lambda\\cdot 1 - \\sum_{i=1}^n (x_i!)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell'(\\lambda)\n&= \\frac{\\left(\\sum_{i=1}^n x_i\\right)}{\\lambda}  - n\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell''(\\lambda)\n&= -\\frac{\\left(\\sum_{i=1}^n x_i\\right)}{\\lambda^2} < 0  \n\\end{align}\n\\] since by the properties of the Poisson distribution \\(x_1,\\dots,x_n>0\\) and \\(\\lambda>0.\\)\n\\[\n\\begin{align}\n&\\frac{\\left(\\sum_{i=1}^n x_i\\right)}{\\hat\\lambda}  - n \\overset{!}{=} 0\\\\[2ex]\n\\Rightarrow & \\hat \\lambda = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\end{align}\n\\]\n\n\n(b) Maximum likelihood estimator \\(\\hat{P}(X=4)\\)\n\\[\n\\begin{align}\nP(X=4) = \\frac{\\lambda^4 \\exp(-\\lambda)}{4!}\n\\end{align}\n\\]\nThus \\(P(X=4)\\) is a function of \\(\\lambda\\) \\[\n\\begin{align}\nP(X=4)\\equiv P(X=4|\\lambda) = \\frac{\\lambda^4 \\exp(-\\lambda)}{4!} = g(\\lambda)\n\\end{align}\n\\]\n\nlambda_vec <- seq(from = .0001, to = 15, len = 100)\ng_vec      <- (lambda_vec^4 * exp(-1*lambda_vec))/( factorial(4) )\n\nplot(x = lambda_vec, y = g_vec, \n     type = \"l\", xlab=expression(lambda), ylab=\"g\")\nabline(v = 4)\naxis(1, at = 4)\n\n\n\n\nThus, for \\(0<\\lambda\\leq 4,\\) \\(g(\\lambda)\\) is one-to-one.\nTherefore, by the equivariance property of the maximum likelihood estimator,\n\\[\n\\begin{align}\n\\hat{P}(X=4)\\equiv \\hat{P}(X=4|\\hat{\\lambda}) = \\frac{\\hat{\\lambda}^4 \\exp(-\\hat{\\lambda})}{4!}\n\\end{align}\n\\] with \\(\\hat{\\lambda}=\\frac{1}{n}\\sum_{i=1}^n x_i.\\)"
  }
]