[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    12:15-13:45 \n    Jur / Hörsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/computational-statistics-script-MSc/\n\n\n\n\n\n\n\nYou can use the Zulip-Chat CompStat (M.Sc.) to post questions, share codes, etc. Happy sharing and discussing!"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html",
    "href": "Ch1_Random_Variable_Generation.html",
    "title": "2  Random Variable Generation",
    "section": "",
    "text": "Main:\n\nMonte Carlo Statistical Methods, Robert, C., and Casella, G., Ch. 2\n\nFurther:\n\nNon-Uniform Random Variate Generation, Devroye, L.\nNonparametric Density Estimation: The L1 View, Devroye, L., Ch. 8\nMonte Carlo and Quasi-Monte Carlo Sampling, Lemieux, C., Ch. 2 and 3"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "href": "Ch1_Random_Variable_Generation.html#uniform-simulation",
    "title": "2  Random Variable Generation",
    "section": "2.1 Uniform Simulation",
    "text": "2.1 Uniform Simulation\nGeneral procedure:\n\nUsually, a random integer with values uniformly in \\([0,m]\\) with a large integer \\(m\\) is generated.\nTo achieve a random number in \\([0, 1]\\), we divide this number by \\(m\\).\nFrom this (pseudo) uniform random numbers we can generate random numbers of almost any other distribution.\n\nThere are many different Random Number Generators (RNGs), we consider the most simple class of RNGs:\n\nDefinition 2.1 (Linear Congruential Generators) Here the \\(i\\)th random integer \\(u_i\\) is generated by \\[\nu_i=(a u_{i-1}+c) \\,\\operatorname{mod}\\, m,\n\\] where the starting value \\(u_0\\) is a chosen and fixed value called seed.\nFurthermore:\n\n\\(m\\), with \\(0<m\\), is called the modulus\n\\(a\\), with \\(0<a<m\\), is called the multiplier\n\\(c\\), with \\(0\\leq c<m\\), is called the increment\n\n\n\n\n\n\n\n\nThe modulo operator: \\(\\operatorname{mod}\\)\n\n\n\n“\\(b\\,\\operatorname{mod}\\,c\\)” denotes the remainder of the division of \\(b\\) by \\(c\\).\nFor instance \\[\n\\begin{align*}\n4\\,&\\operatorname{mod}\\,2 = 0\\\\\n5\\,&\\operatorname{mod}\\,2 = 1\\\\\n1\\,&\\operatorname{mod}\\,2 = 1\\\\\n\\end{align*}\n\\]\n\n# Modulo computation using the modulo operator '%%'\n5 %% 4\n9 %% 4\n4 %% 5\n\n# own modulo-function:\nmy_mod <- function(x,m){\n  t1 <- floor(x/m)\n  return(x-t1*m)\n}\n\n\n\nSome Facts:\n\nThe above recursion generates a completely nonrandom sequence, therefore it is often called a pseudo random sequence.\nUnder appropriate choices of \\(u_0\\) , \\(a\\) and \\(m\\) the generated (deterministic) sequence behaves like a sequence of independent random draws from a uniform distribution on \\([0, m]\\).\nThe cycle length of linear congruential generators will never exceed modulus \\(m\\), but can maximized with the three following conditions (see Knuth (2002) for a proof):\n\nThe increment \\(c\\) is relatively prime to \\(m\\),\n\\(a - 1\\) is a multiple of every prime dividing \\(m\\),\n\\(a - 1\\) is a multiple of \\(4\\) when \\(m\\) is a multiple of \\(4\\).\n\n\n\nBad choice of parameters for the linear congruential random number generator:\n\nm <- 64    # modulus\na <- 33    # multiplier\nc <- 12    # increment\ns <- 57    # seed\nn <- 1000  # length of run (including seed)\n\nr_vec    <- numeric(n) # initialize vector\nr_vec[1] <- s # set seed\n\n## Recursive generation \nfor (i in 1:(n-1)){\n r_vec[i+1] <- (a * r_vec[i] + c) %% m\n}\n\n# scale result from [0,m] to [0,1]:\nmy_bad_runif_vec <- r_vec/m\n\n# BUT! Very short cycle-length (here: period=16)\nr_vec[ 1:16]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\nr_vec[17:32]\n\n [1] 57 37 17 61 41 21  1 45 25  5 49 29  9 53 33 13\n\n\n\n\n\nExample 2.1 (Good vs. Bad RNGs) Average heads ratios \\[\n\\bar{C}_n=\\frac{1}{n}\\sum_{i=1}^nC_i\n\\] for \\(n=1,2,\\dots\\) simulated independent tosses of a fair coin \\(C_i\\) with\n\\[\nC_{i}=\\left\\{\\begin{array}{ll}\n1&\\text{if Head}\\\\\n0&\\text{if Tail}\n\\end{array}\\right.\n\\] and \\[\nP(C_i=0)=P(C_i=1)=0.5.\n\\] By the strong (or weak) law of large numbers this average should converge stochastically (i.e., almost surely or in probability) to \\(0.5\\) as \\(n\\) becomes large \\((n\\to\\infty).\\)\n\n# using the above bad RNG:\nbar_x_bad  <- cumsum(my_bad_runif_vec > 0.5)/(1:n)\n\n# using R's high-quality RNG:\nset.seed(223)\nbar_x_good <- cumsum(runif(n)  > 0.5 )/(1:n)\n\n# plotting the results:\nplot(bar_x_bad, type=\"l\", ylim=c(0.46,0.54), \n     xlab=\"\", ylab=\"\", main=\"Good vs. Bad RNG\")\nlines(bar_x_good, col=\"darkblue\")\n\n\n\n\nFigure 2.1: Two sample paths showing the pseudo random convergence of \\(\\bar{C}_n\\) to the limit 0.5—one based on a good RNG and the other based on a bad RNG.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIBM’s RANDU is a famous example of an miss-specified linear congruential RNG."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-discrete-random-variables",
    "title": "2  Random Variable Generation",
    "section": "2.2 Generation of Discrete Random Variables",
    "text": "2.2 Generation of Discrete Random Variables\n\nAssume the discrete random variable \\(X\\) of interest takes on the values \\[\nX\\in \\{x_1, \\dots , x_k \\}\n\\] with \\[\np_i = \\mathbb{P}(X = x_i ), \\quad i = 1,\\dots , k,\n\\] and \\[\n\\sum_{i=1}^kp_i = 1.\n\\]\nAssume that you can generate pseudo-random realizations \\(u\\in[0,1]\\) from a uniformly distributed random variable \\(U\\sim\\mathcal{U}[0, 1]\\) using an RNG.\nGeneral principle:\n\nSubdivide \\([0, 1]\\) into \\(k\\) intervals with \\[\nI_i = (a_{i-1}, a_i],\n\\] where \\[\na_i = \\sum_{j=1}^ip_j\\quad\\text{and}\\quad a_0 = 0.\n\\]\nDefine the new discrete realizations \\[x=\\left\\{\n\\begin{array}{cc}\n       x_1&\\quad\\text{if}\\quad u\\in I_1\\\\\n       \\vdots& \\vdots\\\\\n       x_k&\\quad\\text{if}\\quad u\\in I_k\n       \\end{array}\\right.\n\\]\n\n\nLemma 2.1 Let \\(u\\) be a realization from \\(\\mathcal{U}[0, 1]\\) and if \\(u\\in I_i\\), set \\(x = x_i\\). Then \\(x\\) is a realizaton from the discrete distribution of \\(X\\).\n\nProof: Done in the lecture.\n\n\nExample 2.2 (Bernoulli Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Bernoulli}(p),\n\\] where \\(p\\) is the probability of success, i.e., \\[\n\\mathbb{P}(X=1)=p\\quad\\text{and}\\quad\\mathbb{P}(X=0)=1-p.\n\\]\nAlgorithm: If \\(U\\sim\\mathcal{U}[0,1]\\) and \\(p\\) is specified, define \\[\nX=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\(X\\sim\\mathrm{Bernoulli}(p)\\).\n\nset.seed(321)\n# Generate one random number from Bernoulli(p) with p=0.5\np  <- 0.5\nU  <- runif(1)\n\nif(U<=p) X=1 else X=0\n\nX\n\n[1] 0\n\n\n\n\nExample 2.3 (Binomial Distribution) Generate random numbers from \\[\nX\\sim\\mathrm{Binomial}(n,p),\n\\] where \\(n\\) is the number of trials and \\(p\\) the probability of success such that \\[\n\\mathbb{P}(X=i)=\\binom{n}{i}p^i(1-p)^{n-1}\n\\] for \\(i=1,\\dots,n.\\)\n\n\n\n\n\n\nTip\n\n\n\nIf \\(X_1,\\dots,X_n\\overset{i.i.d}{\\sim}\\mathrm{Bernoulli}(p),\\) then \\[\nX=\\sum_{i=1}^nX_i \\sim\\mathrm{Binomial}(n,p).\n\\]\n\n\nAlgorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(n\\) and \\(p\\) are specified, define \\[\nX_i=\\left\\{\n  \\begin{matrix}\n  1 & \\text{if }U_i\\leq p\\\\\n  0 & \\text{otherwise}.\\\\\n  \\end{matrix}\n\\right.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Binomial}(n,p).\n\\]\n\nset.seed(321)\n\n# Generate one random number from B(n=10, p=0.5). \nn <- 10\nX <- numeric(n)\np <- 0.5\n\nfor(i in 1:n){\n  U <- runif(1)\n  if(U<=p) X[i]=1 else X[i]=0\n}\nY <- sum(X)\nY \n\n[1] 7\n\n\n\n\nExample 2.4 (Poisson Distribution) Algorithm: If \\(U_1,\\dots,U_n\\) are i.i.d. as \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=\\min\\left\\{n=0,1,2\\dots,\\text{ such that }\\prod_{i=1}^{n+1} U_i \\leq e^{-\\lambda}\n\\right\\}.\n\\] Then \\[\nX=\\left(\\sum_{i=1}^n X_i\\right)\\sim\\mathrm{Poisson}(\\lambda).\n\\]\n\nset.seed(321)\n\n# Generate one random number from Poisson(lambda) \nlambda <- 2\n\n## Initializations\nU <- 1\nn <- 0\n\nwhile(U > exp(-lambda)){\n  U <- U * runif(1)\n  n <- n + 1\n}\nn <- n-1\nn\n\n[1] 3"
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "href": "Ch1_Random_Variable_Generation.html#generation-of-continuous-random-variables",
    "title": "2  Random Variable Generation",
    "section": "2.3 Generation of Continuous Random Variables",
    "text": "2.3 Generation of Continuous Random Variables\n\n2.3.1 The Inverse Method\nA rather general method to generate continuous random variables is the Inverse Method.\n\nTheorem 2.1 (Inverse Method) Let \\(U\\sim\\mathcal{U}[0,1],\\) and let \\(F_X\\) be an invertible distribution function. The transformed random variable\n\\[\nX=F_X^{-1}(U)\n\\] has then the distribution function \\(F_X,\\) \\[\nP(X\\leq x) = F_X(x).\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nTheorem 2.1 can only be used to generate random variables \\(X\\) with invertible distribution functions \\(F_X.\\)\n\n\nProof:\nThe distribution function of the transformed random variable \\[\nX=F^{-1}(U)\n\\] can be derived as \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&= \\mathbb{P}(F_X^{-1}(U)\\leq x) \\\\\n&= \\mathbb{P}(U\\leq F_X(x)) \\\\\n&= F_U(F_X(x)) \\\\\n& = F_X(x),\n\\end{align*}\n\\] which shows the result of Theorem 2.1. The last (and important) equality follows since the distribution function of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\nF_U(u) = \\mathbb{P}(U\\leq u) = u, \\quad 0\\leq u \\leq 1\n\\] since the distribution function \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1]\\) is \\[\nF_U(u) = \\left\\{\n  \\begin{array}{ll}\n  0 & \\text{for } u < 0\\\\\n  u & \\text{for } 0 \\leq  u \\leq 1\\\\\n  1 & \\text{for } 1 < u.\\\\\n  \\end{array}\n\\right.  \n\\tag{2.1}\\]\n\n\n\nExample 2.5 (Exponential Distribution) Since \\[\nF(x)= 1 - \\exp(-\\lambda x),\n\\] we have \\[\nF^{-1}(u) = - \\frac{\\ln(1-u)}{\\lambda}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that \\(1-U\\) has the same distribution as \\(U\\), if \\(U\\sim U[0,1]\\). Therefore also \\(-\\frac{\\ln(u)}{\\lambda}\\) leads to a value from \\(\\mathrm{Exp}(\\lambda).\\)\n\n\nAlgorithm: If \\(U\\sim \\mathcal{U}[0,1]\\) and \\(\\lambda\\) is specified, define \\[\nX=-\\frac{\\ln(U)}{\\lambda}.\n\\] Then \\[\nX\\sim \\mathrm{Exp}(\\lambda).\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe inverse method is mainly a good & general way to think about transforming random variables, in practice, however, we often use other methods.\nThe inverse method often cannot be applied, because the inverse of many important distribution functions cannot be derived explicitly:\n\nThe Gaussian distribution function \\(\\Phi\\) and therefore also its inverse \\(\\Phi^{-1}\\) is not available in explicit terms.\nFor discontinuous random variables we need efficient algorithms for computing the generalized inverse of their distribution function \\(F.\\)\n\n\n\n\n\n2.3.2 Transformation Methods\nIdea: Construct algorithms from theoretical links between distributions.\nPro: These methods can be advantageous if a distribution \\(f\\) is linked (in a relatively simple way) to another distribution that is easy to simulate.\nCon: Generally, these methods are rather case-specific, and difficult to generalize.\n\nExample 2.6 (Building on Exponential RVs) In Example 2.5, we learned to generate an exponential random variable \\(X\\sim\\operatorname{Exp}(\\lambda)\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from exponential random variables \\(X\\sim\\mathrm{Exp}(1):\\)\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\n\n\n\n\n\n\n\nNote\n\n\n\nThere are better algorithms to generate Gamma and Beta random variables.\nWe cannot use exponential random variables to generate Gamma random variable with a non-integer shape parameter \\(\\alpha\\).\n\nThis implies that we cannot generate a \\(\\chi^2_{1}\\)-distributed random variable, because the \\(\\chi^2_{1}\\) distribution is identical to the \\(\\Gamma(\\alpha, 2)\\) distribution with \\(\\alpha=\\frac{1}{2}.\\)\nThis then also implies that we cannot generate a \\(\\mathcal{N}(0,1)\\)-distrbuted random variable, since \\(X^2\\sim \\chi^2_{1}\\) for \\(X\\sim\\mathcal{N}(0,1)\\).\n\n\n\n\nThe well-known Box-Muller algorithm for generating standard normal random variables is based on the following theorem of Box and Muller (1958):\n\nTheorem 2.2 (Normal Variable Generation (Box and Muller, 1958)) If \\(U_1\\) and \\(U_2\\) are i.i.d. as \\(U\\sim\\mathcal{U}[0,1]\\), then \\[\nX_1 =\\sqrt{-2 \\ln(U_1)}\\, \\cos(2\\pi U_2)\n\\] and \\[\nX_2=\\sqrt{-2\\ln(U_1)}\\,\\sin(2\\pi U_2)\n\\] are both i.i.d. as \\(X\\sim\\mathcal{N}(0,1).\\)\n\nProof:\nDefine the random variables \\[\nR = \\sqrt{-2 \\ln(U_1)}\\quad\\text{and}\\quad Q = 2\\pi U_2,\n\\] where \\[\nR\\in[0,\\infty)\\quad\\text{and}\\quad Q\\in[0,2\\pi].\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIdea of the proof:\n\nDerive the bivariate density of \\((R, Q).\\)\nDetermine the functional connection \\(g\\) between \\((R, Q)\\) and \\((X_1, X_2)\\) and note that \\(g\\) is invertible.\nUse the transformation formula for densities to derive the bivariate density of \\((X_1,X_2)\\) using \\(g^{-1}\\) and the bivariate density of \\((R, Q).\\)\nThe result follows, if the bivariate density of \\((X_1,X_2)\\) equals the product of two standard normal densities.\n\nTransformation formula (bivariate case):\nAssume that the bivariate random variable \\(\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\\) has a bivariate density \\(f_{RQ}(r, q)\\) and that there is a one-to-one mapping \\(g\\) between the bivariate random variables \\(\\left(\\begin{matrix}R\\\\ Q\\end{matrix}\\right)\\) and \\(\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\\) such that \\[\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)=\ng(R, Q)=\n\\left(\\begin{matrix}g_1(R, Q)\\\\ g_2 (R, Q)\\end{matrix}\\right),\n\\] where \\(g:\\mathbb{R}^2\\to\\mathbb{R}^2\\) is a one-to-one differentiable transformation with inverse \\(g^{-1}\\).\nThen, the bivariate density of \\(\\left(\\begin{matrix}X_1\\\\X_2\\end{matrix}\\right)\\) is given by \\[\nf_{X_1X_2}(x_1,x_2)=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\,\\left|\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|,\n\\tag{2.2}\\] where \\(\\det\\left(J_{g^{-1}}(x_1,x_2)\\right)\\) denotes the determinant of the Jacobian matrix of \\(g^{-1}\\) evaluated at \\((x_1,x_2),\\) \\[\nJ_{g^{-1}}(x_1,x_2)=\\left(\\begin{matrix}\n\\frac{\\partial g_1^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_1^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\frac{\\partial g_2^{-1}}{\\partial x_1}(x_1,x_2) & \\frac{\\partial g_2^{-1}}{\\partial x_2}(x_1,x_2)\\\\\n\\end{matrix}\\right).\n\\] Note that the Jacobian of \\(g^{-1}\\) equals the inverse of the Jacobian of \\(g,\\) \\[\nJ_{g^{-1}}(x_1,x_2) = \\left(J_{g}(r,q)\\right)^{-1},\n\\] with points \\((x_1,x_2)\\) and \\((r,q)\\) such that \\[\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)=\ng(r, q)=\n\\left(\\begin{matrix}g_1(r, q)\\\\ g_2 (r, q)\\end{matrix}\\right),\n\\]\n\n\nWe can derive the distribution function of \\(R\\) as following \\[\n\\begin{align*}\nF_R(r)=\\mathbb{P}\\left(R\\leq r\\right)\n& = \\mathbb{P}\\left(\\sqrt{-2 \\ln(U_1)}\\leq r\\right) \\\\\n& = \\mathbb{P}\\left(\\ln(U_1)\\geq -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) < -\\frac{r^2}{2}\\right) \\\\\n& = 1 - \\mathbb{P}\\left(\\ln(U_1) \\leq -\\frac{r^2}{2}\\right) \\quad \\text{(continous)}\\\\\n& = 1 - \\mathbb{P}\\left(U_1 \\leq \\exp\\left(-\\frac{r^2}{2}\\right)\\right) \\\\\n& = 1 - F_U\\left(\\exp\\left(-\\frac{r^2}{2}\\right)\\right)\\\\\n& = 1 - \\exp\\left(-\\frac{r^2}{2}\\right),\n\\end{align*}\n\\] where the last step follows from applying the distribution \\(F_U\\) of \\(U\\sim\\mathcal{U}[0,1];\\) see Equation 2.1.\nFor the density function \\(f_R\\) of \\(R\\) we get \\[\nf_R(r)=F_R'(r)=\\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(\\frac{r^2}{2}\\right)\\cdot r&\\text{for }r \\in[0,\\infty)\\\\\n  0&\\text{for }r < 0.\\\\\n  \\end{array}\\right.\n\\] Next, define the random variable \\[\nQ = 2\\pi U_2.\n\\] Since \\(U_2\\sim\\mathcal{U}[0,1],\\) \\[\nQ\\sim\\mathcal{U}[0,2\\pi].\n\\] with density function \\[\nf_Q(q)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{1}{2\\pi}&\\text{for } q\\in [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\] Since \\(U_1\\) and \\(U_2\\) are independent, \\(R=\\sqrt{-2 \\ln(U_1)}\\) and \\(Q = 2\\pi U_2\\) must also be independent, such that \\[\n\\begin{align*}\nf_{RQ}(r,q)\n& = f_R(r)\\cdot f_Q(q) \\\\\n& = \\left\\{\n  \\begin{array}{ll}\n  \\exp\\left(\\frac{r^2}{2}\\right) r\\cdot \\frac{1}{2\\pi} & \\text{for } (r,q) \\in [0,\\infty)\\times [0, 2\\pi] \\\\\n  0&\\text{otherwise}.\\\\\n  \\end{array}\\right.\n\\end{align*}\n\\]\nNow, as we know the bivariate density of \\((R,Q)\\) we can use the functional connection \\[\n\\begin{align*}\n\\left(\\begin{matrix}X_1\\\\ X_2\\end{matrix}\\right)\n& = g(R,Q) \\\\\n& = \\left(\\begin{matrix}\n       g_1(R,Q)\\\\\n       g_2(R,Q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\n  R\\cos(Q)\\\\\n  R\\sin(Q)\n  \\end{matrix}\\right)\n% & = \\left(\\begin{matrix}\n%   \\sqrt{-2\\ln(U_1)}\\cos\\left(2\\pi U_2\\right)\\\\\n%   \\sqrt{-2\\ln(U_1)}\\sin\\left(2\\pi U_2\\right)\\\\\n%   \\end{matrix}\\right)\\\\\n\\end{align*}\n\\] with \\[\nR = \\sqrt{-\\ln(U_1)}\\in [0,\\infty)\n\\] and \\[\nQ=2\\pi U_2\\in[0, 2\\pi].\n\\] \n\n\n\n\n\n\nTip\n\n\n\nNote that, \\(g\\) is just the one-to-one transformation that maps points \\((r,q)\\) of the polar coordinate system (radius \\(r\\in [0,\\infty)\\) and angle \\(q\\in[0, 2\\pi]\\)) to points \\((x_1,x_2)\\) of the Cartesian coordinate system: \\[\n\\begin{align*}\n\\left(\\begin{matrix}x_1\\\\ x_2\\end{matrix}\\right)\n& = g(r,q)\\\\\n& = \\left(\\begin{matrix}g_1(r,q) \\\\ g_2(r,q)\\end{matrix}\\right)\n= \\left(\\begin{matrix}r\\cos(q)\\\\r\\sin(q)\\end{matrix}\\right)\n\\end{align*}\n\\]\nPlay around with this mapping here: https://mathinsight.org/applet/polar_coordinates_map_rectangle\nThe inverse mapping \\(g^{-1}\\) maps points \\((x_1,x_2)\\) from the Cartesian coordinate system to polar-points \\((r,q)\\) in the polar coordinate system \\[\n\\begin{align*}\n\\left(\\begin{matrix}r\\\\ q\\end{matrix}\\right)\n& = g^{-1}(x_1,x_2)\\\\\n& = \\left(\\begin{matrix}g_1^{-1}(x_1,x_2) \\\\ g_2^{-1}(x_1,x_2)\\end{matrix}\\right)\n= \\left(\\begin{matrix}\\sqrt{x_1^2 + x_2^2}\\\\ \\operatorname{atan2}(x_1,x_2)\n\\end{matrix}\\right),\n\\end{align*}\n\\] \n\n\n\\[\n\\begin{align*}\nJ_{g^{-1}}(x_1,x_2)\n&=\\left(J_{g}(r,q)\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\frac{\\partial g_1}{\\partial r}(r,q) & \\frac{\\partial g_1}{\\partial q}(r,q)\\\\\n\\frac{\\partial g_2}{\\partial r}(r,q) & \\frac{\\partial g_2}{\\partial q}(r,q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\\left(\\begin{matrix}\n\\cos(q) & -r\\sin(q)\\\\\n\\sin(q) & \\phantom{-}r\\cos(q)\\\\\n\\end{matrix}\\right)^{-1}\\\\\n&=\n\\frac{1}{r\\cos^2(q) + r\\sin^2(q)}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\\\\\n&=\n\\frac{1}{r}\n\\left(\\begin{matrix}\nr\\cos(q) & r\\sin(q)\\\\\n-\\sin(q) &  \\cos(q)\\\\\n\\end{matrix}\\right)\n\\end{align*},\n\\] where the last step follows from Pythagorean’s identity \\(\\cos^2(q) + \\sin^2(q)=1.\\) So \\[\n\\begin{align*}\n\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\n&=\\left|\\operatorname{det}\\left(\\left(J_{g}(r,q)\\right)^{-1}\\right)\\right|\\\\\n&=\n\\left|\\operatorname{det}\\left(\n  \\begin{matrix}\n            \\cos(q) &            \\sin(q)\\\\\n-\\frac{1}{r}\\sin(q) & \\frac{1}{r}\\cos(q)\\\\\n\\end{matrix}\\right)\n\\right|\\\\\n&=\n\\left|\\frac{1}{r}\\cos^2(q) + \\frac{1}{r}\\sin^2(q)\\right| = \\frac{1}{r},\n\\end{align*}\n\\] again using Pythagorean’s identity \\(\\cos^2(x_2) + \\sin^2(x_2)=1\\) and using that \\(r\\in[0,\\infty).\\)\nThus, by the transformation formula for bivariate densities (Equation 2.2), we have \\[\n\\begin{align*}\nf_{X_1X_2}(x_1,x_2)\n&=f_{RQ}\\left(g^{-1}(x_1,x_2)\\right)\\left|\\operatorname{det}\\left(J_{g^{-1}}(x_1,x_2)\\right)\\right|\\\\\n&=f_{RQ}\\Big(\\underbrace{\\sqrt{x_1^2+x_2^2}}_{=r},\\underbrace{\\operatorname{atan2}(x_1,x_2)}_{=q}\\Big)\\frac{1}{r}\\\\\n&=\\exp\\left(\\frac{x_1^2+x_2^2}{2}\\right) \\sqrt{x_1^2+x_2^2} \\cdot \\frac{1}{2\\pi} \\frac{1}{r}\\\\\n&=\\exp\\left(\\frac{x_1^2+x_2^2}{2}\\right)  \\frac{1}{2\\pi},\n\\end{align*}\n\\] where the last step uses that \\(r=\\sqrt{x_1^2+x_2^2}.\\)\nThis shows the result of Theorem 2.2, since \\[\nf_{X_1X_2}(x_1,x_2) = \\frac{1}{2\\pi}\\exp\\left(\\frac{x_1^2+x_2^2}{2}\\right)\n\\] is known to be the bivariate standard normal density for two uncorrelated (thus independent) standard normal random variables with marginal distributions \\(X_1\\sim\\mathcal{N}(0,1)\\) and \\(X_2\\sim\\mathcal{N}(0,1).\\)\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  # Return result\n  return(c(X1, X2))\n}\n\n# Generate n standard normal random variables:\nset.seed(321)\n\nn     <- 1000\nX_vec <- numeric(n)\n\nfor(i in seq(1, n, by=2)){\n  X_vec[c(i, i+1)] <- BM_Algo()\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE, xlim=c(-4,4))\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test \n# H0: Normality\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99882, p-value = 0.7667\n\n\n\n\n\n\n\n\n\n2.3.3 Accept-Reject Methods\nFor many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the distribution function \\(F\\) is somehow unusable. For instance, surprisingly often there is no explicit form of \\(F\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest—actually, \\(f\\) needs to be known only up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\nTheorem 2.3 (Fundamental Theorem of Simulation) Let \\(X\\in\\mathbb{R}^d\\) be a random variable with density function \\(f_X.\\) Then simulating \\(X\\) is equivalent to simulating from a\n\\[\n(X,U)\\sim\\mathcal{U}(\\mathcal{A}),\n\\] where \\(\\mathcal{U}(\\mathcal{A})\\) denotes the uniform distribution over the area \\[\n\\mathcal{A}=\\left\\{(x,u)\\text{ such that } x\\in\\mathbb{R}^d, 0<u<f_X(x)\\right\\}.\n\\]\n\n\nIt turns out that sampling \\((X, U)\\) uniformly over the set \\(\\mathcal{A}\\) is often challenging.\nHowever, once can consider some superset such that \\(\\mathcal{A}\\subseteq \\mathcal{S},\\) such that simulating a random variable uniformly distributed over \\(\\mathcal{S}\\) is easy.\nA uniform distribution on \\(\\mathcal{A}\\) can then be obtained by drawing from a uniform distribution on \\(\\mathcal{S},\\) and rejecting samples in \\(\\mathcal{S}\\) that are not in \\(\\mathcal{A}.\\)\n\nThe case of densities with compact support\nThe general principle of the accept-reject method is easily explained using a bounded density function \\(f\\) with compact support.\n\n\n\n\n\n\nTip\n\n\n\n\nBounded means that there exists a constant \\(m\\) with \\(0<m<\\infty\\) such that \\[\n\\sup_xf(x)\\leq m\n\\] Note that only degenerated density functions are not bounded.\nAn interval \\([a,b]\\) is called compact if it is closed and the boundaries are finite. For instance, the Gaussian density \\(\\phi\\) has not a compact support, but \\(\\mathrm{supp}(\\phi)=(-\\infty,\\infty)\\).\n\nExample: \\[\nf(x)=\\frac{3}{4}\\left(1-\\left(x-1\\right)^2\\right)\\,1_{(|x-1|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[-1,1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f\\) is bounded from above by \\(3/4\\).\n\n\n\nTo simulate \\[\nX\\sim f_X\n\\] with a bounded and compactly supported density function \\(f_X,\\) simulate the random pair \\[\n(Y,U)\\sim\\mathcal{U}([a,b]\\times[0,m])\n\\] by simulating\n \\[\nY\\sim\\mathcal{U}[a,b]\\quad\\text{and}\\quad U|Y=y \\sim \\mathcal{U}[0,m].\n\\]  Then accept a simulated \\(Y\\) as a simulation for \\(X\\), i.e.  \\[\nX=Y,\n\\] only if \\[\nU\\leq f_X(Y),\n\\] and reject all other \\(Y\\)’s.\n\n\n\n\n\n\nNote\n\n\n\nSimulating \\[\nU|Y=y \\sim \\mathcal{U}[0,m]\n\\] is particularly simple, since the distribution of \\(U|Y=y\\) is here equal to the distribution of the unconditional random variable \\[\nU\\sim \\mathcal{U}[0,m]\n\\] for any possible realization \\(Y=y.\\)\n\n\nThe Accept-Reject Algorithm (for compact densities):\n\n# Accept-Reject Algorithm:\nY <- runif(n, min = a, max = b) \nU <- runif(n, min = 0, max = m) \n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\n\nThe following derivation shows that the simulated random variable \\(X\\) has the correct distribution \\(F_X(x)=\\int_a^xf_X(x)dx.\\) \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n&=\\mathbb{P}(Y\\leq x|U\\leq f_X(Y))\\\\[2ex]\n&= \\frac{\\mathbb{P}(Y\\leq x, U\\leq f_X(Y))}{\\mathbb{P}(U\\leq f_X(Y))}\\\\[2ex]\n&= \\frac{\\mathbb{P}(a\\leq Y\\leq x, \\; 0\\leq U\\leq f_X(Y))}{\\mathbb{P}(0\\leq U\\leq f_X(Y))}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,c\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,c\\;du\\,dy},\n\\end{align*}\n\\] where the constant \\(c\\) is the normalizing constant, i.e., the height of the uniform density function uniquely determined by the equation \\[\n\\int_a^{b} \\int_0^{m}\\,c\\,du\\,dy = 1,\n\\] but which is irrelevant here since \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{c\\;\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{c\\;\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}\\\\[2ex]\n& =\\frac{\\int_a^{x} \\int_0^{f_X(y)}\\,1\\;du\\,dy}{\\int_a^{b}\\int_0^{f_X(y)}\\,1\\;du\\,dy}.\\\\\n\\end{align*}\n\\] Now, using that \\(\\int_{0}^{f_X(y)}1du=\\big[x\\big]^{f_X(y)}_0=f_X(y)\\) yields \\[\n\\begin{align*}\n\\mathbb{P}(X\\leq x)\n& =\\frac{\\int_a^x f_X(y)\\,dy}{\\int_a^b f_X(y)\\,dy}\\\\[2ex]\n& =\\int_a^x f_X(y)dy = F_X(x).\n\\end{align*}\n\\]\nIn the following you see a graphical illustration of this procedure:\n\n\n\n\n\n\n\n\n\n\n\n\nThe good thing is that we only need to evaluate the density function \\(f_X,\\) nothing more.\n\n\nThe case of densities with non-compact support\nThe larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set”, enclosing the pdf \\(f\\), as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of \\(f\\) is unbounded.\nLet the larger set denote by \\[\n\\mathscr{L}=\\{(y,u):\\, 0<u<m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathscr{L}\\) is feasible and\n\n\\(m(x)\\geq f(x)\\) for all \\(x\\).\n\n\n\nFrom the feasibility-requirement it follows that \\(m(.)\\) is necessarily integrable, i.e., that \\[\\int_{\\mathcal{X}}m(x)dx=M,\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathscr{L}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathscr{L}\\).\n\n\nIntegrability of \\(m(.)\\) is crucial here, since it allows us to relate \\(m(.)\\) with a corresponding (auxiliary) pdf \\(g(.)\\) as following: \\[m(x)=M\\,g(x),\\quad\\text{where}\\quad\\int_{\\mathcal{X}}m(x)\\,dx=\\int_{\\mathcal{X}}M\\,g(x)\\,dx=M.\\]\nTerminology:\n\nThe pdf \\(g(.)\\) is called the instrumental density. (Choose \\(g(.)\\) as a pdf from which it is easy to simulate!)\nThe pdf \\(f(.)\\) is called the target density.\n\n\n\nIn order to simulate the pair \\((Y,U)\\sim\\mathrm{Unif}(\\mathscr{L})\\) we can now simulate \\[Y\\sim g\\quad\\text{and}\\quad U|Y={\\color{red}y}\\sim\\mathrm{Unif}[0,M\\,g({\\color{red}y})],\\] but accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\in A)=\\mathbb{P}(Y\\in A|U\\leq f(Y))\n=\\frac{\\int_{\\color{red}A}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_\\mathcal{X}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\n=\\frac{\\int_A f(y)\\,dy}{\\int_\\mathcal{X} f(y)\\,dy}\n=\\int_A f(y)dy,\n\\] for every set \\(A\\),  where we again used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nNote that the above derivation implies that we only need to know the pdf \\(f(.)\\) up to an unkown multiplicative constant \\(c>0\\). I.e., it is enough to know \\(f(x)=c\\,\\tilde{f}_{\\textrm{true}}(x)\\), often written as \\(f(x)\\propto \\tilde{f}_{\\textrm{true}}(x)\\), since the unknown constant \\(c\\) cancels out in the above quotient anyways. This is not so much of importance for us, but useful in Bayesian Statistics.\n\n\nAll this leads to a more general version of the Fundamental Theorem of Simulation:\n\nFundamental Theorem of Simulation (General Version):\n\nLet \\(X\\sim f\\) and let \\(g(.)\\) be a pdf s.t. \\(f(x)\\leq M\\,g(x)\\) for some \\(M\\) with \\(1\\leq M<\\infty\\) and all \\(x\\). Then to simulate \\(X\\sim f\\) it is sufficient to generate \\[Y\\sim g\\quad\\text{and}\\quad U|Y=y\\sim\\mathrm{Unif}[0,M\\,g(y)]\\] if one accepts the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and rejects all others.\n\n\n\n\nThe Accept-Reject Algorithm (General Version):\n# Accept-Reject Algorithm:\nY   <- generate n random numbers from g(.)\n\n# Specify function m():\nm <- function(y){YOUR CODE}\n\nU   <- numeric(n)\nfor(i in 1:n){\n  U[i] <- runif(n=1, min = 0, max = m(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\n\n\nExample\nLet the target “density” be \\[f(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\] with upper bound (or, rather, dominating density) the standard normal density \\[g(x)=\\exp(-x^2/2)/\\sqrt{2\\pi},\\] which is obviously straightforward to generate.\nIn this example we can set \\(m(x)=M\\,g(x)\\) with \\(M=1\\), since we can simply scale the target “density” \\(f\\) such that \\(f(x)\\leq g(x)\\) for all \\(x\\). Specifically, we set \\(f(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\).\nIn the following you see the graphical illustration of this example:\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of the Accept-Reject algorithm:\nStatements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f\\) and \\(g\\) are normalized such that they are both pdfs. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. (The closer \\(M\\) is to \\(1\\) the better.)\n\\(M\\) is a function of how closely \\(g\\) can imitate \\(f\\).\n\nNote that, for such normalized \\(f\\) and \\(g\\) the inequality \\(f(x)\\leq M\\,g(x)\\) with \\(1\\leq M<\\infty\\) for all \\(x\\) is equivalent to saying that the quotient \\(f/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f(x)}{g(x)}\\leq M <\\infty\\quad\\text{for all}\\quad x.\n\\] That is, it is necessary for \\(g\\) to have, e.g., thicker tails than \\(f\\). This makes it, for instance, impossible to simulate a Cauchy distribution \\(f\\) using a normal distribution \\(g\\). The reverse, however, works quite well. \n\n\nExample: Normals from Double Exponentials\nConsider generating a \\(N(0,1)\\) by the Accept-Reject algorithm using a double-exponential distribution \\(\\mathcal{L}(\\alpha)\\), also called Laplace distribution, with density \\(g(x|b)=(1/(2b))\\exp(-\\,|x|/b)\\).  It is then straightforward to show that \\[\n\\frac{f(x)}{g(x|b)}\n%=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\n%=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\n\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf: \\[\n\\frac{f(x)}{g(x|1)}\n\\leq M=\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right).\n\\]\nThe probability of acceptance is then \\(\\sqrt{\\pi/(2e)}=0.76\\). I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average \\(1/0.76\\approx 1.3\\) uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "href": "Ch1_Random_Variable_Generation.html#classical-monte-carlo-integration",
    "title": "2  Random Variable Generation",
    "section": "3.1 Classical Monte Carlo Integration",
    "text": "3.1 Classical Monte Carlo Integration\nThe generic problem here is the evaluation of integrals. (Be aware: Integrals are everywhere in statistics!). For instance, \\[\n\\mathbb{E}_f\\left(h(X)\\right)=\\mathbb{E}\\left(h(X)\\right)=\\int_\\mathcal{X}h(x)\\,f(x)\\,dx.\n\\]\nConvergence:\nGiven our previous developments, it is natural to propose using a realization \\(x_1,\\dots,x_m\\) from a (pseudo random) i.i.d. sample \\(X_1,\\dots,X_m\\) with each \\(X_j\\) distributed as \\(X\\sim f\\) to approximate the above integral by the empirical mean \\[\n\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h(x_j).\n\\] By the Strong Law of Large Numbers we know that the empirical mean \\(\\bar{h}_m\\) converges almost surely (a.s.) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\) as \\(m\\to\\infty\\). (The only prerequisits are that \\(f\\) has finite first moments, i.e., \\(\\mathbb{E}\\left(h(X)\\right)<\\infty\\), and that \\(\\bar{h}_m\\) is constructed from an i.i.d. sample \\(X_1,\\dots,X_m\\).)\nAs we can use the computer to produce realizations from the i.i.d. sample \\(X_1,\\dots,X_m\\), we can in principle choose an arbitrary large sample size \\(m\\) such that \\(\\bar{h}_m\\) can (in principle) be arbitrarily close to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\).\nThough, …\n\n… which sample size \\(m\\) is large enough?\nOr “equivalently”: How fast converges \\(\\bar{h}_m\\) to the desired limit \\(\\mathbb{E}\\left(h(X)\\right)\\)?\n\n\n\nSpeed of Convergence:\nOK, we know now that \\(\\bar{h}_m\\) reaches its limit (here in the “almost surely” sense) as \\(m\\to\\infty\\) under some rather loose conditions on the random sample \\(X_1,\\dots,X_m\\).\nIf we are willing to additionally assume that \\(f\\) has finite second moments, i.e., \\(\\mathbb{E}(h(X)^2)<\\infty\\), we can additionally say something about how fast \\(\\bar{h}_m\\) converges (a.s.) to \\(\\mathbb{E}(h(X))\\).\nThe speed of convergence of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) (i.e., now we think of \\(\\bar{h}_m\\) as the {RV} \\(\\bar{h}_m=\\frac{1}{m}\\sum_{j=1}^m h({\\color{red}{X_{j}}})\\)) to its limit \\(\\mathbb{E}(h(X))\\) can be assessed by answering the question how fast the standard deviation (which is a function of \\(m\\)) of the stochastic sequence converges to zero as \\(m\\to\\infty\\).\n\nThe variance of \\(\\bar{h}_m\\) is given by \\[\n\\mathbb{V}_f\\left(\\bar{h}_m\\right)=\n\\mathbb{V}\\left(\\frac{1}{m}\\sum_{j=1}^m h(X_j)\\right)=\n\\frac{1}{m}\\mathbb{V}\\left(h(X)\\right)\n\\]\nNote that assuming finite second moments \\(\\mathbb{E}(h(X)^2)<\\infty\\) is equivalent to assuming finite variance \\(\\mathbb{V}\\left(h(X)\\right)<\\infty\\). Consequently, we can set \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\) with \\(0<\\mathtt{const}<\\infty\\) such that \\[\n\\sqrt{\\mathbb{V}\\left(\\bar{h}_m\\right)}=m^{-1/2}\\mathtt{const}\\propto m^{-1/2}.\n\\]\n\nI.e., the speed of convergence (or rate) of the stochastic sequence \\(\\{\\bar{h}_m\\}\\) is proportional to the deterministic sequence \\(\\{m^{-1/2}\\}\\).\n\n\nRemark: Even if we would not know the value of \\(\\mathtt{const}=\\sqrt{\\mathbb{V}\\left(h(X)\\right)}\\), we know now that the improvement from \\(m=10\\) to \\(m=100\\) will be much higher than from \\(m=110\\) to \\(m=200\\). In practice, a typical choice is \\(m=10000\\); for moderate standard deviations this choice will guarantee a very good approximation.\n\n\nLimit Distribution:\nOf course, we can estimate the variance of the estimator \\(\\mathbb{V}\\left(\\bar{h}_m\\right)\\) by its empirical version \\[\nv_m=\\frac{1}{m}\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right),\n\\] where again by the Strong Law of Large Numbers (SLLN) \\[\n\\left(\\frac{1}{m}\\sum_{j=1}^m\\left(h(x_j)-\\bar{h}_m\\right)^2\\right)\\to_{\\text{a.s.}}\\mathbb{V}\\left(h(X)\\right).\n\\]  By the Central Limit Theorem (CLT) we have \\[\n\\sqrt{m}\\left(\\frac{\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)}{\\sqrt{\\mathbb{V}\\left(h(X)\\right)}}\\right)\\to_d Z,\n\\] where \\(Z\\sim N(0,1)\\). Note that the the above sequence \\(\\{\\sqrt{m}\\}\\) just hinders the convergence of the sequence \\(\\bar{h}_m - \\mathbb{E}\\left(h(X)\\right)\\to_{a.s.}0\\) such that the quotient converges to a “stable” distribution.\nThe above result can now be used for the construction of (asymptotically valid) convergence tests and confidence intervals with respect to \\(\\bar{h}_m\\), since for large \\(m\\) \\[\n\\bar{h}_m\\,\\overset{d}{\\approx} N\\left(\\mathbb{E}\\left(h(X)\\right),\\frac{\\mathbb{V}\\left(h(X)\\right)}{m}\\right).\n\\]\nAnd as we can use the computer to generate realizations of the i.i.d. sample \\(X_1,\\dots,X_m\\) from a generic \\(X\\sim f\\), we can easily approximate the mean \\(\\mathbb{E}\\left(h(X)\\right)\\) and the variance \\(\\mathbb{V}\\left(h(X)\\right)\\) with arbitrary accuracy as \\(m\\to\\infty\\); by the SLLN (or the WLLN).\n\n\nExample: A first Monte Carlo Integration\nLet’s say we want to integrate the function \\(h(x)=\\left(\\cos(50\\,x)+\\sin(20\\,x)\\right)^2\\). Although this function could be integrated analytically it is a good first test case. The left plot below shows the graph of the function \\(h(.)\\).\nTo approximate the integral \\[\n\\int_\\mathcal{X}h(x)dx\\quad\\text{with}\\quad\\mathcal{X}=[0,1]\n\\] we can use that \\[\n\\int_\\mathcal{X}h(x)dx=\\int_\\mathcal{[0,1]}1\\cdot h(x)dx =\\mathbb{E}_{f_\\text{Unif[0,1]}}(h(X)).\n\\]\nThus, we generate a realization \\((u_1,\\dots,u_n)\\) from the i.i.d. random sample \\(U_1,\\dots,U_n\\sim[0,1]\\) and approximate \\[\n\\int_\\mathcal{X}h(x)dx\\approx \\bar{h}_n=\\frac{1}{n}\\sum_{i=1}^n h(u_i).\n\\]\nIn order to assess how good this approximation is, we need to consider the stochastic propoerties of the RV \\[\n\\frac{1}{n}\\sum_{i=1}^n h(U_i).\n\\] This is done using the above (review of) results on the limit distribution of the sample mean which allows us to construct an approximative \\(95\\%\\) confidence interval, since for large \\(n\\) \\[\n\\left[\\bar{h}_n - 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}, \\bar{h}_n + 1.96\\frac{\\mathtt{std.error}_n}{\\sqrt{n}}\\right]\\approx\n\\left[\\bar{h}_n - 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}, \\bar{h}_n + 1.96  \\sqrt{\\frac{\\mathbb{V}(h(U_i))}{n}}\\right],\n\\] where \\(\\mathtt{std.error}_n^2=n^{-1}\\sum_{i=1}^n(h(u_i)-\\bar{h}_n)^2\\).\nThe right plot below shows one realization of the stochastic sequence \\(\\{\\bar{h}_1,\\dots,\\bar{h}_n\\}\\) with \\(n=10000\\), where the realized value of \\(\\bar{h}_n\\) is \\(0.966\\). This compares favorably with the with the exact value of \\(0.965\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks:\n\nThe approach followed in the above example can be successfully utilized in many cases, even though it is often possible to achieve greater efficiency through numerical methods (e.g., Riemann Sum, Trapezoidal Rule, Simpson’s Rule, etc.) in dimensions 1 or 2.\nThe approach is particularly useful for approximating integrals over higher dimensional sets.\n\n\n\nExample: Approximation of Normal Distribution Tables\nA possible way to construct normal distribution tables is to use MC simulations.\nGenerate a realization \\((x_1,\\dots,x_n)\\) from an i.i.d. standard normal random sample, e.g., using the Box-Muller algorithm.\nThe approximation of the standard normal cdf \\[\n\\Phi(t)=\\int_{-\\infty}^t\\frac{1}{\\sqrt{2\\pi}}e^{-y^2/2}dy\n\\] by the Monte Carlo method is thus \\[\n\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n 1_{(x_i\\leq t)}.\n\\] The corresponding RV \\(\\hat{\\Phi}_n(t)=\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\leq t)}\\) has (exact) variance \\[\n\\mathbb{V}(\\hat{\\Phi}_n(t))=\\frac{\\Phi(t)(1-\\Phi(t))}{n},\n\\] since the single RVs \\(1_{(X_i\\leq t)}\\) are independent Bernoulli with success probability \\(\\Phi(t)\\).\nFor values of \\(t\\) around \\(t=0\\), the variance is thus approximately \\(1/4n\\).\nTo achieve a precision of four decimals by means of a \\(99.9\\%\\) confidence interval, the approximation requires on average \\(n\\approx 10^8\\) simulations.\nThe table below gives the evolution of this approximation for several values of \\(t\\) and shows a very accurate evaluation for \\(n=10^8\\).\n\n\n\\[\n\\begin{array}{cccccccccc}\n\\hline\nn   &t=0.0  &t=0.67 &t=0.84 &t=1.28 &t=1.65 &t=2.32 &t=2.58 &t=3.09 &t=3.72 \\\\\n\\hline\n10^2 &0.485  &0.74   &0.77   &0.9    &0.945  &0.985  &0.995  &1      &1      \\\\\n10^3 &0.4925 &0.7455 &0.801  &0.902  &0.9425 &0.9885 &0.9955 &0.9985 &1      \\\\\n10^4 &0.4962 &0.7425 &0.7941 &0.9    &0.9498 &0.9896 &0.995  &0.999  &0.9999 \\\\\n10^5 &0.4995 &0.7489 &0.7993 &0.9003 &0.9498 &0.9898 &0.995  &0.9989 &0.9999 \\\\\n10^6 &0.5001 &0.7497 &0.8    &0.9002 &0.9502 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^7 &0.5002 &0.7499 &0.8    &0.9001 &0.9501 &0.99   &0.995  &0.999  &0.9999 \\\\\n10^8 &0.5    &0.75   &0.8    &0.9    &0.95   &0.99   &0.995  &0.999  &0.9999 \\\\\n\\end{array}\n\\]\n\n\nRemarks:\n\nTo achieve a precision of two decimals by means of a \\(99.9\\%\\) confidence interval, already \\(n=10^4\\) leads to satisfactory results.\nNote that greater accuracy is achieved in the tails and that more efficient simulation methods could be used (e.g., Importance Sampling)."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "href": "Ch1_Random_Variable_Generation.html#importance-sampling",
    "title": "2  Random Variable Generation",
    "section": "3.2 Importance Sampling",
    "text": "3.2 Importance Sampling\nImportance sampling aims to reduce the variance of the Monte Carlo integral estimate therefore it’s refereed to as a variance reduction technique. This variance reduction is achieved by weighting functions, so-called importance functions.\nAs in the case of Monte Carlo integration the focus lies on evaluating the integral \\[\n\\mathbb{E}_f(h(X))=\\int_\\mathcal{X}h(x)f(x)\\,dx.\n\\]\n\nThough, it turns out that the above approach, i.e., sampling from \\(f\\) is often suboptimal.\nObserve that the value of the above integral can be represented by infinitely many alternative choices of the triplet \\((\\mathcal{X}, h, f)\\). Therefore, the search for an optimal estimator should encompass all these possible representations.\n\nLet’s illustrate this with a simple example.\nExample: Cauchy Tail Probability (from Ripley 1987)\nSuppose that the quantity of interest is the probability, say \\(p\\), that a Cauchy \\(\\mathrm{C}(0,1)\\) RV is larger than \\(2\\), i.e.: \\[\np=\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx.\n\\]\n1. Naive Approach: If \\(p\\) is approximated through the empirical mean \\[\n\\hat{p}_{1}=\\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\n\\] of an i.i.d. sample \\(X_1,\\dots,X_m\\sim\\mathrm{C}(0,1)\\), then the variance of this estimator, a binomial RV scaled by \\(1/m\\), is \\[\n\\mathbb{V}(\\hat{p}_{1})=\\frac{1}{m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(X_j>2)}\\right)=\\frac{p(1-p)}{m},\n\\] which is equal to \\(0.1275/m\\), since (we already know that) \\(p=0.15\\).\n\n\n2. Accounting for Symmetry (i.e., using the ‘Adjusting Screws’ \\(\\mathcal{X}\\) and \\(h\\)): We can achieve a more efficient estimator (i.e., an estimator with lower variance for a given same sample size \\(n\\)) if we take into account the symmetric nature of \\(\\mathrm{C}(0,1)\\). Obviously, our target integral can be equivalently written as \\[\np=\\frac{1}{2}\\left(\\int_{-\\infty}^{-2}\\frac{1}{\\pi(1+x^2)}\\,dx + \\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}\\,dx \\right).\n\\] This representation has the attractive feature that we can use a much higher fraction of the simulated data by using the following new empirical mean: \\[\n\\hat{p}_{2}=\n\\frac{1}{2}\\left(\\frac{1}{m}\\sum_{j=1}^m1_{(X_j<-2)}+ \\frac{1}{m}\\sum_{j=1}^m1_{(X_j>2)}\\right)\\;=\\;\n\\frac{1}{2m}\\sum_{j=1}^m1_{(|X_i|>2)}.\n\\] The variance of this new estimator, \\[\n\\mathbb{V}(\\hat{p}_{2})=\\frac{1}{4m^2}\\mathbb{V}\\left(\\sum_{j=1}^m1_{(|X_i|>2)}\\right)=\\frac{2p(1-2p)}{4m},\n\\] is equal to \\(0.0525/m\\), i.e., lower than in the naive approach.\n\n\n3. Using all ‘Adjusting Screws’ \\(\\mathcal{X}\\), \\(h\\), and \\(f\\): The (relative) inefficiency of the above approaches is due to the generation of RVs outside the domain of interest, \\([2,+\\infty)\\), which are in some sense irrelevant for the approximation of \\(p\\). This motivates the following reformulation of \\(p\\):\nBy symmetry of \\(f\\): \\[\n\\frac{1}{2}=\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx + \\underbrace{\\int_{2}^{+\\infty}\\frac{1}{\\pi(1+x^2)}dx}_{=p}\n\\] \\[\n\\Leftrightarrow \\; p=\\frac{1}{2}-\\int_{0}^2\\frac{1}{\\pi(1+x^2)}dx.\n\\] Furthermore, we can re-arrange the last integral a bit such that \\[\n\\int_{0}^2\\;\\left(\\frac{1}{2}\\cdot 2\\right)\\;\\frac{1}{\\pi(1+x^2)}\\,dx =\n\\int_{0}^2\\;\\underbrace{\\frac{1}{2}}_{f_{\\mathrm{Unif}[0,2]}}\\;\\underbrace{\\frac{2}{\\pi(1+x^2)}}_{=h(x)}\\,dx =\n\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,2].\n\\]\nTherefore a new alternative method for evaluating \\(p\\) is: \\[\n\\hat{p}_{3}=\\frac{1}{2} - \\frac{1}{m}\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U_j\\sim\\mathrm{Unif}[0,2].\n\\] Using integration by parts, it can be shown that \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\). (Compare this to the former results: \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\).)\n\n\nA More General Point of View:\nThe idea of importance sampling is related to weighted and stratified sampling ideas. As illustrated by the above example, when estimating \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx.\n\\]\nSome outcomes of \\(X\\sim f\\) may be more important than others in determining \\(\\theta\\) and we wish to select such values more frequently.\nFor instance, if \\(\\theta\\) denotes the probability of the occurrence of a very rare event, then the only way to estimate \\(\\theta\\) at all accurately may be to produce the rare events more frequently.\nTo achieve this, we can simulate a model which gives pdf \\(g\\) to \\(X\\) instead of the correct pdf \\(f\\), where both pdfs need to be known. This can be easily done, since \\[\n\\theta=\\mathbb{E}_f(h(X))=\\int h(x)\\left(\\frac{g(x)}{g(x)}\\right)\\;f(x)dx=\n\\int \\underbrace{\\left(h(x)\\frac{f(x)}{g(x)}\\right)}_{=\\psi(x)}\\;g(x)dx=\n\\int \\psi(x)\\;g(x)dx=\n\\mathbb{E}_g(\\psi(X)).\n\\]\nThis leads to the following unbiased estimator for \\(\\theta\\) based on sampling from \\(g\\): \\[\n\\hat{\\theta}_g=\\frac{1}{n}\\sum_{i=1}^n\\psi(X_i)\\quad\\text{with}\\quad X_i\\sim g,\n\\] which is a weighted mean of the \\(h(X_i)\\) with weights inversely proportional to the “selection factor” \\(\\frac{g(X_i)}{f(X_i)}\\). \nFor the variance of the estimator \\(\\hat{\\theta}_g\\) we have \\[\n\\mathbb{V}(\\hat{\\theta}_g)=\\frac{1}{n}\\mathbb{V}(\\psi(X_i))=\n\\frac{1}{n}\\int\\left(\\psi(x)-\\theta\\right)^2g(x)dx=\n\\frac{1}{n}\\int\\left(\\frac{h(x)\\,f(x)}{g(x)}-\\theta\\right)^2g(x)dx,\n\\] which, depending on the choice of \\(g(.)\\), can be much smaller (or larger) than the variance of the naive estimator from the classical Monte Carlo Integration using the ordinary empricial mean. \n\n\n\n\nMinimum Variance Theorem\n\nThe importance function \\(g(.)\\) which minimizes the variance \\(\\mathbb{V}(\\psi(X_i))\\), and therefore the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\), is given by \\[\ng^\\ast(x)=\\frac{|h(x)|f(x)}{\\int |h(z)|f(z)dz}.\n\\]\n\n\nProof: Done in the lecture.\n\n\nThough, this result is rather formal (in the sense of “impractical”), since, e.g., if \\(h(x)>0\\) then \\(g^\\ast\\) requires us to know \\(\\int h(z)f(z)dz\\), which is just the integral of interest!\nRemarks:\nThe above minimum variance result is still useful:\n\nIt tells us that a good choice of \\(g(x)\\) shall mimic the shape of \\(|h(x)|f(x)\\), since the optimal \\(g^\\ast(x)\\propto |h(x)|f(x)\\).\nFurthermore, \\(g(x)\\) should be chosen such that it has a thicker tail than \\(f(x)\\), since the variance \\(\\mathbb{V}(\\hat{\\theta}_g)\\) crucially depends on the quotient \\(f(x)/g(x)\\) which would “explode” for \\(g(x)\\approx 0\\).\n\n\n\nLet’s apply our new insights to the above example on the Cauchy tail probability \\(p\\).\nExample: Cauchy Tail Probability (cont.)\nAbove we had:\n\n\\(f(x)=\\frac{1}{\\pi(1+x^2)}\\), the pdf of \\(\\mathrm{C}(0,1)\\) and\n\\(h(x)=1_{(x>2)}\\), i.e., here \\(|h(x)|=h(x)\\).\n\nTherefore \\[\np=\\mathbb{E}_f(h(X))=\\int h(x)f(x)dx=\\int_{2}^{\\infty}f(x)dx=\\int_{2}^{\\infty}\\underbrace{\\frac{f(x)}{g(x)}}_{=\\psi(x)}\\;g(x)dx=\\mathbb{E}_g(\\psi(X)),\n\\] where the \\(h\\) function is absorbed by the formulation of the definite integral.\nA possibly good (and simple) choice of \\(g\\) is, e.g., \\(g(x)=2/(x^2)\\), since this function:\n\n“closely” matches \\(h(x)f(x)\\) and\n\\(g\\) has thicker tails than \\(f\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: It is not straight forward to directly sample from \\(g\\), therefore we need some further steps:\n\n\nThe choice of \\(g\\) leads to \\[\np=\\mathbb{E}_g(\\psi(X))=\n\\int_{2}^{+\\infty}\\left(\\frac{x^2}{2\\,\\pi(1+x^2)}\\right)\\,\\frac{2}{x^2}\\,dx=\n\\int_{2}^{+\\infty}\\left(\\frac{1}{\\pi(1+x^{-2})}\\right)\\,x^{-2}\\,dx.\n\\]\n\n\nNow we can apply some additional (rather case-specific) re-arrangements:\nIntegration by substitution (substituting \\(u=x^{-1}\\)) yields: \\[\np=\\int_{0}^{1/2}\\frac{1}{\\pi(1+u^2)}du.\n\\] Again, we can re-arrange the last integral a bit such that \\[\np=\\int_{0}^{1/2}\\underbrace{2}_{f_{\\mathrm{Unif}[0,1/2]}}\\;\\underbrace{\\frac{1}{2\\,\\pi(1+u^2)}}_{=h(u)}\\,du=\\mathbb{E}(h(U)),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2].\n\\] Therefore, we have a final fourth version of the estimator of \\(p\\): \\[\n\\hat{p}_4=\\sum_{j=1}^m h(U_j),\\quad\\text{where}\\quad U\\sim\\mathrm{Unif}[0,1/2]\n\\] and \\(h(u)=1/(2\\pi(1+u^2))\\).\nThe variance of \\(\\hat{p}_4\\) is \\((\\mathbb{E}(h(U)^2)-\\mathbb{E}(h(U))^2)/m\\) and an integration by parts shows that \\(\\mathbb{V}(\\hat{p}_4)=0.95\\cdot 10^{-4}/m\\). Compare this to the former results: \\(\\mathbb{V}(\\hat p_3)=0.0285/m\\), \\(\\mathbb{V}(\\hat{p}_{2})=0.0525/m\\) and \\(\\mathbb{V}(\\hat{p}_{1})=0.1275/m\\). The variance of \\(\\hat{p}_4\\) is by a factor of \\(10^{-3}\\) lower than the variance of the original \\(\\hat{p}_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox, George EP, and Mervin E Muller. 1958. “A Note on the Generation of Random Normal Deviates.” The Annals of Mathematical Statistics 29 (2): 610–11. https://projecteuclid.org/euclid.aoms/1177706645."
  },
  {
    "objectID": "Ch_Bootstrap.html",
    "href": "Ch_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "href": "Ch_Bootstrap.html#the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.1 The empirical distribution function",
    "text": "3.1 The empirical distribution function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its distribution function \\[\nF(x)=P(X\\leq x)\\quad \\text{for all } x\\in\\mathbb{R}.\n\\]\nData: i.i.d. random sample \\(X_1,\\dots,X_n\\)\nFor given data, the sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet \\(I(\\cdot)\\) denote the indicator function, i.e., \\(I(x\\leq t)=1\\) if \\(x\\leq t\\), and \\(I(x\\leq t)=0\\) if \\(x>t.\\)\n\nDefinition 3.1 (Empirical distribution function) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n I(X_i\\leq x)\n\\] i.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\nProperties:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0\\), if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic; i.e. \\(X_{(1)}\\) is the smallest observation\n\\(F(x)=1\\), if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) monotonically increasing step function\nStructurally, \\(F_n\\) itself is a distribution function; it is equivalent to the distribution function of a discrete random variable \\(X^*\\) with possible values \\(X^*\\in\\{X_1,\\dots,X_n\\}\\) and with \\(P(X^*=X_i)=\\frac{1}{n}\\) for all \\(i=1,\\dots,n.\\)\n\n\nExample 3.1 (Empirical distribution function) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.40, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\\(F_n(x)\\) depends on the observed sample and thus is random. We obtain\n\nFor every \\(x\\in\\mathbb{R}\\) \\[\nnF_n(x)\\sim B(n, p=F(x))\n\\] I.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) and \\(p=F(x)\\).\n\\(E(F_n(x))=F(x)\\)\n\\(Var(F_n(x))=\\frac{F(x)(1-F(x))}{n}\\)\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \\[\nP\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#basic-idea",
    "href": "Ch_Bootstrap.html#basic-idea",
    "title": "3  The Bootstrap",
    "section": "3.2 Basic idea",
    "text": "3.2 Basic idea\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\n\nThe random sample \\(X_1,\\dots,X_n\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\]\nFor large \\(n\\): The empirical distribution \\(F_n\\) of the sample values is “close” to the unknown distribution \\(F\\) (Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nI(X_i\\in[a,b])&\\to_p P(X\\in [a,b])\\\\\n\\Leftrightarrow \\qquad F_n(b)-F_n(a)&\\to_p F(b)-F(a)\n\\end{align*}\n\\]\nThe idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution \\(F\\), the bootstrap uses random sampling from the known \\(F_n\\). This is justified by the insight that the empirical distribution \\(F_n\\) of the observed data is “similar” to the true distribution \\(F\\) (Theorem 3.1)."
  },
  {
    "objectID": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "href": "Ch_Bootstrap.html#the-nonparametric-standard-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The nonparametric (standard) bootstrap",
    "text": "3.3 The nonparametric (standard) bootstrap\nSetup:\n\nData: i.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) from \\(X\\sim F\\).\n\nThe distribution \\(X_i\\sim F,\\) \\(i=1,\\dots,n,\\) depends on an unknown parameter (vector) \\(\\theta.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate an element \\(\\theta\\) of the parameter vector \\(\\theta.\\)\n\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\n\nInference: We are interested in evaluating the distribution of \\(\\hat\\theta-\\theta\\) in order to\n\nprovide standard errors\nconstruct confidence intervals\nperform tests of hypothesis.\n\n\n\nThe bootstrap algorithm\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(X_1,\\dots,X_n.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (e.g. \\(m=2000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\n\\]\nFor large \\(m\\), the estimates \\(\\hat\\theta_1^*,\\hat\\theta_2^*,\\dots,\\hat\\theta_m^*\\) allow to approximate the bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) arbitrarily well. The bootstrap distribution of \\(\\hat\\theta^*-\\hat\\theta\\) is used to approximate the unknown distribution of \\(\\hat\\theta-\\theta\\).\n\nThe theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.\n\n\n\n\n\n\nThe bootstrap does not always work\n\n\n\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\n\n\nThe bootstrap is called consistent, if for large \\(n\\), the bootstrap distribution of \\(\\hat{\\theta}^* -\\hat{\\theta}\\) is a good approximation of the underlying distribution of \\(\\hat{\\theta}-\\theta\\), i.e. \\[\n\\text{distribution}(\\hat{\\theta}^* -\\hat{\\theta}\\ |{\\cal S}_n)\\approx\n\\text{distribution}(\\hat{\\theta}-\\theta).\n\\] The following definition states this more precisely.\n\nDefinition 3.2 (Bootstrap consistency) If for some \\(\\gamma>0\\) (usually: \\(\\gamma=1/2\\)) we have \\(n^\\gamma(\\hat{\\theta}-\\theta)\\rightarrow_d Z\\) for some random variable \\(Z\\) with a non-degenerate distribution, then the bootstrap is consistent if and only if \\[\n\\sup_\\delta \\left|\n   P\\left(n^\\gamma(\\hat\\theta^*-\\hat\\theta)\\le \\delta|{\\cal S}_n\\right)\n  -P\\left(n^\\gamma(\\hat\\theta      -\\theta)\\le \\delta\\right)\n  \\right|\\rightarrow 0\n\\] as \\(n\\to\\infty.\\)\n\nLuckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nThe distribution of the estimator \\(\\hat\\theta\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(X_1,\\dots,X_n\\) does not properly reflect the way how the \\(X_1,\\dots,X_n\\) are generated when \\(X_1,\\dots,X_n\\) is a time-series with auto-correlated data.\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (E.g. in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference about the population mean\nSetup:\n\nPopulation Model: Continuous random variable \\(X\\sim F\\) with unknown \\(F\\) and thus unknown mean \\(\\mu\\)\nData: i.i.d. sample \\(X_1,\\dots,X_n\\) from \\(X\\)\nProblem: What is the distribution of \\(\\bar{X} -\\mu\\)?\n\nNow assume that \\(n=8\\) and that the observed sample is\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n-0.6\n\n\n2\n1.0\n\n\n3\n1.4\n\n\n4\n-0.8\n\n\n5\n1.6\n\n\n6\n1.9\n\n\n7\n-0.1\n\n\n8\n0.7\n\n\n\n\nobservedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)\n\nSo the sample mean is\n\n\\(\\bar X =\\) mean(observedSample) \\(=\\) 0.6375\n\n\nBootstrap:\nThe observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) is taken as underlying empirical “population” in order to generate “bootstrap data” \\(X_1^*,\\dots,X_n^*\\):\n\ni.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\).\n\n\n## generating a bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n\nThe distribution of \\(\\bar X -\\mu\\) is approximated by the conditional distribution of \\(\\bar X^* -\\bar X\\) given the original sample \\({\\cal S}_n\\) \\[\n\\underbrace{P\\left(\\bar{X}-\\mu<\\delta\\right)}_{\\text{unknown}}\\approx\n\\underbrace{P\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)}_{\\text{approximable}}\n\\]\n\nFor the given data with \\(n=8\\) observations, there are \\[\nn^n=8^8=16,777,216\n\\] possible bootstrap samples which are all equally probable.\nThe conditional distribution function of \\(\\bar{X}^*-\\bar{X}\\) given \\(\\mathcal{S}_n\\) \\[\nP\\left(\\bar{X}^*-\\bar{X}<\\delta|\\mathcal{S}_n\\right)\n\\] can be approximated using a Monte-Carlo simulation. For this, we draw new data from \\(F_n,\\) i.e., we sample with replacement data points from the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUsing a large number \\(m\\) (e.g. \\(m=10000\\)) of simulation runs allows us to generate bootstrap estimates \\[\n\\bar{X}^*_1,\\bar{X}^*_2,\\dots,\\bar{X}^*_m\n\\]\n\nThese bootstrap estimates are then used to approximate \\[\n\\frac{1}{m}\\sum_{k=1}^m\nI( \\bar X^*_k-\\bar X\\leq \\delta) \\approx \\underbrace{P\\left(\\bar X^*-\\bar X\\leq \\delta |{\\cal S}_n\\right)}_{\\text{bootstrap distribution}},\n\\] where the approximation will be arbitrarily precise as \\(m\\to\\infty\\).\n\n\n\n\n\n\n\nNotation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one also frequently finds the notation \\(E^*(\\cdot),\\) \\(Var^*(\\cdot),\\) or \\(P^*(\\cdot)\\) to denote conditional expectations \\(E^*(\\cdot)=E(\\cdot|\\mathcal{S}_n),\\), variances \\(Var^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\\), or probabilities \\(P^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\\) given the sample \\({\\cal S}_n.\\)\n\n\n\nThe bootstrap distribution of \\(\\bar X^*\\)\nThe bootstrap focuses on the conditional distribution of \\(X_1^*,\\dots,X_n^*\\) given the observed sample \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\) and the resulting conditional distribution of \\[\n(\\bar X^* -\\bar X)|\\mathcal{S}_n\n\\] Often these conditional distributions are called bootstrap distributions.\n🤟 We can analyze the bootstrap distribution of \\(\\bar X^* -\\bar X\\), since we know the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F.\\)\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\\n&\\vdots\\\\\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the (conditional) discrete random variable \\(X_i^*|\\mathcal{S}_n\\) and can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) is \\[\n\\begin{align*}\nE^*(X_i^*)\n&=E(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_2\\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(X_i^*\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\\n&=E((X_i^* - E(X_i^*|{\\cal S}_n))^2|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\\\\n&=\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any measurable function \\(g\\) we have \\[\nE^*(g(X_i^*))=E(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\]\n\n\n\n\n\n\n\n\nConditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important. The marginal (non-conditional) distribution of \\(X_i^*\\) is equal to the distribution of \\(X_i.\\) This follows from the fact that for \\[\n\\begin{align*}\nP(X_i^*\\leq \\delta)\n&= E\\left(I\\left(X_i^*\\leq \\delta\\right)\\right)\\\\\n&= E\\left[E\\left(I\\left(X_i^*\\leq \\delta\\right)|\\mathcal{S}_n\\right)\\right]\\\\\n&= E\\left[\\frac{1}{n}\\sum_{i=1}^nI\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= E\\left[I\\left(X_i\\leq \\delta\\right)\\right]\\\\\n&= P\\left(X_i\\leq \\delta\\right)=F(\\delta)\n\\end{align*}\n\\] But we do not know \\(F.\\)\n\n\nNow consider the bootstrap distribution of \\(\\bar X^*\\)\nSince we know the distribution of the i.i.d. sample \\[\nX_1|\\mathcal{S}_n, X_2|\\mathcal{S}_n,\\dots, X_n|\\mathcal{S}_n\n\\] it is straight forward to derive the asymptotic distribution of \\(\\bar X^*\\) using the central limit theorem.\nFirstly, let us derive the conditional mean and variance of \\[\n\\bar X^* = \\frac{1}{n}\\sum_{i=1}^nX_i^*\n\\]\n\nThe conditional mean of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nE^*(\\bar X^*)\n&=E(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n \\bar X\\\\\n&=\\frac{n}{n}\\bar X \\\\\n&=\\bar X\n\\end{align*}\n\\]\nThe conditional variance of \\(\\bar X^*\\) is \\[\n\\begin{align*}\nVar^*(\\bar X^*)\n&=Var(\\bar X^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n Var(X_i^*|{\\cal S}_n)\\\\\n&=\\frac{1}{n^2}\\sum_{i=1}^n \\hat\\sigma^2\\\\\n&=\\frac{n}{n^2}\\hat\\sigma^2\\\\\n&=\\frac{1}{n}\\hat\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nWe can apply the CLT to \\(\\bar X^*|\\mathcal{S}_n\\)\n\n\n\nNote that, conditionally on \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}\\),\n\nthe random variables \\(X_1^*,\\dots,X_n^*\\) are i.i.d.\nwith mean \\(E^*(X_i^*)=\\bar X\\)\nand variance \\(Var^*(X^*)=\\hat\\sigma^2\\)\n\nThus, we can apply the central limit theorem (Lindeberg-Lévy) to the appropriately scaled sample mean conditionally on \\({\\cal S}_n\\) \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* - \\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\n\\]\n\n\nThe central limit theorem (Lindeberg-Lévy) implies that \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\hat\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat\\sigma^2\\) is a consistent estimator of \\(\\sigma^2,\\) and thus asymptotically \\(\\hat\\sigma^2\\) may be replaced by \\(\\sigma\\). Therefore, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\bar X^* -\\bar X)}{\\sigma}\\right|{\\cal S}_n\\right)\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nOn the other hand, we also have that \\[\n\\left(\\frac{\\sqrt{n}(\\bar X -\\mu )}{ \\sigma}\\right)\n\\rightarrow_{d} \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nThis means that the bootstrap is consistent. The bootstrap distribution of \\(\\sqrt{n}(\\bar X^* -\\bar X)\\) asymptotically coincides with the distribution of \\(\\sqrt{n}(\\bar X-\\mu)\\) as \\(n\\rightarrow \\infty\\). In other words, for large \\(n\\), \\[\n\\text{distribution}(\\bar X^* -\\bar X|{\\cal S}_n)\\approx \\text{distribution}(\\bar X-\\mu).\n\\]\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nP(\\bar{X}^*-\\bar{X}\\leq \\delta|\\mathcal{S}_n),\n\\] which we can approximate with arbitrary precision (as \\(m\\to\\infty),\\) and which we thus can use as a tool for doing inference about \\(\\mu.\\)\n\n\n\n3.3.2 Example: Inference about a population proportion\nSetup:\n\nData: i.i.d. random sample \\(X_1,\\dots,X_n,\\) where \\(X_i\\in\\{0,1\\}\\) is dichotomous and \\(P(X_i=1)=p\\), \\(P(X_i=0)=1-p\\).\nEstimator: Let \\(S\\) denote the number of \\(X_i\\) which are equal to \\(1.\\) The maximum likelihood estimate of \\(p\\) is \\(\\hat p=S/n.\\)\nProblem: Inference about \\(p\\).\n\nRecall:\n\n\\(n\\hat p=S\\sim B(n,p)\\)\nAs \\(n\\rightarrow\\infty\\) the central limit theorem implies that \\[\n\\frac{\\sqrt{n}(\\hat p -p)}{\\sqrt{p(1-p)}}\\rightarrow_d \\mathcal{N}(0,1)\n\\]\nThus for \\(n\\) large, the distributions of \\(\\sqrt{n}(\\hat p -p)\\) and \\(\\hat p -p\\) can be approximated by \\(\\mathcal{N}(0,p(1-p))\\) and \\(\\mathcal{N}(0,p(1-p)/n)\\), respectively.\n\nBootstrap:\n\nRandom sample \\(X_1^*,\\dots,X_n^*\\) generated by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\n\\]\nLet \\(S^*\\) denote the number of \\(X_i^*\\) which are equal to \\(1.\\)\nBootstrap estimate of \\(p\\): \\(\\hat p^*=S^*/n\\)\n\nThe distribution of \\(\\hat p^*\\) depends on the observed sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\). A different sample \\({\\cal S}_n\\) will lead to a different distribution. The bootstrap now tries to approximate the true distribution of \\(\\hat p - p\\) by the conditional distribution of \\(\\hat p^*-\\hat p\\) given the observed sample \\({\\cal S}_n\\).\nThe bootstrap is called consistent if asymptotically (i.e. for \\(n\\rightarrow \\infty\\)) the conditional distribution of \\(\\hat p^*-\\hat p\\) coincides with the true distribution of \\(\\hat p - p.\\) (Note: a proper scaling is required!)\nWe obtain \\[\n\\begin{align*}\n& P^*(X_i^*=1)=P(X_i^*=1|\\ {\\cal S}_n)=\\hat p, \\\\  \n& P^*(X_i^*=0)=P(X_i^*=0|\\ {\\cal S}_n)=1-\\hat p\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n&   E^*(\\hat p^*)=E(\\hat p^*|\\ {\\cal S}_n)=\\hat p, \\\\\n& Var^*(\\hat p^*)=E[(\\hat p^*-\\hat p)^2|\\ {\\cal S}_n]=\\frac{\\hat p(1-\\hat p)}{n}\n\\end{align*}\n\\]\nThe bootstrap distribution of \\(n\\hat p^*=S^*\\) given \\({\\cal S}_n\\) is equal to the binomial distribution \\(B(n,\\hat p).\\) That is, for large \\(n\\), the bootstrap distribution of \\[\n\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\n\\] is approximately standard normal. In other words, \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{\\hat p(1-\\hat p)}}\\right|{\\cal S}_n\\right) \\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nMoreover, \\(\\hat p\\) is a consistent estimator of \\(p\\) \\[\n\\hat p(1-\\hat p)\\rightarrow_p p(1-p),\\quad n\\rightarrow\\infty.\n\\] This implies that asymptotically \\(\\hat p(1-\\hat p)\\) may be replaced by \\(p(1-p)\\), and \\[\n\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{ \\sqrt{p(1-p)}}\\right|{\\cal S}_n\\right)\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] More precisely, as \\(n\\rightarrow\\infty\\) \\[\n\\sup_\\delta \\left|\n  P\\left(\\left.\\frac{\\sqrt{n}(\\hat p^* -\\hat p)}{\\sqrt{p(1-p)}}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat p^* -\\hat p)|{\\cal S}_n)\\approx\n\\text{distribution}(\\sqrt{n}(\\hat p -p))%\\approx N(0,p(1-p))\n\\] as well as \\[\n\\text{distribution}(\\hat p^* -\\hat p|{\\cal S}_n)\\approx\n\\text{distribution}(\\hat p -p)%\\approx N(0,p(1-p)/n)\n\\]\n\n\n3.3.3 Confidence intervals\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if\n\n\\(\\theta\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2)\\) as \\(n\\to\\infty,\\)\n\nthen one traditionally tries to determine an approximation \\(\\hat v\\) of \\(v\\) (the standard error of \\(\\hat\\theta\\)) from the data. An approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}},\n\\hat{\\theta}+z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v}{\\sqrt{n}}\n\\right]\n\\]\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v\\) of \\(v\\). Statistical inference is then usually based on the bootstrap.\nIn contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates \\(\\hat v\\) of \\(v\\) are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed more precise than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\n\nThe bootstrap approach\n\nSetup:\n\nData: i.i.d. random sample \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter (vector) \\(\\theta.\\)\nProblem: Construct a confidence interval for \\(\\theta.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^* -\\hat{\\theta})|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}-\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.\n\n\nDerivation of the nonparametric bootstrap confidence intervals:\n\nWe can generate \\(m\\) bootstrap estimates \\[\n\\hat\\theta_k^*\\equiv\\hat\\theta(X_{1k}^*,\\dots,X_{nk}^*),\\quad k=1,\\dots,m,\n\\] by drawing bootstrap samples \\(X_{1k}^*,\\dots,X_{nk}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nThe \\(m\\) bootstrap estimates allow us to approximate the \\(\\frac{\\alpha}{2}\\) quantile \\(\\hat t_\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantile \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat t_{p}=\\left\\{\n\\begin{array}{ll}\n\\hat\\theta^*_{(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n(\\hat\\theta^*_{(mp)}+\\hat\\theta^*_{(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.1}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(i)}^*\\) denotes the order statistic \\[\n\\hat\\theta_{(1)}^* \\leq \\hat\\theta_{(2)}^*\\leq \\dots\\leq \\hat\\theta_{(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp.\\)\n\nThen \\[\n\\begin{align*}\n&P^*\\left(\\hat t_\\frac{\\alpha}{2} \\leq \\hat{\\theta}^* \\leq \\hat t_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}^*-\\hat{\\theta} \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Here, \\(P^*\\) denotes probabilities with respect to the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\).\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}.\n\\] Therefore, for large \\(n\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}-\\theta)}\\leq \\sqrt{n}(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta}\\leq\\hat{\\theta}-\\theta \\leq \\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(\\hat{\\theta}-(\\hat t_{1-\\frac{\\alpha}{2}}-\\hat{\\theta})\\le \\theta\\le \\hat{\\theta}-\n(\\hat t_\\frac{\\alpha}{2}-\\hat{\\theta})\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow &P\\left(2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}\\le \\theta\\le 2\\hat{\\theta}-\n\\hat t_\\frac{\\alpha}{2}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\]\nThus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap confidence interval is given by \\[\n\\left[2\\hat{\\theta}-\\hat t_{1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}-\\hat t_\\frac{\\alpha}{2}\\right],\n\\tag{3.2}\\] where \\(\\hat t_\\frac{\\alpha}{2}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) are the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution approximated by the empirical quantiles of the \\(m\\) bootstrap realizations \\(\\hat{\\theta}^*_1, \\hat{\\theta}^*_2,\\dots, \\hat{\\theta}^*_m.\\)\n\n\nExample: Confidence intervals for the population mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample with mean \\(\\mu\\) and variance \\(\\sigma^2\\). \nEstimator: \\(\\bar X=\\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu\\)\nInference Problem: Construct a confidence interval for \\(\\mu.\\)\n\n\n\nThe traditional (non-bootstrap) approach\nTraditional, non-bootstrap approach for constructing a \\((1-\\alpha)\\times 100\\%\\) confidence interval:\n\nBy the CLT: \\(\\bar X\\overset{a}{\\sim} \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n})\\) for large \\(n\\)\nEstimation of \\(\\sigma^2\\): \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X -\\mu)/S)\\overset{a}{\\sim} t_{n-1}\\), and hence \\[\n\\begin{align*}\n&P\\left(-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\bar X -\\mu\\le t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\\n\\Rightarrow\n&P\\left(\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\le \\mu\\le\n      \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\n\\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X-t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}},\n    \\bar X+t_{n-1,1-\\frac{\\alpha}{2}}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis traditional construction relies on the assumption that \\[\n\\bar X \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\] \\(\\bar X\\) is exactly normal distributed (also for small \\(n\\)) if the random sample \\(X_1,\\dots,X_n\\) is i.i.d. normally distributed. If the underlying distribution is not normal, then this condition is approximately fulfilled if the sample size \\(n\\) is sufficiently large (central limit theorem). In this case the constructed confidence interval is an approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\n\n\nThe nonparametric (standard) bootstrap approach\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation.\nConstruction of the nonparametric (standard) bootstrap confidence interval:\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10000\\)) and calculate the corresponding estimates \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\).\nCompute the empirical quantiles \\(\\hat t_{\\frac{\\alpha}{2}}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_1,\\bar X^*_2,\\dots,\\bar X^*_m\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\bar X-\\hat t_{1-\\frac{\\alpha}{2}},\n    2\\bar X-\\hat t_\\frac{\\alpha}{2}\\right]\n\\]"
  },
  {
    "objectID": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "href": "Ch_Bootstrap.html#pivot-statistics-and-the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.4 Pivot statistics and the bootstrap-\\(t\\) method",
    "text": "3.4 Pivot statistics and the bootstrap-\\(t\\) method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of “studentized bootstrap”). The construction relies on so-called pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\nDefinition 3.3 (Asymptotically pivotal statistics) A statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called (exact) pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter. A statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator \\(\\hat{\\theta}\\) satisfies \\[\n\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d\\mathcal{N}(0,v^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator \\[\n\\hat v_n^2\\equiv \\hat v_n(X_1,\\dots,X_n)^2\n\\] of \\(v\\) such that \\[\n\\hat v_n^2 \\rightarrow_p v^2.\n\\] Then, of course, also \\(\\hat v_n\\rightarrow_p v\\), and \\[\nT_n:= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] This means that \\(T_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\\) is asymptotically pivotal.\n\nExample: \\(\\bar{X}\\) is asymptotically pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(E(X)=\\mu\\), variance \\(Var(X)=\\sigma^2>0\\), and \\(E(|X|^4)=\\beta<\\infty\\).\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\sim t_{n-1}\n\\] with \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n:=\\frac{\\sqrt{n}(\\bar X-\\mu)}{S}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistics.\n\n\nBootstrap-\\(t\\) consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*=\\sqrt{n}\\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*},\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\[\n\\hat v_n^*=v_n(X_1^*,\\dots,X_n^*)\n\\] is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*.\\)\n\n\n\n\n\n\nBootstrap-\\(t\\) consistency follows if the standard nonparametric bootstrap is consistent\n\n\n\nIf the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_\\delta \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*-\\hat{\\theta})}{\\hat v_n^*}\\le \\delta\\right|{\\cal S}_n\\right)-\\Phi(\\delta)\\right|\\rightarrow_p 0,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.4.1 Bootstrap-t confidence interval\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample and let the distribution \\(X_i\\sim F\\), \\(i=1,\\dots,n,\\) depend on the unknown parameter (vector) \\(\\theta\\). Assume that bootstrap is consistent and that the estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is asymptotically normal. Furthermore, suppose that a consistent estimator \\[\n\\hat v\\equiv \\hat v(X_1,\\dots,X_n)\n\\] of the asymptotic standard deviation \\(v\\) is available.\nDerivation of the bootstrap-\\(t\\) confidence interval:\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\(\\hat{\\theta}^*\\) and \\(v^*\\) and the bootstrap statistic \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) Repeating this yields \\(m\\) many bootstrap statistics \\[\nT_1^*,T_2^*, \\dots, T_m^*\n\\] which allow us to approximate the bootstrap distribution of of \\(T^*\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}.\\) arbitrarily precise as \\(m\\to\\infty.\\)\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(T^*=\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}\\) using the empirical quantiles based on \\(T_1^*,T_2^*, \\dots, T_m^*.\\) (see Equation 3.1)\n\nThis implies \\[\n\\begin{align*}\n&P^*\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Due to the assumed consistency of the bootstrap, we have that for large \\(n\\) that \\[\n{\\color{red}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v^*}}|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}}\n\\] Therefore, for lage \\(n\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau_\\frac{\\alpha}{2}\\leq {\\color{blue}\\frac{\\hat{\\theta}-\\theta}{\\hat v}} \\leq \\hat \\tau_{1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta-\\hat{\\theta} \\leq -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\\\\\n\\Rightarrow & P\\left(\\hat{\\theta}-\\hat v \\hat \\tau_{1-\\frac{\\alpha}{2}}\\leq \\theta \\leq \\hat{\\theta} -\\hat v\\hat\\tau_\\frac{\\alpha}{2}\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval is given by \\[\n\\left[\\hat{\\theta}-\\hat \\tau_{1-\\frac{\\alpha}{2}}\\hat v,\n      \\hat{\\theta}-\\hat \\tau_{  \\frac{\\alpha}{2}}\\hat v\\right]\n\\tag{3.3}\\]\n\nExample: Bootstrap-\\(t\\) confidence interval for the mean\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(S^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*)^2\\).\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_\\frac{\\alpha}{2}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution of \\(\\frac{\\bar X^*-\\bar X}{S^*}\\)\nThis yields the \\(1-\\alpha\\) confidence interval (using Equation 3.3): \\[\n\\left[\\bar X-\\hat \\tau_{1-\\frac{\\alpha}{2}}S,\n    \\bar X-\\hat \\tau_{\\frac{\\alpha}{2}}S\\right]\n\\]\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the standard nonparametric bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\(T_n^*=\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta})/v^*_n\\) is more direct and hence more accurate (also \\(v^*n\\) depends on the bootstrap sample) than the approximation of the law of \\(\\sqrt{n}(\\hat{\\theta}-\\theta)\\) by the bootstrap law of \\(\\sqrt{n}(\\hat{\\theta}^*-\\hat{\\theta}).\\)\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nNonparametric (standard) boostrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.5 Regression Analysis: Bootstrapping pairs",
    "text": "3.5 Regression Analysis: Bootstrapping pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\nDefinition 3.4 (Random and fixed design) \nRandom design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(E(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a fixed \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(E(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\(E(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\nWe additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=E(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=E(\\varepsilon_i^2X_iX_i^T)=E(\\sigma^2(X_i)X_iX_i^T)\n\\] Note: For homoscedastic errors we have \\(Q=\\sigma^2 M.\\)\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\n\n3.5.1 Bootstrapping pairs: Bootstrap under random design\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.4). Assuming a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\nThis allows to construct basic bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nof the conditional distribution of \\(\\hat\\beta_j^*\\) given \\({\\cal S}_n.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroscedastic. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages."
  },
  {
    "objectID": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Residual bootstrap",
    "text": "3.6 Regression Analysis: Residual bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] under fixed design (Definition 3.4), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(E(\\varepsilon_i)=0\\) and homoscedastic errors \\[\nE(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\nE(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe residual bootstrap algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n.\\)\n\n\nMotivating the residual bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\nE(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\n\\] Thus, by the central limit (Lindeberg-Lévy) one obtains that \\[\n\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 (\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince \\(\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_P \\sigma^2\\) as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.6.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the conditional distribution of \\(\\hat\\beta_j^*|\\mathcal{S}_n\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_1^*,\\hat\\beta_2^*, \\dots, \\hat\\beta_m^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.2: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), \\[\n\\gamma_{jj}:=\\left[(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] is an asymptotically pivotal statistics, since \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nAdditionally compute \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] and approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.3: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right]\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html",
    "href": "Ch_MaximumLikelihood.html",
    "title": "4  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. We must therefore be very specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question always remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment. Let \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)\n\n\n\n\nHow do we combine the information from the \\(n\\) observations to estimate \\(\\theta\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)\n&= \\left(P(\\text{Coin}=H)\\right)^h\\left(P(\\text{Coin}=T)\\right)^{n-h}\\\\\n&= \\theta^h(1-\\theta)^{n-h}  \\\\\n&= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\text{Coin}=H\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\text{Coin}=T\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations \\(\\{x_1,\\dots,n\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\), and where \\(\\theta\\) denotes the (unknown) parameter (vector) of the density function.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density functions \\(f\\) of the i.i.d. sample variables \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) such that \\(f\\) is known up to the parameter (vector) \\(\\theta.\\)\nExamples:\n\n\\(f\\) being the probability mass function of \\(\\mathcal{B}(\\theta)\\) with \\(f(x_i|\\theta)=\\theta\\) if \\(x_i=1\\) and \\(f(x_i|\\theta)=1-\\theta\\) if \\(x_i=0,\\) but unknown propability parameter \\(\\theta.\\)\n\\(f\\) is the normal density \\(f(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\\) with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement can be overly restrictive. In many applications we typically do not know the general distribution family of \\(f.\\) To adress this issue, the quasi maximum likelihood method generalizes classic ML estimation to cases where \\(f\\) is misspecified.\n\n\nEstimation idea: We estimate the unknown \\(\\theta\\) by maximizing the likelihood of the observed data \\(\\{x_1,\\dots,x_n\\}.\\) The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator \\[\n\\begin{align*}\n\\hat\\theta\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\prod_{i=1}^n f(x_i|\\theta)\n\\end{align*}\n\\]\nIn our coin flip example this means to estimate the unknown \\(\\theta\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed \\(0\\) and \\(1\\) outcomes \\(\\{x_1,\\dots,x_n\\}\\) is maximal \\[\n\\hat\\theta_{ML} = \\arg\\max_\\theta \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.\n\\]\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\] Since this is only a monotonic transformation we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_\\theta \\mathcal{L}(\\theta)\\\\\n&=\\arg\\max_\\theta \\ell(\\theta),\n\\end{align*}\n\\] but \\(\\ell(\\theta)\\) gives a more simple structure simplifying the maximization problem.\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn the coin flip example, \\(\\ell(\\theta)\\) is so simple that we can maximize \\(\\ell(\\theta)\\) analytically: \\[\n\\begin{align*}\n\\dfrac{d \\ell(\\theta)}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\dfrac{d \\ell(\\hat\\theta_{ML})}{d \\theta}&\\overset{!}{=}&0\\\\\n\\Leftrightarrow&\\dfrac{h}{\\hat\\theta_{ML}} &=& \\dfrac{n-h}{1-\\hat\\theta_{ML}} \\\\\n\\Leftrightarrow&h-h\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-h\\hat\\theta_{ML}\\\\\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{h}{n}\n\\end{array}\n\\]\nOften, however, the log-likelihood function is so complicated that there is no analytic solution and one needs to apply numeric optimization algorithms."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#properties-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Properties of Maximum Likelihood Estimators",
    "text": "4.2 Properties of Maximum Likelihood Estimators\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta\\) of some parameter \\(\\theta\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: For any other consistent estimator \\(\\tilde\\theta_n\\), \\(\\tilde\\sigma^2\\ge \\sigma^2\\).\n\nThus, maximum likelihood estimators can be very appealing.\n\nExample: Coin Flipping (Bernoulli Trial)\nLet \\(\\theta\\) denote the probability that we get a head \\(H\\) \\[\n\\theta=P(\\text{Coin}=H)\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta=P(\\text{Coin}=T).\n\\]\nWe don’t know the probability \\(\\theta\\) and our goal is to estimate \\(\\theta\\) using an i.i.d. sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}%\\in\\{0,1\\}^n\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    0 & \\text{if Coin}=T\\\\\n    1 & \\text{if Coin}=H\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{B}(\\theta),\\quad i=1,\\dots,n,\n\\] where \\(\\mathcal{B}(\\theta)\\) denotes the Bernoulli distribution with unknown parameter \\(\\theta.\\)\nA given realization of the random sample \\[\n\\{X_1,\\dots,X_n\\}=\\{x_1,\\dots,x_n\\}\n\\] consists of \\[\n0\\leq h\\leq n\n\\] many heads \\(H\\) and \\[\n0\\leq n-h\\leq n\n\\] many tails \\(T.\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "href": "Ch_MaximumLikelihood.html#the-log-likelihood-function",
    "title": "4  Maximum Likelihood",
    "section": "4.3 The (Log-)Likelihood Function",
    "text": "4.3 The (Log-)Likelihood Function\nHow do we combine information from \\(n\\) observations to estimate \\(\\theta\\)?\nIf we assume that all of the observations are drawn from same distribution and are independent, then joint probability of observing \\(h\\) heads and \\(n-h\\) tails in the \\(n\\) coin flips that we actually observed, given \\(\\theta\\), is: \\[\n\\begin{align*}\n\\mathcal{L}(\\theta)&= \\theta^h(1-\\theta)^{n-h}  \\\\\n            &= \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\n\\end{align*}\n\\] where \\(x_i=1\\) stands for \\(\\texttt{HEAD}\\) in \\(i\\)th coin flip and \\(x_i=0\\) for \\(\\texttt{TAIL}\\) in \\(i\\)th coin flip. The function \\(\\mathcal{L}\\) is called the likelihood function.\nIn general, when the observations are identically and independently distributed (i.i.d): \\[\n\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(x_i|\\theta),\n\\] where \\(f(x_i | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_i=x_i\\); the parameter \\(\\theta\\) denotes the density function parameter(s).\nOur goal is to choose a value for \\(\\theta\\) such that the value of the likelihood function is at a maximum, i.e. we choose the value of the parameter(s) that maximize the “probability” or better the likelihood of observing the data that we actually observed. That is: \\[\n\\hat\\theta=\\arg\\max_\\theta \\mathcal{L}(\\theta).\n\\] defines the maximum likelihood (ML) parameter estimator \\(\\hat\\theta\\).\nUsually it’s easier to work with sums rather than products, so we can apply a monotonic transformation (taking \\(\\ln\\)) to the likelihood which yields to the log-likelihood function: \\[\n\\ell(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(x_i|\\theta).\n\\]\nIn our coin flipping example: \\[\n\\ell(\\theta)=\\sum_{i=1}^n\\left( x_i \\ln(\\theta) + (1-x_i)\\ln(1-\\theta)\\right)\n\\]\nIn this case, we can analytically solve for the value of \\(\\theta\\) that maximizes the log likelihood (and hence also the likelihood): \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\sum_{i=1}^n \\left(x_i\\dfrac{1}{\\theta} - (1-x_i)\\dfrac{1}{1-\\theta}\\right)\\\\\n                        &=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\end{align*}\n\\] Setting the above expression to zero and solving gives us our ML estimator (MLE): \\[\n\\hat\\theta_{ML}=\\dfrac{h}{n}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "href": "Ch_MaximumLikelihood.html#optimization-non-analytical-solutions",
    "title": "4  Maximum Likelihood",
    "section": "4.2 Optimization: Non-Analytical Solutions",
    "text": "4.2 Optimization: Non-Analytical Solutions\nUsually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the (log-)likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\nGeneral idea:\n\nStart at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)\nSearch across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or arbitrarily close to zero).\n\n\n4.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nLet \\(f\\) be a two times differentiable function to be optimized (maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial (Order 1)}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial (Order 2)}},\n\\end{align*}\n\\] Locally at \\(\\theta,\\) the Taylor polynomials are good approximations of \\(f\\) provided that \\(h\\) is relatively small (see Figure 4.1).\n\n\n\n\n\nFigure 4.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta_0=1.\\)\n\n\n\n\nIdea: A step-wise (\\(h\\) steps) optimization approach.  Instead of a (possibly infeasible) direct optimization of \\(f,\\) we select some starting value \\(\\theta_0\\) and optimize the second-order Taylor polynomial of \\(f\\) around \\(\\theta_0\\) with respect to \\(h.\\) In each of the following steps, we optimize new second-order Taylor polynomials of \\(f\\) at those values \\(\\theta_\\), for the previous Taylor polynomial was maximal.\nImplementation-Idea: The second-order Taylor-series approximation gives then \\[\n\\begin{align*}\nf(\\theta+h) & \\approx f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2\\\\\n\\Leftrightarrow \\frac{f(\\theta+h)-f(\\theta)}{h}&\\approx f'(\\theta) + \\frac{1}{2} f''(\\theta)h\n\\end{align*}\n\\] which implies \\[\n\\dfrac{\\partial f(\\theta+h)}{\\partial h} \\approx f'(\\theta) + f''(\\theta)h.\n\\]\nTherefore, the first-order condition for the value of \\(h\\) that maximizes the Taylor-series expansion \\(f(\\theta)+f'(\\theta)h + (1/2) f''(\\theta)h^2\\) is \\[\n0=f'(\\theta)+f''(\\theta)\\hat h,\n\\] giving \\[\n\\hat h = -\\frac{f'(\\theta)}{f''(\\theta)}.\n\\]\nThat is, in order to increase the value of \\(f(\\theta)\\) one shall substitute \\(\\theta\\) by \\[\n\\theta + \\hat h = \\theta- \\dfrac{f'(\\theta)}{f''(\\theta)}\n\\]\nThe Newton Raphson optimization algorithm uses this insight as following. We first must provide a starting value, \\(s\\), for \\(\\theta_0=s\\) and, second, decide on some (small) convergence criterion, \\(t\\), e.g. \\(t=10^{-10}\\), for the first derivative. Then the Newton Raphson optimization algorithm is given by: \\[\n\\begin{array}{ll}\n\\texttt{\\textbf{let }} \\theta_0=s  &  \\\\\n\\texttt{\\textbf{let }} i=0                &  \\\\\n\\texttt{\\textbf{while }}  | f'(\\theta_i) | >t & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} i = i+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_i = \\theta_{i-1} - \\frac{f'(\\theta_{i-1})}{f''(\\theta_{i-1})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta=\\theta_i & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta &  \\\\\n\\end{array}\n\\]\nNote: For problems that are globally concave, the starting value \\(s\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nNewton-Raphson Algorithm: Example: Let’s return to our earlier coin-flipping example, with only one head \\(h=1\\) for a sample size of \\(n=5\\). We already know that \\(\\hat\\theta_{ML}=\\frac{h}{n}=\\frac{1}{5}=0.2\\), but let’s apply the Newton-Raphson Algorithm. Recall that \\[\n\\begin{align*}\n\\dfrac{d \\ell}{d \\theta}&=\\dfrac{h}{\\theta} - \\dfrac{n-h}{1-\\theta} \\\\\n\\dfrac{d^2 \\ell}{d \\theta^2} &= -\\dfrac{h}{\\theta^2} - \\dfrac{n-h}{(1-\\theta)^2}\n\\end{align*}\n\\] We have \\(h=1\\) and \\(n=5\\). Choosing \\(t=10^{-10}\\) as our convergence criterion and \\(\\theta_0=0.4\\) as the starting value, allows us to run the algorithm which gives us the results shown in Table Table 4.1.\n\n\nTable 4.1: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data \\(h=1\\) with sample size \\(n=5\\).\n\n\n\n\n\n\n\n\nRepetition \\(i\\)\n\\(\\hat\\theta_i\\)\n\\(\\ell'(\\hat\\theta_i)\\)\n\\(\\ell'(\\hat\\theta_i)/\\ell''(\\hat\\theta_i)\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-4.16\\)\n\\(\\phantom{-}2.40\\cdot 10^{-1}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}1.48\\)\n\\(-3.32\\cdot 10^{-2}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}2.15\\cdot 10^{-1}\\)\n\\(-6.55\\cdot 10^{-3}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}5.43\\cdot 10^{-3}\\)\n\\(-1.73\\cdot 10^{-4}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}3.53\\cdot 10^{-6}\\)\n\\(-1.13\\cdot 10^{-7}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}1.50\\cdot 10^{-12}\\)\n\\(-4.81\\cdot 10^{-14}\\)"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "href": "Ch_MaximumLikelihood.html#ols-estimation-as-ml-estimation",
    "title": "4  Maximum Likelihood",
    "section": "4.3 OLS-Estimation as ML-Estimation",
    "text": "4.3 OLS-Estimation as ML-Estimation\nNow let’s return to the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{4.1}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta=(\\beta_1,\\dots,\\beta_p)^T\\in\\mathbb{R}^p\n\\] denotes the vector of unknown parameter values, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables, where the i.i.d. sample \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design (Definition 3.4).\nFor the following, it is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nTo apply ML-estimation, we must make a distributional assumption about \\(\\varepsilon_i\\) such as, for instance, \\[\n\\begin{equation*}\n\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^2I_n\\right).\n\\end{equation*}\n\\] We could also choose another distributional assumption for \\(\\varepsilon,\\) but the classic ML estimation theory requires us to assumed the correct error distribution. This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, this kind of distributional assumptions allow us to consider also more complicated non-linear regression models such as, for instance, logistic regression.\n\n\n\n\n\nThe multivariate density for \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is then \\[\n\\begin{equation*}\nf(\\varepsilon)=\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\left(\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)}.\n\\end{equation*}\n\\] Noting that \\(\\varepsilon=Y-X\\beta\\), we get the log likelihood \\[\n\\begin{align*}\n\\ell(\\beta,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta)\n\\end{align*}\n\\] with \\(K\\) unknown parameters \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) and \\(\\sigma^2\\) (scalar).\nTaking derivatives gives \\[\n\\begin{align*}\n\\dfrac{\\partial \\ell}{\\partial \\beta}    &= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta) \\\\\n\\dfrac{\\partial \\ell}{\\partial \\sigma^2}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}} \\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] So, we have \\(K+1\\) equations and \\(K+1\\) unknowns. Setting equal to zero and solving gives \\[\n\\begin{align*}\n\\hat\\beta_{ML}&=(X'X)^{-1}X'Y\\\\\ns_{ML}^2&=\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2\n\\end{align*}\n\\] Thus, the MLE of the linear model, \\(\\hat\\beta_{ML}\\), is the same as the OLS estimator, \\(\\hat\\beta\\). Moreover, since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\nAs it is needed for the next chapter, we also give here the second derivatives of the log-likelihood function \\(\\ell\\) as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function \\(\\ell\\):\n\nFirst and second derivative with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}&= - \\dfrac{1}{\\sigma^2}(X'X)\\\\\n\\Rightarrow\\quad (-1)\\cdot E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\beta\\partial \\beta}\\right)&= \\dfrac{1}{\\sigma^2}E(X'X)\\\\\n\\end{align*}\n\\]\nFirst and second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{\\left[(Y-X\\beta)'(Y-X\\beta)\\right]}{\\left(\\sigma^{2}\\right)^{3}} \\\\\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 \\ell}{\\partial \\sigma^2\\partial \\sigma^2} \\right)\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{E\\left[\\sum_{i=1}^n\\varepsilon_i^2\\right]}{\\sigma^{6}} \\\\\n&=-\\frac{n}{2\\sigma^{4}}+\\frac{n\\sigma^2}{\\sigma^{6}}\n=\\frac{n}{2\\sigma^{4}}\\\\\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\dfrac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2}=\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\n&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\\n& =\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\quad\\Rightarrow\\quad (-1)\\cdot  E\\left(\\dfrac{\\partial^2 L}{\\partial \\sigma^2 \\partial \\beta}\\right)\n&=\\frac{E(X'\\varepsilon)}{\\sigma^4}\\\\\n&=\\frac{E[E(X'\\varepsilon|X)]}{\\sigma^4}\\\\\n&=\\frac{E[X'E(\\varepsilon|X)]}{\\sigma^4}=0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "href": "Ch_MaximumLikelihood.html#variance-of-ml-estimators-hatbeta_ml-and-s2_ml",
    "title": "4  Maximum Likelihood",
    "section": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)",
    "text": "4.4 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\nThe variance of an MLE is given by the inverse of the Fisher information matrix. The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. For the OLS case, the Fisher information matrix is \\[\n\\begin{align*}\n\\mathcal{I}\\left(\\begin{array}{cc}\\beta \\\\ \\sigma^2\\end{array}\\right)\n&=\n\\left[\\begin{array}{cc}\n\\frac{1}{\\sigma^2}E(X'X) & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right]\\\\\n&=\n\\left[\\begin{array}{cc}\n\\frac{n}{\\sigma^2}\\Sigma_{X'X} & 0 \\\\\n0 & \\ \\frac{n}{2\\sigma^4}\n\\end{array}\\right],\n\\end{align*}\n\\] where we used that \\[\nE\\left(X'X\\right)=E\\left(\\sum_{i=1}^nX_iX_i'\\right)=n\\underbrace{E\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}}.\n\\] \nTaking the inverse of the Fisher information matrix gives the variance-covariance matrix of the estimators \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\n\\[\n\\begin{equation*}\nVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)=\n\\left[\\begin{array}{cc}\n\\frac{\\sigma^2}{n}\\Sigma_{X'X}^{-1} & 0 \\\\\n0 & \\ \\frac{2\\sigma^4}{n}\n\\end{array}\\right],\n\\end{equation*}\n\\] Given this result, it is easy to see that \\[\nVar(\\hat\\beta_{ML}) \\to 0\\quad\\text{and}\\quad Var(s_{ML}^2) \\to 0\n\\] as \\(n\\to\\infty\\)."
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "href": "Ch_MaximumLikelihood.html#consistency-of-hatbeta_ml-and-s_ml2",
    "title": "4  Maximum Likelihood",
    "section": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)",
    "text": "4.5 Consistency of \\(\\hat\\beta_{ML}\\) and \\(s_{ML}^2\\)\nIf \\(E[\\varepsilon|X]=0\\) (strict exogeneity, follows from the random design (Definition 3.4) assumption), then the bias of \\(\\hat\\beta\\) is zero since \\(E[\\hat\\beta_{ML}]=\\beta\\) \\[\n\\begin{align*}\nE[\\hat\\beta_{ML}]&=E[(X'X)^{-1}X'(X\\beta + \\varepsilon)] \\\\\n                 &=E[E[(X'X)^{-1}X'(X\\beta + \\varepsilon)|X]] \\\\\n                 &=E[E[(X'X)^{-1}X'X\\beta|X]] + E[E[(X'X)^{-1}X'\\varepsilon|X]] \\\\\n                 &=E[E[\\beta|X]] + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta + E[(X'X)^{-1}X'E[\\varepsilon|X]] \\\\\n                 &=        \\beta  \\\\\n\\Leftrightarrow E[\\hat\\beta_{ML}]-\\beta&=\\operatorname{Bias}(\\hat\\beta_{ML})=0\n\\end{align*}\n\\] Of course, from this it also follows that the squared bias is equal to zero \\[\n\\text{Bias}^2(\\hat\\beta_{ML})=0.\n\\]\nThis implies that the mean square error (MSE) of the ML estimator \\(\\hat\\beta_{ML}\\) equals the variance of the ML estimator \\(\\hat\\beta_{ML}\\): \\[\n\\operatorname{MSE}(\\hat\\beta_{ML})=\\underbrace{E[(\\hat\\beta_{ML}-\\beta)^2]=Var(\\hat\\beta_{ML})}_{\\text{MSE}(\\hat\\beta_{ML})=Var(\\hat\\beta_{ML})\\text{ since }\\hat\\beta_{ML}\\text{ is unbiased.}}\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(\\hat\\beta_{ML}\\) is a (weakly) consistent estimator of \\(\\beta\\) \\[\n\\hat\\beta_{ML}\\to_p \\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nMoreover, one can also show that \\(s_{ML}^2\\) is a biased but asymptotically unbiased estimator, that is \\[\n\\left(\\operatorname{Bias}(s^2_{ML})\\right)^2\\to 0\n\\] as \\(n\\to\\infty\\). Together with the result that \\(Var(s^2_{ML})\\to 0\\) as \\(n\\to\\infty\\) we have that \\[\n\\begin{align*}\n\\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\\sigma^2)^2]\\\\\n&=\\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator \\(s^2_{ML}\\) is a (weakly) consistent estimator of \\(\\sigma^2\\) \\[\ns^2_{ML}\\to_p \\sigma^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "href": "Ch_MaximumLikelihood.html#asymptotic-theory-of-maximum-likelihood-estimators",
    "title": "4  Maximum Likelihood",
    "section": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "4.6 Asymptotic Theory of Maximum-Likelihood Estimators\nSo far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume an i.i.d. sample \\(X_1,\\dots,X_n\\) with \\(X_i\\in\\mathbb{R}\\) for all \\(i=1,\\dots,\\), and suppose that the distribution of \\(X_i\\) possesses a density \\(f(x|\\theta),\\) where the true (unknown) parameter \\(\\theta\\in\\mathbb{R}\\) is an interior point of a compact parameter interval \\(\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\\) (“Interior point” means that \\(\\theta_l<\\theta<\\theta_u.\\))\nMoreover let\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i|\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i|\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\left.\\ell_n'(\\theta)\\right|_{\\theta=\\hat\\theta_n}=0\\quad\\text{and}\\quad\\left.\\ell_n''(\\theta)\\right|_{\\theta=\\hat\\theta_n}<0,\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x|\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x|\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x|\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x|\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta)dx\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nA possible example that fits into the above setup is the density of the exponential distribution \\[\nf(x|\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x|\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of \\(\\hat\\theta_n\\) relies on a Taylor expansion (around \\(\\theta\\)) of the derivative of the log-likelihood function \\[\n\\ell_n'(\\cdot).\n\\]\n\nTheorem 4.1 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\nBy the Mean Value Theorem (Theorem 4.1), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta)\n\\tag{4.2}\\] for some \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n).\\)\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 4.2, this implies that \\[\n\\ell_n'(\\theta)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta).\n\\tag{4.3}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x|\\theta)dx=1\n\\] for all possible values of \\(\\theta\\). Therefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1\\\\\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x|\\theta)dx=0.\n\\tag{4.4}\\] And similarly, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1\\\\\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x|\\theta)dx&=0.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x|\\theta)dx=0.\n\\tag{4.5}\\]\nUsing Equation 4.4 and Equation 4.5, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is asymptotically normal.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n}E\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=E\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x|\\theta)}\n{f(x|\\theta)}f(x|\\theta)dx\\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x|\\theta)dx\\\\\n&=0,\n\\end{align*}\n\\] where the last step follows from Equation 4.4.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\underbrace{E\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}{f(X_i|\\theta)}\\right)^2\\right)}_{=:\\mathcal{J}(\\theta)}\\\\\n&=\\frac{1}{n}\\mathcal{J}(\\theta)\n\\end{align*}\n\\]\nMoreover, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta)\n\\] is taken over i.i.d. random variables \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-L'evy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\hat{\\theta}_n)-E(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i|\\theta))}{\\sqrt{\\frac{1}{n}\\mathcal{J}(\\theta)} }=\\frac{\\ell_n'(\\hat{\\theta}_n)}{\\sqrt{n\\mathcal{J}(\\theta)} } \\to_d \\mathcal{N}(0,1)\n\\] Thus using our mean value expression (Equation 4.3), we also have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\cdot \\mathcal{J}(\\theta)}}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{4.6}\\]\nFurther analysis requires us to study the statistic \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\).\n\n\n\n\n\n\nImportant\n\n\n\nHowever, before we consider \\(\\frac{1}{n}\\ell_n''(\\psi_n),\\) with \\(\\psi_n\\in(\\theta,\\hat\\theta_n),\\) we begin this with studying the mean and the variance of the simpler statistic \\[\n\\frac{1}{n}\\ell_n''(\\theta).\n\\]\n\n\nThe mean of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i|\\theta)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n\\frac{1}{n}\\ell_n''(\\theta)\n&=\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i|\\theta)\\right) f(X_i|\\theta)-\\frac{\\partial}{\\partial\\theta}f(X_i|\\theta)\\frac{\\partial}{\\partial\\theta} f(X_i|\\theta)}{\\left(f(X_i|\\theta)\\right)^2}\\right).\n\\end{align*}\n\\] Taking the mean of \\(\\frac{1}{n}\\ell_n''(\\theta)\\) yields: \\[\n\\begin{align*}\nE\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=\\frac{n}{n}E\\left( \\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i|\\theta)}\n{f(X_i|\\theta)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\\n&=0 - E\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i|\\theta)}\n{f(X_i|\\theta)}\\right)^2\\right)\\\\\n&=-\\mathcal{J}(\\theta)\n\\end{align*}\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is an unbiased estimator of \\(-\\mathcal{J}(\\theta)\\), i.e. \\[\n\\begin{align*}\n\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)-\\mathcal{J}(\\theta)\\\\\n&=0.\n\\end{align*}\n\\]\nThe variance of variance of \\(\\frac{1}{n}\\ell_n''(\\theta):\\) \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i|\\theta)\\right)\\\\\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i|\\theta)\\right)}_{=\\text{some fixed, deterministic number}}\\\\\n&=\\frac{1}{n}\\texttt{constant}\n\\end{align*}\n\\] which implies that \\[\nVar\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) of \\(-\\mathcal{J}(\\theta):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(\\frac{1}{n}\\ell_n''(\\theta), -\\mathcal{J}(\\theta)\\right)\\\\\n&=\nE\\left(\\left(\\frac{1}{n}\\ell_n''(\\theta) -\\left(-\\mathcal{J}(\\theta)\\right)\\right)^2\\right)\\\\\n&=\\left(\\operatorname{Bias}\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\right)^2+Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\\\\n&=Var\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\]\nThat is, the estimator \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is a mean square consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_{m.s.} -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta)\\) is also a (weakly) consistent estimator \\[\n\\frac{1}{n}\\ell_n''(\\theta)\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{as}\\quad n\\to\\infty\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 We wanted to study \\(\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 4.6 not \\(\\frac{1}{n}\\ell_n''(\\theta)\\)! Luckily, we are actually close now.\n\n\nWe know that the ML estimator \\(\\hat\\theta_n\\) is (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nSince \\(\\psi_n\\in(\\theta,\\hat{\\theta}_n)\\) is a value between \\(\\theta\\) and \\(\\hat{\\theta}_n\\) (Equation 4.2), the consistency of \\(\\hat{\\theta}_n\\) implies that also \\[\n\\psi_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have (by the continuos mapping theorem) that also \\[\n\\begin{align}\n\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p -\\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\\n-\\frac{1}{n}\\ell_n''(\\psi_n)&\\to_p \\mathcal{J}(\\theta)\\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\] \nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 4.6 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{J}(\\theta)}}\\right)}_{\\to_p \\sqrt{\\mathcal{J}(\\theta)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{J}(\\theta)}\\right)\n\\end{align*}\n\\] which is the asymptotic normality result we aimed for. Note that \\[\n\\begin{align*}\n\\mathcal{J}(\\theta)\n&=-E\\left(\\frac{1}{n}\\ell_n''(\\theta)\\right)\n&=-E\\left(\\left(\\frac{\\partial}{\\partial\\theta^2}\\ln f(X_i|\\theta)\\right)^2\\right)\n=\\mathcal{I}(\\theta)\n\\end{align*}\n\\] , where \\(\\mathcal{I}(\\theta)\\) is called the “Fisher information”.\n\n\n\n\n\n\nNote\n\n\n\nThe above arguments can easily be generalized to multidimensional parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{J}(\\theta)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\hat{\\theta}_n-\\theta\\to_d \\mathcal{N}_p\\left(0,\\frac{1}{n} \\mathcal{J}(\\theta)^{-1}\\right),\n\\] where \\(n\\mathcal{J}(\\theta)=-E(\\ell_n''(\\theta))=\\mathcal{I}(\\theta)\\) is then called “Fisher information matrix”."
  },
  {
    "objectID": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "href": "Ch1_Random_Variable_Generation.html#building-on-exponential-rvs",
    "title": "2  Random Variable Generation",
    "section": "2.4 Building on Exponential RVs",
    "text": "2.4 Building on Exponential RVs\nIn Example 2.5, we learned to generate an exponential random variable \\(X\\) starting from a uniform random variable \\(U\\sim\\mathcal{U}[0,1].\\) In the following we generate random variables starting from an exponential distribution:\nIf the \\(X_1, X_2,\\dots\\) are i.i.d. as \\(X\\sim\\mathrm{Exp}(1),\\) then\n\\[Y\\sim \\chi^2_{2\\nu}\\quad\\text{if}       \\quad Y= 2     \\sum_{i=1}^\\nu X_i,\\quad\\nu=1,2,\\dots \\] \\[Y\\sim \\Gamma(\\alpha,\\beta)\\quad\\text{if}\\quad Y= \\beta \\sum_{i=1}^\\alpha X_i,\\quad \\alpha=1,2,\\dots \\] \\[Y\\sim \\mathrm{Beta}(a,b)\\quad\\text{if}  \\quad Y= \\frac{\\sum_{i=1}^a X_i}{\\sum_{j=1}^{a+b} X_j},\\quad a,b=1,2,\\dots \\]\nSome Limitations:\n\nThere are more efficient algorithms to generate Gamma and Beta RVs.\nWe cannot use exponential RVs to generate Gamma RVs with a non-integer shape parameter \\(\\alpha\\). * This implies that we cannot generate a \\(\\chi^2_{1}\\) RV, which would, in turn, get us a \\(N(0,1)\\) RV. (Reminder: \\(\\chi^2_{1}\\) is identical to \\(\\Gamma(1/2, 2)\\).)\nFor that we look at the Box-Muller Theorem (1958) and the derived algorithm.\n\n\n\n\n\nExample: Normal Variable Generation\nThe well-known Box-Muller algorithm for generating (standard) normal RV is based on the following theorem:\n\nTheorem (Box and Muller, 1958)\n\nIf \\(U_1\\) and \\(U_2\\) are i.i.d. \\(U[0,1]\\), then \\[X_1 =\\sqrt{-2 \\log(U_1)}\\, \\cos(2\\pi U_2)\\quad\\text{and}\\quad X_2=\\sqrt{-2\\log(U_1)}\\,\\sin(2\\pi U_2)\\] are i.i.d. \\(N(0,1)\\).\n\n\nIdea & Proof: Done in the lecture.\n\n\nImplementation of the Box-Muller algorithm:\n\n# Implementation:\nBM_Algo <- function(){\n  # 1. Step: Generate U_1, U_2 iid U[0,1]\n  U <- runif(2)\n  # 2. Step: Transformation\n  X1 <- sqrt(-2 * log(U[1])) * cos(2 * pi * U[2])\n  X2 <- sqrt(-2 * log(U[1])) * sin(2 * pi * U[2])\n  return(c(X1, X2))\n}\n\n# Generation of Stand. Normal RVs through the Box-Muller Algo:\nset.seed(123)\nX_vec <- NULL\nfor(i in 1:500){\n  X_vec <- c(X_vec, BM_Algo())\n}\n\n# Descriptive Plots\npar(mfrow=c(1,2))\nhist(X_vec, freq = FALSE)\ncurve(dnorm, add = TRUE, col=\"blue\", lwd=1.3)\nqqnorm(X_vec)\n\n\n\n\n\n\n\n# Testing for Normality using the Shapiro-Wilk Test (H0: Normality)\nshapiro.test(X_vec)\n\n\n    Shapiro-Wilk normality test\n\ndata:  X_vec\nW = 0.99893, p-value = 0.8323\n\n\n\n\n\n\n\n\n2.4.1 Accept-Reject Methods\nFor many distributions it is difficult (or impossible) to apply the Inverse or Transformation Methods, since the cdf \\(F(.)\\) is somehow unusable. For instance, surprisingly often there is no explicit form of \\(F(.)\\) available or its inverse does not exists.\nAccept-Reject Methods methods can provide a solution here, since they only require the knowledge of the functional form of the density \\(f\\) of interest up to a multiplicative constant. No deep analytic study of \\(f\\) is necessary.\n\n\nGeneral Idea and theoretical justification through the Fundamental Theorem of Simulation: Done in the lecture.\n\n\nThe case of pdfs with compact support:\nThe key-idea is easily explained using a bounded pdf \\(f\\) with compact support.\nNotions:\n\nBounded means that there exists a value \\(m\\) with \\(0<m<\\infty\\) s.t. \\(f(x)\\in[0,m]\\) for all \\(x\\).\nNote that only degenerated pdfs are not bounded.\nAn interval \\([a,b]\\) is called “compact” if it is closed and the boundaries are finite.\nFor instance, the Gaussian has not a compact support, since \\(\\mathrm{supp}(\\phi)=]-\\infty,\\infty[\\).\n\nFor instance, let’s say we want to simulate random numbers \\(X\\sim f\\) with \\[\nf(x)=\\frac{3}{4}\\left(1-\\left(x-1\\right)^2\\right)\\,1_{(|x-1|\\leq 1)},\n\\] where the (compact) support of \\(f\\) is \\([a,b]=[-1,1]\\) and its range is \\([0,m]=[0,3/4]\\), i.e., \\(f\\) is bounded from above by \\(3/4\\).\n\nThe idea is then to simulate the random pair \\((Y,U)\\sim\\mathrm{Unif}([a,b]\\times[0,m])\\) by simulating\n \\[Y\\sim\\mathrm{Unif}[a,b]\\quad\\text{and}\\quad U|Y=y \\sim \\mathrm{Unif}[0,m], \\] but to accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\leq x)=\\mathbb{P}(Y\\leq x|U\\leq f(Y))\n=\\frac{\\int_a^{\\color{red}x} \\int_0^{f(y)}\\,1\\,du\\,dy}{\\int_a^{\\color{red}b}\\int_0^{f(y)}\\,1\\,du\\,dy}\n=\\frac{\\int_a^x f(y)\\,dy}{\\int_a^b f(y)\\,dy}\n=\\int_a^x f(y)dy,\n\\]\nwhere we used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nThe Accept-Reject Algorithm (Simple Version):\n# Accept-Reject Algorithm:\nY <- runif(n, min = a, max = b) \nU <- runif(n, min = 0, max = m) \n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\nIn the following you see a graphical illustration of this procedure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe good thing is that we only need to evaluate the pdf \\(f(.)\\); nothing more.\n\n\nGeneralization: pdfs with non-compact support.\nThe larger set does not necessarily need to be a surrounding closed box as used above. In fact, it can be any “larger set”, enclosing the pdf \\(f\\), as long as simulating uniformly over this larger set is feasible. This generalization allows for cases where the support of \\(f\\) is unbounded.\nLet the larger set denote by \\[\n\\mathscr{L}=\\{(y,u):\\, 0<u<m(y)\\},\n\\] where:\n\nsimulation of a uniform on \\(\\mathscr{L}\\) is feasible and\n\n\\(m(x)\\geq f(x)\\) for all \\(x\\).\n\n\n\nFrom the feasibility-requirement it follows that \\(m(.)\\) is necessarily integrable, i.e., that \\[\\int_{\\mathcal{X}}m(x)dx=M,\\] where \\(M\\) exists and is finite (and positive), since otherwise, \\(\\mathscr{L}\\) would not have finite mass and a uniform distribution would not exists on \\(\\mathscr{L}\\).\n\n\nIntegrability of \\(m(.)\\) is crucial here, since it allows us to relate \\(m(.)\\) with a corresponding (auxiliary) pdf \\(g(.)\\) as following: \\[m(x)=M\\,g(x),\\quad\\text{where}\\quad\\int_{\\mathcal{X}}m(x)\\,dx=\\int_{\\mathcal{X}}M\\,g(x)\\,dx=M.\\]\nTerminology:\n\nThe pdf \\(g(.)\\) is called the instrumental density. (Choose \\(g(.)\\) as a pdf from which it is easy to simulate!)\nThe pdf \\(f(.)\\) is called the target density.\n\n\n\nIn order to simulate the pair \\((Y,U)\\sim\\mathrm{Unif}(\\mathscr{L})\\) we can now simulate \\[Y\\sim g\\quad\\text{and}\\quad U|Y={\\color{red}y}\\sim\\mathrm{Unif}[0,M\\,g({\\color{red}y})],\\] but accept the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and to reject all others.\nThis results in the correct distribution of the accepted value of \\(Y\\), call it \\(X\\), because \\[\n\\mathbb{P}(X\\in A)=\\mathbb{P}(Y\\in A|U\\leq f(Y))\n=\\frac{\\int_{\\color{red}A}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}{\\int_\\mathcal{X}\\int_0^{f(y)}\\,\\frac{1}{M}\\,du\\,dy}\n=\\frac{\\int_A f(y)\\,dy}{\\int_\\mathcal{X} f(y)\\,dy}\n=\\int_A f(y)dy,\n\\] for every set \\(A\\),  where we again used that \\(f(y)=\\int_{0}^{f(y)}du\\).\nNote that the above derivation implies that we only need to know the pdf \\(f(.)\\) up to an unkown multiplicative constant \\(c>0\\). I.e., it is enough to know \\(f(x)=c\\,\\tilde{f}_{\\textrm{true}}(x)\\), often written as \\(f(x)\\propto \\tilde{f}_{\\textrm{true}}(x)\\), since the unknown constant \\(c\\) cancels out in the above quotient anyways. This is not so much of importance for us, but useful in Bayesian Statistics.\n\n\nAll this leads to a more general version of the Fundamental Theorem of Simulation:\n\nFundamental Theorem of Simulation (General Version):\n\nLet \\(X\\sim f\\) and let \\(g(.)\\) be a pdf s.t. \\(f(x)\\leq M\\,g(x)\\) for some \\(M\\) with \\(1\\leq M<\\infty\\) and all \\(x\\). Then to simulate \\(X\\sim f\\) it is sufficient to generate \\[Y\\sim g\\quad\\text{and}\\quad U|Y=y\\sim\\mathrm{Unif}[0,M\\,g(y)]\\] if one accepts the pair \\((Y,U)\\) only if \\(U\\leq f(Y)\\) and rejects all others.\n\n\n\n\nThe Accept-Reject Algorithm (General Version):\n# Accept-Reject Algorithm:\nY   <- generate n random numbers from g(.)\n\n# Specify function m():\nm <- function(y){YOUR CODE}\n\nU   <- numeric(n)\nfor(i in 1:n){\n  U[i] <- runif(n=1, min = 0, max = m(Y[i])) \n}\n\n# A-R Step:\naccept <- U <= f(Y)\nX      <- Y[accept]\n\n\nExample\nLet the target “density” be \\[f(x)\\propto \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\] with upper bound (or, rather, dominating density) the standard normal density \\[g(x)=\\exp(-x^2/2)/\\sqrt{2\\pi},\\] which is obviously straightforward to generate.\nIn this example we can set \\(m(x)=M\\,g(x)\\) with \\(M=1\\), since we can simply scale the target “density” \\(f\\) such that \\(f(x)\\leq g(x)\\) for all \\(x\\). Specifically, we set \\(f(x)=0.075 \\cdot \\exp(-x^2/2)\\,(\\sin(6x)^2 + 3\\cos(x)^2\\,\\sin(4x)^2 + 1)\\).\nIn the following you see the graphical illustration of this example:\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiency of the Accept-Reject algorithm:\nStatements with respect to the efficiency of the Accept-Reject algorithm can be made if \\(f\\) and \\(g\\) are normalized such that they are both pdfs. Then:\n\nThe constant \\(M\\) is necessarily larger than \\(1\\).\nThe probability of acceptance is \\(1/M\\). (See Exercises.)\n\\(M\\) is interpreted as the efficiency of the Accept-Reject algorithm. (The closer \\(M\\) is to \\(1\\) the better.)\n\\(M\\) is a function of how closely \\(g\\) can imitate \\(f\\).\n\nNote that, for such normalized \\(f\\) and \\(g\\) the inequality \\(f(x)\\leq M\\,g(x)\\) with \\(1\\leq M<\\infty\\) for all \\(x\\) is equivalent to saying that the quotient \\(f/g\\) is bounded, i.e., that \\[\n0\\leq \\frac{f(x)}{g(x)}\\leq M <\\infty\\quad\\text{for all}\\quad x.\n\\] That is, it is necessary for \\(g\\) to have, e.g., thicker tails than \\(f\\). This makes it, for instance, impossible to simulate a Cauchy distribution \\(f\\) using a normal distribution \\(g\\). The reverse, however, works quite well. \n\n\nExample: Normals from Double Exponentials\nConsider generating a \\(N(0,1)\\) by the Accept-Reject algorithm using a double-exponential distribution \\(\\mathcal{L}(\\alpha)\\), also called Laplace distribution, with density \\(g(x|b)=(1/(2b))\\exp(-\\,|x|/b)\\).  It is then straightforward to show that \\[\n\\frac{f(x)}{g(x|b)}\n%=\\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)}\n%=\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(-\\frac{1}{2}x^2+\\frac{|x|}{b}\\right)\n\\leq\\sqrt{\\frac{2}{\\pi}}\\,b\\,\\exp\\left(\\frac{1}{2\\,b^2}\\right)\n\\] and that the minimum of the bound (in \\(b\\)) is attained for \\(b=1\\).\nThis leads to the following optimal (i.e. most efficient) specification of the double-exponential distribution as instrumental pdf: \\[\n\\frac{f(x)}{g(x|1)}\n\\leq M=\\sqrt{\\frac{2}{\\pi}}\\,\\exp\\left(\\frac{1}{2}\\right).\n\\]\nThe probability of acceptance is then \\(\\sqrt{\\pi/(2e)}=0.76\\). I.e., to produce one normal random variable, this Accept-Reject algorithm requires on the average \\(1/0.76\\approx 1.3\\) uniform variables. This is to be compared with the Box-Muller algorithm for which the ratio between produced normals and required uniforms is 1."
  }
]