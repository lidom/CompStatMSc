## Solutions {-}

#### Solutions of Exercise 1. {-} 


Below I use the same data (one H, four T) that was used to produce the results in @tbl-NR of our script. However, you can produce new data by setting another seed-value

```{r}
theta_true <- 0.2    # unknown true theta value
n          <-  5     # sample size

set.seed(1)

# simulate data: n many (unfair) coin tosses
x <- sample(x          = c(0,1), 
               size    = n, 
               replace = TRUE, 
               prob    = c(1-theta_true, theta_true)) 

## number of heads (i.e., the number of "1"s in x)
h <- sum(x)

## First derivative of the log-likelihood function
Lp_fct   <- function(theta, h = h, n = n){
    (h/theta) - (n - h)/(1 - theta)    
}
## Second derivative of the log-likelihood function
Lpp_fct   <- function(theta, h = h, n = n){
    - (h/theta^2) - (n - h)/(1 - theta)^2    
}


t     <- 1e-10   # convergence criterion
check <- TRUE    # for stopping the while-loop
i     <- 0       # count iterations


theta <- 0.4     # starting value 
Lp    <- Lp_fct( theta, h=h, n=n)
Lpp   <- Lpp_fct(theta, h=h, n=n)



while(check){
    i         <- i + 1
    ##
    theta_new <- theta[i] - (Lp_fct(theta[i], h=h, n=n) / Lpp_fct(theta[i], h=h, n=n))    
    Lp_new    <- Lp_fct( theta_new, h = h, n = n)
    Lpp_new   <- Lpp_fct(theta_new, h = h, n = n)
    ##
    theta     <- c(theta, theta_new) 
    Lp        <- c(Lp,    Lp_new) 
    Lpp       <- c(Lpp,   Lpp_new) 
    ##
    if( abs(Lp_fct(theta_new, h=h, n=n)) < t ){check <- FALSE}
}

cbind(theta, Lp, Lp/Lpp)
```




#### Solutions of Exercise 2. {-} 

##### (a) Log-likelihood function {-}

The log-likelihood function is given by
$$
\begin{align*}
\ell(\theta)
&=\sum_{i=1}^n \ln (\theta\exp(-\theta X_i))\\
&=\sum_{i=1}^n (\ln \theta -\theta X_i)\\
&=n \ln \theta -\sum_{i=1}^n \theta X_i
\end{align*}
$$

##### (b) ML-estimator  {-}

The ML estimator is defined as $\hat{\theta}_{n}=\arg\max\ell(\theta)$. Deriving the ML estimator $\hat\theta_n$:
$$
\begin{align*}
\ell_n'(\theta)&=n\frac{1}{\theta} - \sum_{i=1}^n X_i\\
\ell_n'(\hat\theta_n)=0\quad \Leftrightarrow &\quad 0=n\frac{1}{\hat\theta_n} - \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad n\frac{1}{\hat\theta_n} = \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad \hat\theta_n = \frac{1}{\frac{1}{n}\sum_{i=1}^n X_i}= \frac{1}{\bar{X}}
\end{align*}
$$

##### (b) Fisher information {-}

The Fisher information is given by 
$$
\mathcal{I}(\theta)=n\mathcal{J}(\theta),
$$ 
where $\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))$. The second derivative of $\ell(\theta)$ is given by
$$
\ell''(\theta)=-n\frac{1}{\theta^2}
$$
So, the expression for $\ell''(\theta)$ is here *deterministic* as it doesn't depend on the random variables $X_i$. 
$$
\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))=-\frac{1}{n}\left(-n\frac{1}{\theta^2}\right)=\frac{1}{\theta^2}
$$
That is, the Fisher information is $\mathcal{I}(\theta)=n\mathcal{J}(\theta)=n/\theta^2$. 
Therefore, the asymptotic distribution of $\hat\theta_n$ is 
$$
\begin{align*}
(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\frac{\theta^2}{n}\right)\\
\Leftrightarrow\quad \sqrt{n}(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\theta^2\right)
\end{align*}
$$