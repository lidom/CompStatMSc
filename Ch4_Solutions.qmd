## Solutions {-}

#### Solutions of Exercise 1. {-} 


Below I use the same data (one H, four T) that was used to produce the results in @tbl-NR of our script. However, you can produce new data by setting another seed-value

```{r}
theta_true <- 0.2    # unknown true theta value
n          <-  5     # sample size

set.seed(1)

# simulate data: n many (unfair) coin tosses
x <- sample(x          = c(0,1), 
               size    = n, 
               replace = TRUE, 
               prob    = c(1-theta_true, theta_true)) 

## number of heads (i.e., the number of "1"s in x)
h <- sum(x)

## First derivative of the log-likelihood function
Lp_fct   <- function(theta, h = h, n = n){
    (h/theta) - (n - h)/(1 - theta)    
}
## Second derivative of the log-likelihood function
Lpp_fct   <- function(theta, h = h, n = n){
    - (h/theta^2) - (n - h)/(1 - theta)^2    
}


t     <- 1e-10   # convergence criterion
check <- TRUE    # for stopping the while-loop
i     <- 0       # count iterations


theta <- 0.4     # starting value 
Lp    <- Lp_fct( theta, h=h, n=n)
Lpp   <- Lpp_fct(theta, h=h, n=n)



while(check){
    i         <- i + 1
    ##
    theta_new <- theta[i] - (Lp_fct(theta[i], h=h, n=n) / Lpp_fct(theta[i], h=h, n=n))    
    Lp_new    <- Lp_fct( theta_new, h = h, n = n)
    Lpp_new   <- Lpp_fct(theta_new, h = h, n = n)
    ##
    theta     <- c(theta, theta_new) 
    Lp        <- c(Lp,    Lp_new) 
    Lpp       <- c(Lpp,   Lpp_new) 
    ##
    if( abs(Lp_fct(theta_new, h=h, n=n)) < t ){check <- FALSE}
}

cbind(theta, Lp, Lp/Lpp)
```




#### Solutions of Exercise 2. {-} 

##### (a) Log-likelihood function {-}

The log-likelihood function is given by
$$
\begin{align*}
\ell(\theta)
&=\sum_{i=1}^n \ln (\theta\exp(-\theta X_i))\\
&=\sum_{i=1}^n (\ln \theta -\theta X_i)\\
&=n \ln \theta -\sum_{i=1}^n \theta X_i
\end{align*}
$$

##### (b) ML-estimator  {-}

The ML estimator is defined as $\hat{\theta}_{n}=\arg\max\ell(\theta)$. Deriving the ML estimator $\hat\theta_n$:
$$
\begin{align*}
\ell_n'(\theta)&=n\frac{1}{\theta} - \sum_{i=1}^n X_i\\
\ell_n'(\hat\theta_n)=0\quad \Leftrightarrow &\quad 0=n\frac{1}{\hat\theta_n} - \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad n\frac{1}{\hat\theta_n} = \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad \hat\theta_n = \frac{1}{\frac{1}{n}\sum_{i=1}^n X_i}= \frac{1}{\bar{X}}
\end{align*}
$$

##### (b) Fisher information {-}

The Fisher information is given by 
$$
\mathcal{I}(\theta)=n\mathcal{J}(\theta),
$$ 
where $\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))$. The second derivative of $\ell(\theta)$ is given by
$$
\ell''(\theta)=-n\frac{1}{\theta^2}
$$
So, the expression for $\ell''(\theta)$ is here *deterministic* as it doesn't depend on the random variables $X_i$. 
$$
\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))=-\frac{1}{n}\left(-n\frac{1}{\theta^2}\right)=\frac{1}{\theta^2}
$$
That is, the Fisher information is $\mathcal{I}(\theta)=n\mathcal{J}(\theta)=n/\theta^2$. 
Therefore, the asymptotic distribution of $\hat\theta_n$ is 
$$
\begin{align*}
(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\frac{\theta^2}{n}\right)\\
\Leftrightarrow\quad \sqrt{n}(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\theta^2\right)
\end{align*}
$$


#### Solutions of Exercise 3. {-}


##### (a) Likelihood function {-}

Recall that the density function of $\mathcal{Unif}(0,\theta)$ is
$$
f(x|\theta)
=\left\{
\begin{array}{ll}
\frac{1}{\theta} & 0\leq x\leq \theta\\
0                & \text{otherwise}\\
\end{array}
\right.
$$

Thus the likelihood function is
$$
\mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i|\theta)
$$
If any $X_i>\theta,$ we have that $\mathcal{L}_n(\theta)=0.$ Putting it differently, let $X_{(n)}=\max\{X_1,\dots,X_n\}$ ($n$th order-statistic), then
$\mathcal{L}_n(\theta)=0$ for all $\theta<X_{(n)}.$ 

However, for all $\theta \geq X_{(n)}$ we have that $f(X_i|\theta)=1/\theta$ such that $\mathcal{L}_n(\theta)=(1/\theta)^n$. Summing up, 
$$
\mathcal{L}_n(\theta)
=\left\{
\begin{array}{ll}
\left(\frac{1}{\theta}\right)^n & \theta \geq  X_{(n)}\\
0                               & \theta < X_{(n)}\\
\end{array}
\right.
$${#eq-MLMaxestim}



#### (b) Maximum likelihood estimator of $\theta$ {-}

$\mathcal{L}_n(\theta)$ is strictly decreasing over the interval $[X_{(n)},\infty);$ see @fig-MLMaxestim.

```{r}
#| label: fig-MLMaxestim
#| fig-cap: Graph of the likelihood function $\mathcal{L}_n(\theta)$ given in @eq-MLMaxestim.
n          <- 20   # sample size
X_max      <- 0.25

theta_vec  <- seq(from = 0, 
                  to   = X_max * 1.5, 
                  len  = 100) 
likelihood_fun <- function(theta, X_max, n){ 
    likelihood              <- 1/(theta^n)
    likelihood[theta < X_max] <- 0 
    return(likelihood) 
}

likelihood_vec <- likelihood_fun(theta = theta_vec,
                                 X_max = X_max, 
                                 n     = n)

plot(y = likelihood_vec, 
     x = theta_vec, 
     type = "l", 
     xlab = expression(theta),
     ylab = "Likelihood", 
     main = "")            
axis(1, at = X_max, labels = expression(X[(n)]))                  
```

Thus, the maximum likelihood estimator is 
$$
\begin{align}
\hat{\theta}_{ML} 
& =\arg\max_{\theta>0}\mathcal{L}_n(\theta)\\
& = X_{(n)}.
\end{align}
$$


#### Solutions of Exercise 4. {-}


##### (a) Maximum likelihood estimator $\hat{\lambda}$ {-}


$$
\begin{align}
\mathcal{L}(\lambda) 
& = \prod_{i=1}^n f(x_i|\lambda)\\[2ex]
& = \prod_{i=1}^n \frac{\lambda^{x_i} \exp(-\lambda)}{x_i!} \\[2ex]
& = \frac{\lambda^{\sum_{i=1}^n x_i}  \exp(-n \lambda)}{\prod_{i=1}^n (x_i!)} \\[4ex]
\ell(\lambda) 
&= \left(\sum_{i=1}^n x_i\right) \ln(\lambda) -n\lambda\cdot 1 - \sum_{i=1}^n (x_i!)
\end{align}
$$

$$
\begin{align}
\ell'(\lambda) 
&= \frac{\left(\sum_{i=1}^n x_i\right)}{\lambda}  - n 
\end{align}
$$

$$
\begin{align}
\ell''(\lambda) 
&= -\frac{\left(\sum_{i=1}^n x_i\right)}{\lambda^2} < 0  
\end{align}
$$
since by the properties of the Poisson distribution $x_1,\dots,x_n>0$ and $\lambda>0.$


$$
\begin{align}
&\frac{\left(\sum_{i=1}^n x_i\right)}{\hat\lambda}  - n \overset{!}{=} 0\\[2ex]
\Rightarrow & \hat \lambda = \frac{1}{n}\sum_{i=1}^n x_i.
\end{align}
$$


##### (b) Maximum likelihood estimator $\hat{P}(X=4)$ {-}


$$
\begin{align}
P(X=4) = \frac{\lambda^4 \exp(-\lambda)}{4!}
\end{align}
$$

Thus $P(X=4)$ is a function of $\lambda$
$$
\begin{align}
P(X=4)\equiv P(X=4|\lambda) = \frac{\lambda^4 \exp(-\lambda)}{4!} = g(\lambda)
\end{align}
$$


```{r}
lambda_vec <- seq(from = .0001, to = 15, len = 100)
g_vec      <- (lambda_vec^4 * exp(-1*lambda_vec))/( factorial(4) )

plot(x = lambda_vec, y = g_vec, 
     type = "l", xlab=expression(lambda), ylab="g")
abline(v = 4)
axis(1, at = 4)
```

Thus, for $0<\lambda\leq 4,$ $g(\lambda)$ is one-to-one. 

Therefore, by the equivariance property of the maximum likelihood estimator,

$$
\begin{align}
\hat{P}(X=4)\equiv \hat{P}(X=4|\hat{\lambda}) = \frac{\hat{\lambda}^4 \exp(-\hat{\lambda})}{4!}
\end{align}
$$
with $\hat{\lambda}=\frac{1}{n}\sum_{i=1}^n x_i.$

