## Solutions {-}

#### Solutions of Exercise 1. {-} 


Below I use the same data that was used to produce the results in Table 6.1 of our script. However, you can produce new data by setting another seed-value

```{r}
theta_true <- 0.2    # unknown true theta value
n          <-  5     # sample size

## Use a common Random Number Generator:
RNGkind(sample.kind = "Rounding")

set.seed(1)

# simulate data: n many (unfair) coin tosses
x <- sample(x          = c(0,1), 
               size    = n, 
               replace = TRUE, 
               prob    = c(1-theta_true, theta_true)) 

## number of heads (i.e., the number of "1"s in x)
h <- sum(x)

## First derivative of the log-likelihood function
Lp_fct   <- function(theta, h = h, n = n){
    (h/theta) - (n - h)/(1 - theta)    
}
## Second derivative of the log-likelihood function
Lpp_fct   <- function(theta, h = h, n = n){
    - (h/theta^2) - (n - h)/(1 - theta)^2    
}


t     <- 1e-10   # convergence criterion
check <- TRUE    # for stopping the while-loop
i     <- 0       # count iterations


theta <- 0.4     # starting value 
Lp    <- Lp_fct( theta, h=h, n=n)
Lpp   <- Lpp_fct(theta, h=h, n=n)



while(check){
    i         <- i + 1
    ##
    theta_new <- theta[i] - (Lp_fct(theta[i], h=h, n=n) / Lpp_fct(theta[i], h=h, n=n))    
    Lp_new    <- Lp_fct( theta_new, h = h, n = n)
    Lpp_new   <- Lpp_fct(theta_new, h = h, n = n)
    ##
    theta     <- c(theta, theta_new) 
    Lp        <- c(Lp,    Lp_new) 
    Lpp       <- c(Lpp,   Lpp_new) 
    ##
    if( abs(Lp_fct(theta_new, h=h, n=n)) < t ){check <- FALSE}
}

cbind(theta, Lp, Lp/Lpp)
```





 <!-- Exercies 2 -->

#### Solutions of Exercise 2. {-} 

##### (a) Mean Squared Errors {-} 

First, note that 
$$
s^2_{UB}=\left(\frac{n}{n-K}\right)s^2_{ML}\quad\Leftrightarrow\quad \left(\frac{n-K}{n}\right)s^2_{UB}=s^2_{ML}
$$
Since $s_{UB}^2$ is unbiased, we have that
$$
\operatorname{MSE}(s^2_{UB})=\V(s^2_{UB})=\frac{n2\sigma^4}{(n-K)^2}
$$
The bias and the squared bias of $s_{ML}^2$:
$$
\begin{align*}
\operatorname{Bias}(s^2_{ML})
&=E(s^2_{ML})-\sigma^2\\
&=\left(\frac{n-K}{n}\right)E(s^2_{UB})-\sigma^2\\
&=\left(\frac{n-K}{n}\right)\sigma^2-\sigma^2=\frac{-K}{n}\sigma^2\\
\Rightarrow\operatorname{Bias}^2(s^2_{ML})&=\frac{K^2}{n^2}\sigma^4
\end{align*}
$$

The variance of $s_{ML}^2$:

$$
\begin{align*}
\V(s^2_{ML})
&=\left(\frac{n-K}{n}\right)^2\V(s^2_{UB})\\
&=\left(\frac{n-K}{n}\right)^2\frac{n}{(n-K)^2}2\sigma^4=\frac{2\sigma^4}{n}
\end{align*}
$$
(which coincides with the variance of $s^2_{ML}$ as derived in the script.)



The mean squared error of $s_{ML}^2$:
$$
\begin{align*}
\operatorname{MSE}(s^2_{ML})&=\operatorname{Bias}^2(s^2_{ML}) + \V(s^2_{ML})\\
&=\frac{K^2}{n^2}\sigma^4 + \frac{2\sigma^4}{n}
\end{align*}
$$


Checking the inequality of interest:
$$
\begin{align*}
\operatorname{MSE}(s^2_{ML})&\leq\operatorname{MSE}(s^2_{UB})\\
\frac{K^2\sigma^4}{n^2} + \frac{2\sigma^4}{n}&<\frac{n2\sigma^4}{(n-K)^2}\\
\frac{K^2}{2n^2} + \frac{1}{n}&<\frac{n}{(n-K)^2}\\
\frac{K^2}{2n^2} + \frac{1}{n} - \frac{n}{(n-K)^2}&<0\\
%\frac{K^2}{2n^2} & <\frac{n}{(n-K)^2} - \frac{1}{n}\\
%\frac{K^2}{2n^2} & <\frac{n^2-(n-K)^2}{n(n-K)^2} \\
%\frac{K^2}{2} & < n\frac{n^2-(n-K)^2}{(n-K)^2} \\
%\frac{K^2}{2} & < n\;\;\underbrace{\left(\frac{n^2}{(n-K)^2}-1\right)}_{>0}\\
\end{align*}
$$
It turns out that the derivation of the sufficiently large $n$ for each given $K$ is very tedious. But we can at least do a simple visual check:
```{r}
myfun  <- function(n,K){(K^2)/(2*n^2) + 1/n - n/((n-K)^2)}

K      <- 5
n      <- (K+1):100

plot(y= myfun(n=n,K=K), x=n, type="o", ylab="")
K      <- 10
n      <- (K+1):100
lines(y= myfun(n=n,K=K), x=n, type="o", col="red")
K      <- 20
n      <- (K+1):100
lines(y= myfun(n=n,K=K), x=n, type="o", col="blue")
abline(h=0)
legend("bottomright", col=c("black", "red", "blue"), legend = c("K=5", "K=10", "K=20"))
```



<!-- $K=1:$ -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \frac{K^2}{n^2} -\frac{2(K-1)}{n} - (1-\frac{4}{K}) & < 0 \\ -->
<!-- \frac{1^2}{n^2} -\frac{2(1-1)}{n} - (1-\frac{4}{1}) & < 0 \\ -->
<!-- \frac{1^2}{n^2}  +3 & < 0 \\ -->
<!-- \end{align*} -->
<!-- $$ -->

<!-- \begin{align*} -->
<!-- \operatorname{MSE}(s^2_{ML})&<\operatorname{MSE}(s^2_{UB})\\ -->
<!-- \frac{K^2}{n^2}\sigma^4 + \frac{2\sigma^4}{n}&<\frac{n2\sigma^4}{(n-K)^2}\\ -->
<!-- \frac{2\sigma^4}{(n-K)}\left(\frac{K^2(n-K)}{2n^2} + \frac{n-K}{n}\right)&<\frac{2\sigma^4}{n-K}\\ -->
<!-- \frac{2\sigma^4}{(n-K)}\left(1+\frac{K^2(n-K)}{2n^2}-\frac{K}{n}\right)&<\frac{2\sigma^4}{n-K}\\ -->
<!-- \frac{K^2(n-K)}{2n^2}-\frac{K}{n}&<0\\ -->
<!-- \frac{K^2(n-K)}{2n^2}&<\frac{K}{n}\\ -->
<!-- nK^2(n-K)&<K2n^2\\ -->
<!-- Kn-K^2&<2n\\ -->
<!-- -K^2&<n(2-K)\\ -->
<!-- %-K^2/(2-K)&<n\\ -->
<!-- \frac{K^2}{K-2}&<n\\ -->
<!-- \end{align*} -->




So, indeed for every $K$ there's a large enough $n$ which fulfills the above inequalities:
\begin{itemize}
\item For $1\leq K\leq 2$: \quad $n\geq 1$
\item For $K\geq 3$: \quad  $n>\frac{K^2}{K-2}$
\end{itemize}

```{r, echo=FALSE, eval=FALSE}
K <- 3
K * (2 -K)/(4-K)

For K > 4
K <- 3
K * (2 - K)/(4 - K) < 1



K <- 4
n <- 2
2 * K - K^2 <(4-K) * n
```

<!-- ```{r} -->
<!-- K <- 5 -->
<!-- n <- 1000 -->


<!-- (2*K^2-K^3)/((4*K-K^2)) -->

<!-- kk <- 1:10 -->
<!-- n <- 2:100 -->

<!-- K <- kk[1] -->
<!-- plot(y= c(1+ ((K^2-4*K)/(2*n)) + ((2*K^2-K^3)/(2*n^2))), x=n, type="l", ylim=c(-.4,1.5)) -->
<!-- for(i in 1:length(kk)){ -->
<!--   K <- kk[i] -->
<!--   lines(y= c(1+ ((K^2-4*K)/(2*n)) + ((2*K^2-K^3)/(2*n^2))), x=n, type="l")   -->
<!-- } -->


<!-- ((n-K)*K^2)/(2*n^2) + ((n-K)/n)^2 -->
<!-- ``` -->


Meta-information
============
extype: num
exsolution: 0
exname: ch6_2
extol: 0





















```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.cap = "")
```


Question
========

Assume an i.i.d. random sample $X_1,\dots,X_n$ from an exponential distribution, i.e. the underlying density of $X_i$ is given by $f(x|\theta)=\theta\exp(-\theta x)$. We then have $\mu:=E(X_i)=\frac{1}{\theta}$ as well as $\V(X_i)=\frac{1}{\theta^2}$. 
\begin{itemize}
\item[(a)] What is the log-likelihood function for the i.i.d. random sample $X_1,\dots,X_n$?
\item[(b)] Derive the maximum likelihood (ML) estimator $\hat\theta_n$ of $\theta$.
\item[(c)] From maximum likelihood theory we know that 
$$
(\hat\theta_n-\theta)\to_d \mathcal{N}\left(0,\frac{1}{n \mathcal{J}(\theta)}\right)
$$
Derive the expression for the Fischer information $\mathcal{I}(\theta)=n\mathcal{J}(\theta)$. Use the Fisher information to give the explizit formula for the asymptotic distribution of $\hat\theta_n$.
\end{itemize}

Solution
========

\begin{itemize}
\item[(a)] The log-likelihood function is given by
\begin{align*}
\ell(\theta)
&=\sum_{i=1}^n \ln (\theta\exp(-\theta X_i)))\\
&=\sum_{i=1}^n (\ln \theta -\theta X_i)\\
&=n \ln \theta -\sum_{i=1}^n \theta X_i\\
\end{align*}

\item[(b)] The ML estimator is defined as $\hat{\theta}_{n}=\arg\max\ell(\theta)$. Deriving the ML estimator $\hat\theta_n$:

\begin{align*}
\ell_n'(\theta)&=n\frac{1}{\theta} - \sum_{i=1}^n X_i\\
\ell_n'(\hat\theta_n)=0\quad \Leftrightarrow &\quad 0=n\frac{1}{\hat\theta_n} - \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad n\frac{1}{\hat\theta_n} = \sum_{i=1}^n X_i\\
\Leftrightarrow &\quad \hat\theta_n = \frac{1}{\frac{1}{n}\sum_{i=1}^n X_i}= \frac{1}{\bar{X}}
\end{align*}

\item[(c)] The Fisher information is given by $\mathcal{I}(\theta)=n\mathcal{J}(\theta)$, where $\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))$. The second derivative of $\ell(\theta)$ is given by
$$
\ell''(\theta)=-n\frac{1}{\theta^2}
$$
So, the expression for $\ell''(\theta)$ is here deterministic as it doesn't depend on the random variables $X_i$. 
$$
\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))=-\frac{1}{n}\left(-n\frac{1}{\theta^2}\right)=\frac{1}{\theta^2}
$$
That is, the Fisher information is $\mathcal{I}(\theta)=n\mathcal{J}(\theta)=n/\theta^2$. Therefore, the asymptotic distribution of $\hat\theta_n$ is 
\begin{align*}
(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\frac{\theta^2}{n}\right)\\
\Leftrightarrow\quad \sqrt{n}(\hat\theta_n-\theta)&\to_d \mathcal{N}\left(0,\theta^2\right)
\end{align*}
\end{itemize}




Meta-information
============
extype: num
exsolution: 0
exname: ch6_3
extol: 0