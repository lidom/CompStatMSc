[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    14:15-15:45 \n    Jur / Hörsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\nEM Algorithm & Cluster Analysis\nBootstrap\nNonparametric Regression\nFunctional Data Analysis\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/Script-CompStat-MSc/ (pwd: compstat)\nWe’ll use an eWhiteboard for derivations and some extra explanations.\nBasic material from our econometrics course:\n\nIntroduction to R\nProbability\n\n\n\n\n\n\n\n\n\nConsider using git/github for your personal course notes.\n\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html",
    "href": "Ch1_MaximumLikelihood.html",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g. the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed.\nIn (classical) maximum likelihood estimation we must be rather specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta_n\\) of some parameter \\(\\theta_0\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta_0\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta_0) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: This means that no consistent estimator has lower asymptotic mean squared error than the maximum likelihood estimator.\n\nLikewise for multivariate parameter \\(\\theta_0\\in\\mathbb{R}^p.\\)\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density/probability mass functions \\(f\\) of the random variables in the (i.i.d.) random sample \\(X_1,\\dots,X_n\\) such that \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) where \\(f\\) is known up to an unknown parameter \\(\\theta,\\) where \\(\\theta\\in\\mathbb{R}^K\\) is allowed to be a finite (\\(1\\leq K<\\infty\\)) dimensional vector.\nExamples:\n\n\\(f\\) being the probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta)\\) with \\[\nf(x_i|\\theta)=\n\\left\\{\n\\begin{array}{ll}\n\\theta,&\\text{if } x_i=1\\\\\n1-\\theta, & \\text{if } x_i=0\n\\end{array}\n\\right.\n\\] and unknown parameter \\(0\\leq \\theta\\leq 1.\\)\n\\(f\\) is the normal density \\[\nf(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\n\\] with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement (fixing the family of density functions) can be overly restrictive. In many applications we typically do not know the family of \\(f.\\) To address this issue, the quasi maximum likelihood theory generalizes classic maximum likelihood estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Such coin-flips can be modeled using Bernoulli random variables \\[\nX\\sim\\mathcal{Bern}(\\theta_0)\n\\] where \\[\nX=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\n    \\end{matrix}\n    \\right.\n\\] The probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta_0)\\) with unknown probability of success parameter \\(0<\\theta_0<1,\\) is given by \\[\nf(x|\\theta_0)=\n\\left\\{\n  \\begin{array}{ll}\n  \\theta_0,&\\text{if } x=1\\\\\n  1-\\theta_0, & \\text{if } x=0\n  \\end{array}\n\\right.\n\\] I.e. \\[\n\\theta_0 = f(1|\\theta_0) = P(X=1) = P(\\text{Coin}=H),\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta_0 = f(0|\\theta_0) = P(X=0) = P(\\text{Coin}=T).\n\\]\nOur goal is to estimate the unknown \\(\\theta_0\\) using a random (i.i.d.) sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\text{ in $i$th coin flip}\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\\text{ in $i$th coin flip}\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{Bern}(\\theta_0),\\quad i=1,\\dots,n.\n\\] \nA given observed realization of the random sample \\[\n\\{X_{1,obs},X_{2,obs},\\dots,X_{n,obs}\\}=\\{0,1,\\dots,0\\}\n\\] consists of \\(0\\leq N_{H,obs}\\leq n\\) \\[\nN_{H,obs}=\\sum_{i=1}^n X_{i,obs}\n\\] many heads and of \\[\n0\\leq n-N_{H,obs} \\leq n\n\\] many tails.\n\n\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] to estimate the unknown \\(\\theta_0\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta)\n&=\\prod_{i=1}^nf(X_{i,obs}|\\theta)\\\\[2ex]\n%&=\\left(P(X=1)\\right)^{N_{H,obs}}\\left(P(X=0)\\right)^{n-N_{H,obs}}\\\\[2ex]\n%&= \\theta^{N_{H,obs}}(1-\\theta)^{n-N_{H,obs}}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\end{align*}\n\\]\nThe function \\(\\mathcal{L}_n(\\theta)\\) is called the likelihood function.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.1 (Likelihood Function) More generally, when the observations \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\] where \\(f(X_{i,obs} | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_{i,obs},\\) and where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of the density function. (A definition for dependent data (e.g. time series) is also possible.)\n\n\n\n\n\n\nWe estimate the unknown parameter \\(\\theta_0\\) by maximizing the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) over the range of possible parameter values. The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}_n(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Maximum Likelihood (ML) Estimator) \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\end{align*}\n\\] where \\(\\Theta\\) denotes the parameter space.\n\n\n\nIn our coin flip example this means to estimate the unknown \\(\\theta_0\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) is maximal, \\[\n\\hat\\theta_{ML} = \\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\]\nUsually it’s easier to work with sums rather than products—also for doing the asymptotics in Section 1.4. So we apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\ln\\prod_{i=1}^n f(X_{i,obs}|\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n \\ln f(X_{i,obs}|\\theta).\n\\end{align*}\n\\] Since this is only a monotonic transformation, we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\ell_n(\\theta).\n\\end{align*}\n\\] \nIn our coin flipping example, taking the natural logarithm (\\(\\ln\\)) yields, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta) &= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}} \\\\[2ex]\n\\Rightarrow\\quad \\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n\\left( X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\right).\n\\end{align*}\n\\]\nThe coin flip example is actually so simple that we can maximize \\(\\ell_n(\\theta)\\) analytically. Computing the first derivative yields \\[\n\\begin{align*}\n\\ell'_n(\\theta)&=\\sum_{i=1}^n \\left(X_{i,obs}\\dfrac{1}{\\theta} - (1-X_{i,obs})\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta}\n\\end{align*}\n\\] Setting the first derivative to zero determines the maximum likelihood estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\ell_n'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{N_{H,obs}}{\\hat\\theta_{ML}} &=& \\dfrac{n-N_{H,obs}}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&N_{H,obs}-N_{H,obs}\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-N_{H,obs}\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{N_{H,obs}}{n}\n\\end{array}\n\\tag{1.1}\\]\nUsually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE, \\(\\hat\\theta_{ML}.\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "href": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "title": "1  Maximum Likelihood",
    "section": "1.2 Numeric Optimization",
    "text": "1.2 Numeric Optimization\nUsually we are not so fortunate as to have an analytical solution for the MLE, and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\n\nGeneral idea:\n\nStart at some value, \\(\\theta_{(0)},\\) in the parameter space \\(\\Theta.\\)\nSearch across the parameter space \\(\\Theta\\) using a step-wise procedure until a updated parameter value \\(\\ell'(\\theta_{(m)})\\) is found that yield a derivative of the log likelihood that is effectively zero (i.e. smaller than some convergence/stopping criterion), \\(\\ell'(\\theta_{(m)})\\approx 0.\\)\n\n\n1.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nIn the following, we consider the univariate case \\(\\theta\\in\\mathbb{R}.\\) However, the multivariate case \\(\\theta\\in\\mathbb{R}^K\\) is treated likewise, but requires substituting first derivatives by gradients, second derivatives by the Hessian, etc.\n\n\n\n\n\n\nNote\n\n\n\n\nMinimization and maximization are essentially the same problems since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial of order 1}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial of order 2}},\n\\end{align*}\n\\] Locally, i.e. for \\(h\\approx 0,\\) (e.g. \\(h=\\pm 0.04\\)) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure 1.1.\n\n\n\n\n\nFigure 1.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Taylor’s Theorem)  Today, there are many different versions of Taylor’s theorem. We consider the following two:\n1. Peano form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k\\) times differentiable at \\(x\\in\\mathbb{R}\\) and let \\(h\\in\\mathbb{R}.\\) Then there exists a function \\(P_{k,x}:\\mathbb{R}\\to\\mathbb{R}\\) such that \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ P_{k,x}(h)\\;(h)^k\n\\end{align*}\n\\] with \\[\nP_{k,x}(h)\\to 0\\quad\\text{as}\\quad h\\to 0,\n\\] where \\(f^{(\\ell)}(x)\\) denotes the \\(\\ell\\)th derivative of \\(f\\) at \\(x.\\)\n2. Lagrange or Mean-value form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k+1\\) times differentiable on the open interval between \\(x\\) and \\(x+h,\\) with \\(h\\in\\mathbb{R},\\) and let \\(f^{(k)}\\) be continuous on the closed interval between \\(x\\) and \\(x+h,\\). Then \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ M_{k,x}(h)\n\\end{align*}\n\\] with \\[\nM_{k,x}(h)=\\frac{f^{(k+1)}(\\xi)}{(k+1)!}(h)^{k+1}\n\\] for some real number \\(\\xi\\) between \\(x\\) and \\(x+h.\\) (This form of Taylor’s theorem is based on the mean-value Theorem 1.2.)\n\n\n\n\n\n\nQualitative version using the small-\\(o\\) notation:\n\n\n\n\\[\n\\begin{align*}\nf(x + h) & =\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ o\\big(|h|^k\\big),\n\\end{align*}\n\\] where \\(o\\big(|h|^k\\big)\\) denotes the family of real-valued functions, \\(g(h)\\) say, that are of a strictly smaller \\(o\\)rder of magnitude than the function \\(|h|^k\\) as \\(h\\to 0;\\) i.e.\n\\[\no\\big(|h|^k\\big)=\\left\\{g(h)\\to 0\\;\\text{ as }\\; h\\to 0:\\frac{|g(h)|}{|h|^k}\\to 0\\quad\\text{as}\\quad h\\to 0\\right\\}.\n\\]\n1. Note: Peano form of the remainder term: \\(P_{k,x}(h)\\;(h)^k=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|P_{k,x}(h)\\;(h)^k|}{|h|^k}\n%=\\frac{|P_k(x+h)|\\cdot |h|^k|}{|h|^k}\n=|P_{k,x}(h)|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n2. Note: Mean-value form of the remainder term: \\(M_{k,x}(h)=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|M_{k,x}(h)|}{|h|^k}=\n\\left|\\frac{f^{(k+1)}(\\xi)}{(k+1)!}\\right|\\cdot|h|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell_n\\) be a log-likelihood function with continuous first, \\(\\ell_n',\\) and second, \\(\\ell_n'',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell_n,\\) we try to find the root of \\(\\ell_n',\\) i.e. the value of \\(\\theta\\in\\Theta\\) such that \\[\n\\ell_n'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order condition of the optimization problem. We do so using a step-wise optimization approach, where each step has a smallish size \\(h.\\)\nInitialization: Let \\(\\theta_{(0)}\\in\\Theta\\) be our first guess of the root of \\(\\ell'_n.\\)\n\\(h\\)-Steps: Typically, our guess is not perfect and thus \\(\\ell_n'(\\theta_{(0)})\\neq 0.\\) Therefore, we want to move from \\(\\theta_{(0)}\\) to a new root-candidate \\(\\theta_{(1)}\\) by doing an \\(h\\)-step update \\[\n\\theta_{(1)} = \\theta_{(0)} + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell_n'\\) around our first guess \\(\\theta_{(0)}\\) gives \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)} + h) & \\approx \\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)})h\n\\end{align*}\n\\] Thus, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell_n',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)}) h_{(0)} = 0\\\\[2ex]\n\\Rightarrow h_{(0)} = -\\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\begin{align*}\n\\theta_{(1)}\n& = \\theta_{(0)} + h_{(0)}\\\\[2ex]\n& = \\theta_{(0)} - \\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\end{align*}\n\\] Likewise, the \\(m\\)th root-candidate is \\[\n\\begin{align*}\n\\theta_{(m)}\n& = \\theta_{(m-1)} + h_{(m-1)}\\\\[2ex]\n& = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})};\n\\end{align*}\n\\] see also Figure 1.2.\n\n\n\n\n\nFigure 1.2: The \\(m\\)th update step in the Newton-Raphson root-finding algorithm.\n\n\n\n\n\n\n\n1.2.2 Convergence of the Newton-Raphson Algorithm\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.  \\[\n\\ell_n'(\\theta_{root})=0.\n\\] We aim to find \\(\\theta_{root}\\) using the Newton-Raphson algorithm and call our best approximation of \\(\\theta_{root}\\) the maximum likelihood estimate; i.e. \\(\\hat{\\theta}_{ML}\\approx\\theta_{root}.\\)\nLet \\[\ne_{(0)}=\\theta_{root}-\\theta_{(0)}\n\\] denote the start value error and let \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start value error neighborhood around \\(\\theta_{root}.\\)\nOne can shown that if \\(\\ell_n'\\) is “well behaved” over \\(I;\\) i.e. \n\nif \\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\nif \\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I,\\)\n\nand if our first guess \\(\\theta_{(0)}\\) is “close enough;” i.e. \n\nif \\(M|e_{(0)}|<1,\\) where \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\n\nthen \\(\\theta_{(m)}\\) will converge to \\(\\theta_{root}\\) as \\(m\\to\\infty.\\)\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, we typically don’t know if \\(\\ell_n'\\) is “well behaved” and we usually don’t know whether our first guess is “close enough”. So, typically we cannot guarantee convergence of the Newton-Raphson algorithm. 😭 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor problems that are globally concave, the starting value \\(\\theta_0\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nIn actual practice, implementation of the Newton-Raphson algorithm can be tricky. We may have \\(\\ell_n''(\\theta_{(m)})=0,\\) in which case the function looks locally like a straight line, with no solution to the Taylor series approximation \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(m)} + h) & \\approx \\ell_n'(\\theta_{(m)}) + \\ell_n''(\\theta_{(m)})h.\n\\end{align*}\n\\] In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on \\(\\ell_n'(\\theta_m).\\)\nIn other cases where \\(\\theta_{(m)}\\) is too far from the true maximizer \\(\\theta\\), the Taylor approximation may be so inaccurate that \\(\\ell_n(\\theta_{(m+1)})\\) is actually smaller than \\(\\ell_n(\\theta_{(m)}).\\) When this happens one may replace \\(\\theta_{(m+1)}\\) with \\((\\theta_{(m+1)}+\\theta_{(m)})/2\\) (or some other value between \\(\\theta_{(m)}\\) and \\(\\theta_{(m+1)}\\)) in the hope that a smaller step will produce better results.\n\n\n\nStopping Criterion: Since we are expecting that \\(\\ell_n'(\\theta_{(m)})\\to 0,\\) as \\(m\\to\\infty,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell_n'(\\theta_{(m)})|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon>0.\\)\n\n\n\n\n\n\nPseudo-Code: Newton-Raphson Algorithm\n\n\n\n\\[\n\\begin{array}{ll}\n\\texttt{\\textbf{select }} \\theta_{(0)}\\in\\Theta\\;\\;\\text{ and}&\\varepsilon>0 \\\\[2ex]\n\\texttt{\\textbf{let }} m=0         &  \\\\\n\\texttt{\\textbf{while }}  | \\ell_n'(\\theta_{(m)}) | >\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} m = m+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_{(m)} = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta_{ML}=\\theta_{(m)} & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta_{ML} &  \\\\\n\\end{array}\n\\]\n\n\n\n\n1.2.3 Newton-Raphson Algorithm: Coin-Flipping Example\nLet’s return to our earlier coin flipping example.\nIf we observe, for instance, only one head \\(N_{H,obs}=1\\) for a sample size of \\(n=5,\\) we already know from Equation 1.1 that \\[\n\\hat\\theta_{ML}=\\frac{N_{H,obs}}{n}=\\frac{1}{5}=0.2,\n\\] but let us, nevertheless, apply the Newton-Raphson algorithm.\nThe first and second derivatives of \\[\n\\ell_n(\\theta)=\\sum_{i=1}^n\\big(X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\big)\n\\] are \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta} \\\\[2ex]\n\\ell_n''(\\theta) &= -\\dfrac{N_{H,obs}}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{N_{H,obs}}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{N_{H,obs}}{\\theta^2} - \\dfrac{n-N_{H,obs}}{(1-\\theta)^2}.\n\\end{align*}\n\\]\nWe consider a sample size of \\(n=5\\) with the following observed outcome:\n\nOne Head: \\(\\quad N_{H,obs}=1\\)\nFour Tails: \\(\\quad n-N_{H,obs}=4\\)\n\nSetting \\(\\varepsilon=10^{-10}\\) as our stopping criterion and \\(\\theta_{(0)}=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table 1.1. The numeric optimization solution is \\(\\hat\\theta_{ML} = 0.2\\) which equals the analytic solution.\n\n\nTable 1.1: Result of applying the Newton Raphson optimization algorithm to our coin flipping example for given data with \\(N_{H,obs}=1,\\) sample size \\(n=5,\\) starting value \\(\\theta_{(0)}=0.4,\\) and convergence criterion \\(\\varepsilon=10^{-10}.\\)\n\n\n\n\n\n\n\n\n\\(m\\)\n\\(\\hat\\theta_{(m)}\\)\n\\(h_{{(m)}}=\\frac{-\\ell_n'(\\hat\\theta_{(m)})}{\\ell_n''(\\hat\\theta_{(m)})}\\)\n\\(\\ell_n'(\\hat\\theta_{(m)})\\gtrless \\varepsilon\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-2.4\\cdot 10^{-1}\\)\n\\({\\color{red}-4.2 > \\varepsilon}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}3.3\\cdot 10^{-2}\\)\n\\({\\color{red}\\phantom{-}1.5 > \\varepsilon}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}6.6\\cdot 10^{-3}\\)\n\\({\\color{red}\\phantom{-}2.2\\cdot 10^{-1} > \\varepsilon}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}1.7\\cdot 10^{-4}\\)\n\\({\\color{red}\\phantom{-}5.4\\cdot 10^{-3} > \\varepsilon}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}1.1\\cdot 10^{-7}\\)\n\\({\\color{red}\\phantom{-}3.5\\cdot 10^{-6} > \\varepsilon}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}4.8\\cdot 10^{-14}\\)\n\\({\\color{darkgreen}\\phantom{-}1.5\\cdot 10^{-12} < \\varepsilon}\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "href": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "title": "1  Maximum Likelihood",
    "section": "1.3 Linear Regression under Normality",
    "text": "1.3 Linear Regression under Normality\nNow let’s return to the linear regression model \\[\nY_i=X_i'\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{1.2}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta_0\\in\\mathbb{R}^p\n\\] denotes the vector of unknown slope parameters, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})'\\in\\mathbb{R}^K\n\\] denotes the vector of predictor variables, where the (i.i.d.) random sample\n\\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design with homoskedastic errors (see Definition 1.3).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Random Design (Regression Analysis)) \nA random desgin in regression analysis is given by the following setup:\nLet \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] or equivalently \\[\n(X_1,\\varepsilon_1), (X_2,\\varepsilon_2), \\dots, (X_n,\\varepsilon_n)\n\\] denote a (i.i.d.) random sample with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\), intertable \\((K\\times K)\\) matrix \\(\\mathbb{E}(X_iX_i')=\\Sigma_{X'X}\\), \\(i=1,\\dots,n,\\) and with either\n\nhomoskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0<\\infty\\)\n\nor\n\nheteroskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), for a strictly positive and finite variance function \\(\\sigma^2_0(\\cdot).\\)\n\n\n\n\nFor the following, it is convenient to write Equation 1.2 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta_0} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nUnder normally distributed (and homoskedastic) error terms, \\(\\varepsilon_i,\\) we have that \\[\n\\begin{align}\n\\underset{(n\\times 1)}{\\varepsilon} &\\sim \\mathcal{N}_n\\left(0, \\sigma_0^2I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y-X\\beta_0)|X &\\sim \\mathcal{N}_n\\left(0, \\sigma^2_0I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y_i-X_i'\\beta_0)|X_i &\\sim \\mathcal{N}\\left(0, \\sigma^2_0\\right)\\\\[2ex]\n\\Rightarrow\\quad\nY_i|X_i &\\sim \\mathcal{N}\\left(X_i'\\beta_0, \\sigma^2_0\\right)\n\\end{align}\n\\tag{1.3}\\]\n\n\n\n\n\n\n\nUnder Equation 1.3, we have \\[\nf(Y_i|X_i;\\beta_0',\\sigma_0^2)=\n\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta_0)^2}{2\\sigma_0^2}\\right),\n\\] where \\[\n\\theta_0=(\\beta_0',\\sigma_0^2)'\\in\\mathbb{R}^K\\times\\mathbb{R}_{>0}\n\\] denotes the unknown parameter vector.\nThis allows us to setup the likelihood function, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\beta',\\sigma^2)\n& =\\prod_{i=1}^n f(Y_i|X_i;\\beta',\\sigma^2)\\\\[2ex]\n& =\\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n& =\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{\\sum_{i=1}^n (Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n%& =\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)\\\\[2ex]\n& =(2\\pi)^{-n/2} \\cdot (\\sigma^2)^{-n/2}\\cdot  \\exp\\left(-\\frac{(Y-X\\beta)'(Y-X\\beta)}{2\\sigma^2}\\right),\\\\[2ex]\n\\end{align*}\n\\]   and the log-likelihood function, \\[\n\\begin{align*}\n\\ell_n(\\beta',\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta).\n\\end{align*}\n\\] \nTaking first derivatives gives \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}}\\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] Putting the above derivative functions into one column vector yields the \\(((K+1)\\times 1)\\)-dimensional gradient called score function in ML-theory: \\[\n\\nabla\\ell_n(\\theta')=\n\\left(\\begin{matrix}\n\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)\\\\\n\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)\n\\end{matrix}\\right)\n\\tag{1.4}\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Score Function) More generally, let \\(\\ell_n(\\theta)\\) denote the log-likelihood function evaluated at a \\(p\\)-dimensional parameter vector \\(\\theta=(\\theta_1,\\dots,\\theta_p)'.\\)\nThen the gradient vector \\[\\nabla\\ell_n(\\theta')=\\left(\\begin{matrix}\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_1}(\\theta')\\\\ \\vdots\\\\\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_p}(\\theta')\n  \\end{matrix}\n  \\right)\n\\] is called the score-function.\nThe score function is random, since it depends on the random sample.\nAt the true parameter vector \\(\\theta_0\\in\\mathbb{R}^p,\\) the score function satisfies \\[\n\\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_j}(\\theta_0')\\right)=0\n\\] for all \\(j=1,\\dots,p.\\) We show this below in Section 1.4.\n\n\n\nSetting the score function in Equation 1.4 equal to zero yields a system of \\(K+1\\) equations with \\(K+1\\) unknowns, which we can solve for the maximum likelihood estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}:\\)\nSolving for \\(\\hat\\beta_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad& - \\dfrac{1}{\\hat\\sigma^2_{ML}}(-X'Y + X'X\\hat\\beta_{ML})  \\overset{!}{=}0\\\\[2ex]\n\\Rightarrow\\quad & \\hat\\beta_{ML}=(X'X)^{-1}X'Y\\\\[2ex]\n\\end{align*}\n\\] Solving for \\(s^2_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad&-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}  \\overset{!}{=}0\\ \\\\[2ex]\n\\Rightarrow\\quad  &\ns_{ML}^2 =\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\\\[2ex]\n&\\phantom{s_{ML}^2}=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2,\n\\end{align*}\n\\] where \\(\\hat\\varepsilon_i = Y_i - X_i'\\hat{\\beta}_{ML}.\\)\nObservations:\n\n\\(\\hat\\beta_{ML}\\) equals the OLS estimator \\(\\hat\\beta=(X'X)^{-1}X'Y.\\)  Since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\n\\(s_{ML}^2\\) differs from the unbiased variance estimator \\(s_{UB}^2=\\frac{1}{n-K}\\hat{\\varepsilon}_i^2.\\)\n\n\n1.3.1 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\n\n\n\n\n\n\nComputing the Asymptotic Variance\n\n\n\nTo compute the asymptotic variance of the ML-estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML},\\) we need to\n\ncompute the Hessian matrix (i.e. all second partial derivatives) of \\(\\ell_n,\\)\n\ntake the expectation of this Hessian matrix and multiply it by \\(-1/n\\), which gives us the Fisher Information matrix.\nInverting the Fisher information matrix give the asymptotic variance expression.\n\n\n\nLet’s do this preliminary work in the following:\n\nPartial second derivatives with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\Rightarrow\\quad\n\\underset{(K\\times K)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)}\n&= - \\dfrac{1}{\\sigma^2}(X'X)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{1}{\\sigma^2} \\mathbb{E}(X'X)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{n}{\\sigma^2} \\Sigma_{X'X}\\right)\\\\[2ex]\n&=  \\dfrac{1}{\\sigma^2} \\Sigma_{X'X},\n\\end{align*}\n\\] where\n\\[\n\\mathbb{E}\\left(X'X\\right)\n=\\mathbb{E}\\left(\\sum_{i=1}^nX_iX_i'\\right)\n=n\\underbrace{\\mathbb{E}\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}} = n\\Sigma_{X'X}.\n\\]\nSecond derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2}\\frac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{2}}\\\\[2ex]\n\\Rightarrow\\quad\\underset{(1\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{3}} \\\\[2ex]\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\[2ex]\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{\\mathbb{E}\\left(\\sum_{i=1}^n\\varepsilon_i^2\\right)}{\\sigma^{6}} \\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{n\\sigma^2}{\\sigma^{6}}\\right)\\\\[2ex]\n&=\\left(-\\frac{1}{2\\sigma^{4}}+\\frac{1}{\\sigma^{4}}\\right)\\\\[2ex]\n&=\\frac{1}{2\\sigma^{4}}\\\\[2ex]\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}(X')(Y - X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}X'\\varepsilon\\\\[2ex]\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\Rightarrow\\quad\n\\underset{(K\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta \\partial \\sigma^2}(\\beta',\\sigma^2)}\n=   \\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)'\n%&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\[2ex]\n& = \\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot  \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\left(\\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\right)'\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\varepsilon)}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(\\mathbb{E}(X'\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\mathbb{E}(\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot 0=0,\n\\end{align*}\n\\] since \\(\\mathbb{E}(\\varepsilon|X)=0\\) is an \\((n\\times 1)\\) zero vector.\nCollecting the above results, allows us to write down the expression for \\((-1/n)\\) times the expectation of the Hessian matrix of \\(\\ell_n\\) which yields the Fisher Information (Matrix):\n\\[\n\\begin{align*}\n&\\mathcal{I}(\\theta) :=\\; -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=\n-\\frac{1}{n}\\cdot \\mathbb{E}\n\\left[\\begin{array}{cc}\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2) \\right)\n\\end{array}\\right]\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.5 (Fisher Information Matrix) The matrix \\[\n\\mathcal{I}(\\theta) := -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\theta)\\right)\n\\] is called Fisher Information Matrix.\n\n\n\n\nAsymptotic Variance and Fisher Information Matrix\nThe asymptotic variance of the MLE \\[\n\\hat{\\theta}_{ML}=\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n\\] is given by the inverse of the Fisher information matrix evaluated at the true parameter values \\(\\beta_0\\) and \\(\\sigma^2_0.\\) \\[\n\\begin{align*}\n&AVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\\left(\\mathcal{I}(\\beta'_0,\\sigma^2_0)\\right)^{-1}\\\\[2ex]\n&=\\left(-\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta'_0,\\sigma^2_0)\\right)\\right)^{-1}\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right)\\\\\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta'_0,\\sigma^2_0) \\right)\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2_0}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4_0}}\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\sigma^2_0\\Sigma_{X'X}^{-1}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{2\\sigma^4_0}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\nThat is, \\[\n\\begin{align*}\nAVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n&=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\sigma^2_0\\Sigma_{X'X}^{-1} & 0 \\\\[2ex]\n0 & \\ 2\\sigma^4_0\n\\end{array}\\right].\n\\end{align*}\n\\tag{1.5}\\]\n\n\nOf course, the variance expressions in Equation 1.5 contain unknown quantities and thus are not directly usable in practice. However, we can plug in estimates of the unknown quantities; namely \\[\ns_{ML}^2                         \\quad\\text{for}\\quad \\sigma^2_0\n\\] and \\[\nS_{X'X}^{-1}=\\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1} \\quad \\text{for}\\quad \\Sigma_{X'X}^{-1}.\n\\]\nThis leads to estimators of the asymptotic variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{AVar}(s^2_{ML})\n&=2\\left(s_{ML}^2\\right)^2\n\\end{align}\n\\] and thus to estimators of the variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{Var}(\\hat{\\beta}_{ML})\n=\\frac{1}{n}\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 \\frac{1}{n}S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{Var}(s^2_{ML})\n=\\frac{1}{n}\\widehat{AVar}(s^2_{ML})\n&=\\frac{1}{n}2\\left(s_{ML}^2\\right)^2.\n\\end{align}\n\\]"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "href": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "title": "1  Maximum Likelihood",
    "section": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x;\\theta_0),\n\\] where the true (unknown, univariate) parameter \\(\\theta_0\\in\\Theta\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta_0\\) is an “interior point” of \\(\\Theta\\) if \\(\\theta_l<\\theta_0<\\theta_u.\\)\nMoreover, we consider the following setup.\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i;\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i;\\theta)\n\\]\nMaximum-likelihood estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_n=\\arg\\max_{\\theta\\in\\Theta}\\ell_n(\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\ell_n'(\\hat\\theta_n)=0\\quad\\text{and}\\quad\\ell_n''(\\hat\\theta_n)<0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x;\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x;\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x;\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x;\\theta)dx\n\\end{align*}\n\\] for all \\(\\theta\\in\\Theta.\\)\n\n\n\n\n\n\n\nExample\n\n\n\nAn example that fits into the above setup is the density of the exponential distribution \\[\nf(x;\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x;\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on a Taylor expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta_0\\) (see Equation 1.6). To derive this expression, we use the mean value theorem (Theorem 1.2).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\nBy the Mean Value Theorem (Theorem 1.2), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\tag{1.6}\\] for some \\(\\psi_n\\) between \\(\\hat{\\theta}_n\\) and \\(\\theta_0;\\) i.e.\n\n\\(\\psi_n\\in(\\theta_0,\\hat{\\theta}_n)\\quad\\) if \\(\\quad\\theta_0<\\hat{\\theta}_n\\)\n\\(\\psi_n\\in(\\hat{\\theta}_n,\\theta_0)\\quad\\) if \\(\\quad\\hat{\\theta}_n<\\theta_0\\)\n\n\nNote: Equation 1.6 is simply the first-order version of the mean-value form of Taylor’s theorem (Theorem 1.1).\n\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 1.6, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0).\n\\tag{1.7}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x;\\theta)dx=1\n\\] for all possible values of \\(\\theta\\in\\Theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\underbrace{\\int_{-\\infty}^{\\infty} f(x;\\theta)dx}_{=1}&=\\frac{\\partial}{\\partial \\theta}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x;\\theta)dx\n=\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx\n=0\n\\tag{1.8}\\] for all \\(\\theta\\in\\Theta.\\)\nLikewise, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\underbrace{\\int_{-\\infty}^{\\infty} f(x;\\theta)dx}_{=1}&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x;\\theta)dx\n=\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx\n=0\n\\tag{1.9}\\] for all \\(\\theta\\in\\Theta.\\)\nUsing Equation 1.8 and Equation 1.9, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta_0)=\\frac{1}{n}\\underbrace{\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)}_{\\ell_n'(\\theta_0)}\n\\] is asymptotically normal. This is done in the following by checking the three conditions for applying the Lindeberg-Lévy central limit theorem.\nFirstly, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\n\\] is taken over i.i.d. random variables: \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_1;\\theta_0),\\dots,\\frac{\\partial}{\\partial \\theta} \\ln f(X_n;\\theta_0)\\overset{\\text{i.i.d.}}{\\sim}\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\n\\]\nSecondly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X;\\theta_0)}{f(X;\\theta_0)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)}\n{f(x;\\theta_0)}f(x;\\theta_0)dx\\quad[\\text{Def. of $\\mathbb{E}$}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\tag{1.10}\\] where the last step follows from Equation 1.8.\nThirdly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X;\\theta_0)}{f(X|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\\\\\n&=\\frac{1}{n}\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\] where the simplification of the variance expression to a second moment expression follows from Equation 1.10. \n\nWe can write the last expression using the Fisher Information \\(\\mathcal{I}(\\theta_0)-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0))\\) since below in Equation 1.12 we’ll see that \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n& =-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)) = \\mathcal{I}(\\theta_0).\n\\end{align*}\n\\]\n\n\nThus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\theta_0)-\\overbrace{\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)}^{=0}}{\\sqrt{\\frac{1}{n}\\mathcal{I}(\\theta_0)} } = \\frac{\\ell_n'(\\theta_0)}{\\sqrt{n\\mathcal{I}(\\theta_0)} } \\to_d \\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\)\nBy our mean value expression in Equation 1.7 \\[\n\\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] we thus have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\mathcal{I}(\\theta_0)}}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{1.11}\\] The \\(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\)-part in Equation 1.11 is our object of interest.\nThe further analysis requires us to study the asymptotic behavior of\n\\[\n-\\frac{1}{n}\\ell_n''(\\psi_n)\n\\] which will help us to understand the behavior of \\(\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\) in Equation 1.11.\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(-\\frac{1}{n}\\ell_n''(\\psi_n),\\) we begin with studying the mean and the variance of the simpler statistic \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0).\n\\]\n\n\nFirst, the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i;\\theta_0)\\right) f(X_i;\\theta_0)-\\frac{\\partial}{\\partial\\theta}f(X_i;\\theta_0)\\frac{\\partial}{\\partial\\theta} f(X_i;\\theta_0)}{\\left(f(X_i;\\theta_0)\\right)^2}\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2  \n\\right).\n\\end{align*}\n\\] Taking the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}\n{f(X;\\theta_0)}+\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)+\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] From Equation 1.10, we know that \\(\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2} f(X;\\theta_0)}{f(X;\\theta_0)}\\right)=0\\) thus \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=0 + \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\\\\[2ex]\n&=\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\tag{1.12}\\] \\[\n\\Rightarrow \\qquad \\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)=\\mathcal{I}(\\theta_0)\\qquad\n\\]\nThis means that \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n\\] is an unbiased estimator of the Fisher information \\(\\mathcal{I}(\\theta_0).\\)\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nFor multivariate (\\(p\\)-dimensional) parameters \\(\\theta_0,\\) the Fisher information \\(\\mathcal{I}(\\theta_0)=(-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta_0)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section 1.3.1).\n\n\nSecond, the variance of variance of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=Var\\left(-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X;\\theta_0)\\right)}_{=\\texttt{constant}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results for \\(-\\frac{1}{n}\\ell_n''(\\theta_0),\\) we can write down the Mean Squared Error (MSE) of the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) of \\(\\mathcal{I}(\\theta_0):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(-\\frac{1}{n}\\ell_n''(\\theta_0) -\\mathcal{I}(\\theta_0)\\right)^2\\right)\\\\[2ex]\n&=\\underbrace{\\left(\\operatorname{Bias}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\right)^2}_{=0}+Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[3ex]\n&=Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) is a mean square consistent estimator, i.e. \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_{m.s.} \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta_0)\\) is also a (weakly) consistent estimator, i.e.  \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_p \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 Remember, we wanted to study \\(-\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 1.11 not \\(-\\frac{1}{n}\\ell_n''(\\theta_0).\\) Studying \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) was only the simpler thing to do.\nLuckily, we are actually close now.\n\n\nNext, we use that ML estimators \\(\\hat\\theta_n\\) are (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\nExample: Our results in Section 1.3 imply, for instance, that the ML estimator \\(\\hat{\\beta}_n\\) is consistent for \\(\\beta.\\)\n\nSince \\(\\psi_n\\) is a mean value between \\(\\theta_0\\) and \\(\\hat{\\theta}_n\\) (Equation 1.6), consistency of \\(\\hat{\\theta}_n\\) implies that \\[\n\\psi_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have by the continuous mapping theorem that \\[\n\\begin{align}\n-\\frac{1}{n}\\ell_n''(\\psi_n) & \\to_p \\phantom{-}\\mathcal{I}(\\theta_0)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\qquad\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)&\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} \\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\]\nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 1.11 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)}_{\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right),\n\\end{align*}\n\\tag{1.13}\\] where \\(1/\\mathcal{I}(\\theta_0)\\) is the asymptotic variance of the ML estimator \\(\\hat{\\theta}_n\\) and equals the inverse of the (here scalar valued) Fisher information \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)).\n\\]\nEquation 1.13 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{I}(\\theta_0)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d \\mathcal{N}_p\\left(0, \\mathcal{I}(\\theta_0)^{-1}\\right),\n\\] where \\(\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\\) is the \\((p\\times p)\\) Fisher information matrix with \\(H_{\\ell_n}(\\theta_0)\\) denoting the Hesse matrix of \\(\\ell_n(\\cdot)\\) evaluated at \\(\\theta_0.\\)\n\n\n\n\n\n\n\n\nML-Theory and Machine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020))."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "href": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "title": "1  Maximum Likelihood",
    "section": "1.5 Invariance Property of the ML-Estimator",
    "text": "1.5 Invariance Property of the ML-Estimator\nSuppose that a distribution has the parameter \\(\\theta_0,\\) but we are interested in finding an estimator of a function of \\(\\theta_0,\\) say \\[\n\\eta_0=\\tau(\\theta_0).\n\\] The invariance property of ML-estimators says that if \\(\\hat{\\theta}_n\\) is the ML-estimator of \\(\\theta_0,\\) then \\(\\tau(\\hat{\\theta}_n)\\) is the ML-estimator of \\(\\eta_0=\\tau(\\theta_0).\\)\n\nOne-to-One Functions\nLet the function \\[\n\\eta = \\tau(\\theta)\n\\] be a one-to-one function. That is, for each value of \\(\\theta\\) there is a unique value of \\(\\eta\\) and vice versa.\nImportant property of one-to-one functions: A one-to-one function \\(\\eta = \\tau(\\theta)\\) possesses a well-defined inverse \\[\n\\theta=\\tau^{-1}(\\eta).\n\\]\n\n\n\n\n\n\nExample:\n\n\n\nFor instance, the functions \\[\n\\begin{align*}\n\\eta = \\tau(\\theta) & = \\theta + 3\n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = \\eta -3 \\\\[2ex]\n\\eta = \\tau(\\theta) & = \\theta/5  \n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = 5 \\eta\n\\end{align*}\n\\] are one-to-one functions. However, for instance, the functions \\[\n\\begin{align*}\n\\tau(\\theta) & = \\sin(\\theta)\\\\[2ex]\n\\tau(\\theta) & = \\theta^2  \n\\end{align*}\n\\] are not one-to-one functions.\n\n\nIn this one-to-one case, it is easily seen that it makes no difference whether we maximize the likelihood function as a function of \\(\\theta\\) or as a function of \\(\\eta = \\tau(\\theta)\\)—in each case we get the same answer.\nThe likelihood function of \\(\\tau(\\theta),\\) written as a function of \\(\\eta,\\) is given by \\[\n\\begin{align*}\n\\mathcal{L}^*(\\eta)\n&= \\prod_{i=1}^n f\\big(X_i;\\tau^{-1}(\\eta)\\big)\n= \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n  \\sup_{\\eta}  \\mathcal{L}^*(\\eta)\n= \\sup_{\\eta}  \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n= \\sup_{\\theta}\\mathcal{L}\\big(\\theta\\big).\n\\end{align*}\n\\] Thus, the maximum of \\(\\mathcal{L}^*(\\eta)\\) is attained at \\[\n\\eta=\\tau(\\theta)=\\tau(\\hat\\theta_n),\n\\] showing that the ML-estimator of \\(\\tau(\\theta_0)\\) is \\(\\tau(\\hat\\theta_n).\\)\n\n\n\n\n\n\n\nMore general (not one-to-one) functions\n\n\n\nIn many cases, however, this simply version of the invariance of ML-estimators is not useful since many functions of interest are not one-to-one.\nLuckily, the invariance property of the ML-estimator also holds for functions that are not one-to-one; see Chapter 7 in Casella and Berger (2001)."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#exercises",
    "href": "Ch1_MaximumLikelihood.html#exercises",
    "title": "1  Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nProgram the Newton-Raphson algorithm for a numerical computation of the ML estimate \\(\\hat\\theta\\) of the parameter \\(\\theta=P(\\text{Coin}=\\texttt{HEAD})\\) in our coin toss example of this chapter. Replicate the results shown in Table 1.1.\n\n\nExercise 2.\nAssume an i.i.d. random sample \\(X_1,\\dots,X_n\\) from an exponential distribution, i.e. the underlying density of \\(X_i\\) is given by \\[\nf(x;\\theta_0)=\n\\left\\{\\begin{array}{ll}\\theta_0\\exp(-\\theta_0 x),&x\\geq 0\\\\0,&x<0\\end{array}\\right.\n\\] with \\(\\theta_0>0,\\) where \\[\n\\mu:=\\mathbb{E}(X_i)=\\frac{1}{\\theta_0}\n\\] and \\[\nVar(X_i)=\\frac{1}{\\theta_0^2}.\n\\]\n\nWhat is the log-likelihood function for the i.i.d. random sample \\(X_1,\\dots,X_n\\)?\nDerive the maximum likelihood (ML) estimator \\(\\hat\\theta_n\\) of \\(\\theta_0.\\)\nFrom maximum likelihood theory we know that \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\to_d \\mathcal{N}\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right).\n\\] Derive the expression for the Fisher information \\(\\mathcal{I}(\\theta_0).\\) Use the Fisher information to give the explicit formula for the asymptotic distribution of \\(\\hat\\theta_n\\).\n\n\n\nExercise 3.\n\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Unif}(0,\\theta_0).\\)\n\nWhat is the likelihood function?\nWhat is the maximum likelihood estimator of \\(\\theta_0\\)?\n\n\n\nExercise 4.\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Poisson}(\\lambda_0).\\) That is \\(X\\sim f\\) with density function \\[\nf(x;\\lambda_0) = \\frac{\\lambda_0^x \\exp(-\\lambda_0)}{x!}.\n\\]\n\nFind the maximum likelihood estimator, \\(\\hat{\\lambda},\\) of \\(\\lambda_0.\\)\nLet \\(0<\\lambda_0\\leq 4.\\) Find the maximum likelihood estimator, \\(\\hat{P}(X=4),\\) of \\(P(X=4).\\)\n\n\n\nExercise 5.\nShow that the Newton-Raphson algorithm converges; i.e. that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\] under the setup outlined in Section 1.2.2.\nTip: Use the first-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with explicit reminder term \\(R\\) given by \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + R,\n\\end{align*}\n\\] where \\[\nR=\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\] for a mean-value \\(\\xi_{(m)}\\) between \\(\\theta_{(m)}\\) and \\(\\theta_{root}\\). This is called the Lagrange form of the Taylor-Series reminder term and follows from the Mean-Value Theorem Theorem 1.2."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#references",
    "href": "Ch1_MaximumLikelihood.html#references",
    "title": "1  Maximum Likelihood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCasella, George, and Roger Berger. 2001. Statistical Inference. 2nd ed. Duxbury.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017. “Overcoming Catastrophic Forgetting in Neural Networks.” Proceedings of the National Academy of Sciences 114 (13): 3521–26.\n\n\nMartens, James. 2020. “New Insights and Perspectives on the Natural Gradient Method.” The Journal of Machine Learning Research 21 (1): 5776–5851.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 1–25."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html",
    "href": "Ch2_EMAlgorithmus.html",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "",
    "text": "The Expectation Maximization (EM) algorithm is often used to simplify or to facilitate complex maximum likelihood estimation problems. In this chapter, we present the EM algorithm for estimating Gaussian mixture distributions, as this is probably its most well-known application. Even the original work on the EM algorithm (Dempster, Laird, and Rubin 1977) already dealt with the estimation of Gaussian mixture distributions."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "href": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models",
    "text": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models\nAs a data example we use the palmerpenguins data (Horst, Hill, and Gorman (2020)).\nThese data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (Figure 2.1). We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.\n\n\n\nFigure 2.1: Cheeky penguin in action.\n\n\nThe following code chunk prepares the data\n\n\n\n\n\n\nCaution\n\n\n\nWe have the information about the different penguin species (penguin_species) but in the following we pretend not to know this information.\nWe want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (penguin_flipper) alone.\nAfterwards we can use the data in penguin_species to check how good our cluster analysis is.\n\n\n\n## Select a color palette\ncol_v <- RColorBrewer::brewer.pal(n = 3, name = \"Set2\")\n\n## Preparing the data:\npenguins <- palmerpenguins::penguins %>%  # penguin data\n  tidyr::as_tibble() %>%                  # 'tibble'-dataframe\n  dplyr::filter(species!=\"Adelie\") %>%    # remove penguin species 'Adelie' \n  droplevels() %>%                        # remove the non-used factor level\n  tidyr::drop_na() %>%                    # remove NAs\n  dplyr::mutate(species = species,        # rename variables \n                flipper = flipper_length_mm) %>% \n  dplyr::select(species, flipper)         # select variables \n\n##  \nn      <- nrow(penguins)                  # sample size (n=187)\n\n## Pulling out the variable 'penguin_species':\npenguin_species <- dplyr::pull(penguins, species)\n\n## Pulling out the variable 'penguin_flipper':\npenguin_flipper <- dplyr::pull(penguins, flipper)\n\n## Plot\n## Histogramm:\nhist(x = penguin_flipper, freq = FALSE, \n     xlab=\"Flipper-Length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))\n## Stipchart hinzufügen:\nstripchart(x = penguin_flipper, method = \"jitter\", \n           jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[3],.5), \n           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)\n\n\n\n\n\n\n\n\n\nClustering using Gaussian Mixture Distributions\nAt the end of this chapter, we’ll be able to\n\nEstimate the Gaussian mixture distribution using the EM algorithm\nAssign the predictors \\(x_i\\) (flipper length) to the group (penguine species) that maximizes the “posterior probability” (see Figure 2.2 and Section 2.3.2)\n\n\n\n\n\n\nFigure 2.2: Cluster analysis based on a mixture distribution with two weighted normal distributions.\n\n\n\n\nFigure Figure 2.2 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. Cluster result: 95% of the penguins could be correctly assigned - based only on their flipper lengths.\nThe following R codes can be used to reproduce the above cluster analysis (using the R package mclust) and Figure 2.2:\n\n## mclust R package:\n## Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(\"mclust\"))\n\n## Number of Groups\nG <- 2 \n\n## Schätzung des Gaußschen Mischmodells (per EM Algorithmus)\n## und Clusteranalyse\nmclust_obj <- mclust::Mclust(data       = penguin_flipper, \n                             G          = G, \n                             modelNames = \"V\", \n                             verbose    = FALSE)\n\n# summary(mclust_obj)\n# str(mclust_obj)\n\n## estimated group assignment \nclass <- mclust_obj$classification\n\n## Fraction of correct group assignments:\n# cbind(class, penguin_species)\nround(sum(class == as.numeric(penguin_species))/n, 2)\n\n## estimated means of the two Gaussian distributions\nmean_m <- t(mclust_obj$parameters$mean)\n\n## estimated variances (and possibly covariances) \ncov_l  <- list(\"Cov1\" = mclust_obj$parameters$variance$sigmasq[1], \n               \"Cov2\" = mclust_obj$parameters$variance$sigmasq[2])\n\n## estimated mixture weights (prior-probabilities) \nprop_v <- mclust_obj$parameters$pro\n\n## evaluating the Gaussian mixture density function \nnp      <- 100 # number of evaluation points\nxxd     <- seq(min(penguin_flipper)-3, \n               max(penguin_flipper)+5, \n               length.out = np)\n## mixture density\nyyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +\n           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n## single densities\nyyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]\nyyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n\n## Plot\nhist(x = penguin_flipper, xlab=\"Flipper length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))\nlines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))\nlines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)\nlines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)\nabline(v=203.1, lty=3)\nstripchart(penguin_flipper[class==1], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)\nstripchart(penguin_flipper[class==2], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)\n\nBut coding is nothing without understanding. We’ll learn the underlying statistical method it in this chapter."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "href": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions",
    "text": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions\n\n2.2.1 Gaussian Mixture Models (GMM)\nWe denote a random variable \\(X\\) that follows a Gaussian mixture distribution as \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\]\nThe corresponding density function of a Gaussian mixture distribution is defined as follows: \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g \\varphi(x;\\mu_g,\\sigma_g)\n\\tag{2.1}\\]\n\nWeights: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) with \\(\\pi_g>0\\) and \\(\\sum_{g=1}^G\\pi_g=1\\)\nMeans: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) with \\(\\mu_g\\in\\mathbb{R}\\)\nStandard deviations: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) with \\(\\sigma_g>0\\)\nNormal density of group \\(g=1,\\dots,G\\): \\[\n\\varphi(x;\\mu_g,\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right)\n\\]\nUnknown parameters: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)\n\n\n\n2.2.2 Maximum Likelihood (ML) Estimation\nWe could try to estimate the unknown parameters \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) using the maximum likelihood method.\n\nI’ll say it right away: The attempt will fail.\n\n\nBasic Idea of ML Estimation\n\nAssumption: The data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) is a realization of a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] with \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}).\n\\]\n\n\n\nEstimation Idea: Choose \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) such that \\(f_{GMM}(\\cdot;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) “optimally” fits the observed data \\(\\mathbf{x}\\).\nImplementation of the Estimation Idea: Maximize (with respect to \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\)) the likelihood function \\[\n\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})=\\prod_{i=1}^nf_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\] Or, equivalently, maximize the log-likelihood function (simpler maximization) \\[\n\\begin{align*}\n%\\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\\right)=\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n=&\\sum_{i=1}^n\\ln\\left(f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\\n=&\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\end{align*}\n\\tag{2.2}\\]\n\n\n\n\n\n\n\nMaximization constraints\n\n\n\nThe maximization must take into account the parameter constraints in Equation 2.1; namely,\n\n\\(\\sigma_g>0\\) and\n\\(\\pi_g>0\\) for all \\(g=1,\\dots,G\\) such that\n\\(\\sum_{g=1}^G\\pi_g=1\\).\n\n\n\nThe maximizing parameter values \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\hat{\\boldsymbol{\\sigma}}\\) are the ML-Estimators:\n\\[\n(\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n\\]\n😒 Problems with singularities in numerical solutions: If one tries to solve the above maximization problem numerically with the help of the computer, one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities.\nFor real GMMs (i.e. GMMs with more than one group \\(G>1\\)), problems with singularities occur very easily during a numerical maximization. This happens whenever one (or more) of the normal distribution component(s), say \\(\\varphi_g(x_i;\\mu_g,\\sigma_g),\\) tries to describe only single data points. This leads to a Gaussian density function centered around a single data point \\(x_i\\) such that\n\\[\n\\varphi(x_i;{\\color{red}\\mu_g=x_i},\\sigma_g),\n\\] where \\[\n\\sigma_g\\to 0.\n\\] This degenerating situation leads to very large density function values, \\[\n\\varphi(x_i;\\mu_g=x_i,\\sigma_g)\\to\\infty\\quad\\text{for}\\quad \\sigma_g\\to 0,\n\\] and thus maximize the log-likelihood in an undesirable way (see Figure 2.3).\n\n\n\n\n\nFigure 2.3: Gaussian density with \\(\\mu_g=x_i\\) for \\(\\sigma_g\\to 0\\).\n\n\n\n\nSuch undesirable, trivial maximization solutions typically lead to implausible, non-useful estimation results.\n🤓 Analytic solution: It is a bit tedious, but one can maximize the log-likelihood function of the GMM (see Equation 2.2) analytically. If you do this, you will get the following expressions: \\[\n\\begin{align*}\n\\hat\\pi_g&=\\frac{1}{n}\\sum_{i=1}^np_{ig},\\quad\n\\hat\\mu_g=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\[2ex]\n\\hat\\sigma_g&=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2},\n\\end{align*}\n\\tag{2.3}\\] where \\[\np_{ig}=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\tag{2.4}\\] for \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G\\).\n\n\n\n\n\n\nNote\n\n\n\nDeriving the expressions for \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) and \\(\\hat{\\pi}_g\\) in Equation 2.3 is really a bit tedious (multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints) but in principle doable.\n\n\n\n🙈 However: The above expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) and \\(\\hat\\sigma_g\\) depend themselves on the unknown parameters\n\n\\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\),\n\\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and\n\\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\),\n\nbecause the probabilities \\(0\\leq p_{ig}\\leq 1\\) (defined in Equation 2.4) depend on these unknown parameters.\nThus, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3 do not allow direct estimation of the unknown parameters \\(\\pi_g\\), \\(\\mu_g\\), and \\(\\sigma_g\\).\n\n\n\n\n\n\nPrior and Posterior Probabilities\n\n\n\n\nThe probability \\[\n\\pi_g = \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g\\right)\n\\]\nin Equation 2.4 is called the prior probability. The prior probability \\(\\pi_g\\) is the probability that a penguine \\(i\\), from which we know nothing about its flipper length, belongs to group \\(g\\).\nThe conditional probability \\[\np_{ig} = \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i = x_i\\right)\n\\] in Equation 2.4 is called the posterior probability. The posterior probability \\(p_{ig}\\) is the probability that penguine \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g.\\)\n\nWe’ll discuss the prior and the posterior probability in more detail in Section 2.3.2.\n\n\n🥳 Solution: The EM Algorithm\n\n\n\n2.2.3 The EM Algorithm for GMMs\nThe expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3, however, suggest a simple iterative maximum likelihood estimation procedure: An alternating estimation of\n\nthe (unknown) posterior probabilities \\[\n\\begin{align*}\np_{ig}\n&= \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i = x_i\\right)\\\\[2ex]\n& = \\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\] for \\(i=1,\\dots,n,\\) and \\(g=1,\\dots,G,\\) and of\nthe (unknown) parameters \\[\n(\\pi_g,\\mu_g,\\sigma_g) \\quad\\text{for}\\quad g=1,\\dots,G\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nOnce you know \\(p_{ig},\\) you can compute \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) using Equation 2.3.\nOnce you know \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) you can compute \\(p_{ig}\\) using Equation 2.4.\n\n\nThe EM Algorithm:\n\nInitialization:  Set starting values \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\)\nLoop:  For \\(r=1,2,\\dots\\)\n\n(Expectation) Compute: \\[p_{ig}^{(r-1)}=\\frac{\\pi_g^{(r-1)}\\varphi(x_i;\\mu^{(r-1)}_g,\\sigma_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\]\n(Maximization) Compute:\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence:  Stop if the value of the maximized log-likelihood function, \\(\\ell(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)};\\mathbf{x})\\), does not change anymore substantially.\n\nThe above pseudo code is implemented in the following code chunk:\n\nlibrary(\"MASS\")\nlibrary(\"mclust\")\n\n## data:\nx <- cbind(penguin_flipper) # data [n x d]-dimensional. \nd <- ncol(x)                # dimension (d=1: univariat)\nn <- nrow(x)                # sample size\nG <- 2                      # number of groups\n\n## further stuff \nllk       <- matrix(NA, n, G)\np         <- matrix(NA, n, G)  \nloglikOld <- 1e07\ntol       <- 1e-05\nit        <- 0\ncheck     <- TRUE \n\n## EM Algorithm\n\n## 1. Starting values for pi, mu and sigma:\npi    <- rep(1/G, G)              # naive pi \nsigma <- array(diag(d), c(d,d,G)) # varianz = 1\nmu    <- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )\n\nwhile(check){\n  \n  ## 2.a Expectation step\n  for(g in 1:G){\n    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  p <- sweep(p, 1, STATS = rowSums(p), FUN = \"/\")\n  \n  ## 2.b Maximization step \n  par   <- mclust::covw(x, p, normalize = FALSE)\n  mu    <- par$mean\n  sigma <- par$S\n  pi    <- colMeans(p)\n  \n  ## 3. Check convergence \n  for(g in 1:G) {\n    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  loglik <- sum(log(rowSums(llk))) # current max. log-likelihood value\n  ##\n  diff      <- abs(loglik - loglikOld)/abs(loglik) # rate of change\n  loglikOld <- loglik\n  it        <- it + 1\n  ## Check whether rate of change is still large enough (> tol)?\n  check     <- diff > tol\n}\n\n## Estimation results:\nresults <- matrix(c(pi, mu, sqrt(sigma)), \n                  nrow = 3, \n                  ncol = 2, \n                  byrow = TRUE,\n                  dimnames = list(c(\"weights\", \n                                    \"means\", \n                                    \"standard-deviations\"),\n                                  c(\"group 1\", \n                                    \"group 2\"))) \n##\nresults %>% round(., 2)\n\n                    group 1 group 2\nweights                0.69    0.31\nmeans                216.19  194.24\nstandard-deviations    7.32    6.25"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "href": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x;\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.$\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_g = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\[\np_{ig}=P(Z_{ig}=1|X_i=x_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem (see Equation 2.5) which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).”\n\n\n\n\n\n\n\n\nBayes’ Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere’s the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) is the Expectation-step of the EM algorithm (Section 2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e. the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^nz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation 2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of density functions. This simplifies the maximization problem considerably, since the normal density belongs to the exponential family (see Section 1.4) which is not the case for the normal mixture distribution.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, we can calculate the conditional expected value (using Equation 2.6), which motivates the “Expectation”-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector.\nThe following EM algorithm differs only in notation from the version already discussed in Section 2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\np_{ig}^{(r-1)} = \\frac{\\pi_g^{(r-1)} \\varphi(x_i;\\mu_g^{(r-1)},\\sigma_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\n\\] does not change anymore substantially."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "href": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 (Unsupervised) Classification",
    "text": "2.4 (Unsupervised) Classification\nThe problem of predicting a discrete random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] where \\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and \\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n(Note: Above we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.)\nExample: Predict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\nA classification rule is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\nIf there are learning/training data with group-labels \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nIf there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] From Bayes’s theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where \\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown quantities and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nBut well, the average penguin probably doesn’t care about the EM Algorithm.\n\n\n\nFigure 2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#exercises",
    "href": "Ch2_EMAlgorithmus.html#exercises",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\n\nConsider \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\sim\\text{Bernoulli}(p).\\) Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nNow let \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\) is a Bernoulli mixture random variable with parameters \\(p_g\\) and prior probabilities \\(\\pi_g\\), \\(g=1,\\dots,G\\). Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nLet \\((Z_{i1},\\dots,Z_{iG})\\in\\{0,1\\}^G\\) be the vector of latent group indicator random variables with \\(Z_{i1}+\\dots + Z_{iG}=1\\) and \\(P(Z_{ig}=1)=\\pi_g\\), \\(g=1,\\dots,G\\). Thus, the realization \\(z_i=(0,1,0,\\dots,0)\\) means that the \\(i\\)th observation comes from the \\(2\\)nd Bernoulli distribution \\(\\text{Bernoulli}(p_2)\\). Write the expressions of the (log) likelihood functions \\(\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\) and \\(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\) that take into account the latend group indicator random variables.\nWrite down the expression for the posterior probability \\[\n\\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i).\n\\]\nDerive the expectation of \\(\\tilde\\ell,\\) \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\).\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\) with respect to \\(p_g\\) for \\(g=1,\\dots,G.\\)\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\) with respect to \\(\\pi_g\\) for \\(g=1,\\dots,G\\) such that \\(\\sum_{g=1}^G\\pi_g=1.\\)\nSketch the EM-Algorithm"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#references",
    "href": "Ch2_EMAlgorithmus.html#references",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nTraiberman, Sharon. 2019. “Occupations and Import Competition: Evidence from Denmark.” American Economic Review 109 (12): 4260–4301."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "href": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 Cluster Analysis",
    "text": "2.4 Cluster Analysis\nThe problem of predicting a discrete random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\\overset{\\text{i.i.d.}}{\\sim}(Y,X)\n\\] where\n\n\\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and\n\\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n\n\nNote: Above in Section 2.3, we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.\n\n\n\n\n\n\n\nExample\n\n\n\nPredict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\n\n\nA classification rule \\(h\\) is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\n\n\n\n\n\n\n(Un-)Supervised Classification\n\n\n\n\nSupervised Classification: If there are learning/training data with group-labels \\[\n({\\color{red}Y_1},X_1),\\dots,({\\color{red}Y_n},X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nUnsupervised Classification/Cluster Analysis: If there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n\n\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] That is, the conditonal mean \\(m(x)\\) is the posterior probability, i.e. the probability of \\(Y=1\\) given \\(X=x.\\)\nFrom Bayes’ theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where\n\n\\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\n\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown quantities and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nBut well, the average penguin probably doesn’t care about the EM Algorithm.\n\n\n\nFigure 2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "href": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x;\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.$\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_g = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\[\np_{ig}=P(Z_{ig}=1|X_i=x_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem (see Equation 2.5) which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).”\n\n\n\n\n\n\n\n\nBayes’ Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere’s the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) is the Expectation-step of the EM algorithm (Section 2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e. the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^nz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation 2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of density functions. This simplifies the maximization problem considerably, since the normal density belongs to the exponential family (see Section 1.4) which is not the case for the normal mixture distribution.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, we can calculate the conditional expected value (using Equation 2.6), which motivates the “Expectation”-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector.\nThe following EM algorithm differs only in notation from the version already discussed in Section 2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\np_{ig}^{(r-1)} = \\frac{\\pi_g^{(r-1)} \\varphi(x_i;\\mu_g^{(r-1)},\\sigma_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}^{(r)}=(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)})\\) with\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\n\\] does not change anymore substantially."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#solutions",
    "href": "Ch1_MaximumLikelihood.html#solutions",
    "title": "1  Maximum Likelihood",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\nBelow I use the same data (one H, four T) that was used to produce the results in Table 1.1 of our script. However, you can produce new data by setting another seed-value\n\ntheta_true <- 0.2    # unknown true theta value\nn          <-  5     # sample size\n\nset.seed(1)\n\n# simulate data: n many (unfair) coin tosses\nx <- sample(x       = c(0,1), \n            size    = n, \n            replace = TRUE, \n            prob    = c(1-theta_true, theta_true)) \n\n## number of heads (i.e., the number of \"1\"s in x)\nN_H <- sum(x)\n\n## First derivative of the log-likelihood function\nLp_fct   <- function(theta, N_H = N_H, n = n){\n    (N_H/theta) - (n - N_H)/(1 - theta)    \n}\n## Second derivative of the log-likelihood function\nLpp_fct   <- function(theta, N_H = N_H, n = n){\n    - (N_H/theta^2) - (n - N_H)/(1 - theta)^2    \n}\n\nt     <- 1e-10   # convergence criterion\ncheck <- TRUE    # check object to stop the while-loop\ni     <- 0       # count iterations\n\n## Initializations \ntheta  <- 0.4     # starting value theta_{(0)}\nh_step <- NULL    # empty value \nLp     <- Lp_fct( theta, N_H=N_H, n=n)\nLpp    <- Lpp_fct(theta, N_H=N_H, n=n)\n\nwhile(check){\n    i         <- i + 1\n    ##\n    h_step_new <- -1 * (Lp_fct(theta[i], N_H=N_H, n=n) / Lpp_fct(theta[i], N_H=N_H, n=n))    \n    h_step     <- c(h_step, h_step_new)\n    theta_new  <- theta[i] + h_step_new\n    Lp_new     <- Lp_fct( theta_new, N_H=N_H, n=n)\n    Lpp_new    <- Lpp_fct(theta_new, N_H=N_H, n=n)\n    ##\n    theta      <- c(theta, theta_new) \n    Lp         <- c(Lp,    Lp_new) \n    Lpp        <- c(Lpp,   Lpp_new) \n    ##\n    if( abs(Lp_fct(theta_new, N_H=N_H, n=n)) < t ){\n      check <- FALSE\n    }\n}\n\nresults           <- cbind(1:length(theta)-1, theta, -Lp/Lpp, Lp)\ncolnames(results) <- c(\"m\", \"theta_m\", \"h_m\", \"Lp(theta_m)\")\nresults\n\n     m   theta_m           h_m   Lp(theta_m)\n[1,] 0 0.4000000 -2.400000e-01 -4.166667e+00\n[2,] 1 0.1600000  3.326733e-02  1.488095e+00\n[3,] 2 0.1932673  6.558924e-03  2.159084e-01\n[4,] 3 0.1998263  1.736356e-04  5.433195e-03\n[5,] 4 0.1999999  1.132731e-07  3.539786e-06\n[6,] 5 0.2000000  4.814638e-14  1.504574e-12\n\n\n\n\nSolutions of Exercise 2.\n\n(a) Log-Likelihood Function\nThe log-likelihood function is given by \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\sum_{i=1}^n \\ln (\\theta\\exp(-\\theta X_i))\\\\\n&=\\sum_{i=1}^n (\\ln \\theta -\\theta X_i)\\\\\n&=n \\ln \\theta -\\sum_{i=1}^n \\theta X_i\n\\end{align*}\n\\]\n\n\n(b) ML-Estimator\nThe ML estimator is defined as \\(\\hat{\\theta}_{n}=\\arg\\max_{\\theta}\\ell(\\theta)\\). Deriving the ML estimator \\(\\hat\\theta_n\\): \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=n\\frac{1}{\\theta} - \\sum_{i=1}^n X_i\\\\\n\\ell_n'(\\hat\\theta_n)=0\\quad \\Leftrightarrow &\\quad 0=n\\frac{1}{\\hat\\theta_n} - \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad n\\frac{1}{\\hat\\theta_n} = \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad \\hat\\theta_n = \\frac{1}{\\frac{1}{n}\\sum_{i=1}^n X_i}= \\frac{1}{\\bar{X}_n}\n\\end{align*}\n\\]\n\n\n(b) Fisher Information\nThe Fisher information is given by \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell''(\\theta_0)).\n\\] The second derivative of \\(\\ell_n(\\theta)\\) evaluated at \\(\\theta_0\\) is given by \\[\n\\ell''_n(\\theta)=-n\\frac{1}{\\theta^2_0}.\n\\] Thus, \\[\n\\begin{align*}\n\\mathcal{I}(\\theta_0)\n&=-\\frac{1}{n}\\mathbb{E}(\\ell''_n(\\theta_0))\\\\[2ex]\n&=-\\frac{1}{n}\\left(-n\\frac{1}{\\theta^2_0}\\right)\\\\[2ex]\n&=\\frac{1}{\\theta^2_0}.\n\\end{align*}\n\\] Therefore, the asymptotic distribution of \\(\\hat\\theta_n\\) is \\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\theta_n-\\theta)&\\to_d \\mathcal{N}\\left(0,\\theta^2_0\\right),\n\\end{align*}\n\\] \\(n\\to\\infty.\\)\nSince we do not know the asymptotic variance \\(\\theta_0^2,\\) we need to plug-in a consistent estimator; namely, \\[\n\\hat{\\theta}_n^2 = \\left(\\frac{1}{\\bar{X}_n}\\right)^2 \\approx \\theta^2_0.\n\\] This allows us to use the following Normal approximation to construct statistical hypothesis tests and confidence intervals, etc: \\[\n\\begin{align*}\n\\hat\\theta_n&\\overset{a}{\\sim}\\mathcal{N}\\left(\\theta,\\frac{\\hat{\\theta}_n^2}{n}\\right)\n\\end{align*}\n\\] This approximation is good for largish sample sizes (roughly \\(n\\geq 30\\)).\n\n\n\nSolutions of Exercise 3.\n\n(a) Likelihood Function\nRecall that the density function of \\(\\mathcal{Unif}(0,\\theta)\\) is \\[\nf(x;\\theta_0)\n=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{\\theta} & 0\\leq x\\leq \\theta_0\\\\\n0                & \\text{otherwise}\\\\\n\\end{array}\n\\right.\n\\] Thus, the likelihood function is \\[\n\\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i;\\theta).\n\\] Note: If any \\(\\theta<X_i,\\) we have that \\[\n\\mathcal{L}_n(\\theta)=0.\n\\] Putting it differently, let \\[\nX_{(n)}=\\max\\{X_1,\\dots,X_n\\}\n\\] denote the \\(n\\)th order-statistic, then \\[\n\\mathcal{L}_n(\\theta)=0\\quad\\text{for all}\\quad \\theta<X_{(n)}.\n\\]\nHowever, for all values of \\(\\theta\\) with \\(\\theta \\geq X_{(n)}\\) we have that \\[\nf(X_i;\\theta)=\\frac{1}{\\theta}\\quad\\textbf{for all}\\quad i=1,\\dots,n.\n\\] Thus, for all values of \\(\\theta\\) with \\(\\theta \\geq X_{(n)},\\) \\[\n\\mathcal{L}_n(\\theta)=\\left(\\frac{1}{\\theta}\\right)^n.\n\\]\nSumming up, \\[\n\\mathcal{L}_n(\\theta)\n=\\left\\{\n\\begin{array}{ll}\n\\left(\\frac{1}{\\theta}\\right)^n & \\theta \\geq  X_{(n)}\\\\\n0                               & \\theta < X_{(n)}\\\\\n\\end{array}\n\\right.\n\\tag{1.14}\\]\n\n\n\n(b) Maximum Likelihood Estimator of \\(\\theta_0\\)\n\\(\\mathcal{L}_n(\\theta)\\) is strictly decreasing over the interval \\([X_{(n)},\\infty);\\) see Figure 1.3.\n\nn          <- 20   # sample size\nX_max      <- 0.25\n\ntheta_vec  <- seq(from = 0, \n                  to   = X_max * 1.5, \n                  len  = 100) \nlikelihood_fun <- function(theta, X_max, n){ \n    likelihood                <- 1/(theta^n)\n    likelihood[theta < X_max] <- 0 \n    return(likelihood) \n}\n\nlikelihood_vec <- likelihood_fun(theta = theta_vec,\n                                 X_max = X_max, \n                                 n     = n)\n\nplot(y = likelihood_vec, \n     x = theta_vec, \n     type = \"l\", \n     xlab = expression(theta),\n     ylab = \"Likelihood\", \n     main = \"\")            \naxis(1, at = X_max, labels = expression(X[(n)]))                  \n\n\n\n\nFigure 1.3: Graph of the likelihood function \\(\\mathcal{L}_n(\\theta)\\) given in Equation 1.14.\n\n\n\n\nThus, the maximum likelihood estimator of \\(\\theta_0\\) is \\[\n\\begin{align}\n\\hat{\\theta}_{ML}\n& =\\arg\\max_{\\theta>0}\\mathcal{L}_n(\\theta)\\\\\n& = X_{(n)}.\n\\end{align}\n\\]\n\n\nSolutions of Exercise 4.\n\n(a) Finding the Maximum Likelihood Estimator of \\(\\lambda_0\\)\n\\[\n\\begin{align}\n\\mathcal{L}_n(\\lambda)\n& = \\prod_{i=1}^n f(X_i;\\lambda)\\\\[2ex]\n& = \\prod_{i=1}^n \\frac{\\lambda^{X_i} \\exp(-\\lambda)}{X_i!} \\\\[2ex]\n& = \\frac{\\lambda^{\\sum_{i=1}^n X_i}  \\exp(-n \\lambda)}{\\prod_{i=1}^n (X_i!)} \\\\[4ex]\n\\ell(\\lambda)\n&= \\left(\\sum_{i=1}^n X_i\\right) \\ln(\\lambda) -n\\lambda\\cdot 1 - \\sum_{i=1}^n \\ln(X_i!)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell'_n(\\lambda)\n&= \\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda}  - n\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell''_n(\\lambda)\n&= -\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda^2} < 0  \n\\end{align}\n\\] since by the properties of the Poisson distribution \\(X_1,\\dots,X_n>0\\) and \\(\\lambda>0.\\)\nThus the maximum likelihood estimator of \\(\\lambda_0\\) is given by \\[\n\\begin{align}\n&\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\hat\\lambda_n}  - n \\overset{!}{=} 0\\\\[2ex]\n\\Rightarrow & \\hat \\lambda_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\end{align}\n\\]\n\n\n(b) Finding the Maximum Likelihood Estimator of \\(P(X=4)\\)\n\\[\n\\begin{align}\nP(X=4) = \\frac{\\lambda_0^4 \\exp(-\\lambda_0)}{4!}\n\\end{align}\n\\]\nThus \\(P(X=4)\\) is a function of \\(\\lambda\\) \\[\n\\begin{align}\nP(X=4)\\equiv P(X=4|\\lambda) = \\frac{\\lambda^4 \\exp(-\\lambda)}{4!} = \\tau(\\lambda)\n\\end{align}\n\\]\n\nlambda_vec <- seq(from = .0001, to = 15, len = 100)\ng_vec      <- (lambda_vec^4 * exp(-1*lambda_vec))/( factorial(4) )\n\nplot(x = lambda_vec, y = g_vec, \n     type = \"l\", xlab=expression(lambda), ylab=expression(tau(lambda)))\nabline(v = 4)\naxis(1, at = 4)\n\n\n\n\n(For \\(0<\\lambda\\leq 4,\\) \\(g(\\lambda)\\) is even a one-to-one mapping.)\nBy the invariance property of the maximum likelihood estimator (which also applies to functions \\(\\tau\\) that are not one-to-one) we thus have \\[\n\\begin{align}\n\\hat{P}(X=4)\\equiv \\hat{P}(X=4|\\hat{\\lambda}_n) = \\frac{\\hat{\\lambda}^4_n \\exp(-\\hat{\\lambda}_n)}{4!}\n\\end{align}\n\\] with \\(\\hat{\\lambda}_n=\\frac{1}{n}\\sum_{i=1}^n X_i.\\)\n\n\n\nSolutions of Exercise 5.\nSetup of Section 1.2.2:\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.  \\[\n\\ell_n'(\\theta_{root})=0.\n\\] Let \\[\n\\begin{align*}\ne_{(0)}&=\\theta_{root}-\\theta_{(0)}\\\\[2ex]\ne_{(m)}&=\\theta_{root}-\\theta_{(m)}\n\\end{align*}\n\\] denote the start-value error and the \\(m\\)th step error, respectively.\nLet \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start-value neighborhood around \\(\\theta_{root}.\\)\nLet \\(\\ell_n'\\) be “well behaved” over \\(I;\\) such that\n\n\\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\n\\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I.\\)\n\nLet \\(\\theta_{(0)}\\) be “close enough;” i.e. let\n\n\\(M|e_{(0)}|<1,\\)\n\nwhere \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\nIn the following, we show that the Newton-Raphson algorithm converges under this setup.\n\nBy the second-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with the mean-value form of the remainder term, we have that \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) +\n    \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) +\n    \\frac{1}{2}\\ell'''(\\xi)(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] for some real-valued number \\(\\xi_{(m)}\\) between \\(\\theta_{root}\\) and \\(\\theta_{(m)}.\\)\nSince \\(\\ell'(\\theta_{root})=0,\\) we have that \\[\n\\begin{align*}\n0\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + \\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] Some rearrangments lead \\[\n\\begin{align*}\n\\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)})\n& = -\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n{\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\qquad[\\text{dividing by}\\;\\ell''(\\theta_{(m)})]\\\\[2ex]\n\\end{align*}\n\\] Using the update steps of the alrorithm \\[\n\\theta_{(m+1)} = \\theta_{(m)} - \\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}\n\\quad\\Leftrightarrow\\quad\n\\theta_{(m)} - \\theta_{(m+1)}  = {\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}}\n\\] yields \\[\n\\begin{align*}\n\\theta_{(m)} - \\theta_{(m+1)} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\n\\theta_{root} - \\theta_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\ne_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(e_{(m)})^2\n\\end{align*}\n\\] Taking absolute values, since we are not interested in the sign of the approximation errors, yields \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&= \\frac{1}{2} \\; |\\ell'''(\\xi_{(m)})|\\;\\frac{1}{|\\ell''(\\theta_{(m)})|}(e_{(m)})^2\n\\end{align*}\n\\] Considering the worst case within \\(I,\\) leads to the following inequality \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&\\leq  \\overbrace{\\frac{1}{2} \\; \\left(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|\\right)\\;\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell''(\\theta_{(m)})|}\\right)}^{=M}\\;(e_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad |e_{(m+1)}|  \n&\\leq  M\\;(e_{(m)})^2\\\\[-2ex]\n\\end{align*}\n\\tag{1.15}\\] To show that the Newton-Raphon algorithm converges, we need to show that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\]\nFor \\(0\\leq M\\,|e_{(0)}|<1\\) the inequality in Equation 1.15 becomes a sharp inequality \\[\n\\begin{align*}\n&|e_{(1)}|\\leq \\overbrace{M\\;|e_{(0)}|}^{<1}\\,|e_{(0)}|\n\\quad\\Rightarrow\\quad |e_{(1)}|< \\,|e_{(0)}|\\\\[3ex]\n&{\\color{darkgreen}[\\text{Using that $M\\;|e_{(0)}|<1$ and that $|e_{(1)}|<|e_{(0)}|$}]}\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(2)}|\\leq {\\color{darkgreen}\\overbrace{M\\;|e_{(1)}|}^{<1}}\\,|e_{(1)}|\n\\quad\\Rightarrow\\quad |e_{(2)}|< \\,|e_{(1)}|\\\\[2ex]\n&\\phantom{|e_{(2)}|\\leq \\overbrace{M\\;|e_{(1)}|}^{<1}\\,|e_{(1)}|}\\vdots\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(m+1)}|< \\,|e_{(m)}| \\quad\\text{for all }m=0,1,2\\dots\n\\end{align*}\n\\] which shows the convergence of the Newton Raphson algorithm. (The inequality in Equation 1.15 implies that the convergence is even quadratic; i.e. very fast.)\nSpecial Case \\(M=0\\):  For \\[\n\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\n\\] we have that \\(M=0\\) which implies that we find the root, \\(\\theta_{root},\\) already in the first \\((m=1)\\) update step, since\n\\[\n\\begin{align*}\n|e_{(1)}|  \n&\\leq \\overbrace{\\left(M |e_{(0)}|\\right)}^{=0}\\;|e_{(0)}|\\\\[2ex]\n\\Rightarrow\\quad |e_{(1)}|&=0\\\\[2ex]\n\\Rightarrow\\quad \\theta_{(1)}&=\\theta_{root},\n\\end{align*}\n\\] even when \\(|e_{(0)}|\\gg 0.\\)\nThis makes sense, since \\(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\\) implies that \\(\\ell'(\\theta)\\) has no curvature; i.e. \\(\\ell'(\\theta)\\) is a straight line for all \\(x\\in I\\) which implies that the Taylor approximation used in the update steps of the Newton-Raphson algorithm is just perfect (no approximation error)."
  },
  {
    "objectID": "Ch3_Bootstrap.html",
    "href": "Ch3_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-Illustration",
    "href": "Ch3_Bootstrap.html#sec-Illustration",
    "title": "3  The Bootstrap",
    "section": "3.1 Illustration: When are you happy about the Bootstrap?",
    "text": "3.1 Illustration: When are you happy about the Bootstrap?\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y.\\) These returns \\(X\\) and \\(Y\\) are random with\n\n\\(Var(X)=\\sigma^2_X\\)\n\\(Var(Y)=\\sigma^2_Y\\)\n\\(Cov(X,Y)=\\sigma_{XY}\\)\n\nWe want to invest a fraction \\(\\alpha\\in(0,1)\\) in \\(X\\) and invest the remaining \\(1-\\alpha\\) in \\(Y.\\)\nOur aim is to minimize the variance (risk) of our investment, i.e., we want to minimize \\[\nVar\\left(\\alpha X + (1-\\alpha)Y\\right).\n\\] One can show that the value \\(\\alpha\\) that minimizes this variance is \\[\n\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n\\tag{3.1}\\] Using a data set that contains past measurements \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] for \\(X\\) and \\(Y,\\) we can estimate the unknown \\(\\alpha\\) by plugging in estimates of the variances and covariances \\[\n\\hat\\alpha = \\frac{\\hat\\sigma^2_Y - \\hat\\sigma_{XY}}{\\hat\\sigma^2_X + \\hat\\sigma^2_Y - 2\\hat\\sigma_{XY}}\n\\tag{3.2}\\] with \\[\n\\begin{align*}\n\\hat{\\sigma}^2_X&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2\\\\\n\\hat{\\sigma}^2_Y&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2\\\\\n\\hat{\\sigma}_{XY}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right),\n\\end{align*}\n\\] where \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\) and \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i.\\)\nIt is natural to wish to quantify the accuracy of our estimator \\[\n\\hat\\alpha\\approx \\alpha.\n\\]\nFor instance, to construct a confidence interval we need to know the standard error of the estimator \\(\\hat\\alpha\\), \\[\n\\sqrt{Var(\\hat\\alpha)} = \\operatorname{SE}(\\hat\\alpha)=?\n\\] However, computing \\(\\operatorname{SE}(\\hat\\alpha)\\) is here difficult due to the definition of \\(\\hat\\alpha\\) in Equation 3.2 which contains variance estimates also in the denominator.\n\n\n\n\n\n\nConclusion: Why Bootstrap?\n\n\n\nIn cases as described above, we are happy to use the Basic Bootstrap Method (Section 3.4) which allows us to approximate \\(\\operatorname{SE}(\\hat\\alpha)\\) simply by resampling from the data; i.e. without having to derive an explicite formula for \\(\\operatorname{SE}(\\hat\\alpha).\\) The Basic Bootstrap Method is found to be as accurate as the standard asymptotic Normality results.\nIf we have a consistent estimator for the \\(\\operatorname{SE}(\\hat\\alpha),\\) then we can make use of this estimator by applying the Bootstrap-\\(t\\) Method (Section 3.5). The Bootstrap-\\(t\\) Method is found to be more accurate than the standard asymptotic Normality results."
  },
  {
    "objectID": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "href": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.2 Recap: The Empirical Distribution Function",
    "text": "3.2 Recap: The Empirical Distribution Function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its (cumulative) distribution function\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.1 ((Cumulative) Distribution Function (CDF)) \\[\nF(x)=P(X \\leq x)\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\]\n\n\n\nThe sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\n\\] denote a real-valued random sample with \\(X\\sim F,\\) and let \\(1_{(\\cdot)}\\) denote the indicator function, i.e., \\[\n\\begin{align*}\n1_{(\\text{TRUE})} &=1\\quad\\text{and}\\quad 1_{(\\text{FALSE})}=0.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.2 (Empirical (Cumulative) Distribution Function (ECDF)) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\] I.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\n\n\nProperties of the ECDF:\n\n\\(0\\le F_n(x)\\le 1\\)\n\\(F_n(x)=0,\\) if \\(x<X_{(1)}\\), where \\(X_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\\) denotes the order-statistic.\n\\(F_n(x)=1,\\) if \\(x\\ge X_{(n)}\\), where \\(X_{(n)}\\) is largest observation\n\\(F_n\\) is a monotonically increasing step function\n\n\n\n\n\n\n\nThe empirical distribution function \\(F_n\\) is itself is a distribution function\n\n\n\nThe empirical distribution funciton \\(F_n\\) is the distribution function (Definition 3.1) of the discrete random variable \\(X^*,\\) where\n\n\\(X^*\\in\\{X_1,\\dots,X_n\\}\\)\n\nand\n\n\\(P(X^*=X_i)=\\frac{1}{n}\\) for each \\(i=1,\\dots,n.\\)\n\nThus\n\\[\n\\begin{align*}\nF_n(x)\n&=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\[2ex]\n&= P\\left(X^*\\leq x\\right)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3.1 (Computing the empirical distribution function in R) \nSome data:\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.30, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\nThe R function ecdf() returns a function that gives the values of \\(F_n(x):\\)\n\n## Note: ecdf() returns a function!\nmyecdf_fun(5.0)\n\n[1] 0.25\n\n\n\n\n\n\nStatistical Properties of \\(F_n\\)\n\\(F_n(x)\\) depends on the i.i.d. random sample \\(X_1,\\dots,X_n\\) and thus is itself a random function.\nWe obtain \\[\nnF_n(x)\\sim B(n, p=F(x))\\quad x\\in\\mathbb{R}\n\\]\nI.e., \\(nF_n(x)\\) has a binomial distribution with parameters \\(n\\) (“number of trials”) and \\(p=F(x)\\) (“probability of success on a single trial”).\nThus, \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x))& = np = nF(x)\\\\[2ex]\n\\Rightarrow \\quad \\mathbb{E}(F_n(x))& = p = F(x)\\\\[2ex]\n\\Rightarrow \\quad \\operatorname{Bias}(F_n(x))& = \\mathbb{E}(F_n(x)) - F(x) =0\\\\\n\\end{align*}\n\\] and \\[\n\\begin{align*}\nVar(nF_n(x))& = np(1-p) = nF(x)(1-F(x))\\\\[2ex]\n\\Rightarrow \\quad Var(F_n(x))& = \\frac{nF(x)(1-F(x))}{n^2}=\\frac{F(x)(1-F(x))}{n}\n\\end{align*}\n\\] such that \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n& = (\\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\\\[2ex]\n& =\\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\]\nThis allows us to conclude that \\[\n\\begin{align*}\nF_n(x) & \\to_{m.s.} F(x)\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow \\quad F_n(x) & \\to_{p} F(x)\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, \\(F_n(x)\\) is point-wise for each \\(x\\in\\mathbb{R}\\) a weakly consistent estimator of \\(F(x).\\)\nThe Clivenko-Cantelli Theorem 3.1 states that \\(F_n\\) is even uniformly over \\(\\mathbb{R}\\) a consistent estimator of \\(F.\\)\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\\) denote a real-valued random sample with \\(X\\sim F.\\) Then \\[\n\\begin{align*}\n&\\quad P\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\\\\[2ex]\n\\Leftrightarrow &\\quad\n\\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|\\to_{a.s.} 0.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "href": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 Basic Idea of the Bootstrap",
    "text": "3.3 Basic Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\nSampling from the population distribution \\(F\\) (infeasible Monte Carlo simulation) The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\] Let \\(\\theta_0\\) denote a distribution parameter of \\(F\\) which we want to estimate, and let \\(\\hat\\theta_n\\) denote an estimator of \\(\\theta_0.\\) If we would know \\(F,\\) we could generate arbitrarily many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1}, \\hat{\\theta}_{n,2}, \\dots, \\hat{\\theta}_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these realizations. Unfortunately, we don’t know \\(F,\\) thus Monte Carlo inference is infeasible.\nThe idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:  Instead of random sampling from \\(F,\\) which is infeasible, the bootstrap uses random sampling from the known empirical distribution function \\(F_n\\) to generate arbitrarily many bootstrap realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}^*_{n,1}, \\hat{\\theta}^*_{n,2}, \\dots, \\hat{\\theta}^*_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these bootstrap realizations. This is justified asymptotically since for large \\(n,\\) the empirical distribution \\(F_n\\) is “close” to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "href": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "title": "3  The Bootstrap",
    "section": "3.4 The Basic Bootstrap Method",
    "text": "3.4 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption. The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference: In order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e. in learning the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could do asymptotic statistics. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the Normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section 3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=5000\\) or \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem 3.1) the bootstrap estimators \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x).\\) That is, we can (and will do so) treat the bootstrap distribution \\(H^{Boot}_{n}(x)\\) as known.\nThe crucial question is, however, whether the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is able to approximate the unknown distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\n\\] as \\(n\\to\\infty.\\) This is an important requirement called bootstrap consistency.\n\nBootstrap Consistency\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\\) is a good approximation of the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\), i.e. \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] The following definition states this more precisely.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. However, there are some requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nTypically, the distribution of the estimator \\(\\hat\\theta_n-\\theta_0\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) does not properly reflect the way how \\(X_1,\\dots,X_n\\) are generated in a first place. (For instance, when \\(X_1,\\dots,X_n\\) is generated by a time-series process with auto-correlated data.)\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.4.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\sim F\\)\nNon-zero, finite variance \\(0<Var(X)=\\sigma_0^2<\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\n\\] as \\(n\\to\\infty\\)?\n\n\n\n\n\n\nRecall: Inference using Classic Asymptotic Statistics\n\n\n\nThis example is so simple that we know (by the Lindeberg-Lévy CLT) that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., that \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\[\n\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution, i.e. \\[\n\\Phi_{\\sigma_0}(x)\n=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/\\sigma_0}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz.\n\\]\n\n\nYes, the asymptotic result is simple here (boring), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] able to approximate \\(\\Phi_{\\sigma_0}(x)\\) for all \\(x\\in\\mathbb{R}\\)?\n\n3.4.1.1 Practice: Empirical Consideration of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\nLet us consider the following observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) shown in Table 3.1. The data was generated by drawing from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) That is, \\(Var(X)=\\sigma_0^2=2\\cdot \\operatorname{df}=4.\\)\n\n\n\n\n\nTable 3.1: Observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n7\n7.77\n\n\n8\n1.93\n\n\n\n\n\nobservedSample <- c(0.36, 3.39, 3.24, 4.90, \n                    1.76, 5.33, 7.77, 1.93)\n\nSo the observed sample mean is\n\n\\(\\bar X_{n,obs} =\\) mean(observedSample) \\(=\\) 3.585\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical “population” in order to generate the i.i.d. bootstrap sample \\[\nX_1^*,\\dots,X_n^*\n\\]\nThese i.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\(\\bar{X}^*_n\\) as demonstrated in the following R code:\n\n## generating one realization of the bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n## computing the corresponding realization of the bootstrap estimator\nmean(bootSample)\n\n[1] 4.21375\n\n\nWe can now approximate the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] using the empirical distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\n\\] based on the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\n\\] generated using the bootstrap algorithm\n\nGenerate bootstrap sample\nCompute bootstrap estimator\nRepeat Steps 1 and 2 \\(m\\) times\n\nwith a (very) large \\(m.\\) The following R code demonstrates this:\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\n\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)     \nlegend(\"topleft\", \n       legend = c(\"Bootstrap Distribution\", \n                  \"Normal Limit Distribution with\\nMean = 0 and Variance = 4\"), \n      col = c(\"black\", \"red\"), lty = c(1,1))\n\n\n\n\nNote: To plot the Normal limit distribution we need to make use of our knowledge that \\(X_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^2_{(\\operatorname{df}=2)}\\) which implies that we know (the usually unknpown) asymptotic variance of the estimator \\(\\bar{X}_n,\\) \\[\nnVar(\\bar{X}_n)=Var(\\sqrt{n}(\\bar{X}_n-\\mu_0))=\\sigma_0^2=2\\cdot\\operatorname{df}=4,\n\\] for each \\(n=1,2,\\dots,\\) thus also \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=4.\\)\nUsually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.)\nBy contrast, we get the complete bootstrap distribution directly from the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\n\\] That is, to estimate the usually unknown value of the asymptotic variance \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=\\sigma_0^2=4,\\) we can simply use the empirical variance of the bootstrap estimators \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\) multiplied by \\(n,\\) as done in the following R-code:\n\nround(n * var(Xbar_boot), 2)\n\n[1] 4.82\n\n\n\n\n\n3.4.1.2 Theory (Part 1): Mean and Variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;Var^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e. on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know 🤟 the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable: \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the discrete conditional random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_0.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_{0}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq \\delta)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq x)\n&= P(1_{(X_i^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq x)}=1) \\cdot 1 + P(1_{(X_i^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X_i\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X_i\\leq x)}=1) \\cdot 1 + P(1_{(X_i\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X_i\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nNow we can consider the mean and the variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0.\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\quad[\\text{cond.~on $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant}]\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_0,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_0\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThus, we know now that for large \\(n\\) (\\(n\\to\\infty\\)) the mean and the variance of the bootstrap distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] matches the mean (zero) and the variance (\\(\\sigma_0^2\\)) of the limit distribution \\(\\Phi_{\\sigma_0}.\\)\nBootstrap consistency, however, addresses the total distribution—not only the first two moments.\n\n\n\n\n3.4.1.3 Theory (Part 2): Bootstrap Consistency\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function …\n\n… uniquely determines its associated probability distribution.\n… can be used to easily derive (all) the moments of a random variable.\n… is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\psi_{\\Phi_{\\sigma_0}}(t)=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\tag{3.3}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\) are i.i.d., is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\left(\\psi_{W_1}(t)\\right)^n.\n\\tag{3.4}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.5}\\] where \\(|\\lambda(t)|\\leq |t^2|\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following can be found in Example 3.1 in Shao and Tu (1996)\n\nIt follows from the Lindeberg-Lévy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all \\(x\\in\\mathbb{R}.\\) This result can be proven by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) To see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\frac{X_i-\\mu_0}{\\sqrt{n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where\n\n\\(W_{1,n},\\dots,W_{n,n}\\) are i.i.d. with\n\\(\\mathbb{E}(W_{i,n})=0\\) and\n\\(Var(W_{i,n})=\\frac{1}{n}\\sigma_0^2.\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,\\left|W_{1,n}\\right|^3, \\left|W_{1,n}\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_1-\\mu_0\\right|^3, n^{-1}\\left|X_1-\\mu_0\\right|^2\\big)\\right).\n\\end{align*}\n\\] That is, \\[\nn|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] which means that \\(|\\lambda_n(t)|\\to 0\\) faster than \\(n^{-1}.\\)\nThus, by Equation 3.3 \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t)\n\\end{align*}\n\\]\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) (I.e. we have shown the Lindeberg-Lévy CLT.)\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above prove, by showing that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}^*_n- \\bar{X}_n\\right)|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\n\\(W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\) is i.i.d. with\n\\(\\mathbb{E}^*(W^*_{n})=\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)=0\\) and\n\\(Var^*(W^*_{n})=Var(W^*_{n}|\\mathcal{S}_n)=\\frac{1}{n}\\hat{\\sigma}_0^2=\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right)\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_0^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i^* - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\nn{\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., \\({\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\) faster than \\(n^{-1}.\\) Moreover, since \\[\n{\\color{darkgreen}\\hat\\sigma_0^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\n\\] we have that (using Equation 3.3) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nH_n^{Boot}(x)=P\\left(\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x|\\mathcal{S}_n)\\right) \\approx\n\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar X^*_{n,j}-\\bar X_n\\right)\\leq x\\right)}=H_{n,m}^{Boot}(x),  \n\\] as done in Section 3.4.1.1.\n\n\n\n\n3.4.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\n\\(\\theta_0\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\) as \\(n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\) as \\(n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution. This confidence interval is approximate, since it is only asymptotically justified; i.e. it is not exact in finite samples.\n\n\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v_n\\) of \\(v_0\\) (see ?sec-Illustraction). Statistical inference is then usually based on the bootstrap confidence intervals.\nIn many situations it can be shown that bootstrap confidence intervals (or tests) are even more precise than asymptotic normality based confidence intervals. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0\\):\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles \\[\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\quad\\text{and}\\quad \\hat q^*_{n,1-\\frac{\\alpha}{2}}\n\\] of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.6}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g. \\(\\lfloor 4.9\\rfloor = 4\\)).\nThe approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.7}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we’ll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem 3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation 3.7) for \\(\\theta_0\\): \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-\n(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation 3.7 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically valid (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(s^2_n=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/s_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{s_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}},\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right]\n\\]\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\mu_0\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section 3.4.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval according to Equation 3.7: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "href": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.5 The Bootstrap-\\(t\\) Method",
    "text": "3.5 The Bootstrap-\\(t\\) Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption.\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the (nonparametric) bootstrap-\\(t\\) method (one also speaks of the “studentized bootstrap”). The construction relies on so-called (asymptotically) pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\] which implies that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\bar{X}_n\\) is a Pivotal Statistic\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0<Var(X)=\\sigma_0^2<\\infty\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic.\n\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*=\\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_{x\\in\\mathbb{R}} \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat v_n^*}\\le x \\;\\right|\\;{\\cal S}_n\\right)-\\Phi(x)\\right|\\rightarrow_p 0,\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.5.1 The Bootstrap-\\(t\\) Confidence Interval\nSetup:\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2\\) of \\(\\hat{\\theta}_n,\\) i.e.  \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\hat v^2_n\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\]\nand that \\[\n\\hat v_n\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\nAlgorithm of the Bootstrap-\\(t\\) Confidence Interval for \\(\\theta_0\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g. \\(m=100,000\\)) many bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\]\nUse the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) empirical quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) (see Equation 3.6).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.8}\\] where \\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) based on the original sample \\(X_1,\\dots,X_n.\\)\n\nJustifying the Bootstrap-\\(t\\) CI (Equation 3.8) for \\(\\theta_0\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem 3.1).\nThus, the empirical bootstrap quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat \\tau_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nMoreover, due to the assumed consistencies of the bootstrap and of the estimator \\(\\hat v_n,\\) we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(- \\hat{\\theta}_n + \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval (Equation 3.8) \\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance of \\(\\bar{X}_n\\) is \\(s^2\\approx \\lim_{n\\to\\infty}n Var(\\bar{X}_n)=\\sigma_0^2\\), where \\(s^2\\) denotes the sample variance \\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s_n^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*_n)^2}\\) to generate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\] using Equation 3.6.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation 3.8): \\[\n\\left[\\bar X_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where \\(s_n\\) is computed from the original sample, i.e., \\[\ns_n=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}.\n\\]\n\n\n\n\n3.5.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{v^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(v^*_n\\) depends on the bootstrap sample — not the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. basic bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Bootstrapping Pairs",
    "text": "3.6 Regression Analysis: Bootstrapping Pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Random and fixed design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables and \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\) with either\n\nhomoscedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2<\\infty\\) or\nheteroscedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean \\(\\mathbb{E}(\\varepsilon_i)=0\\) and homoscedastic errors \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2\\) for all \\(i=1,\\dots,n.\\)\n\n\n\nThe least squares estimator \\(\\hat\\beta\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\\\\\n&=\\beta+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.6.1 Bootstrapping Pairs: Bootstrap under Random Design\nUnder the random design, we additionally assume that there exists a positive definite (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T)\n\\] and a positive semi-definite matrix \\(Q\\) such that \\[\nQ=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)=\\mathbb{E}(\\sigma^2(X_i)X_iX_i^T)\n\\]\n\n\n\n\n\n\nTip\n\n\n\nFor homoscedastic errors we have \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2(X_i)X_iX_i^T)\\\\\n&=\\sigma^2\\mathbb{E}(X_iX_i^T)\\, =\\sigma^2 M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see your econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\rightarrow_d\\mathcal{N}(0,M^{-1}QM^{-1}),\\quad n\\to\\infty.\n\\]\nBootstrapping regression estimates \\(\\hat\\beta\\) is straightforward under a random design (Definition 3.6).\nUnder a random design, \\((Y_i,X_i)\\) are i.i.d. and one may apply the basic bootstrap in order to approximate the distribution of the estimation errors \\[\n\\hat\\beta-\\beta.\n\\] In the literature this procedure is usually called bootstrapping pairs, namely, \\((Y_i, X_i)\\)-pairs.\nAlgorithm:\n\nOriginal data: i.i.d. sample \\({\\cal S}_n:=\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\}\\)\nGenerate bootstrap samples \\[\n(Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nEach bootstrap sample \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\\) leads to a bootstrap realization of the least squares estimator \\[\n\\hat\\beta^*=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\\approx\\mathcal{N}(0,M^{-1}QM^{-1})\n\\]\n\nConfidence Intervals\nThis allows to construct basic bootstrap confidence intervals for the \\(j\\)th regression coefficient \\(\\beta_j\\), \\(j=1,\\dots,p\\):\n\nGenerate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j1}^*,\\dots,\\hat\\beta_{jm}^*\n\\]\nDetermine the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\)\nfrom the bootstrap realizations \\(\\hat{\\beta}_{j1}^*,\\dots,\\hat\\beta_{jm}^*\\) using Equation 3.6.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval as in Equation 3.7: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_j-\\hat t_{\\frac{\\alpha}{2},j}\\right]\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThis bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroskedastic. This is not true for the basic confidence intervals intervals usually provided by standard software packages."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.7 Regression Analysis: Residual Bootstrap",
    "text": "3.7 Regression Analysis: Residual Bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoscedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta+ \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under fixed design (Definition 3.6), i.e., where \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. with zero mean \\(\\mathbb{E}(\\varepsilon_i)=0\\) and homoscedastic errors \\[\n\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated \\(X\\)-variables (time-series). In these cases all arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\). The above assumptions on the error terms then of course have to be satisfied conditionally on \\(X_1,\\dots,X_n\\).\n\n\nThe idea of the residual bootstrap is very simple: The model implies that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i:=Y_i-X_i^T\\hat\\beta, \\quad i=1,\\dots,n,\n\\] where again \\[\n\\hat\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator.\nIt is well known that \\[\n\\hat\\sigma^2:= \\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides an unbiased, consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\mathbb{E}(\\hat\\sigma^2)=\\sigma^2 \\quad \\text{and}\\quad \\hat\\sigma^2\\rightarrow_p \\sigma^2.\n\\]\n\nThe Residual Bootstrap Algorithm\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta+\\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g. \\(m=100,000\\)) bootstrap estimators \\[\n\\hat\\beta^*_1,\\hat\\beta^*_2,\\dots,\\hat\\beta^*_m\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*-\\hat\\beta|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\n\nMotivating the Residual Bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoscedastic (!) errors. We have \\[\n\\hat\\beta-\\beta=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) the distribution of \\(\\sqrt{n}(\\hat\\beta-\\beta)\\) is approximately normal with mean 0 and covariance matrix \\(\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\) \\[\n\\sqrt{n}(\\hat\\beta-\\beta)\\to_d\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)\n\\]\nOn the other hand (the bootstrap world), we have construction \\[\n\\hat\\beta^*-\\hat\\beta\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*\n\\] Conditional on \\({\\cal S}_n,\\) the bootstrap error terms are i.i.d with \\[\n\\mathbb{E}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2.\n\\] An appropriate central limit theorem argument implies that \\[\n\\left.\\sqrt{n}(\\hat\\beta^*-\\hat\\beta)\\right|\\mathcal{S}_n\\to_d\\mathcal{N}\\left(0,\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right),\n\\] for \\(n\\to\\infty.\\)\nSince\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2\\rightarrow_p \\sigma^2\n\\] as \\(n\\rightarrow\\infty\\), the bootstrap is consistent. That is, for large \\(n\\), we have approximately \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*-\\hat\\beta) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta-\\beta))}_{\\mathcal{N}\\left(0,\\sigma^2 \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right)}\n\\]\n\n\n3.7.1 Bootstrap confidence intervals for the regression coefficients\n\nNonparametric bootstrap confidence intervals\nBasic nonparametric bootstrap confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat t_{\\frac{\\alpha}{2},j}\\) and \\(\\hat t_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\(\\hat\\beta_j^*\\) using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles (see Equation 3.6) based on the \\(m\\) bootstrap estimates \\[\n\\hat\\beta_{j1}^*,\\hat\\beta_{j2}^*, \\dots, \\hat\\beta_{jm}^*.\n\\] \nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) nonparametric bootstrap confidence interval as in Equation 3.7: \\[\n\\left[2\\hat\\beta_j-\\hat t_{1-\\frac{\\alpha}{2},j},\n      2\\hat\\beta_j-\\hat t_{ \\frac{\\alpha}{2},j }\\right]\n\\]\n\n\nBootstrap-\\(t\\) confidence intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), i.e., \\[\n\\gamma_{jj}:=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat{\\sigma}=\\sqrt{\\frac{1}{n-p}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\n\\] is an asymptotically pivotal statistic, \\[\n\\frac{\\sqrt{n}(\\hat\\beta_j-\\beta_j)}{\\hat\\sigma\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_j\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\nApproximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau_{\\frac{\\alpha}{2},j}\\) and \\(\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\) of the bootstrap distribution of \\[\nT^*=\\frac{\\hat\\beta_j^*-\\hat\\beta_j}{\\hat\\sigma^* \\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat\\sigma^{*2}:=\\frac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2},\n\\] using the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles (see Equation 3.6) based on the \\(m\\) bootstrap realizations \\[\nT^*_1,T_2^*,\\dots, T_m^*.\n\\]\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.8: \\[\n\\left[\n  \\hat\\beta_j-\\hat \\tau_{1-\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}},\\;\n  \\hat\\beta_j-\\hat \\tau_{\\frac{\\alpha}{2},j}\\hat\\sigma \\sqrt{\\gamma_{jj}}\n\\right],\n\\] where \\(\\hat{\\sigma}=\\sqrt{\\frac{1}{n-p}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nThere are many more bootstrap procedures. In case of heteroscedastic errors, for instance, there’s also the “Wild Bootstrap.”\nFor high-dimensional problems (\\(p\\) as large as \\(n\\) or larger), one can use (under certain regularity assumptions) the “Multiplier Bootstrap”."
  },
  {
    "objectID": "Ch3_Bootstrap.html#exercises",
    "href": "Ch3_Bootstrap.html#exercises",
    "title": "3  The Bootstrap",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider the empirical distribution function \\[\nF_n(x) = \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] for a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} F.\n\\]\n\nDerive the exact distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nDerive the asymptotic distribution of \\(F_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nShow that \\(F_n(x)\\) is a point-wise (weakly) consistent estimator of \\(F(x)\\) for each given \\(x\\in\\mathbb{R}\\).\n\n\n\nExercise 2.\n\n\n\n\n\n\nTip\n\n\n\nExercise 1 shows that the empirical distribution function is a point-wise consistent estimator for each given \\(x\\in\\mathbb{R}.\\) However, point-wise consistency generally does not imply uniformly consistency for all \\(x\\in\\mathbb{R},\\) and therefore the Clivenko-Cantelli (Theorem 3.1) is so famous.\nThis exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.\n\n\nPoint-wise convergence of a function \\(g_n(x),\\) i.e., \\[\n|g_n(x) - g(x)|\\to 0\n\\] for each \\(x\\in\\mathcal{X}\\subset\\mathbb{R}\\) as \\(n\\to\\infty\\) generally does not imply uniform convergence, i.e., \\[\n\\sup_{x\\in\\mathcal{X}}|g_n(x) - g(x)|\\to 0\n\\] as \\(n\\to\\infty.\\)\nShow this by providing an example for \\(g_n\\) which converges point-wise, but not uniformly for \\(x\\in\\mathcal{X}\\).\n\n\n\nExercise 3.\nConsider the following setup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(Var(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\n\nDerive the classic confidence interval for \\(\\mu\\) using the asymptotic normality of the estimator \\(\\bar{X}.\\) Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of \\(n=20\\) and,\n\n\nPart 1: For \\(F\\) being the normal distribution with \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\), and\nPart 2: For \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom.\n\n\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-\\(t\\) confidence interval.\n\n\n\nExercise 4.\n\nLet \\(\\mathcal{S}_n = \\{Y_1 , \\dots, Y_n\\}\\) be a random sample from a population with mean \\(\\mu\\), variance \\(\\sigma^2,\\) and distribution function \\(F.\\) Let \\(F_n\\) be the empirical distribution function. Let \\(\\bar{Y}\\) be the sample mean for \\(\\mathcal{S}_n.\\) Let \\(\\mathcal{S}^*_n = \\{Y_1^∗,\\dots, Y_n^∗\\}\\) be a random sample taken independently and with replacement from \\(\\mathcal{S}_n.\\) Let \\(\\bar{Y}^*\\) be the sample mean for \\(\\mathcal{S}^*_n.\\)\n\nShow that \\[\n\\mathbb{E}^*(\\bar{Y}^*) = \\bar{Y}\n\\]\nShow that \\[\n\\mathbb{E}(\\bar{Y}^*) = \\mu\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#references",
    "href": "Ch3_Bootstrap.html#references",
    "title": "3  The Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. 3rd ed. Wiley.\n\n\nDavison, Anthony Christopher, and David Victor Hinkley. 2013. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\n\n\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Science.\n\n\nHorowitz, Joel L. 2001. “The Bootstrap.” In Handbook of Econometrics, 5:3159–3228.\n\n\nShao, Jun, and Dongsheng Tu. 1996. The Jackknife and Bootstrap. Springer Science."
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "href": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "title": "3  The Bootstrap",
    "section": "3.4 The Basic Bootstrap Method",
    "text": "3.4 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption. The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference: In order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e. in learning the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could do asymptotic statistics. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the Normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section 3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=5000\\) or \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem 3.1) the bootstrap estimators \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x).\\) That is, we can (and will do so) treat the bootstrap distribution \\(H^{Boot}_{n}(x)\\) as known.\nThe crucial question is, however, whether the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is able to approximate the unknown distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\n\\] as \\(n\\to\\infty.\\) This is an important requirement called bootstrap consistency.\n\nBootstrap Consistency\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\\) is a good approximation of the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\), i.e. \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] The following definition states this more precisely.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. However, there are some requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nTypically, the distribution of the estimator \\(\\hat\\theta_n-\\theta_0\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) does not properly reflect the way how \\(X_1,\\dots,X_n\\) are generated in a first place. (For instance, when \\(X_1,\\dots,X_n\\) is generated by a time-series process with auto-correlated data.)\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.4.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\sim F\\)\nNon-zero, finite variance \\(0<Var(X)=\\sigma_0^2<\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\n\\] as \\(n\\to\\infty\\)?\n\n\n\n\n\n\nRecall: Inference using Classic Asymptotic Statistics\n\n\n\nThis example is so simple that we know (by the Lindeberg-Lévy CLT) that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., that \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\[\n\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution, i.e. \\[\n\\Phi_{\\sigma_0}(x)\n=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/\\sigma_0}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz.\n\\]\n\n\nYes, the asymptotic result is simple here (boring), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] able to approximate \\(\\Phi_{\\sigma_0}(x)\\) for all \\(x\\in\\mathbb{R}\\)?\n\n3.4.1.1 Practice: Empirical Consideration of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\nLet us consider the following observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) shown in Table 3.1. The data was generated by drawing from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) That is, \\(Var(X)=\\sigma_0^2=2\\cdot \\operatorname{df}=4.\\)\n\n\n\n\n\nTable 3.1: Observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n7\n7.77\n\n\n8\n1.93\n\n\n\n\n\nobservedSample <- c(0.36, 3.39, 3.24, 4.90, \n                    1.76, 5.33, 7.77, 1.93)\n\nSo the observed sample mean is\n\n\\(\\bar X_{n,obs} =\\) mean(observedSample) \\(=\\) 3.585\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical “population” in order to generate the i.i.d. bootstrap sample \\[\nX_1^*,\\dots,X_n^*\n\\]\nThese i.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\(\\bar{X}^*_n\\) as demonstrated in the following R code:\n\n## generating one realization of the bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n## computing the corresponding realization of the bootstrap estimator\nmean(bootSample)\n\n[1] 4.21375\n\n\nWe can now approximate the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] using the empirical distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\n\\] based on the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\n\\] generated using the bootstrap algorithm\n\nGenerate bootstrap sample\nCompute bootstrap estimator\nRepeat Steps 1 and 2 \\(m\\) times\n\nwith a (very) large \\(m.\\) The following R code demonstrates this:\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\n\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)     \nlegend(\"topleft\", \n       legend = c(\"Bootstrap Distribution\", \n                  \"Normal Limit Distribution with\\nMean = 0 and Variance = 4\"), \n      col = c(\"black\", \"red\"), lty = c(1,1))\n\n\n\n\nNote: To plot the Normal limit distribution we need to make use of our knowledge that \\(X_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^2_{(\\operatorname{df}=2)}\\) which implies that we know (the usually unknpown) asymptotic variance of the estimator \\(\\bar{X}_n,\\) \\[\nnVar(\\bar{X}_n)=Var(\\sqrt{n}(\\bar{X}_n-\\mu_0))=\\sigma_0^2=2\\cdot\\operatorname{df}=4,\n\\] for each \\(n=1,2,\\dots,\\) thus also \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=4.\\)\nUsually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.)\nBy contrast, we get the complete bootstrap distribution directly from the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\n\\] That is, to estimate the usually unknown value of the asymptotic variance \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=\\sigma_0^2=4,\\) we can simply use the empirical variance of the bootstrap estimators \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\) multiplied by \\(n,\\) as done in the following R-code:\n\nround(n * var(Xbar_boot), 2)\n\n[1] 4.82\n\n\n\n\n\n3.4.1.2 Theory (Part 1): Mean and Variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;Var^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e. on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know 🤟 the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable: \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the discrete conditional random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_0.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_{0}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq \\delta)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq x)\n&= P(1_{(X_i^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq x)}=1) \\cdot 1 + P(1_{(X_i^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X_i\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X_i\\leq x)}=1) \\cdot 1 + P(1_{(X_i\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X_i\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nNow we can consider the mean and the variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0.\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\quad[\\text{cond.~on $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant}]\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_0,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_0\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThus, we know now that for large \\(n\\) (\\(n\\to\\infty\\)) the mean and the variance of the bootstrap distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] matches the mean (zero) and the variance (\\(\\sigma_0^2\\)) of the limit distribution \\(\\Phi_{\\sigma_0}.\\)\nBootstrap consistency, however, addresses the total distribution—not only the first two moments.\n\n\n\n\n3.4.1.3 Theory (Part 2): Bootstrap Consistency\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function …\n\n… uniquely determines its associated probability distribution.\n… can be used to easily derive (all) the moments of a random variable.\n… is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\psi_{\\Phi_{\\sigma_0}}(t)=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\tag{3.3}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\) are i.i.d., is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\left(\\psi_{W_1}(t)\\right)^n.\n\\tag{3.4}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.5}\\] where \\(|\\lambda(t)|\\leq |t^2|\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following can be found in Example 3.1 in Shao and Tu (1996)\n\nIt follows from the Lindeberg-Lévy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all \\(x\\in\\mathbb{R}.\\) This result can be proven by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) To see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\frac{X_i-\\mu_0}{\\sqrt{n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where\n\n\\(W_{1,n},\\dots,W_{n,n}\\) are i.i.d. with\n\\(\\mathbb{E}(W_{i,n})=0\\) and\n\\(Var(W_{i,n})=\\frac{1}{n}\\sigma_0^2.\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,\\left|W_{1,n}\\right|^3, \\left|W_{1,n}\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_1-\\mu_0\\right|^3, n^{-1}\\left|X_1-\\mu_0\\right|^2\\big)\\right).\n\\end{align*}\n\\] That is, \\[\nn|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] which means that \\(|\\lambda_n(t)|\\to 0\\) faster than \\(n^{-1}.\\)\nThus, by Equation 3.3 \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t)\n\\end{align*}\n\\]\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) (I.e. we have shown the Lindeberg-Lévy CLT.)\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above prove, by showing that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}^*_n- \\bar{X}_n\\right)|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\n\\(W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\) is i.i.d. with\n\\(\\mathbb{E}^*(W^*_{n})=\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)=0\\) and\n\\(Var^*(W^*_{n})=Var(W^*_{n}|\\mathcal{S}_n)=\\frac{1}{n}\\hat{\\sigma}_0^2=\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right)\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_0^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i^* - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\nn{\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., \\({\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\) faster than \\(n^{-1}.\\) Moreover, since \\[\n{\\color{darkgreen}\\hat\\sigma_0^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\n\\] we have that (using Equation 3.3) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nH_n^{Boot}(x)=P\\left(\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x|\\mathcal{S}_n)\\right) \\approx\n\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar X^*_{n,j}-\\bar X_n\\right)\\leq x\\right)}=H_{n,m}^{Boot}(x),  \n\\] as done in Section 3.4.1.1.\n\n\n\n\n3.4.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\n\\(\\theta_0\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\) as \\(n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\) as \\(n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution. This confidence interval is approximate, since it is only asymptotically justified; i.e. it is not exact in finite samples.\n\n\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v_n\\) of \\(v_0\\) (see ?sec-Illustraction). Statistical inference is then usually based on the bootstrap confidence intervals.\nIn many situations it can be shown that bootstrap confidence intervals (or tests) are even more precise than asymptotic normality based confidence intervals. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0\\):\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles \\[\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\quad\\text{and}\\quad \\hat q^*_{n,1-\\frac{\\alpha}{2}}\n\\] of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.6}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g. \\(\\lfloor 4.9\\rfloor = 4\\)).\nThe approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.7}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we’ll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem 3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation 3.7) for \\(\\theta_0\\): \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-\n(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation 3.7 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically valid (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(s^2_n=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/s_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{s_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}},\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right]\n\\]\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\mu_0\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section 3.4.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval according to Equation 3.7: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BootT",
    "href": "Ch3_Bootstrap.html#sec-BootT",
    "title": "3  The Bootstrap",
    "section": "3.5 The Bootstrap-\\(t\\) Method",
    "text": "3.5 The Bootstrap-\\(t\\) Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption.\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the (nonparametric) bootstrap-\\(t\\) method (one also speaks of the “studentized bootstrap”). The construction relies on so-called (asymptotically) pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\] which implies that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\bar{X}_n\\) is a Pivotal Statistic\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0<Var(X)=\\sigma_0^2<\\infty\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic.\n\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*=\\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_{x\\in\\mathbb{R}} \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat v_n^*}\\le x \\;\\right|\\;{\\cal S}_n\\right)-\\Phi(x)\\right|\\rightarrow_p 0,\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.5.1 The Bootstrap-\\(t\\) Confidence Interval\nSetup:\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2\\) of \\(\\hat{\\theta}_n,\\) i.e.  \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\hat v^2_n\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\]\nand that \\[\n\\hat v_n\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\nAlgorithm of the Bootstrap-\\(t\\) Confidence Interval for \\(\\theta_0\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g. \\(m=100,000\\)) many bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\]\nUse the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) empirical quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) (see Equation 3.6).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.8}\\] where \\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) based on the original sample \\(X_1,\\dots,X_n.\\)\n\nJustifying the Bootstrap-\\(t\\) CI (Equation 3.8) for \\(\\theta_0\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem 3.1).\nThus, the empirical bootstrap quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat \\tau_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nMoreover, due to the assumed consistencies of the bootstrap and of the estimator \\(\\hat v_n,\\) we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(- \\hat{\\theta}_n + \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval (Equation 3.8) \\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance of \\(\\bar{X}_n\\) is \\(s^2\\approx \\lim_{n\\to\\infty}n Var(\\bar{X}_n)=\\sigma_0^2\\), where \\(s^2\\) denotes the sample variance \\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s_n^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*_n)^2}\\) to generate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\] using Equation 3.6.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation 3.8): \\[\n\\left[\\bar X_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where \\(s_n\\) is computed from the original sample, i.e., \\[\ns_n=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}.\n\\]\n\n\n\n\n3.5.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{v^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(v^*_n\\) depends on the bootstrap sample — not the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. basic bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field."
  }
]