[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    14:15-15:45 \n    Jur / Hörsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\nEM Algorithm & Cluster Analysis\nBootstrap\nNonparametric Regression\nFunctional Data Analysis\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/Script-CompStat-MSc/ (pwd: compstat)\nWe’ll use an eWhiteboard for derivations and some extra explanations.\nBasic material from our econometrics course:\n\nIntroduction to R\nProbability\n\n\n\n\n\n\n\n\n\nConsider using git/github for your personal course notes.\n\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html",
    "href": "Ch1_MaximumLikelihood.html",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g. the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed.\nIn (classical) maximum likelihood estimation we must be rather specific about the process that generated the data. This is a trade off – by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta_n\\) of some parameter \\(\\theta_0\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta_0\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta_0) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: This means that no consistent estimator has lower asymptotic mean squared error than the maximum likelihood estimator.\n\nLikewise for multivariate parameter \\(\\theta_0\\in\\mathbb{R}^p.\\)\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density/probability mass functions \\(f\\) of the random variables in the (i.i.d.) random sample \\(X_1,\\dots,X_n\\) such that \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) where \\(f\\) is known up to an unknown parameter \\(\\theta,\\) where \\(\\theta\\in\\mathbb{R}^K\\) is allowed to be a finite (\\(1\\leq K<\\infty\\)) dimensional vector.\nExamples:\n\n\\(f\\) being the probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta)\\) with \\[\nf(x_i|\\theta)=\n\\left\\{\n\\begin{array}{ll}\n\\theta,&\\text{if } x_i=1\\\\\n1-\\theta, & \\text{if } x_i=0\n\\end{array}\n\\right.\n\\] and unknown parameter \\(0\\leq \\theta\\leq 1.\\)\n\\(f\\) is the normal density \\[\nf(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\n\\] with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement (fixing the family of density functions) can be overly restrictive. In many applications we typically do not know the family of \\(f.\\) To address this issue, the quasi maximum likelihood theory generalizes classic maximum likelihood estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Such coin-flips can be modeled using Bernoulli random variables \\[\nX\\sim\\mathcal{Bern}(\\theta_0)\n\\] where \\[\nX=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\n    \\end{matrix}\n    \\right.\n\\] The probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta_0)\\) with unknown probability of success parameter \\(0<\\theta_0<1,\\) is given by \\[\nf(x|\\theta_0)=\n\\left\\{\n  \\begin{array}{ll}\n  \\theta_0,&\\text{if } x=1\\\\\n  1-\\theta_0, & \\text{if } x=0\n  \\end{array}\n\\right.\n\\] I.e. \\[\n\\theta_0 = f(1|\\theta_0) = P(X=1) = P(\\text{Coin}=H),\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta_0 = f(0|\\theta_0) = P(X=0) = P(\\text{Coin}=T).\n\\]\nOur goal is to estimate the unknown \\(\\theta_0\\) using a random (i.i.d.) sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\text{ in $i$th coin flip}\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\\text{ in $i$th coin flip}\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{Bern}(\\theta_0),\\quad i=1,\\dots,n.\n\\] \nA given observed realization of the random sample \\[\n\\{X_{1,obs},X_{2,obs},\\dots,X_{n,obs}\\}=\\{0,1,\\dots,0\\}\n\\] consists of \\(0\\leq N_{H,obs}\\leq n\\) \\[\nN_{H,obs}=\\sum_{i=1}^n X_{i,obs}\n\\] many heads and of \\[\n0\\leq n-N_{H,obs} \\leq n\n\\] many tails.\n\n\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] to estimate the unknown \\(\\theta_0\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta)\n&=\\prod_{i=1}^nf(X_{i,obs}|\\theta)\\\\[2ex]\n%&=\\left(P(X=1)\\right)^{N_{H,obs}}\\left(P(X=0)\\right)^{n-N_{H,obs}}\\\\[2ex]\n%&= \\theta^{N_{H,obs}}(1-\\theta)^{n-N_{H,obs}}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\end{align*}\n\\]\nThe function \\(\\mathcal{L}_n(\\theta)\\) is called the likelihood function.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.1 (Likelihood Function) More generally, when the observations \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\] where \\(f(X_{i,obs} | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_{i,obs},\\) and where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of the density function. (A definition for dependent data (e.g. time series) is also possible.)\n\n\n\n\n\n\nWe estimate the unknown parameter \\(\\theta_0\\) by maximizing the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) over the range of possible parameter values. The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}_n(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Maximum Likelihood (ML) Estimator) \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\end{align*}\n\\] where \\(\\Theta\\) denotes the parameter space.\n\n\n\nIn our coin flip example this means to estimate the unknown \\(\\theta_0\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) is maximal, \\[\n\\hat\\theta_{ML} = \\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\]\nUsually it’s easier to work with sums rather than products—also for doing the asymptotics in Section 1.4. So we apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\ln\\prod_{i=1}^n f(X_{i,obs}|\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n \\ln f(X_{i,obs}|\\theta).\n\\end{align*}\n\\] Since this is only a monotonic transformation, we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\ell_n(\\theta).\n\\end{align*}\n\\] \nIn our coin flipping example, taking the natural logarithm (\\(\\ln\\)) yields, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta) &= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}} \\\\[2ex]\n\\Rightarrow\\quad \\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n\\left( X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\right).\n\\end{align*}\n\\]\nThe coin flip example is actually so simple that we can maximize \\(\\ell_n(\\theta)\\) analytically. Computing the first derivative yields \\[\n\\begin{align*}\n\\ell'_n(\\theta)&=\\sum_{i=1}^n \\left(X_{i,obs}\\dfrac{1}{\\theta} - (1-X_{i,obs})\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta}\n\\end{align*}\n\\] Setting the first derivative to zero determines the maximum likelihood estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\ell_n'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{N_{H,obs}}{\\hat\\theta_{ML}} &=& \\dfrac{n-N_{H,obs}}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&N_{H,obs}-N_{H,obs}\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-N_{H,obs}\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{N_{H,obs}}{n}\n\\end{array}\n\\tag{1.1}\\]\nUsually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE, \\(\\hat\\theta_{ML}.\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "href": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "title": "1  Maximum Likelihood",
    "section": "1.2 Numeric Optimization",
    "text": "1.2 Numeric Optimization\nUsually we are not so fortunate as to have an analytical solution for the MLE, and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\n\nGeneral idea:\n\nStart at some value, \\(\\theta_{(0)},\\) in the parameter space \\(\\Theta.\\)\nSearch across the parameter space \\(\\Theta\\) using a step-wise procedure until a updated parameter value \\(\\ell'(\\theta_{(m)})\\) is found that yield a derivative of the log likelihood that is effectively zero (i.e. smaller than some convergence/stopping criterion), \\(\\ell'(\\theta_{(m)})\\approx 0.\\)\n\n\n1.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nIn the following, we consider the univariate case \\(\\theta\\in\\mathbb{R}.\\) However, the multivariate case \\(\\theta\\in\\mathbb{R}^K\\) is treated likewise, but requires substituting first derivatives by gradients, second derivatives by the Hessian, etc.\n\n\n\n\n\n\nNote\n\n\n\n\nMinimization and maximization are essentially the same problems since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial of order 1}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial of order 2}},\n\\end{align*}\n\\] Locally, i.e. for \\(h\\approx 0,\\) (e.g. \\(h=\\pm 0.04\\)) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure 1.1.\n\n\n\n\n\nFigure 1.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Taylor’s Theorem)  Today, there are many different versions of Taylor’s theorem. We consider the following two:\n1. Peano form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k\\) times differentiable at \\(x\\in\\mathbb{R}\\) and let \\(h\\in\\mathbb{R}.\\) Then there exists a function \\(P_{k,x}:\\mathbb{R}\\to\\mathbb{R}\\) such that \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ P_{k,x}(h)\\;(h)^k\n\\end{align*}\n\\] with \\[\nP_{k,x}(h)\\to 0\\quad\\text{as}\\quad h\\to 0,\n\\] where \\(f^{(\\ell)}(x)\\) denotes the \\(\\ell\\)th derivative of \\(f\\) at \\(x.\\)\n2. Lagrange or Mean-value form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k+1\\) times differentiable on the open interval between \\(x\\) and \\(x+h,\\) with \\(h\\in\\mathbb{R},\\) and let \\(f^{(k)}\\) be continuous on the closed interval between \\(x\\) and \\(x+h,\\). Then \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ M_{k,x}(h)\n\\end{align*}\n\\] with \\[\nM_{k,x}(h)=\\frac{f^{(k+1)}(\\xi)}{(k+1)!}(h)^{k+1}\n\\] for some real number \\(\\xi\\) between \\(x\\) and \\(x+h.\\) (This form of Taylor’s theorem is based on the mean-value Theorem 1.2.)\n\n\n\n\n\n\nQualitative version using the small-\\(o\\) notation:\n\n\n\n\\[\n\\begin{align*}\nf(x + h) & =\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ o\\big(|h|^k\\big),\n\\end{align*}\n\\] where \\(o\\big(|h|^k\\big)\\) denotes the family of real-valued functions, \\(g(h)\\) say, that are of a strictly smaller \\(o\\)rder of magnitude than the function \\(|h|^k\\) as \\(h\\to 0;\\) i.e.\n\\[\no\\big(|h|^k\\big)=\\left\\{g(h)\\to 0\\;\\text{ as }\\; h\\to 0:\\frac{|g(h)|}{|h|^k}\\to 0\\quad\\text{as}\\quad h\\to 0\\right\\}.\n\\]\n1. Note: Peano form of the remainder term: \\(P_{k,x}(h)\\;(h)^k=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|P_{k,x}(h)\\;(h)^k|}{|h|^k}\n%=\\frac{|P_k(x+h)|\\cdot |h|^k|}{|h|^k}\n=|P_{k,x}(h)|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n2. Note: Mean-value form of the remainder term: \\(M_{k,x}(h)=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|M_{k,x}(h)|}{|h|^k}=\n\\left|\\frac{f^{(k+1)}(\\xi)}{(k+1)!}\\right|\\cdot|h|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell_n\\) be a log-likelihood function with continuous first, \\(\\ell_n',\\) and second, \\(\\ell_n'',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell_n,\\) we try to find the root of \\(\\ell_n',\\) i.e. the value of \\(\\theta\\in\\Theta\\) such that \\[\n\\ell_n'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order condition of the optimization problem. We do so using a step-wise optimization approach, where each step has a smallish size \\(h.\\)\nInitialization: Let \\(\\theta_{(0)}\\in\\Theta\\) be our first guess of the root of \\(\\ell'_n.\\)\n\\(h\\)-Steps: Typically, our guess is not perfect and thus \\(\\ell_n'(\\theta_{(0)})\\neq 0.\\) Therefore, we want to move from \\(\\theta_{(0)}\\) to a new root-candidate \\(\\theta_{(1)}\\) by doing an \\(h\\)-step update \\[\n\\theta_{(1)} = \\theta_{(0)} + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell_n'\\) around our first guess \\(\\theta_{(0)}\\) gives \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)} + h) & \\approx \\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)})h\n\\end{align*}\n\\] Thus, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell_n',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)})h = 0\\\\[2ex]\n\\Rightarrow h = -\\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\theta_{(1)} = \\theta_{(0)} - \\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\] Likewise, the \\(m\\)th root-candidate is \\[\n\\theta_{(m)} = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})};\n\\] see also Figure 1.2.\n\n\n\n\n\nFigure 1.2: The \\(m\\)th update step in the Newton-Raphson root-finding algorithm.\n\n\n\n\n\n\n\n1.2.2 Convergence of the Newton-Raphson Algorithm\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.  \\[\n\\ell_n'(\\theta_{root})=0.\n\\] We aim to find \\(\\theta_{root}\\) using the Newton-Raphson algorithm and call our best approximation of \\(\\theta_{root}\\) the maximum likelihood estimate; i.e. \\(\\hat{\\theta}_{ML}\\approx\\theta_{root}.\\)\nLet \\[\ne_{(0)}=\\theta_{root}-\\theta_{(0)}\n\\] denote the start value error and let \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start value error neighborhood around \\(\\theta_{root}.\\)\nOne can shown that if \\(\\ell_n'\\) is “well behaved” over \\(I;\\) i.e. \n\nif \\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\nif \\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I,\\)\n\nand if our first guess \\(\\theta_{(0)}\\) is “close enough;” i.e. \n\nif \\(M|e_{(0)}|<1,\\) where \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\n\nthen \\(\\theta_{(m)}\\) will converge to \\(\\theta_{root}\\) as \\(m\\to\\infty.\\)\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, we typically don’t know if \\(\\ell_n'\\) is “well behaved” and we usually don’t know whether our first guess is “close enough”. So, typically we cannot guarantee convergence of the Newton-Raphson algorithm. 😭 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor problems that are globally concave, the starting value \\(\\theta_0\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nIn actual practice, implementation of the Newton-Raphson algorithm can be tricky. We may have \\(\\ell_n''(\\theta_{(m)})=0,\\) in which case the function looks locally like a straight line, with no solution to the Taylor series approximation \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(m)} + h) & \\approx \\ell_n'(\\theta_{(m)}) + \\ell_n''(\\theta_{(m)})h.\n\\end{align*}\n\\] In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on \\(\\ell_n'(\\theta_m).\\)\nIn other cases where \\(\\theta_{(m)}\\) is too far from the true maximizer \\(\\theta\\), the Taylor approximation may be so inaccurate that \\(\\ell_n(\\theta_{(m+1)})\\) is actually smaller than \\(\\ell_n(\\theta_{(m)}).\\) When this happens one may replace \\(\\theta_{(m+1)}\\) with \\((\\theta_{(m+1)}+\\theta_{(m)})/2\\) (or some other value between \\(\\theta_{(m)}\\) and \\(\\theta_{(m+1)}\\)) in the hope that a smaller step will produce better results.\n\n\n\nStopping Criterion: Since we are expecting that \\(\\ell_n'(\\theta_{(m)})\\to 0,\\) as \\(m\\to\\infty,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell_n'(\\theta_{(m)})|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon>0.\\)\n\n\n\n\n\n\nPseudo-Code: Newton-Raphson Algorithm\n\n\n\n\\[\n\\begin{array}{ll}\n\\texttt{\\textbf{select }} \\theta_{(0)}\\in\\Theta\\;\\;\\text{ and}&\\varepsilon>0 \\\\[2ex]\n\\texttt{\\textbf{let }} m=0         &  \\\\\n\\texttt{\\textbf{while }}  | \\ell_n'(\\theta_{(m)}) | >\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} m = m+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_{(m)} = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta_{ML}=\\theta_{(m)} & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta_{ML} &  \\\\\n\\end{array}\n\\]\n\n\n\n\n1.2.3 Newton-Raphson Algorithm: Coin-Flipping Example\nLet’s return to our earlier coin flipping example.\nIf we observe, for instance, only one head \\(N_{H,obs}=1\\) for a sample size of \\(n=5,\\) we already know from Equation 1.1 that \\[\n\\hat\\theta_{ML}=\\frac{N_{H,obs}}{n}=\\frac{1}{5}=0.2,\n\\] but let us, nevertheless, apply the Newton-Raphson algorithm.\nThe first and second derivatives of \\[\n\\ell_n(\\theta)=\\sum_{i=1}^n\\big(X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\big)\n\\] are \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta} \\\\[2ex]\n\\ell_n''(\\theta) &= -\\dfrac{N_{H,obs}}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{N_{H,obs}}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{N_{H,obs}}{\\theta^2} - \\dfrac{n-N_{H,obs}}{(1-\\theta)^2}.\n\\end{align*}\n\\]\nWe consider a sample size of \\(n=5\\) with the following observed outcome:\n\nOne Head: \\(\\quad N_{H,obs}=1\\)\nFour Tails: \\(\\quad n-N_{H,obs}=4\\)\n\nSetting \\(\\varepsilon=10^{-10}\\) as our stopping criterion and \\(\\theta_{(0)}=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table 1.1. The numeric optimization solution is \\(\\hat\\theta_{ML} = 0.2\\) which equals the analytic solution.\n\n\nTable 1.1: Result of applying the Newton Raphson optimization algorithm to our coin flipping example for given data with \\(N_{H,obs}=1,\\) sample size \\(n=5,\\) starting value \\(\\theta_{(0)}=0.4,\\) and convergence criterion \\(\\varepsilon=10^{-10}.\\)\n\n\n\n\n\n\n\n\n\\(m\\)\n\\(\\hat\\theta_{(m)}\\)\n\\(-\\ell_n'(\\hat\\theta_{(m)})/\\ell_n''(\\hat\\theta_{(m)})\\)\n\\(\\ell_n'(\\hat\\theta_{(m)})\\gtrless \\varepsilon\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-2.4\\cdot 10^{-1}\\)\n\\({\\color{red}-4.2 > \\varepsilon}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}3.3\\cdot 10^{-2}\\)\n\\({\\color{red}\\phantom{-}1.5 > \\varepsilon}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}6.6\\cdot 10^{-3}\\)\n\\({\\color{red}\\phantom{-}2.2\\cdot 10^{-1} > \\varepsilon}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}1.7\\cdot 10^{-4}\\)\n\\({\\color{red}\\phantom{-}5.4\\cdot 10^{-3} > \\varepsilon}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}1.1\\cdot 10^{-7}\\)\n\\({\\color{red}\\phantom{-}3.5\\cdot 10^{-6} > \\varepsilon}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}4.8\\cdot 10^{-14}\\)\n\\({\\color{darkgreen}\\phantom{-}1.5\\cdot 10^{-12} < \\varepsilon}\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "href": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "title": "1  Maximum Likelihood",
    "section": "1.3 Linear Regression under Normality",
    "text": "1.3 Linear Regression under Normality\nNow let’s return to the linear regression model \\[\nY_i=X_i'\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{1.2}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta_0\\in\\mathbb{R}^p\n\\] denotes the vector of unknown slope parameters, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})'\\in\\mathbb{R}^K\n\\] denotes the vector of predictor variables, where the (i.i.d.) random sample\n\\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design with homoskedastic errors (see Definition 1.3).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Random Design (Regression Analysis)) \nA random desgin in regression analysis is given by the following setup:\nLet \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] or equivalently \\[\n(X_1,\\varepsilon_1), (X_2,\\varepsilon_2), \\dots, (X_n,\\varepsilon_n)\n\\] denote a (i.i.d.) random sample with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\), intertable \\((K\\times K)\\) matrix \\(\\mathbb{E}(X_iX_i')=\\Sigma_{X'X}\\), \\(i=1,\\dots,n,\\) and with either\n\nhomoskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0<\\infty\\)\n\nor\n\nheteroskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), for a strictly positive and finite variance function \\(\\sigma^2_0(\\cdot).\\)\n\n\n\n\nFor the following, it is convenient to write Equation 1.2 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta_0} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nUnder normally distributed (and homoskedastic) error terms, \\(\\varepsilon_i,\\) we have that \\[\n\\begin{align}\n\\underset{(n\\times 1)}{\\varepsilon} &\\sim \\mathcal{N}_n\\left(0, \\sigma_0^2I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y-X\\beta_0)|X &\\sim \\mathcal{N}_n\\left(0, \\sigma^2_0I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y_i-X_i'\\beta_0)|X_i &\\sim \\mathcal{N}\\left(0, \\sigma^2_0\\right)\\\\[2ex]\n%\\Rightarrow\\quad\n%\\underset{(n\\times 1)}{Y|X} &\\sim \\mathcal{N}_n\\left(X\\beta_0, \\sigma^2_0I_n\\right)\n\\end{align}\n\\tag{1.3}\\]\n\n\n\n\n\n\n\nUnder Equation 1.3, we have \\[\nf(Y_i|X_i;\\beta_0',\\sigma_0^2)=\n\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta_0)^2}{2\\sigma_0^2}\\right),\n\\] where \\[\n\\theta_0=(\\beta_0',\\sigma_0^2)'\\in\\mathbb{R}^K\\times\\mathbb{R}_{>0}\n\\] denotes the unknown parameter vector.\nThis allows us to setup the likelihood function, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\beta',\\sigma^2)\n& =\\prod_{i=1}^n f(Y_i|X_i;\\beta',\\sigma^2)\\\\[2ex]\n& =\\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n& =\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{\\sum_{i=1}^n (Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n%& =\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)\\\\[2ex]\n& =(2\\pi)^{-n/2} \\cdot (\\sigma^2)^{-n/2}\\cdot  \\exp\\left(-\\frac{(Y-X\\beta)'(Y-X\\beta)}{2\\sigma^2}\\right),\\\\[2ex]\n\\end{align*}\n\\]   and the log-likelihood function, \\[\n\\begin{align*}\n\\ell_n(\\beta',\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta).\n\\end{align*}\n\\] \nTaking first derivatives gives \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}}\\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] Putting the above derivative functions into one column vector yields the \\(((K+1)\\times 1)\\)-dimensional gradient called score function in ML-theory: \\[\n\\nabla\\ell_n(\\theta')=\n\\left(\\begin{matrix}\n\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)\\\\\n\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)\n\\end{matrix}\\right)\n\\tag{1.4}\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Score Function) More generally, let \\(\\ell_n(\\theta)\\) denote the log-likelihood function evaluated at a \\(p\\)-dimensional parameter vector \\(\\theta=(\\theta_1,\\dots,\\theta_p)'.\\)\nThen the gradient vector \\[\\nabla\\ell_n(\\theta')=\\left(\\begin{matrix}\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_1}(\\theta')\\\\ \\vdots\\\\\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_p}(\\theta')\n  \\end{matrix}\n  \\right)\n\\] is called the score-function.\nThe score function is random, since it depends on the random sample.\nAt the true parameter vector \\(\\theta_0\\in\\mathbb{R}^p,\\) the score function satisfies \\[\n\\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_j}(\\theta_0')\\right)=0\n\\] for all \\(j=1,\\dots,p.\\) We show this below in Section 1.4.\n\n\n\nSetting the score function in Equation 1.4 equal to zero yields a system of \\(K+1\\) equations with \\(K+1\\) unknowns, which we can solve for the maximum likelihood estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}:\\)\nSolving for \\(\\hat\\beta_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad& - \\dfrac{1}{\\hat\\sigma^2_{ML}}(-X'Y + X'X\\hat\\beta_{ML})  \\overset{!}{=}0\\\\[2ex]\n\\Rightarrow\\quad & \\hat\\beta_{ML}=(X'X)^{-1}X'Y\\\\[2ex]\n\\end{align*}\n\\] Solving for \\(s^2_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad&-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}  \\overset{!}{=}0\\ \\\\[2ex]\n\\Rightarrow\\quad  &\ns_{ML}^2 =\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\\\[2ex]\n&\\phantom{s_{ML}^2}=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2,\n\\end{align*}\n\\] where \\(\\hat\\varepsilon_i = Y_i - X_i'\\hat{\\beta}_{ML}.\\)\nObservations:\n\n\\(\\hat\\beta_{ML}\\) equals the OLS estimator \\(\\hat\\beta=(X'X)^{-1}X'Y.\\)  Since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\n\\(s_{ML}^2\\) differs from the unbiased variance estimator \\(s_{UB}^2=\\frac{1}{n-K}\\hat{\\varepsilon}_i^2.\\)\n\n\n1.3.1 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\n\n\n\n\n\n\nTip\n\n\n\nTo compute the asymptotic variance of the ML-estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML},\\) we need to\n\ncompute the Hessian matrix (i.e. all second partial derivatives) of \\(\\ell_n,\\)\n\ntake the expectation of this Hessian matrix and multiply it by \\(-1/n\\), which gives us the Fisher Information matrix.\nInverting the Fisher information matrix give the asymptotic variance expression.\n\n\n\nLet’s do this preliminary work in the following:\n\nPartial second derivatives with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\Rightarrow\\quad\n\\underset{(K\\times K)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)}\n&= - \\dfrac{1}{\\sigma^2}(X'X)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{1}{\\sigma^2} \\mathbb{E}(X'X)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{n}{\\sigma^2} \\Sigma_{X'X}\\right)\\\\[2ex]\n&=  \\dfrac{1}{\\sigma^2} \\Sigma_{X'X},\n\\end{align*}\n\\] where\n\\[\n\\mathbb{E}\\left(X'X\\right)\n=\\mathbb{E}\\left(\\sum_{i=1}^nX_iX_i'\\right)\n=n\\underbrace{\\mathbb{E}\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}} = n\\Sigma_{X'X}.\n\\]\nSecond derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2}\\frac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{2}}\\\\[2ex]\n\\Rightarrow\\quad\\underset{(1\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{3}} \\\\[2ex]\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\[2ex]\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{\\mathbb{E}\\left(\\sum_{i=1}^n\\varepsilon_i^2\\right)}{\\sigma^{6}} \\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{n\\sigma^2}{\\sigma^{6}}\\right)\\\\[2ex]\n&=\\left(-\\frac{1}{2\\sigma^{4}}+\\frac{1}{\\sigma^{4}}\\right)\\\\[2ex]\n&=\\frac{1}{2\\sigma^{4}}\\\\[2ex]\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}(X')(Y - X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}X'\\varepsilon\\\\[2ex]\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\Rightarrow\\quad\n\\underset{(K\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta \\partial \\sigma^2}(\\beta',\\sigma^2)}\n=   \\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)'\n%&= -\\frac{X'(Y-X\\beta)}{\\sigma^4}\\\\[2ex]\n& = \\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot  \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\left(\\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\right)'\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\varepsilon)}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(\\mathbb{E}(X'\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\mathbb{E}(\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=-\\frac{1}{n}\\cdot 0=0,\n\\end{align*}\n\\] since \\(\\mathbb{E}(\\varepsilon|X)=0\\) is an \\((n\\times 1)\\) zero vector.\nCollecting the above results, allows us to write down the expression for \\((-1/n)\\) times the expectation of the Hessian matrix of \\(\\ell_n\\) which yields the Fisher Information (Matrix):\n\\[\n\\begin{align*}\n&\\mathcal{I}(\\theta) :=\\; -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=\n-\\frac{1}{n}\\cdot \\mathbb{E}\n\\left[\\begin{array}{cc}\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2) \\right)\n\\end{array}\\right]\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.5 (Fisher Information Matrix) The matrix \\[\n\\mathcal{I}(\\theta) := -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\theta)\\right)\n\\] is called Fisher Information Matrix.\n\n\n\n\nAsymptotic Variance and Fisher Information Matrix\nThe asymptotic variance of the MLE \\[\n\\hat{\\theta}_{ML}=\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n\\] is given by the inverse of the Fisher information matrix evaluated at the true parameter values \\(\\beta_0\\) and \\(\\sigma^2_0.\\) \\[\n\\begin{align*}\n&AVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\\left(\\mathcal{I}(\\beta'_0,\\sigma^2_0)\\right)^{-1}\\\\[2ex]\n&=\\left(-\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta'_0,\\sigma^2_0)\\right)\\right)^{-1}\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right)\\\\\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta'_0,\\sigma^2_0) \\right)\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2_0}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4_0}}\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\sigma^2_0\\Sigma_{X'X}^{-1}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{2\\sigma^4_0}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\nThat is, \\[\n\\begin{align*}\nAVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n&=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\sigma^2_0\\Sigma_{X'X}^{-1} & 0 \\\\[2ex]\n0 & \\ 2\\sigma^4_0\n\\end{array}\\right].\n\\end{align*}\n\\tag{1.5}\\]\n\n\nOf course, the variance expressions in Equation 1.5 contain unknown quantities and thus are not directly usable in practice. However, we can plug in estimates of the unknown quantities; namely \\[\ns_{ML}^2                         \\quad\\text{for}\\quad \\sigma^2_0\n\\] and \\[\nS_{X'X}^{-1}=\\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1} \\quad \\text{for}\\quad \\Sigma_{X'X}^{-1}.\n\\]\nThis leads to estimators of the asymptotic variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{AVar}(s^2_{ML})\n&=2\\left(s_{ML}^2\\right)^2\n\\end{align}\n\\] and thus to estimators of the variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{Var}(\\hat{\\beta}_{ML})\n=\\frac{1}{n}\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 \\frac{1}{n}S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{Var}(s^2_{ML})\n=\\frac{1}{n}\\widehat{AVar}(s^2_{ML})\n&=\\frac{1}{n}2\\left(s_{ML}^2\\right)^2.\n\\end{align}\n\\]"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "href": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "title": "1  Maximum Likelihood",
    "section": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x;\\theta_0),\n\\] where the true (unknown, univariate) parameter \\(\\theta_0\\in\\Theta\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta_0\\) is an “interior point” of \\(\\Theta\\) if \\(\\theta_l<\\theta_0<\\theta_u.\\)\nMoreover, we consider the following setup.\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i;\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i;\\theta)\n\\]\nMaximum-likelihood estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_n=\\arg\\max_{\\theta\\in\\Theta}\\ell_n(\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\ell_n'(\\hat\\theta_n)=0\\quad\\text{and}\\quad\\ell_n''(\\hat\\theta_n)<0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x;\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x;\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x;\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x;\\theta)dx\n\\end{align*}\n\\] for all \\(\\theta\\in\\Theta.\\)\n\n\n\n\n\n\n\nExample\n\n\n\nAn example that fits into the above setup is the density of the exponential distribution \\[\nf(x;\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown “rate” parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x;\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on a Taylor expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta_0\\) (see Equation 1.6). To derive this expression, we use the mean value theorem (Theorem 1.2).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\nBy the Mean Value Theorem (Theorem 1.2), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\tag{1.6}\\] for some \\(\\psi_n\\) between \\(\\hat{\\theta}_n\\) and \\(\\theta_0;\\) i.e.\n\n\\(\\psi_n\\in(\\theta_0,\\hat{\\theta}_n)\\quad\\) if \\(\\quad\\theta_0<\\hat{\\theta}_n\\)\n\\(\\psi_n\\in(\\hat{\\theta}_n,\\theta_0)\\quad\\) if \\(\\quad\\hat{\\theta}_n<\\theta_0\\)\n\n\nNote: Equation 1.6 is simply the first-order version of the mean-value form of Taylor’s theorem (Theorem 1.1).\n\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 1.6, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0).\n\\tag{1.7}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x;\\theta)dx=1\n\\] for all possible values of \\(\\theta\\in\\Theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx&=\\frac{\\partial}{\\partial \\theta}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x;\\theta)dx=0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\tag{1.8}\\] And likewise, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x;\\theta)dx=0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\tag{1.9}\\]\nUsing Equation 1.8 and Equation 1.9, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta_0)=\\frac{1}{n}\\underbrace{\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)}_{\\ell_n'(\\theta_0)}\n\\] is asymptotically normal.\nThis is done in the following by checking the three conditions for applying the Lindeberg-Lévy central limit theorem.\nFirstly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)}\n{f(x;\\theta_0)}f(x;\\theta_0)dx\\quad[\\text{Def. of $\\mathbb{E}$}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\tag{1.10}\\] where the last step follows from Equation 1.8.\nSecondly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X_i;\\theta_0)}{f(X_i|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)^2\\right)\\\\\n&=:\\frac{1}{n}\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\] where the simplification of the variance expression to a second moment expression follows from Equation 1.10.\n\\(\\mathcal{I}(\\theta_0)\\) is the Fisher information (here a scalar) evaluated at \\(\\theta_0.\\)\nThirdly, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\n\\] is taken over i.i.d. random variables: \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0),\\quad i=1,\\dots,n.\n\\] Thus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\theta_0)-\\overbrace{\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)}^{=0}}{\\sqrt{\\frac{1}{n}\\mathcal{I}(\\theta_0)} } = \\frac{\\ell_n'(\\theta_0)}{\\sqrt{n\\mathcal{I}(\\theta_0)} } \\to_d \\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\)\nThus, that by our mean value expression in Equation 1.7 \\[\n\\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] we thus have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\mathcal{I}(\\theta_0)}}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{1.11}\\] The \\(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\)-part in Equation 1.11 is our object of interest.\nThe further analysis requires us to study the asymptotic behavior of\n\\[\n-\\frac{1}{n}\\ell_n''(\\psi_n)\n\\] which will help us to understand the behavior of \\(\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\) in Equation 1.11.\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(-\\frac{1}{n}\\ell_n''(\\psi_n),\\) we begin with studying the mean and the variance of the simpler statistic \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0).\n\\]\n\n\nFirst, the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i;\\theta_0)\\right) f(X_i;\\theta_0)-\\frac{\\partial}{\\partial\\theta}f(X_i;\\theta_0)\\frac{\\partial}{\\partial\\theta} f(X_i;\\theta_0)}{\\left(f(X_i;\\theta_0)\\right)^2}\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2  \n\\right).\n\\end{align*}\n\\] Taking the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}+\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)+\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] From Equation 1.10, we know that \\(\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2} f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)=0\\) thus \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=0 + \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)^2\\right)\\\\[2ex]\n&=\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\] \\[\n\\Rightarrow \\qquad \\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)=\\mathcal{I}(\\theta_0)\\qquad\n\\]\nThis means that \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n\\] is an unbiased estimator of the Fisher information \\(\\mathcal{I}(\\theta_0).\\) \n\n\n\n\n\n\nMultivariate Settings\n\n\n\nFor multivariate (\\(p\\)-dimensional) parameters \\(\\theta_0,\\) the Fisher information \\(\\mathcal{I}(\\theta_0)=(-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta_0)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section 1.3.1).\n\n\nSecond, the variance of variance of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=Var\\left(-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X_i;\\theta_0)\\right)}_{=\\texttt{constant}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results for \\(-\\frac{1}{n}\\ell_n''(\\theta_0),\\) we can write down the Mean Squared Error (MSE) of the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) of \\(\\mathcal{I}(\\theta_0):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(-\\frac{1}{n}\\ell_n''(\\theta_0) -\\mathcal{I}(\\theta_0)\\right)^2\\right)\\\\[2ex]\n&=\\underbrace{\\left(\\operatorname{Bias}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\right)^2}_{=0}+Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[3ex]\n&=Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) is a mean square consistent estimator, i.e. \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_{m.s.} \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta_0)\\) is also a (weakly) consistent estimator, i.e.  \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_p \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 Remember, we wanted to study \\(-\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 1.11 not \\(-\\frac{1}{n}\\ell_n''(\\theta_0).\\) Studying \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) was only the simpler thing to do.\nLuckily, we are actually close now.\n\n\nNext, we use that ML estimators \\(\\hat\\theta_n\\) are (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty;\n\\] this is, for instance, implied by our results in Section 1.3 for the case of ML estimation in linear regression under normality.\nSince \\(\\psi_n\\) is a mean value between \\(\\theta_0\\) and \\(\\hat{\\theta}_n\\) (Equation 1.6), consistency of \\(\\hat{\\theta}_n\\) implies that \\[\n\\psi_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have by the continuous mapping theorem that \\[\n\\begin{align}\n-\\frac{1}{n}\\ell_n''(\\psi_n) & \\to_p \\phantom{-}\\mathcal{I}(\\theta_0)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\qquad\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)&\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} \\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\]\nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 1.11 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)}_{\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right),\n\\end{align*}\n\\tag{1.12}\\] where \\(1/\\mathcal{I}(\\theta_0)\\) is the asymptotic variance of the ML estimator \\(\\hat{\\theta}_n\\) and equals the inverse of the (here scalar valued) Fisher information \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)).\n\\]\nEquation 1.12 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{I}(\\theta_0)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d \\mathcal{N}_p\\left(0, \\mathcal{I}(\\theta_0)^{-1}\\right),\n\\] where \\(\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\\) is the \\((p\\times p)\\) Fisher information matrix with \\(H_{\\ell_n}(\\theta_0)\\) denoting the Hesse matrix of \\(\\ell_n(\\cdot)\\) evaluated at \\(\\theta_0.\\)\n\n\n\n\n\n\n\n\nML-Theory and Machine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020))."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#equivariance-property-of-the-ml-estimator",
    "href": "Ch1_MaximumLikelihood.html#equivariance-property-of-the-ml-estimator",
    "title": "1  Maximum Likelihood",
    "section": "1.5 Equivariance Property of the ML-Estimator",
    "text": "1.5 Equivariance Property of the ML-Estimator\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.3 (Equivariance (or Invariance) Property of the ML-Estimator) Let \\[\n\\tau_0=g(\\theta_0),\n\\] where \\(g(\\theta)\\) is a one-to-one (or injective) function of \\(\\theta\\in\\Theta;\\) i.e. \\(g\\) maps distinct elements of its domain to distinct elements such that the outputs never repeat. (One-to-one functions are invertible.)\nLet \\(\\hat{\\theta}_{n}\\arg\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta)\\) be the maximum likelihood estimator of \\(\\theta_0.\\) Then \\[\n\\hat{\\tau}_{n} = g\\left(\\hat{\\theta}_{n}\\right)\n\\] is the maximum likelihood estimator of \\(\\tau_0.\\)\n\n\n\nExample: \\(g(\\theta)=\\theta + 5.\\)\nProof of Theorem 1.3:\nSince \\(g\\) is a one-to-one function, it possesses an inverse \\(g^{-1}.\\)\nThus \\[\n\\hat{\\theta}_{n} = g^{-1}(\\hat{\\tau}_{n})\n\\] and \\[\n\\theta = g^{-1}(\\tau)\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\] This allows us to express the likelihood function \\(\\mathcal{L}_n(\\theta)\\) used for estimating \\(\\theta_0\\) in terms of a likelihood function \\(\\tilde{\\mathcal{L}}_n(\\tau)\\) in \\(\\tau\\): \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta)\n& = \\mathcal{L}_n(g^{-1}(\\tau))\\\\[2ex]\n& = \\prod_{i=1}^n f(X_i|)\\\\[2ex]\n& = \\tilde{\\mathcal{L}}_n(\\tau)\\\\[2ex]\n\\end{align*}\n\\] for all \\(\\theta\\in\\Theta.\\) At \\(\\hat{\\theta}_n\\) we thus have \\[\n\\mathcal{L}_n(\\hat{\\theta}_n) = \\mathcal{L}_n(\\overbrace{g^{-1}(\\hat{\\tau}_n)}^{=\\hat{\\theta}_n})=\\tilde{\\mathcal{L}}_n(\\hat{\\tau}_n).\n\\] To show now that \\(\\hat\\tau\\) is the maximum likelihood estimator of \\(\\tau_0,\\) we need to show that \\[\n\\tilde{\\mathcal{L}}_n(\\tau)\\leq\\tilde{\\mathcal{L}}_n(\\hat\\tau_n)\\quad\\text{for all}\\quad\\tau.\n\\]\nSince \\(\\hat\\theta_n\\) denotes the ML estimator of \\(\\theta_0,\\) we know that \\[\n\\mathcal{L}_n(\\theta)\\leq\\mathcal{L}_n(\\hat\\theta_n)\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\] But since for any \\(\\tau=g(\\theta)\\) \\[\n\\tilde{\\mathcal{L}}(\\tau)   = \\mathcal{L}(\\theta)\n\\leq\n\\mathcal{L}(\\hat{\\theta}_n) = \\tilde{\\mathcal{L}}(\\hat{\\tau}_{n}),\n\\] we know also that \\[\n\\tilde{\\mathcal{L}}_n(\\tau)\\leq\\tilde{\\mathcal{L}}_n(\\hat\\tau_n)\\quad\\text{for all}\\quad\\tau;\n\\] i.e., \\(\\hat{\\tau}_n\\) is indeed a maximum likelihood estimator of \\(\\tau_0.\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#exercises",
    "href": "Ch1_MaximumLikelihood.html#exercises",
    "title": "1  Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nProgram the Newton-Raphson algorithm for a numerical computation of the ML estimate \\(\\hat\\theta\\) of the parameter \\(\\theta=P(\\text{Coin}=\\texttt{HEAD})\\) in our coin toss example of this chapter. Replicate the results shown in Table 1.1.\n\n\nExercise 2.\nAssume an i.i.d. random sample \\(X_1,\\dots,X_n\\) from an exponential distribution, i.e. the underlying density of \\(X_i\\) is given by \\[\nf(x;\\theta_0)=\n\\left\\{\\begin{array}{ll}\\theta_0\\exp(-\\theta_0 x),&x\\geq 0\\\\0,&x<0\\end{array}\\right.\n\\] with \\(\\theta_0>0,\\) where \\[\n\\mu:=\\mathbb{E}(X_i)=\\frac{1}{\\theta_0}\n\\] and \\[\nVar(X_i)=\\frac{1}{\\theta_0^2}.\n\\]\n\nWhat is the log-likelihood function for the i.i.d. random sample \\(X_1,\\dots,X_n\\)?\nDerive the maximum likelihood (ML) estimator \\(\\hat\\theta_n\\) of \\(\\theta_0.\\)\nFrom maximum likelihood theory we know that \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\to_d \\mathcal{N}\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right).\n\\] Derive the expression for the Fischer information \\(\\mathcal{I}(\\theta_0).\\) Use the Fisher information to give the explicit formula for the asymptotic distribution of \\(\\hat\\theta_n\\).\n\n\n\nExercise 3.\n\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Unif}(0,\\theta_0).\\)\n\nWhat is the likelihood function?\nWhat is the maximum likelihood estimator of \\(\\theta_0\\)?\n\n\n\nExercise 4.\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Poisson}(\\lambda_0).\\) That is \\(X\\sim f\\) with density function \\[\nf(x|\\lambda_0) = \\frac{\\lambda_0^x \\exp(-\\lambda_0)}{x!}.\n\\]\n\nFind the maximum likelihood estimator, \\(\\hat{\\lambda},\\) of \\(\\lambda_0.\\)\nLet \\(0<\\lambda_0\\leq 4.\\) Find the maximum likelihood estimator, \\(\\hat{P}(X=4),\\) of \\(P(X=4).\\)\n\n\n\nExercise 5.\nShow that the Newton-Raphson algorithm converges; i.e. that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\] under the setup outlined in Section 1.2.2.\nTip: Use the first-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with explicit reminder term \\(R\\) given by \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + R,\n\\end{align*}\n\\] where \\[\nR=\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\] for a mean-value \\(\\xi_{(m)}\\) between \\(\\theta_{(m)}\\) and \\(\\theta_{root}\\). This is called the Lagrange form of the Taylor-Series reminder term and follows from the Mean-Value Theorem Theorem 1.2."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#references",
    "href": "Ch1_MaximumLikelihood.html#references",
    "title": "1  Maximum Likelihood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017. “Overcoming Catastrophic Forgetting in Neural Networks.” Proceedings of the National Academy of Sciences 114 (13): 3521–26.\n\n\nMartens, James. 2020. “New Insights and Perspectives on the Natural Gradient Method.” The Journal of Machine Learning Research 21 (1): 5776–5851.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 1–25."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html",
    "href": "Ch2_EMAlgorithmus.html",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "",
    "text": "The Expectation Maximization (EM) algorithm is often used to simplify, or make possible, complex maximum likelihood estimation problems. In this chapter, we present the EM algorithm for estimating Gaussian mixture distributions, as this is probably its most well-known application. Even the original work on the EM algorithm (Dempster, Laird, and Rubin 1977) already dealt with the estimation of Gaussian mixture distributions."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "href": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models",
    "text": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models\nAs a data example we use the palmerpenguins data (Horst, Hill, and Gorman (2020)).\nThese data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (Figure 2.1). We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.\n\n\n\nFigure 2.1: Cheeky penguin in action.\n\n\nThe following code chunk prepares the data\n\n\n\n\n\n\nCaution\n\n\n\nWe have the information about the different penguin species (penguin_species) but in the following we pretend not to know this information.\nWe want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (penguin_flipper) alone.\nAfterwards we can use the data in penguin_species to check how good our cluster analysis is.\n\n\n\n## Select a color palette\ncol_v <- RColorBrewer::brewer.pal(n = 3, name = \"Set2\")\n\n## Preparing the data:\npenguins <- palmerpenguins::penguins %>%  # penguin data\n  tidyr::as_tibble() %>%                  # 'tibble'-dataframe\n  dplyr::filter(species!=\"Adelie\") %>%    # remove penguin species 'Adelie' \n  droplevels() %>%                        # remove the non-used factor level\n  tidyr::drop_na() %>%                    # remove NAs\n  dplyr::mutate(species = species,        # rename variables \n                flipper = flipper_length_mm) %>% \n  dplyr::select(species, flipper)         # select variables \n\n##  \nn      <- nrow(penguins)                  # sample size\n\n## Pulling out the variable 'penguin_species':\npenguin_species <- dplyr::pull(penguins, species)\n\n## Pulling out the variable 'penguin_flipper':\npenguin_flipper <- dplyr::pull(penguins, flipper)\n\n## Plot\n## Histogramm:\nhist(x = penguin_flipper, freq = FALSE, \n     xlab=\"Flipper-Length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))\n## Stipchart hinzufügen:\nstripchart(x = penguin_flipper, method = \"jitter\", \n           jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[3],.5), \n           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)\n\n\n\n\n\n\n\n\n\nClustering using Gaussian Mixture Distributions\n\nEstimate the Gaussian mixture distribution using the EM algorithm\nAssign the data points \\(x_i\\) to the group that maximizes the “posterior probability” (see Figure 2.2 and Section 2.3.2)\n\n\n\n\n\n\nFigure 2.2: Cluster analysis based on a mixture distribution with two weighted normal distributions.\n\n\n\n\nFigure Figure 2.2 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. Cluster result: 95% of the penguins could be correctly assigned - based only on their flipper lengths.\nThe following R codes can be used to reproduce the above cluster analysis (using the R package mclust) and Figure 2.2. We’ll learn everything about it in this chapter:\n\n## mclust R package:\n## Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(\"mclust\"))\n\n## Number of Groups\nG <- 2 \n\n## Schätzung des Gaußschen Mischmodells (per EM Algorithmus)\n## und Clusteranalyse\nmclust_obj <- mclust::Mclust(data       = penguin_flipper, \n                             G          = G, \n                             modelNames = \"V\", \n                             verbose    = FALSE)\n\n# summary(mclust_obj)\n# str(mclust_obj)\n\n## estimated group assignment \nclass <- mclust_obj$classification\n\n## Fraction of correct group assignments:\n# cbind(class, penguin_species)\nround(sum(class == as.numeric(penguin_species))/n, 2)\n\n## estimated means of the two Gaussian distributions\nmean_m <- t(mclust_obj$parameters$mean)\n\n## estimated variances (and possibly covariances) \ncov_l  <- list(\"Cov1\" = mclust_obj$parameters$variance$sigmasq[1], \n               \"Cov2\" = mclust_obj$parameters$variance$sigmasq[2])\n\n## estimated mixture weights (prior-probabilities) \nprop_v <- mclust_obj$parameters$pro\n\n## evaluating the Gaussian mixture density function \nnp      <- 100 # number of evaluation points\nxxd     <- seq(min(penguin_flipper)-3, \n               max(penguin_flipper)+5, \n               length.out = np)\n## mixture density\nyyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +\n           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n## single densities\nyyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]\nyyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n\n## Plot\nhist(x = penguin_flipper, xlab=\"Flipper length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))\nlines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))\nlines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)\nlines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)\nabline(v=203.1, lty=3)\nstripchart(penguin_flipper[class==1], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)\nstripchart(penguin_flipper[class==2], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "href": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions",
    "text": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions\n\n2.2.1 Gaussian Mixture Models (GMM)\nWe denote a random variable \\(X\\) that follows a Gaussian mixed distribution as \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\]\nThe corresponding density function of a Gaussian mixture distribution is defined as follows: \\[\nf_{GMM}(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g \\varphi(x|\\mu_g,\\sigma_g)\n\\tag{2.1}\\]\n\nWeights: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) with \\(\\pi_g>0\\) and \\(\\sum_{g=1}^G\\pi_g=1\\)\nMeans: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) with \\(\\mu_g\\in\\mathbb{R}\\)\nStandard deviations: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) with \\(\\sigma_g>0\\)\nNormal density of group \\(g=1,\\dots,G\\): \\[\n\\varphi(x|\\mu_g,\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right)\n\\]\nUnknown parameters: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)\n\n\n\n2.2.2 Maximum Likelihood (ML) Estimation\nWe could try to estimate the unknown parameters \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) using the maximum likelihood method.\n\nI’ll say it right away: The attempt will fail.\n\n\nBasic Idea of ML Estimation\n\nAssumption: The data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) is a realization of a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] with \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}).\n\\]\n\n\nThat is, in a certain sense, the observed data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) “know” the unknown parameters \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) and we “only” have to elicit this information from them.\n\n\nEstimation Idea: Choose \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) such that \\(f_{GMM}(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) “optimally” fits the observed data \\(\\mathbf{x}\\).\nImplementation of the Estimation Idea: Maximize (with respect to \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\)) the likelihood function \\[\n\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_{GMM}(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\] Or maximize the log-likelihood function (simpler maximization) \\[\n\\begin{align*}\n%\\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)=\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n=&\\sum_{i=1}^n\\ln\\left(f_{GMM}(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\\n=&\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\n\\end{align*}\n\\tag{2.2}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe maximization must take into account the parameter constraints in Equation 2.1; namely, \\(\\sigma_g>0\\) and \\(\\pi_g>0\\) for all \\(g=1,\\dots,G\\) and \\(\\sum_{g=1}^G\\pi_g=1\\).\n\n\nThe maximizing parameter values \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\hat{\\boldsymbol{\\sigma}}\\) are the ML-Estimators:\n\\[\n(\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n\\]\n😒 Problems with singularities in numerical solutions: If one tries to solve the above maximization problem numerically with the help of the computer, one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities.\nFor real GMMs (i.e. \\(G>1\\)), problems with singularities occur very easily during a numerical maximization. This happens whenever one (or more) of the normal distribution component(s) tries to describe only single data points. This leads to a Gaussian density function centered around a single data point \\(x_i\\) such that\n\\[\n\\mu_g=x_i.\n\\] and \\[\n\\sigma_g\\to 0.\n\\] This degenerating situation leads to very large density function values, \\[\n\\varphi(x_i|\\mu_g=x_i,\\sigma_g)\\to\\infty\\quad\\text{for}\\quad \\sigma_g\\to 0,\n\\] and thus maximize the log-likelihood in an undesirable way (see Figure 2.3). Such undesirable, trivial maximization solutions typically lead to implausible estimation results.\n\n\n\n\n\nFigure 2.3: Gaussian density with \\(\\mu_g=x_i\\) for \\(\\sigma_g\\to 0\\).\n\n\n\n\n🤓 Analytic solution: It is a bit tedious, but one can maximize the log-likelihood function of the GMM (see Equation 2.2) analytically. If you do this, you will get the following expressions: \\[\n\\begin{align*}\n\\hat\\pi_g&=\\frac{1}{n}\\sum_{i=1}^np_{ig},\\quad\n\\hat\\mu_g=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\[2ex]\n\\hat\\sigma_g&=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2},\n\\end{align*}\n\\tag{2.3}\\] where \\[\np_{ig}=\\frac{\\pi_g\\varphi(x_i|\\mu_g,\\sigma_g)}{f_{GMM}(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\tag{2.4}\\] for \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G\\).\n\n\n\n\n\n\nNote\n\n\n\nDeriving the above expressions for \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) and \\(\\hat{\\pi}_g\\) is really a bit tedious (multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints) but in principle doable.\n\n\n\n🙈 However: The above expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) and \\(\\hat\\sigma_g\\) depend themselves on the unknown parameters\n\n\\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\),\n\\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and\n\\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\),\n\nbecause the probabilities \\(0\\leq p_{ig}\\leq 1\\) (defined in Equation 2.4) depend on these unknown parameters.\n\nThe probability \\(p_{ig}\\) in Equation 2.4 is called the posterior probability. The posterior probability \\(p_{ig}\\) is the probability that penguine \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).\nThe probability \\(\\pi_g\\) in Equation 2.4 is called the prior probability. The prior probability \\(\\pi_g\\) is the probability that a penguine \\(i\\), from which we know nothing about its flipper length, belongs to group \\(g\\).\n\n\n\n\n\n\n\nNote\n\n\n\nStill, we cannot properly define what we mean by “prior probability” and “posterior probability”. We’ll discuss the prior and the posterior probability in more detail in Section 2.3.2.\n\n\nThus, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3 do not allow direct estimation of the unknown parameters \\(\\pi_g\\), \\(\\mu_g\\), and \\(\\sigma_g\\).\n🥳 Solution: The EM Algorithm\n\n\n\n2.2.3 The EM Algorithm for GMMs\nThe expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3, however, suggest a simple iterative maximum likelihood estimation procedure: An alternating estimation of\n\nthe (unknown) posterior probabilities \\[\np_{ig}\\quad\\text{for}\\quad i=1,\\dots,n,\\; g=1,\\dots,G\n\\]\n\nand of\n\nthe (unknown) parameters \\[\n(\\pi_g,\\mu_g,\\sigma_g) \\quad\\text{for}\\quad g=1,\\dots,G\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nOnce you know \\(p_{ig},\\) you can compute \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) using Equation 2.3.\nOnce you know \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) you can compute \\(p_{ig}\\) using Equation 2.4.\n\n\nThe EM Algorithm:\n\nInitialization:  Set starting values \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\)\nLoop:  For \\(r=1,2,\\dots\\)\n\n(Expectation) Compute: \\[p_{ig}^{(r-1)}=\\frac{\\pi_g^{(r-1)}\\varphi(x_i|\\mu^{(r-1)}_g,\\sigma_g^{(r-1)})}{f_{GMM}(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\]\n(Maximization) Compute:\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence:  Stop if the value of the maximized log-likelihood function, \\(\\ell(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)}|\\mathbf{x})\\), does not change anymore substantially.\n\nThe above pseudo code is implemented in the following code chunk:\n\nlibrary(\"MASS\")\nlibrary(\"mclust\")\n\n## data:\nx <- cbind(penguin_flipper) # data [n x d]-dimensional. \nd <- ncol(x)                # dimension (d=1: univariat)\nn <- nrow(x)                # sample size\nG <- 2                      # number of groups\n\n## further stuff \nllk       <- matrix(NA, n, G)\np         <- matrix(NA, n, G)  \nloglikOld <- 1e07\ntol       <- 1e-05\nit        <- 0\ncheck     <- TRUE \n\n## EM Algorithm\n\n## 1. Starting values for pi, mu and sigma:\npi    <- rep(1/G, G)              # naive pi \nsigma <- array(diag(d), c(d,d,G)) # varianz = 1\nmu    <- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )\n\nwhile(check){\n  \n  ## 2.a Expectation step\n  for(g in 1:G){\n    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  p <- sweep(p, 1, STATS = rowSums(p), FUN = \"/\")\n  \n  ## 2.b Maximization step \n  par   <- mclust::covw(x, p, normalize = FALSE)\n  mu    <- par$mean\n  sigma <- par$S\n  pi    <- colMeans(p)\n  \n  ## 3. Check convergence \n  for(g in 1:G) {\n    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  loglik <- sum(log(rowSums(llk))) # current max. log-likelihood value\n  ##\n  diff      <- abs(loglik - loglikOld)/abs(loglik) # rate of change\n  loglikOld <- loglik\n  it        <- it + 1\n  ## Check whether rate of change is still large enough (> tol)?\n  check     <- diff > tol\n}\n\n## Estimation results:\nresults <- matrix(c(pi, mu, sqrt(sigma)), \n                  nrow = 3, \n                  ncol = 2, \n                  byrow = TRUE,\n                  dimnames = list(c(\"weights\", \n                                    \"means\", \n                                    \"standard-deviations\"),\n                                  c(\"group 1\", \n                                    \"group 2\"))) \n##\nresults %>% round(., 2)\n\n                    group 1 group 2\nweights                0.69    0.31\nmeans                216.19  194.24\nstandard-deviations    7.32    6.25"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "href": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\n=\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\nMoreover, so far, we were not able to propertly state what we mean by prior and posterior probability.\n\nAll this can be solved my adding unobserved group labels as missing variables.\n\n\n\n2.3.1 Completion of the Data\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) where, however, it must hold true that \\[\n\\sum_{g=1}^Gz_{ig}=1.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\n\\] means that for each (penguin) \\(i\\) there is only one group. This is an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x|\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot|\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a (unobserved) realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] for each \\(i=1,\\dots,n.\\)\nNote that the condition \\[\n\\sum_{g=1}^GZ_{ig}=1\n\\] implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\(\\pi_g\\) If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\(p_{ig}\\) If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem (see Equation 2.5) which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).”\n\n\n\n\n\n\n\n\nBayes’ Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i|\\mu_g,\\sigma_g)}{f_{GMM}(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i|\\mu_g,\\sigma_g)}{f_{GMM}(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere’s the Expection in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) in the Expectation-step of the EM algorithm (Section 2.2.3) is indeed a computation of an expectation.\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e. the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i|\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation 2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: we can directly calculate the logarithm of the normal distribution. This simplifies the maximization problem considerably, since the normal distribution belongs to the exponential family (see Section 1.4) which is not the case for the normal mixture distribution.\nWe do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but we know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, we can calculate the conditional expected value (using Equation 2.6), which motivates the “Expectation”-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z}))\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\]\nThe following EM algorithm differs only in notation from the version already discussed in Section 2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step. Moreover, the chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems.\nIn the following, the parameter vector \\((\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) will be denoted as \\(\\boldsymbol{\\theta}\\) for simplicity. \n\nInitialization: Set starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z}))\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i|\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\np_{ig}^{(r-1)} = \\frac{\\pi_g^{(r-1)} \\varphi(x_i|\\mu_g^{(r-1)},\\sigma_g^{(r-1)})}{f_{GMM}(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function, \\[\n\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)}),\n\\] does not change anymore substantially."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "href": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 (Unsupervised) Classification",
    "text": "2.4 (Unsupervised) Classification\nThe problem of predicting a discrete random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] where \\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and \\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n(Note: Above we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.)\nExample: Predict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\nA classification rule is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\nIf there are learning/training data with group-labels \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nIf there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] From Bayes’s theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where \\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown quantities and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nBut well, the average penguin probably doesn’t care about the EM Algorithm.\n\n\n\nFigure 2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#exercises",
    "href": "Ch2_EMAlgorithmus.html#exercises",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\n\nConsider \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\sim\\text{Bernoulli}(p).\\) Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nNow let \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\) is a Bernoulli mixture random variable with parameters \\(p_g\\) and prior probabilities \\(\\pi_g\\), \\(g=1,\\dots,G\\). Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nLet \\((Z_{i1},\\dots,Z_{iG})\\in\\{0,1\\}^G\\) be the vector of latent group indicator random variables with \\(Z_{i1}+\\dots + Z_{iG}=1\\) and \\(P(Z_{ig}=1)=\\pi_g\\), \\(g=1,\\dots,G\\). Thus, the realization \\(z_i=(0,1,0,\\dots,0)\\) means that the \\(i\\)th observation comes from the \\(2\\)nd Bernoulli distribution \\(\\text{Bernoulli}(p_2)\\). Write the expressions of the (log) likelihood functions \\(\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\) and \\(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\) that take into account the latend group indicator random variables.\nWrite down the expression for the posterior probability \\[\n\\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i).\n\\]\nDerive the expectation of \\(\\tilde\\ell,\\) \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\).\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\) with respect to \\(p_g\\) for \\(g=1,\\dots,G.\\)\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x},\\mathbf{Z})\\right)\\) with respect to \\(\\pi_g\\) for \\(g=1,\\dots,G\\) such that \\(\\sum_{g=1}^G\\pi_g=1.\\)\nSketch the EM-Algorithm"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#references",
    "href": "Ch2_EMAlgorithmus.html#references",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218."
  }
]