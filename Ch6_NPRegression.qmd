# Nonparametric Regression

## Introduction

Let us consider the case of univariate nonparametric regression, i.e., with one single explanatory variable $X\in\mathbb{R}$.

**Data:** 
$$
(Y_{1},X_{1}),\dots,(Y_{n},X_{n})\overset{\text{i.i.d}}{\sim}(Y,X)
$$

* $Y_{i}$ response variable
* $X_{i}\in [a,b]\subset \mathbb{R}$ explanatory  variable
* $n$ sufficiently large (e.g., $n\geq 40$)


#### The Nonparametric Regression Model {-}

$$
Y_i=m(X_i)+\varepsilon_i
$$

* $m(X)=\mathbb{E}(Y_i|X=X_i)$ regression function
* $\mathbb{E}(\varepsilon_i)=0$, $Var(\varepsilon_i)=\sigma^2$
* $\varepsilon_i$  independent of $X_i$ or at least $\mathbb{E}(\varepsilon_i|X_i)=0$


Special cases of **parametric**  regression models:

* Linear regression: $m(x)$ is a straight line
$$
m(X)=\beta_0+\beta_1 X
$$

* Possible generalizations: $m(x)$ quadratic or cubic polynomial
$$
\begin{align*}
                m(X)&=\beta_0 +\beta_1 X+\beta_2 X^2\\
\text{or} \quad m(X)&=\beta_0+\beta_1 X+\beta_2 X^2+\beta_3 X^3
\end{align*}
$$

Many important applications lead to regression functions possessing a complicated structure. Standard models then are "too simple" and do not provide useful approximations of $m(x)$.

::: {.callout-tip}
# As George Box is saying it:
"All models are false, but some are useful" (G. Box)
:::



An important point in theoretical analysis is the way how the observations $X_1,\dots,X_n$ have been generated. One distinguishes between "fixed" and "random design".

* **Fixed design:** The observation points $X_1,\dots,X_n$ are fixed  (non stochastic) values.
   * **Example:** Crop yield ($Y$) in dependence of the amount of fertilizer ($X$) used.
* **Equidistant Design:**  (Most important special case of fixed design)  
   $$
   X_{i+1}-X_i=\frac{b-a}{n}.
   $$
* **Random design:** The observation points $X_1,\dots,X_n$ are (realizations of) i.i.d. random variables with density $f$. The density $f$ is called "design density". Throughout this chapter it will be assumed that $f(x)>0$ for all $x\in [a,b]$.
   * **Example:** Sample $(Y_1,X_1),\dots,(Y_n,X_n)$ of log-wages ($Y$) and age ($X$) of randomly selected individuals.


In the case of random design, $m(x)$ is the *conditional* expectation of $Y$ given $X=x$,
$$
m(x)=\mathbb{E}(Y|\ X=x)
$$

and $Var(\varepsilon_i|X_i)=\sigma^2$.

::: {.callout-note}
For random design all expectations (as well as variances) have to be interpreted as *conditional* expectations (variances) given $X_1,\dots,X_n$.
:::


**Example:** Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13); see @fig-CanadianIncome. 

```{r}
#| label: fig-CanadianIncome
#| fig-cap: Canadian cross-section wage data.
suppressPackageStartupMessages(library("np"))
data("cps71")
plot(cps71$age, cps71$logwage, xlab="Age", ylab="log(wage)")
```

What would be a good/reasonable model assumption for $m(x)$ to estimate the conditional mean function for the data shown in @fig-CanadianIncome?


::: {.callout-important}

# No specific model assumption

There are no specific assumptions about the structure of the regression function. It is only assumed that $m$ is **smooth** (sufficiently often differentiable).
:::


## Basis function expansions

Some frequently used approaches to nonparametric regression rely on expansions of the form
$$
m(x)\approx \sum_{j=1}^p \beta_j b_j(x),
$$
where $b_1(x),b_2(x),\dots$ are suitable basis functions. 


The basis functions $b_1,b_2,\dots$ have to be chosen in such a way that for *any possible* smooth function $m$ the approximation error 
$$
\min_\beta \left|m(x)-\sum_{j=1}^p \beta_j b_j(x)\right|^a
$$ 
tends to zero as $p\rightarrow\infty$ (approximation theory) for some $a\geq 1$; typically $a=2$.


For a fixed value $p$ an estimator $\hat m_p$ is determined by
$$
\hat m(x)=\sum_{j=1}^p \hat\beta_j b_j(x),
$$
where the coefficients  $\hat\beta_j$ are obtained by ordinary least squares
$$
\sum_{i=1}^n \left( Y_i-\sum_{j=1}^p \hat\beta_j \underbrace{b_j(X_i)}_{X_{ij}}\right)^2
=\min_{\beta_1,\dots,\beta_p} \sum_{i=1}^n \left( Y_i-\sum_{j=1}^p \beta_j \underbrace{b_j(X_i)}_{X_{ij}}\right)^2
$$


Examples are approximations by polynomials, spline functions, wavelets or Fourier expansions (for periodic functions).


### Polynomial Regression

**Theoretical Justification:** Every smooth function can be well approximated by a polynomial of sufficiently high degree.

**Approach:**

* Choose $p$ and fit a polynomial of degree $p$:
  $$
  \min_{\beta_1,\dots,\beta_p}\sum_{i=1}^n \left(Y_i-\sum_{j=1}^p {\beta}_{j} X^{j-1}\right)^2
  $$
  $$
  \Rightarrow\quad {\hat m}_p(X)={\hat \beta}_{1}+\sum_{j=2}^{p-1}
  {\hat \beta}_{j} X_i^{j-1}
  $$
* This corresponds to an approximation with basis functions
  $$
  b_1(x)=1, b_2(x)=x, b_3(x)=x^2, \dots, b_{p}(x)=x^{p-1}.
  $$

**Note:** It is only assumed that $m$ is well approximated by a polynomial of degree $p$. That is, there will usually still exist an **approximation error** (i.e., bias of $\hat{m}_p \neq 0$).


**R-Code to compute polynomial regressions:**

Generate some data:

```{r}
#| eval: true
#| echo: true
set.seed(1)
# Generate some data: 
n      <- 100     # Sample Size
x_vec  <- (1:n)/n # Equidistant X 
# Gaussian iid error term 
e_vec  <- rnorm(n = n, mean = 0, sd = .5)
# Dependent variable Y
y_vec  <-  sin(x_vec * 5) + e_vec
# Save all in a dataframe
db     <-  data.frame(x=x_vec,y=y_vec)
```

Compute the ordinary least squares regressions of different polynomial regression models:

```{r}
#| eval: true
#| echo: true
# Fitting of polynomials to the data (parametric models):
# Constant line fit: (Basis function x^0)
reg_p1 <- lm(y ~ 1, data=db)
# Basis functions: x^0 + ... + x^3
reg_p3 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)
# Basis functions: x^0 + ... + x^6
reg_p6 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)
```

Take a look at the fits:

```{r}
par(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))
plot(db, main="Truth")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
plot(db, main="Degree 0")
lines(y = predict(reg_p1, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
plot(db, main="Degree 3")
lines(y = predict(reg_p3, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
plot(db, main="Degree 6")
lines(y = predict(reg_p6, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
```


The quality of the approximation obviously depends on the choice of $p$ which serves as a "smoothing parameter"


```{r}
m_true    <- sin(x_vec * 5)

n_MCrepl  <- 200 # MC-replications

m_hat_p1  <- matrix(NA, n, n_MCrepl)
m_hat_p3  <- matrix(NA, n, n_MCrepl)
m_hat_p6  <- matrix(NA, n, n_MCrepl)

for(r in 1:n_MCrepl){
    # Generate some data: 
    e_vec  <- rnorm(n = n, mean = 0, sd = .5)
    y_vec  <-  sin(x_vec * 5) + e_vec
    db     <-  data.frame(x = x_vec,y = y_vec)
    # Estimations
    reg_p1 <- lm(y ~ 1, data=db)
    reg_p3 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)
    reg_p6 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)
    # Save predictions (y hat)
    m_hat_p1[,r] <- predict(reg_p1, newdata = db)
    m_hat_p3[,r] <- predict(reg_p3, newdata = db)
    m_hat_p6[,r] <- predict(reg_p6, newdata = db)
}

par(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))
plot(db, main="Truth")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
subSelect <- 25
matplot(y = m_hat_p1[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",  
        ylim = range(m_hat_p1[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=0.5, main = "Degree p=0")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
matplot(y = m_hat_p3[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",
        ylim = range(m_hat_p3[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=.5, main = "Degree p=3")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
matplot(y = m_hat_p6[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",
        ylim = range(m_hat_p6[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=.5, main = "Degree p=6")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
```



```{r}
## Pointwise (for each x) biases of \hat{m}(x): 
Pt_Bias_p1 <- rowMeans(m_hat_p1) - m_true
Pt_Bias_p3 <- rowMeans(m_hat_p3) - m_true
Pt_Bias_p6 <- rowMeans(m_hat_p6) - m_true

## Pointwise (for each x) variances \hat{m}(x):
Pt_Var_p1  <- apply(m_hat_p1, 1, var)
Pt_Var_p3  <- apply(m_hat_p3, 1, var)
Pt_Var_p6  <- apply(m_hat_p6, 1, var)

par(mfrow=c(1,2))
matplot(y = cbind(Pt_Bias_p1, Pt_Bias_p3, Pt_Bias_p6), 
        x = x_vec, type = "l", 
        main = "Pointwise Bias", ylab="", xlab="x")
legend("topleft", col = c(1,2,3), lty = c(1,2,3), 
legend = c("Degree p=0", "Degree p=3", "Degree p=6"))
matplot(y = cbind(Pt_Var_p1, Pt_Var_p3, Pt_Var_p6), 
        x = x_vec, type = "l", 
        main = "Pointwise Variance", ylab="", xlab="x")
legend("top", col = c(1,2,3), lty = c(1,2,3), 
legend = c("Degree p=0", "Degree p=3", "Degree p=6"))
```

**The Bias-Variance Trade-Off:**

* $p$ small: variability of the estimator is small, but there may exist a high systematic error (bias).
* $p$ large: bias is small, but variability of the estimator is high.

::: {.callout-note}
# Remark

Polynomial regression is not very popular in practice. Reasons are numerical problems in fitting high dimensional polynomials. Furthermore, high order polynomials often posses an erratic, difficult to interpret behavior at the boundaries.
:::

### Regression Splines


The practical disadvantages of global basis functions (like polynomials), explain the success of local basis functions. A frequently used system of basis functions are **local polynomials**, i.e., so-called "spline functions".  


A spline function is a *piece wise* polynomial function. They are defined with respect to a pre-specified sequence of $q$ "knots" 
$$
a=\tau_1<\tau_2\leq\dots\leq \tau_{q-1}<\tau_q=b.
$$ 
Different specifications of the knot sequence lead to different splines.


More precisely, for a given knot sequence a spline function $s(x)$ of degree $k$ is defined by the following properties:

* $s(x)$ is a polynomial of degree $k$ in every interval $[\tau_j,\tau_{j+1}]$, i.e.
$$
s(x)=s_0+s_1x+s_2x^2+\dots+s_kx^{k},\quad x\in[\tau_j,\tau_{j+1}]
$$
with $s_0,\dots,s_k\in\mathbb{R}.$
* $s(x)$ is  $k-1$ times continuously differentiable at each knot point $x=\tau_j$, $j=1,\dots,q$.

   * $s(x)$ is called a *linear spline* if $k=1$

   * $s(x)$ is a *quadratic spline* if $k=2$

   * $s(x)$ is a *cubic spline*  if $k=3$.

In practice, the most frequently used splines are **cubic** spline functions based on an equidistant sequence of $q$ knots, i.e.,
$$
\tau_{j+1}-\tau_j=\tau_j-\tau_{j-1}\quad\text{for all } j.
$$


The space of all spline functions of degree $k$ defined with respect to a given knot sequence
$$
a=\tau_1<\tau_2\leq\dots\leq \tau_{q-1}<\tau_q=b
$$ 
is a 
$$
p:=q+k-1
$$ 
dimensional linear function space
$$
{\cal{S}}_{k,\tau_1,\dots,\tau_q}=\operatorname{span}(b_{1,k},\dots,b_{p,k}),
$$
where $b_{1,k},\dots,b_{p,k}$ are **basis-functions**.


<!-- Possible basis functions are
$\tilde{b}_1(x)=1,\tilde{b}_2(x)=x,\dots,\tilde{b}_{k}=x^{k-1},\tilde{b}_{k+1}=(x-\tau_1)^k_+,\dots,
\tilde{b}_{k+q-1}=(x-\tau_{q-1})^k_+$, where
$$
(x-\tau_j)^k_+=
\left\{ \begin{matrix}  (x-\tau_j)^k & \text{ if } x\geq  \tau_j\\
0 & \text{ else} \end{matrix}\right.
$$
Each spline function $s\in {\cal{S}}_{k,\tau_1,\dots,\tau_q}$ can then be written as
$$
s(x)=\sum_{j=1}^{k} \beta_{j} x^{j-1} +\sum_{j=1}^{q-1}\beta_{j+k}(x-\tau_j)^k_+\quad\text{ for } x\in[a,b]
$$
and suitable parameters $\beta_1,\dots,\beta_{k+q-2}$. -->


#### B-Spline Basis Functions {-} 

The so-called **B-spline** basis functions are almost always used in practice, since they possess a number of advantages from a numerical point of view.


The B-Spline basis functions 
$$
b_{j,k},\quad j=1,\dots,p=q+k-1,
$$ 
for splines of order $k$ based on a knot sequence 
$$
a=\tau_1<\tau_2\leq\dots\leq \tau_{q-1}<\tau_q=b
$$ 
are calculated by a recursive procedure:
$$
b_{j,0}(x)=\left\{ \begin{matrix}  1 & \text{ if } \tau_{j}^*\leq x <\tau_{j+1}^*\\
0 & \text{ else} \end{matrix}\right., 
$$
for $j=1,\dots,p=q+2k-1.$


And for $l=1,\dots,k$
$$
b_{j,l}(x)=\frac{x-\tau_j^*}{\tau_{l+j}^*-\tau_j^*}b_{j,l-1}(x)+
\frac{\tau_{l+j+1}^*-x}{\tau_{l+j+1}^*-\tau_{j+1}^*}b_{j+1,l-1}(x),
$$
$j=1,\dots,q+k-1$, and $x\in [a,b],$

where

* $\tau_1^*=\dots=\tau_{k+1}^*=\tau_1$ (One left boundary knot $+$ $k$ replications), 
* $\tau_{k+2}^*=\tau_2,\dots,\tau^*_{k+q}=\tau_q$ ($q-2$ interior knots) and 
* $\tau_{k+q+1}^*=\dots=\tau_{2k+q}^*=\tau_q$ (One right boundary knot $+$ $k$ replications).


The following `R` code generates  
$$
p=\underbrace{\texttt{Numbr.of Knots}}_{q=7} + \underbrace{\texttt{degree}}_{k=3} - 1=9
$$ 
cubic B-spline basis functions. Let's take a look at them:

```{r}
library("splines2")

degree         <- 3
ord            <- degree + 1
internal_knots <- seq(from = 0.1, to = 0.9, by = 0.2)
boundary_knots <- c(0, 1)
all_knots      <- sort(c(rep(boundary_knots[1], ord), 
                         internal_knots, 
                         rep(boundary_knots[2], ord)))

## evaluation points
x_vec <- seq(from = boundary_knots[1], 
             to   = boundary_knots[2], len = 100)

X_mat_degree3 <- splines2::bSpline(
        x              = x_vec, 
        knots          = internal_knots, 
        degree         = degree,
        intercept      = TRUE, 
        Boundary.knots = boundary_knots
    )

matplot(x = x_vec, 
        y = X_mat_degree3, 
        type = "l", main = "B-Spline Basis Functions \nDegree 3",
        ylab = "", xlab = "x", axes = FALSE)
axis(1)
axis(2)
abline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), 
       col = "gray")
dim(X_mat_degree3)
```


#### Regression Splines {-}


The so-called "regression spline" (or "B-spline") approach to estimating a regression function $m(x)$ is based on fitting a set of spline basis functions to the data. 

Frequently, cubic splines ($k=3$) with equidistant knots are applied. 

* $\tau_1=a$ 
* $\tau_{j+1}=\tau_j + (b-a)/(q-1),\quad j=1,\dots,q-1$  
* such that $\tau_q=b$


In this case the number of knots $q,$ or more precisely the total number of basis functions 
$$
p=q+k-1
$$ 
with $k=3$ in the case of cubic B-splines serves as the **smoothing parameter** which has to be selected by the statistician.


An estimator $\hat{m}_p(x)$ is then given by
$$
\hat m_p(x)=\sum_{j=1}^p \hat\beta_j b_{j,k}(x),
$$
and the coefficients $\hat\beta_j$ are determined by ordinary least squares.



Let 
$$
Y=(Y_1,\dots,Y_n)^\top 
$$ 
denote the vector of response variables and let 
$$
\mathbf{X}
$$ 
denote the $n\times p$ matrix with elements 
$$
X_{ij}=b_{j,k}(X_i),\quad i=1,\dots,n,\quad j=1,\dots,p.
$$


Then the OLS estimator
$$
\hat \beta=(\hat\beta_1,\dots,\hat\beta_p)^\top
$$ 
can be written as
$$
\hat\beta=(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top Y.
$$
The fitted values are given by
$$
% \left(\begin{array}{c}
% {\hat Y}_1\\
% \vdots%\\ \cdot\\ \cdot
% \\ {\hat Y}_n
% \end{array}\right)=
\left(\begin{array}{c}
{\hat m}_p(X_1)\\
\vdots%\\ \cdot\\ \cdot
\\ {\hat m}_p(X_n)
\end{array}\right)=\mathbf{X}\hat\beta=\underbrace{\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top }_{=:S_p}Y
$$

The matrix $S_p$ is referred to as the **smoothing matrix** and the number of B-spline basis function $p$ is referred to as the **smoothing parameter**. 


::: {.callout-note}
# Remark

Quite generally, the most important nonparametric regression procedures are **linear smoothing methods**. This means that in dependence of some smoothing parameter (here $p$), estimates of the vector
$$
(m(X_1),\dots,m(X_n))^\top
$$ 
are obtained by multiplying a **smoother matrix** $S_p$ with $Y$. 

That is, 
$$
\left(\begin{array}{c}
m(X_1)\\
\vdots%\\ \cdot\\ \cdot
\\ m(X_n)
\end{array}\right)\approx
\left(\begin{array}{c}
{\hat m}_p(X_1)\\
\vdots%\\ \cdot\\ \cdot
\\ {\hat m}_p(X_n)
\end{array}\right)=S_p Y
$$
:::


`R` code to compute regression splines:

First, we generate some data.
```{r}
set.seed(1)
# Generate some data: #################
n      <- 100     # Sample Size
x_vec  <- (1:n)/n # Equidistant X 
# Gaussian iid error term 
e_vec  <- rnorm(n = n, mean = 0, sd = .5)
# Dependent variable Y
y_vec  <-  sin(x_vec * 5) + e_vec
```

Then, we generate cubic B-spline basis functions with (different) equidistant knot sequences and evaluate them at `x_vec`: 

```{r}
degree      <- 3 # piecewise cubic splines

knot_seq_5  <- seq(from = 0, to = 1, len = 5)# knots
X_mat_p7    <- splines2::bSpline(
        x              = x_vec, # evaluation points
        knots          = knot_seq_5[-c(1, length(knot_seq_5))], 
        degree         = degree,
        intercept      = TRUE, 
        Boundary.knots = knot_seq_5[ c(1, length(knot_seq_5))]
    )

knot_seq_15  <- seq(from = 0, to = 1, len = 15)# knots
X_mat_p17    <- splines2::bSpline(
        x              = x_vec, # evaluation points
        knots          = knot_seq_15[-c(1, length(knot_seq_15))], 
        degree         = degree,
        intercept      = TRUE, 
        Boundary.knots = knot_seq_15[ c(1, length(knot_seq_15))]
    )    
```

Compute the smoothing matrices $S_p$ for $p=7$ and $p=17$:

```{r}
S_p7  <- X_mat_p7  %*% solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) 
S_p17 <- X_mat_p17 %*% solve(t(X_mat_p17) %*% X_mat_p17) %*% t(X_mat_p17) 
```

Compute the estimates $\hat{m}_p(X_1),\dots,\hat{m}_p(X_n)$ for $p=7$ and $p=17$:

```{r}
m_hat_p7  <- S_p7  %*% y_vec
m_hat_p17 <- S_p17 %*% y_vec
```

Let's plot the results:

```{r}
plot(y=y_vec, x=x_vec, xlab="X", ylab="Y", 
     main="Regression Splines")
lines(y=sin(x_vec * 5), x=x_vec, col="red", lty=2, lwd=1.5)
lines(y=m_hat_p7, x=x_vec, col="blue", lwd=1.5)
lines(y=m_hat_p17, x=x_vec, col="darkorange", lwd=1.5)
legend("bottomleft", 
       c("(Unknown) Regression Function m", 
         "Regr.-Spline Fit with p=7", 
         "Regr.-Spline Fit with p=17"), 
       col=c("red","blue", "darkorange"), 
       lty=c(2,1,1), lwd=c(2,2,2))

```


### Mean Average Squared Error of Regression Splines

In a nonparametric regression context we do **not** assume that the unknown true regression function $m(x)$ exactly corresponds to a spline function. Thus, 
$$
\hat m_p=(\hat{m}_p(X_1),\dots,\hat{m}_p(X_n))^\top
$$ 
possesses a systematic estimation error (bias). That is, 
$$
\mathbb{E}_\varepsilon(\hat{m}_p(X_i))\neq m(X_i).
$$

To simplify notation, we will in the following write 
$$
\mathbb{E}_\varepsilon(\cdot)\quad\text{and}\quad Var_\varepsilon(\cdot)
$$ 
to denote expectation and variance "with respect to the random variable $\varepsilon$, only". 


In the case of random design,
$$
\mathbb{E}_\varepsilon(\cdot)\quad\text{and}\quad Var_\varepsilon(\cdot)
$$
thus denote the conditional expectation 
$$
\mathbb{E}(\cdot|X_1,\dots,X_n)
$$ 
and the conditional variance 
$$
Var(\cdot|X_1,\dots,X_n)
$$ 
given the observed $X$-values. 

For random design, these conditional expectations depend on the observed sample, and thus are random. For fixed design, such expectations are of course fixed values.


It will always be assumed that the matrix 
$$
\mathbf{X}^\top \mathbf{X},
$$ 
with $\mathbf{X}=(b_{j,k}(X_i))_{i,j}$, is invertible (under our conditions on the design density this holds with probability 1 for the random design).


The behavior of nonparametric function estimates is usually evaluated with respect to **quadratic risk**. 

A commonly used measure of accuracy of a spline estimator $\hat m_p$ is the **Mean Average Squared Error (MASE):**
$$
\begin{align*}
&\operatorname{MASE}(\hat m_p):=\frac{1}{n}\sum_{i=1}^n \mathbb{E}_\varepsilon\left(m(X_i)-\hat{m}_p(X_i)\right)^2\\
= &\frac{1}{n}\sum_{i=1}^n \underbrace{\left(\mathbb{E}_\varepsilon(\hat{m}_p(X_i))-m(X_i)\right)^2}_{(\operatorname{Bias}_\varepsilon(\hat{m}_p(X_i)))^2} + \\[2ex]
&\frac{1}{n}\sum_{i=1}^n \underbrace{\mathbb{E}_\varepsilon\left((\hat{m}_p(X_i)-\mathbb{E}_\varepsilon(\hat{m}_p(X_i))\right)^2}_{Var_\varepsilon(\hat{m}_p(X_i))}
\end{align*}
$$

Another frequently used measure is the **Mean Integrated Squared Error (MISE):**
$$
\begin{align*}
\operatorname{MISE}(\hat m_p):=\int_a^b \mathbb{E}_\varepsilon\left(m(x)-\hat m_p(x)\right)^2dx
\end{align*}
$$


##### MASE versus MISE: {-}

* Equidistant design:
$$
\operatorname{MISE}(\hat m_p)=\operatorname{MASE}(\hat m_p) + O(n^{-1})
$$
* MISE and MASE are generally not asymptotically equivalent in the case of random design
$$
\operatorname{MASE}(\hat m_p)=\int_a^b \mathbb{E}_\varepsilon\left(m(x)-\hat m_p(x)\right)^2 f(x)dx + O_P(n^{-1}).
$$

::: {.callout-note}
# Landau symbol "Big Oh" $O(r_n)$

$$
O(r_n)\quad\text{with}\quad r_n>0,\;n=1,2,\dots
$$
is a placeholder symbol describing the family of all sequences $x_n$, $n=1,2,\dots,$ such that

* $\dfrac{|x_n|}{r_n}\to c$ as $n\to\infty,$ where $c$ is a constant with $0\leq c < \infty$. 

Examples: 

* $-\dfrac{1}{n}=O(n^{-1})$
* $\dfrac{1}{n^2}=O(n^{-1})$
* $\displaystyle\frac{1}{m}\sum_{j=1}^{m-1} g\left(x_j\right)(x_{j+1}-x_j) = \int_a^b g(x)dx + O(m^{-1})$, <br> where $x_j=a+\dfrac{j-1}{m-1}(b-a)$ for sufficiently smooth (continuously differentiable over $(a,b)$) $g$. 
:::

::: {.callout-note}
# Landau symbol "Small oh" $o(r_n)$
$$
o(r_n)\quad\text{with}\quad r_n>0,\;n=1,2,\dots
$$
is a placeholder symbol describing the family of all sequences $x_n$, $n=1,2,\dots,$ such that

* $\dfrac{|x_n|}{r_n}\to 0$ as $n\to\infty.$

Examples: 

* $\dfrac{1}{n^2}=o(n^{-1})$
* $n^{-a}=o(n^{-b})$ for all $a>b>0.$


**Note:** For every sequence $x_n=o(r_n)$ it holds that $x_n=O(r_n),$ but not the other way round.
:::

::: {.callout-note}
# Special Cases $O(1)$ and $o(1)$

$$
O(1)\quad \text{and}\quad o(1)
$$

Examples: 

* $1 + \dfrac{1}{n} = O(1)$
* $\dfrac{1}{n^2}=o(1)$ 
:::


::: {.callout-note}
# Stochastic Landau symbol "Big Oh Pee" $O_P(r_n)$
$$
O_P(r_n)\quad\text{with}\quad r_n>0,\;n=1,2,\dots
$$
is a placeholder symbol describing the family of all *stochastic* sequences $X_n$, $n=1,2,\dots,$ such that 

* $\displaystyle P\left(\frac{|X_n|}{r_n}>\delta\right)<\epsilon$ for all sufficiently large $n$ and all $\delta>0$ and $\epsilon>0$. <br>
Plain English: "such that $\frac{|X_n|}{r_n}$ is **bounded in probability** for all large enough $n$"

Example: 

* If $\displaystyle \sqrt{n}(\bar{X}_n-\mu)\to_d\mathcal{N}(0,\sigma^2),$ then 
$$
\begin{align*}
\sqrt{n}(\bar{X}_n-\mu) &= O_P(1)\\[2ex]
(\bar{X}_n-\mu) &= O_P(n^{-1/2})
\end{align*}
$$

:::

::: {.callout-note}
# Stochastic Landau symbol "Small oh Pee" $o_P(r_n)$
$$
o_P(r_n)\quad\text{with}\quad r_n>0,\;n=1,2,\dots
$$
is a placeholder symbol describing the family of all *stochastic* sequences $X_n$, $n=1,2,\dots,$ such that

* $\displaystyle \frac{|X_n|}{r_n}\to_P 0\quad$ as $\quad n\to\infty$

Example: 

* If $\displaystyle \sqrt{n}(\bar{X}_n-\mu)\to_d\mathcal{N}(0,\sigma^2),$ then 
$$
\begin{align*}
(\bar{X}_n-\mu)         &= o_P(1)
\end{align*}
$$
:::


In the following we focus on the **MASE** which has the advantage that we can use matrix algebra. 

$\operatorname{Bias}_\varepsilon(\hat{m}_p(X_i))=\mathbb{E}_\varepsilon(\hat m_p(X_i))-m_p(X_i)$:
$$
\begin{align*}
  \mathbb{E}_\varepsilon(\hat m_p(X_i))&=\mathbb{E}_\varepsilon\Big(\sum_{j=1}^p \hat{\beta}_j b_{j,k}(X_i)\Big)\\
 &=\sum_{j=1}^p\mathbb{E}_\varepsilon(\hat{\beta}_j) b_{j,k}(X_i),
\end{align*}
$$
where $\hat{\beta}=(\hat{\beta}_1,\dots,\hat{\beta}_p)^\top=(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top Y$.


Then
$$
\begin{align*}
\mathbb{E}_\varepsilon(\hat\beta)
&=\mathbb{E}_\varepsilon\Big((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top  (\underbrace{m+\varepsilon}_{=Y})\Big)\\
&=\underbrace{(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top  m}_{=:(\beta_1,\dots,\beta_p)^\top=\beta}+0,%\\
%&=:(\beta_1,\dots,\beta_p)^\top=\beta
\end{align*}
$$
where $m=(m(X_1),\dots, m(X_n))^\top$ and $\varepsilon=(\varepsilon_1,\dots,\varepsilon_n)^\top$.


Note that 
$$
\beta=(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top  m
$$ 
is a least squares solution; namely of the following least squares problem that tries to approximate the unknown $m$ using a spline function $s$:
$$
\begin{align*}
\sum_i (m(X_i)-\sum_{j=1}^p \beta_j b_{j,k}(X_i))^2 
&=\min_{\vartheta_1,\dots,\vartheta_p}\sum_i (m(X_i)-\sum_{j=1}^p \vartheta_j  b_{j,k}(X_i))^2\\
&=\min_{s\in {\cal{S}}_{k,\tau_1,\dots,\tau_q}} \sum_i (m(X_i)-s(X_i))^2.
\end{align*}
$$

That is, the mean of our spline regression estimator
$$
\mathbb{E}_\varepsilon(\hat m_p(x))=\sum_{j=1}^p \beta_j b_j(x)=:\tilde m_p(x)
$$
is the best ($L_2$) approximation of the true, but unknown, regression function $m(x)$ by means of spline functions in ${\cal{S}}_{k,\tau_1,\dots,\tau_q}$.

::: {.callout-important}
If the true, but unknown, regression function $m$ happens to be an element of the space of spline functions ${\cal{S}}_{k,\tau_1,\dots,\tau_q},$ then 
$$
\operatorname{Bias}_\varepsilon(\hat m_p) = \tilde m_p(x) - m(x) = 0
$$
However, generally we do not expect that $m$ is actually an element of ${\cal{S}}_{k,\tau_1,\dots,\tau_q},$ such that  
$$
\operatorname{Bias}_\varepsilon(\hat m_p) = \tilde m_p(x) - m(x) \neq 0.
$$
We rather hope that 
$$
\operatorname{Bias}_\varepsilon(\hat m_p) = \tilde m_p(x) - m(x) \to 0
$$
as $n\to\infty$ and $p\to\infty.$ 
:::

By the general approximation properties of cubic splines ($k=3$) with $q=p-2$ equidistant knots, we will thus expect that (@DeBoor_1978 or @Eubank_1999):

* If $m$ is twice continuously differentiable over $(a,b)$, then
$$
(\operatorname{Bias}_\varepsilon(\hat m_p))^2=\frac{1}{n}\sum_{i=1}^n \left(m(X_i)-\tilde m_p(X_i))\right)^2=O_p(p^{-4})
$$

* If $m$ is four times continuously differentiable, then
$$
(\operatorname{Bias}_\varepsilon(\hat m_p))^2=\frac{1}{n}\sum_{i=1}^n \left(m(X_i)-\tilde m_p(X_i))\right)^2=O_p(p^{-8})
$$



The next step is to compute the (average) **variance** of the estimator, which can be obtained by the usual type of arguments applied in parametric regression:
<!-- % Let
% $\tilde m_p=(\tilde m(X_1),\dots,\tilde m(X_n))^\top $ and $\varepsilon=(\varepsilon_1,\dots,\varepsilon_n)^\top $. Then
%{\small -->
$$
\begin{align*}
%\V_\varepsilon(\hat m_p)&=:
\frac{1}{n}\sum_{i=1}Var_\varepsilon(\hat{m}_p(X_i))&=
% \frac{1}{n}\mathbb{E}_\varepsilon\left(\Vert \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top Y-
% \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \tilde m_p\Vert_2^2\right)\\
\frac{1}{n}\mathbb{E}_\varepsilon\left(\Vert \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top Y-
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top m\Vert_2^2\right)\\[2ex]
&=\frac{1}{n}\mathbb{E}_\varepsilon\left(\Vert \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \varepsilon\Vert_2^2\right)\\[2ex]
&= \frac{1}{n}\mathbb{E}_\varepsilon\left(\varepsilon^\top  (\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top )^\top\;\;\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \varepsilon\right)\\[2ex]
&= \frac{1}{n}\mathbb{E}_\varepsilon\left(\varepsilon^\top  \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \varepsilon\right)\\[2ex]
&= \frac{1}{n}\mathbb{E}_\varepsilon\left(\operatorname{trace}\left(\varepsilon^\top  \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \varepsilon\right)\right)\\[2ex]
&=\frac{1}{n}\operatorname{trace}\left((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top  \mathbb{E}_\varepsilon(\varepsilon\varepsilon^\top ) \mathbf{X}\right)\quad(\text{with }\mathbb{E}_\varepsilon(\varepsilon\varepsilon^\top )=I_n\sigma_\varepsilon)\\[2ex]
&=\frac{1}{n} \sigma^2 \text{trace}\left((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top  \mathbf{X}\right)\\[2ex]
&=\frac{1}{n} \sigma^2 \text{trace}\left(I_p\right)\\[2ex]
&=\sigma^2  \frac{p}{n}=:Var_\varepsilon(\hat{m}_p)
\end{align*}
$$


::: {.callout-tip}
# Trace-Trick 
For any $j\times l$ matrix $A$ and any $l\times j$ matrix $B$ we have the identity
$$\text{trace}(AB)=\text{trace}(BA)$$
:::


#### Summary {-} 

For cubic ($k=3$) splines with $q$ equidistant knots and a twice differentiable function $m$ we will expect that:

* $\displaystyle(\operatorname{Bias}_\varepsilon(\hat m_p))^2=O_p(p^{-4})$

* $\displaystyle Var_\varepsilon(\hat m_p)= \sigma^2\frac{p}{n}$

where $p=q+2$ is the number of basis functions. 

This leads to the classical **trade-off between (average) squared bias and (average) variance** that is typical for nonparametric statistics:

* $\displaystyle (\operatorname{Bias}_\varepsilon(\hat m_p))^2$ **decreases** as $p$ increases.
* $\displaystyle Var_\varepsilon(\hat m_p)$  **increases** as $p$ increases.

<!-- p^-4 = p/n
     p^-5 = 1/n
     p    = (1/n)^-1/5  -->

Thus if 

* $p\to\infty$ as $n\to\infty$ such that $\dfrac{p}{n}\to 0$

then 

$$
\operatorname{MASE}(\hat m_{p}) = (\operatorname{Bias}_\varepsilon(\hat m_p))^2 + Var_\varepsilon(\hat m_p) \to 0
$$
as $n\to 0.$

An **optimal smoothing parameter** $p_{opt}$ that minimizes $\operatorname{MASE}(\hat m_{p})$ will balance the squared bias and variance 
$$
\begin{align}
p_{opt}^{-4} & \sim \frac{p_{opt}}{n}\\[2ex]
%p_{opt}^{-5} & \sim \frac{1}{n}\\[2ex]
\Leftrightarrow\;p_{opt}      & \sim n^{1/5},
\end{align}
$$ 
i.e., $p_{opt}=\texttt{const} \cdot n^{1/5}$ for some finite and positive constant $0 <\texttt{const}<\infty.$

Then
$$
\operatorname{MASE}(\hat m_{p_{opt}})=O_p(n^{-4/5}).
$$

::: {.callout-note}
For an estimator $\hat m$ based on a **valid** (!) parametric model we have
$$
\operatorname{MASE}(\hat m_{p_{opt}})=O_p(n^{-1}).
$$
:::



Similar results can be obtained for the mean integrated squared error (MISE): If $m$ is twice continuously differentiable, and $p_{opt} \sim n^{1/5}$, then
$$
\operatorname{MISE}(\hat m_{p_{opt}})=\mathbb{E}_\varepsilon\left(\int_a^b(m(x)-\hat m_{p_{opt}}(x))^2dx\right)=O_p(n^{-4/5}).
$$


## Selecting the Smoothing Parameter $p$ 

**Problem:** 
Since $m$ is unknown, we cannot directly compute $\operatorname{MASE}$ and $p_{opt}$. However, we need to choose the smoothing parameter $p$ in an (somehow) optimal and objective manner.

**Approach:** 
Determine an estimate $\hat p_{opt}$ of the unknown "optimal" number $p_{opt}$ of basis functions by minimizing a suitable error criterion with the following properties:

* For every possible $p$ the corresponding criterion function can be calculated from the *data*.
* For any $p$ the error criterion provides "information" about the respective  $\operatorname{MASE}$



Recall: With 
$$
\hat m_p=(\hat m_p(X_1),\dots,\hat m_p(X_n))^\top 
$$ 
we have
$$
\hat m_p=\mathbf{X}\hat\beta=\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top Y=S_pY,
$$
where 
$$
p=\operatorname{trace}(S_p).
$$ 


That is, for given $p$, the number of parameters to estimate by the spline method (one also speaks of the "degrees of freedom" of the smoothing procedure) is equal to $p$ which corresponds to the trace of the **smoother matrix** 
$$
S_p=\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top. 
$$

Most frequently used error criteria are **Cross-Validation (CV)** and **Generalized Cross-Validation (GCV)**: 

* **Cross-Validation (CV):** <br>
For a given value $p,$ cross-validation tries to approximate the corresponding prediction error.
$$
\operatorname{CV}(p)=\frac{1}{n} \sum_{i=1}^n\biggl( Y_i-
{\hat m}_{p,-i}(X_i)\biggr)^2.
$$
Here, for any $i=1,\dots,n$, ${\hat m}_{p,-i}$ is the "leave-one-out" estimator of $m$ to be obtained when a spline function is fitted to the $n-1$ observations:
$$
(Y_1,X_1),\dots,(Y_{i-1},X_{i-1}),(Y_{i+1},X_{i+1}),\dots,(Y_{n},X_{n}).
$$
**Motivation:**
We have
$$
\begin{align*}
&\mathbb{E}_\varepsilon(\operatorname{CV}(p))
= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_\varepsilon\left[\biggl( \overbrace{m(X_i)+\varepsilon_i}^{=Y_i}-
{\hat m}_{p,-i}(X_i)\biggr)^2\right]\\[2ex]
= & \frac{1}{n}\sum_{i=1}^n \mathbb{E}_\varepsilon\left[\left(\left( m(X_i)-
{\hat m}_{p,-i}(X_i) \right)^2 +2\left( m(X_i)-
{\hat m}_{p,-i}(X_i) \right)\varepsilon_i +\varepsilon_i^2\right)\right]\\[2ex]
= &\underbrace{\frac{1}{n}\sum_{i=1}^n \mathbb{E}_\varepsilon\left[\left(m(X_i)-
{\hat m}_{p,-i}(X_i)\right)^2\right]}_{\operatorname{MASE}(\hat m_p)} \\[2ex]
&+ 2\frac{1}{n} \sum_{i=1}^n
\underbrace{\mathbb{E}_\varepsilon\left[( m(X_i)-
{\hat m}_{p,-i}(X_i))\varepsilon_i\right]}_{=0}+\sigma^2
\end{align*}
$$
Thus, 
$$
\begin{align*}
\mathbb{E}_\varepsilon(\operatorname{CV}(p)) = \operatorname{MASE}(\hat m_p) + \sigma^2,
\end{align*}
$$
such that
$$
\begin{align*}
p_{opt} 
&= \arg\min_p\mathbb{E}_\varepsilon(\operatorname{CV}(p))\\[2ex]
&= \arg\min_p\operatorname{MASE}(\hat m_p).
\end{align*}
$$
That is, at least on average, minimizing $CV(p)$ is equivalent to minimizing $\operatorname{MASE}(\hat m_p)$. 


* **$k$-fold Cross-Validation (CV):** <br>
In practice, one usually works with $k$-fold CV ($k=5$ or $k=10$). For this the index set $I=\{1,\dots,n\}$ is partitioned into $k$ disjoint index sets $I_1,\dots,I_k$ of (roughly) equal sizes, i.e. $|I_1|\approx|I_2|\approx\dots\approx|I_k|$, such that $I_1\cup I_1\cup \dots \cup I_k=I$ and $I_j\cap I_k=\emptyset$ for all $j\neq k$.
$$
\operatorname{CV}_k(p)=\frac{1}{k}\sum_{k=1}^K\frac{1}{|I_k|} \sum_{i\in I_k}\left( Y_i-
{\hat m}_{p,-I_k}(X_i)\right)^2.
$$


* **Generalized Cross-Validation (GCV):**
$$
\operatorname{GCV}(p)=\frac{1}{n\left(1-\frac{p}{n}\right)^2}\sum_{i=1}^n \left( Y_i-
{\hat m}_p(X_i)\right)^2
$$
**Motivation:**
The average residual sum of squares are given by
$$
\operatorname{ARSS}(p):=\frac{1}{n}\sum_{i=1}^n\biggl( Y_i-
\hat{m}_{p}(X_i)\biggr)^2
$${#eq-ARSS}
which allows us to rewrite $\operatorname{GCV}(p)$ as 
$$
\operatorname{GCV}(p)=\frac{1}{\left(1-\frac{p}{n}\right)^2}\operatorname{ARSS}(p)
$$
If $p\rightarrow\infty$ such that $p/n\rightarrow 0$, a Taylor expansion yields
$$
\begin{align*}
\operatorname{GCV}(p)= 
&\operatorname{ARSS}(p)+2\frac{p}{n}\overbrace{\text{ARSS}(p)}^{=\sigma^2+o_p(1)}+O_p\left(\left(\frac{p}{n}\right)^2\right)\\[2ex]
&\operatorname{ARSS}(p)+2\sigma^2\frac{p}{n}+o_p(1)
\end{align*}
$$
Some lengthy derivations show that the expected value of $\operatorname{ARSS}(p)$ is
$$
\begin{align*}
\mathbb{E}_\varepsilon(\operatorname{ARSS}(p))=\operatorname{MASE}(\hat m_p)-2\sigma^2\frac{p}{n}+\sigma^2
\end{align*}
$$
Thus
$$
\begin{align*}
\mathbb{E}_\varepsilon(\operatorname{GCV}(p))\approx \operatorname{MASE}(\hat m_p) +\sigma^2
\end{align*}
$$

::: {.callout-note}
# Over-fitting 
Note that in econometrics (where we assumed to know the model apart from the model parameters) we used @eq-ARSS as an estimator for $\sigma^2.$ 

In non-parametrics, where we do not assume to know the model, but try to learn the model form the data, @eq-ARSS cannot be used as an estimator for $\sigma^2$, firstly, since we do not know $p$, and secondly, since we expect $\hat{m}_{p}$ to be biased for each $p$. 

In fact, for a large enough $p,$ the regression function $\hat{m}_{p}$ becomes so flexible that $Y_i\approx \hat{m}_{p}(X_i)$ for all $i=1,\dots,n$ such that $\operatorname{ARSS}(p)\approx 0.$

However, an estimate $\hat{m}_{p}$ for which $\operatorname{ARSS}(p)\approx 0$ typically has fitted the noise component $\varepsilon$---additionally to the signal component $m$.  Such an over-fitted estimate  $\hat{m}_{p}$ will typically perform very poorly when used to predict a *new* $Y_{new}$ for a given new $X_{new}$ by $\hat{Y}_{new} = \hat{m}_{p}(X_{new}),$ i.e.
$$
Y_{new}\not\approx \hat{m}_{p}(X_{new}).
$$

The term $2\sigma^2\frac{p}{n}$ in 
$$
\begin{align*}
\mathbb{E}_\varepsilon(\operatorname{ARSS}(p))=\operatorname{MASE}(\hat m_p)-2\sigma^2\frac{p}{n}+\sigma^2
\end{align*}
$$
is called the **optimism** of the model and quantifies the amount by which the in-sample average residual sum of squares (ARSS) systematically  under-estimates the true mean average squared error (MASE) of $\hat m_p.$ 
:::

<!-- $$
\begin{align*}
&\mathbb{E}_\varepsilon(\operatorname{ARSS}(p))
=\frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\left[\left( Y_i - \hat{m}_{p}(X_i)\right)^2\right]\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n Var_\varepsilon\left[Y_i - \hat{m}_{p}(X_i)\right] + \frac{1}{n}\sum_{i=1}^n\left(\mathbb{E}_\varepsilon\left[ Y_i - \hat{m}_{p}(X_i)\right] \right)^2\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n \underbrace{Var_\varepsilon\left(Y_i\right)}_{=\sigma^2} + \frac{1}{n}\sum_{i=1}^n Var_\varepsilon\left(\hat{m}_{p}(X_i)\right)\\[2ex] 
&\phantom{=}-2\frac{1}{n}\sum_{i=1}^n Cov_{\varepsilon}\left(Y_i,\hat{m}_{p}(X_i)\right)\\[2ex] 
&\phantom{=}+ \frac{1}{n}\sum_{i=1}^n\left(\mathbb{E}_\varepsilon\left[ Y_i - \hat{m}_{p}(X_i)\right] \right)^2\\[2ex]
\end{align*}
$$
Using that $\hat{m}_{p}(X_i)=[S_pY]_{ii}$, one can show that 
$$
Cov_{\varepsilon}\left(Y_i,\hat{m}_{p}(X_i)\right) = \sigma^2[S_p]_{ii}
$$
such that
$$
\begin{align*}
2\frac{1}{n}\sum_{i=1}^n Cov_{\varepsilon}\left(Y_i,\hat{m}_{p}(X_i)\right)
&=2\sigma^2\frac{1}{n}\operatorname{trace}(S_p)\\[2ex]
&=2\sigma^2\frac{p}{n}.
\end{align*}
$$
Thus
$$
\begin{align*}
&\mathbb{E}_\varepsilon(\operatorname{ARSS}(p))= \sigma^2 - 2\sigma^2\frac{p}{n} +\\[2ex]
&\underbrace{\frac{1}{n}\sum_{i=1}^n Var_\varepsilon\left(\hat{m}_{p}(X_i)\right) + \frac{1}{n}\sum_{i=1}^n\left(\mathbb{E}_\varepsilon\left[ Y_i - \hat{m}_{p}(X_i)\right] \right)^2}_{test}  
\end{align*}
$$
$$
\begin{align*}
\mathbb{E}_\varepsilon(\operatorname{ARSS}(p))
&=\frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\Big[\big( \overbrace{m(X_i) +\varepsilon_i}^{=Y_i} - \hat{m}_{p}(X_i)\big)^2\Big]\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\left[\left(\varepsilon_i - \left( \hat{m}_{p}(X_i) - m(X_i) \right) \right)^2\right]\\[2ex] 
&=\frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\left[\left(\hat{m}_{p}(X_i) - m(X_i)\right)^2\right]\\[2ex] 
&\phantom{=} - \frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\left[2\varepsilon_i (\hat{m}_{p}(X_i)  - m(X_i) ) \right] \\[2ex] 
&\phantom{=} + \frac{1}{n}\sum_{i=1}^n\mathbb{E}_\varepsilon\left[\varepsilon_i^2\right]\\[2ex]
&=\operatorname{MASE}(\hat m_p)-2\sigma^2\frac{p}{n}+\sigma^2
\end{align*}
$$ -->



`R` code to compute an estimate of the optimal smoothing parameter

First, we generate some data.
```{r}
set.seed(1)
# Generate some data: #################
n      <- 100     # Sample Size
x_vec  <- (1:n)/n # Equidistant X 
# Gaussian iid error term 
e_vec  <- rnorm(n = n, mean = 0, sd = .5)
# Dependent variable Y
y_vec  <-  sin(x_vec * 5) + e_vec
```

Then, compute the GCV scores for different numbers of basis functions $p$ and plot them to select an estimate for the optimal value of the smoothing parameter $p$. 

```{r}
p_vec <- 6:12 
GCV_p <- numeric(length(p_vec))

for(j in 1:length(p_vec)){

p         <- p_vec[j] # number of basis functions
q         <- p - 2    # number of equidistant knots 
knot_seq  <- seq(from = 0, to = 1, len = q)# knots

X_mat     <- splines2::bSpline(
        x              = x_vec, # evaluation points
        knots          = knot_seq[-c(1, length(knot_seq))], 
        degree         = degree,
        intercept      = TRUE, 
        Boundary.knots = knot_seq[ c(1, length(knot_seq))]
    )

S_p      <- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) 
m_hat_p  <- S_p   %*% y_vec
ARSS     <- mean(c(y_vec - m_hat_p)^2)
GCV_p[j] <- ARSS/((1-p/n)^2)
}
plot(y = GCV_p, x = p_vec, type="o")
```

Compute the nonparametric regression estimate using the GCV optimal smoothing parameter $p=8.$

```{r}
p         <- 8
q         <- p - 2    # number of equidistant knots 
knot_seq  <- seq(from = 0, to = 1, len = q)# knots

X_mat     <- splines2::bSpline(
        x              = x_vec, # evaluation points
        knots          = knot_seq[-c(1, length(knot_seq))], 
        degree         = degree,
        intercept      = TRUE, 
        Boundary.knots = knot_seq[ c(1, length(knot_seq))]
    )

S_p      <- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) 
m_hat_p  <- S_p   %*% y_vec
```

Let's plot the results:

```{r}
plot(y=y_vec, x=x_vec, xlab="X", ylab="Y", 
     main="Regression Splines")
lines(y=sin(x_vec * 5), x=x_vec, col="red", lty=2, lwd=1.5)
lines(y=m_hat_p, x=x_vec, col="blue", lwd=1.5)
legend("bottomleft", 
       c("(Unknown) Regression Function m", 
         "Regr.-Spline Fit with GCV optimal p=8"), 
       col=c("red","blue"), 
       lty=c(2,1,1), lwd=c(2,2,2))

```

## References {-}