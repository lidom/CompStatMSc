# Nonparametric Regression


Let us consider the case of univariate nonparametric regression, i.e., with one single explanatory variable $X\in\mathbb{R}$.

**Data:** 
$$
(Y_{1},X_{1}),\dots,(Y_{n},X_{n})\overset{\text{i.i.d}}{\sim}(Y,X)
$$

* $Y_{i}$ response variable
* $X_{i}\in [a,b]\subset \mathbb{R}$ explanatory  variable
* $n$ sufficiently large (e.g., $n\geq 40$)


#### The Nonparametric Regression Model {-}

$$
Y_i=m(X_i)+\varepsilon_i
$$

* $m(X_i)=\mathbb{E}(Y_i|X=X_i)$ regression function
* $\mathbb{E}(\varepsilon_i)=0$, $Var(\varepsilon_i)=\sigma^2$
* $\varepsilon_i$  independent of $X_i$.

Special cases of **parametric**  regression models:

* Linear regression: $m(x)$ is a straight line
$$
m(X)=\beta_0+\beta_1 X
$$

* Possible generalizations: $m(x)$ quadratic or cubic polynomial
$$
\begin{align*}
                m(X)&=\beta_0 +\beta_1 X+\beta_2 X^2\\
\text{or} \quad m(X)&=\beta_0+\beta_1 X+\beta_2 X^2+\beta_3 X^3
\end{align*}
$$

Many important applications lead to regression functions possessing a complicated structure. Standard models then are "too simple" and do not provide useful approximations of $m(x)$.

::: {.callout-tip}
# As George Box is saying it:
"All models are false, but some are useful" (G. Box)
:::

**Example:**

Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13). 

```{r}
suppressPackageStartupMessages(library("np"))
data("cps71")
plot(cps71$age, cps71$logwage, xlab="Age", ylab="log(wage)")
```



#### Nonparametric Regression {-} 

There are no specific assumptions about the structure of the regression function. It is only assumed that $m$ is **smooth**.


An important point in theoretical analysis is the way how the observations $X_1,\dots,X_n$ have been generated. One distinguishes between "fixed" and "random design".

* **Fixed design:** The observation points $X_1,\dots,X_n$ are fixed  (non stochastic) values.
   * **Example:** Crop yield ($Y$) in dependence of the amount of fertilizer ($X$) used.
* **Equidistant Design:**  (Most important special case of fixed design)  
   $$
   X_{i+1}-X_i=\frac{b-a}{n}.
   $$
* **Random design:** The observation points $X_1,\dots,X_n$ are (realizations of) i.i.d. random variables with density $f$. The density $f$ is called "design density". Throughout this chapter it will be assumed that $f(x)>0$ for all $x\in [a,b]$.
   * **Example:** Sample $(Y_1,X_1),\dots,(Y_n,X_n)$ of log-wages ($Y$) and age ($X$) of randomly selected individuals.


In the case of random design, $m(x)$ is the *conditional* expectation of $Y$ given $X=x$,
$$
m(x)=\mathbb{E}(Y|\ X=x)
$$

and $Var(\varepsilon_i|X_i)=\sigma^2$.

::: {.callout-note}
For random design all expectations (as well as variances) have to be interpreted as *conditional* expectations (variances) given $X_1,\dots,X_n$.
:::

## Basis function expansions

Some frequently used approaches to nonparametric regression rely on expansions of the form
$$
m(x)\approx \sum_{j=1}^p \beta_j b_j(x),
$$
where $b_1(x),b_2(x),\dots$ are suitable basis functions. 


The basis functions $b_1,b_2,\dots$ have to be chosen in such a way that for *any possible* smooth function $m$ the approximation error 
$$
\min_\beta |m(x)-\sum_{j=1}^p \beta_j b_j(x)|
$$ 
tends to zero as $p\rightarrow\infty$ (approximation theory).


For a fixed value $p$ an estimator $\hat m_p$ is determined by
$$
\hat m(x)=\sum_{j=1}^p \hat\beta_j b_j(x),
$$
where the coefficients  $\hat\beta_j$ are obtained by ordinary least squares
$$
\sum_{i=1}^n \left( Y_i-\sum_{j=1}^p \hat\beta_j b_j(X_i)\right)^2
=\min_{\beta_1,\dots,\beta_p} \sum_{i=1}^n \left( Y_i-\sum_{j=1}^p \beta_j \underbrace{b_j(X_i)}_{X_{ij}}\right)^2
$$


Examples are approximations by polynomials, spline functions, wavelets or Fourier expansions (for periodic functions).


### Polynomial Regression

**Theoretical Justification:** Every smooth function can be well approximated by a polynomial of sufficiently high degree.

**Approach:**

* Choose $p$ and fit a polynomial of degree $p$:
  $$
  \min_{\beta_1,\dots,\beta_p}\sum_{i=1}^n \left(Y_i-\sum_{j=1}^p {\beta}_{j} X^{j-1}\right)^2
  $$
  $$
  \Rightarrow\quad {\hat m}_p(X)={\hat \beta}_{1}+\sum_{j=2}^{p-1}
  {\hat \beta}_{j} X_i^{j-1}
  $$
* This corresponds to an approximation with basis functions
  $$
  b_1(x)=1, b_2(x)=x, b_3(x)=x^2, \dots, b_{p}(x)=x^{p-1}.
  $$

**Note:** It is only assumed that $m$ is well approximated by a polynomial of degree $p$. That is, there will usually still exist an **approximation error** (i.e., bias of $\hat{m}_p \neq 0$).


**R-Code to compute polynomial regressions:**

Generate some data:

```{r}
#| eval: true
#| echo: true
set.seed(1)
# Generate some data: 
n      <- 100     # Sample Size
x_vec  <- (1:n)/n # Equidistant X 
# Gaussian iid error term 
e_vec  <- rnorm(n = n, mean = 0, sd = .5)
# Dependent variable Y
y_vec  <-  sin(x_vec * 5) + e_vec
# Save all in a dataframe
db     <-  data.frame(x=x_vec,y=y_vec)
```

Compute the ordinary least squares regressions of different polynomial regression models:

```{r}
#| eval: true
#| echo: true
# Fitting of polynomials to the data (parametric models):
# Constant line fit: (Basis function x^0)
reg_p1 <- lm(y ~ 1, data=db)
# Basis functions: x^0 + ... + x^3
reg_p3 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)
# Basis functions: x^0 + ... + x^6
reg_p6 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)
```

Take a look at the fits:

```{r}
par(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))
plot(db, main="Truth")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
plot(db, main="Degree 0")
lines(y = predict(reg_p1, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
plot(db, main="Degree 3")
lines(y = predict(reg_p3, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
plot(db, main="Degree 6")
lines(y = predict(reg_p6, newdata = db), 
      x = x_vec, col="red", lwd=1.5)
```


The quality of the approximation obviously depends on the choice of $p$ which serves as a "smoothing parameter"


```{r}
m_true    <- sin(x_vec * 5)

n_MCrepl  <- 200 # MC-replications

m_hat_p1  <- matrix(NA, n, n_MCrepl)
m_hat_p3  <- matrix(NA, n, n_MCrepl)
m_hat_p6  <- matrix(NA, n, n_MCrepl)

for(r in 1:n_MCrepl){
    # Generate some data: 
    e_vec  <- rnorm(n = n, mean = 0, sd = .5)
    y_vec  <-  sin(x_vec * 5) + e_vec
    db     <-  data.frame(x = x_vec,y = y_vec)
    # Estimations
    reg_p1 <- lm(y ~ 1, data=db)
    reg_p3 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)
    reg_p6 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)
    # Save predictions (y hat)
    m_hat_p1[,r] <- predict(reg_p1, newdata = db)
    m_hat_p3[,r] <- predict(reg_p3, newdata = db)
    m_hat_p6[,r] <- predict(reg_p6, newdata = db)
}

par(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))
plot(db, main="Truth")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
subSelect <- 25
matplot(y = m_hat_p1[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",  
        ylim = range(m_hat_p1[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=0.5, main = "Degree p=0")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
matplot(y = m_hat_p3[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",
        ylim = range(m_hat_p3[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=.5, main = "Degree p=3")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
##
matplot(y = m_hat_p6[,1:subSelect], 
        x = x_vec, type = "l", lty = 1, ylab = "", xlab = "x",
        ylim = range(m_hat_p6[,1:subSelect], sin(x_vec * 5)), 
        col=rep("red",n), lwd=.5, main = "Degree p=6")
lines(y=sin(x_vec * 5), x=x_vec, col="blue", lwd=1.5)
```



```{r}
## Pointwise (for each x) biases of \hat{m}(x): 
Pt_Bias_p1 <- rowMeans(m_hat_p1) - m_true
Pt_Bias_p3 <- rowMeans(m_hat_p3) - m_true
Pt_Bias_p6 <- rowMeans(m_hat_p6) - m_true

## Pointwise (for each x) variances \hat{m}(x):
Pt_Var_p1  <- apply(m_hat_p1, 1, var)
Pt_Var_p3  <- apply(m_hat_p3, 1, var)
Pt_Var_p6  <- apply(m_hat_p6, 1, var)

par(mfrow=c(1,2))
matplot(y = cbind(Pt_Bias_p1, Pt_Bias_p3, Pt_Bias_p6), 
        x = x_vec, type = "l", 
        main = "Pointwise Bias", ylab="", xlab="x")
legend("topleft", col = c(1,2,3), lty = c(1,2,3), 
legend = c("Degree p=0", "Degree p=3", "Degree p=6"))
matplot(y = cbind(Pt_Var_p1, Pt_Var_p3, Pt_Var_p6), 
        x = x_vec, type = "l", 
        main = "Pointwise Variance", ylab="", xlab="x")
legend("top", col = c(1,2,3), lty = c(1,2,3), 
legend = c("Degree p=0", "Degree p=3", "Degree p=6"))
```

* $p$ small: variability of the estimator is small, but there may exist a high systematic error (bias).
* $p$ large: bias is small, but variability of the estimator is high.

::: {.callout-note}
# Remark

Polynomial regression is not very popular in practice. Reasons are numerical problems in fitting high dimensional polynomials. Furthermore, high order polynomials often posses an erratic, difficult to interpret behavior at the boundaries.
:::

### Regression Splines


The practical disadvantages of global basis functions (like polynomials), explain the success of local basis functions. A frequently used system of basis functions are **local polynomials**, i.e., so-called "spline functions".  


A spline function is a *piece wise* polynomial function. They are defined with respect to a pre-specified sequence of $q$ "knots" 
$$
a= \tau_1<\tau_2<\dots<\tau_q=b.
$$ 
Different specifications of the knot sequence lead to different splines.


More precisely, for a given knot sequence a spline function $s(x)$ of degree $k$ is defined by the following properties:

* $s(x)$ is a polynomial of degree $k$ in every interval $[\tau_j,\tau_{j+1}]$, i.e.
$$
s(x)=s_0+s_1x+s_2x^2+\dots+s_kx^{k},\quad x\in[\tau_j,\tau_{j+1}]
$$
with $s_0,\dots,s_k\in\mathbb{R}.$
* $s(x)$ is  $k-1$ times continuously differentiable at each knot point $x=\tau_j$, $j=1,\dots,q$.

   * $s(x)$ is called a *linear spline* if $k=1$, 

   * $s(x)$ is a *quadratic spline* if $k=2$, and

   * $s(x)$ is a *cubic spline*  if $k=3$.

In practice, the most frequently used splines are **cubic** spline functions based on an equidistant sequence of $q$ knots, i.e.,
$$
\tau_{j+1}-\tau_j=\tau_j-\tau_{j-1}\quad\text{for all } j.
$$


The space of all spline functions of degree $k$ defined with respect to a given knot sequence
$a=\tau_1,\dots,\tau_q=b$ is a 
$$
p:=q+k-1
$$ 
dimensional linear function space
$$
{\cal{S}}_{k,\tau_1,\dots,\tau_q}=\operatorname{span}(b_{1,k},\dots,b_{p,k}),
$$
where $b_{1,k},\dots,b_{p,k}$ are **basis-functions**.


<!-- Possible basis functions are
$\tilde{b}_1(x)=1,\tilde{b}_2(x)=x,\dots,\tilde{b}_{k}=x^{k-1},\tilde{b}_{k+1}=(x-\tau_1)^k_+,\dots,
\tilde{b}_{k+q-1}=(x-\tau_{q-1})^k_+$, where
$$
(x-\tau_j)^k_+=
\left\{ \begin{matrix}  (x-\tau_j)^k & \text{ if } x\geq  \tau_j\\
0 & \text{ else} \end{matrix}\right.
$$
Each spline function $s\in {\cal{S}}_{k,\tau_1,\dots,\tau_q}$ can then be written as
$$
s(x)=\sum_{j=1}^{k} \beta_{j} x^{j-1} +\sum_{j=1}^{q-1}\beta_{j+k}(x-\tau_j)^k_+\quad\text{ for } x\in[a,b]
$$
and suitable parameters $\beta_1,\dots,\beta_{k+q-2}$. -->


#### B-Spline Basis Functions {-} 

The so-called **B-spline** basis functions are almost always used in practice, since they possess a number of advantages from a numerical point of view.


The B-Spline basis functions $b_{j,k}$, $j=1,\dots,p=k+q-2$, for splines of order $k$ based on a knot sequence 
$$
a\le \tau_1,\dots,\tau_q\le b
$$ 
are calculated by a recursive procedure:
$$
b_{j,0}(x)=\left\{ \begin{matrix}  1 & \text{ if } x\in[\tau_{j}^*,\tau_{j+1}^*]\\
0 & \text{ else} \end{matrix}\right., \quad j=1,\dots,p=q+2k-1 
$$
and
$$
b_{j,l}(x)=\frac{x-\tau_j^*}{\tau_{l+j}^*-\tau_j^*}b_{j,l-1}(x)+
\frac{\tau_{l+j+1}^*-x}{\tau_{l+j+1}^*-\tau_{j+1}^*}b_{j+1,l-1}(x),
$$
for $l=1,\dots,k$, $j=1,\dots,q+k-1$, and $x\in [a,b]$. 

Here, 

* $\tau_1^*=\dots=\tau_{k+1}^*=\tau_1$,
* $\tau_{k+2}^*=\tau_2,\dots,\tau^*_{k+q}=\tau_q$ and 
* $\tau_{k+q+1}^*=\dots=\tau_{2k+q}^*=\tau_q$.
