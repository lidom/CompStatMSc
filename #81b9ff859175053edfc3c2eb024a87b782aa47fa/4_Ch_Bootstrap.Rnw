\documentclass[17pt]{extreport}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry} 
\usepackage{amsmath} 
\usepackage{amsfonts} 
\usepackage[english]{babel} 
\usepackage[round]{natbib}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr} 
\setlength{\parindent}{0mm}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % Remove headings-line
% \rhead{Winter Term 14/15}
% \lhead{Econometrics I (BGSE)}
\cfoot{\thechapter-\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[framemethod=tikz]{mdframed}
\usepackage[babel=true]{csquotes}
\definecolor{MidnightBlue}{rgb}{0, 0, 0.551}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\R}{\hbox{I\hskip-0.23em R}}
\newcommand{\adem}{{\alpha\over 2}}
\newcommand{\al}{\alpha}
\newcommand{\gammah}{{\hat \gamma}}
\newcommand{\rhoh}{{\hat \rho}}
\newcommand{\ep}{\epsilon}
\newcommand{\be}{\beta}
\newcommand{\ra}{\rightarrow}
\newcommand{\tht}{\theta}
\newcommand{\thth}{\hat\theta}
\newcommand{\alphad}{{\alpha\over 2}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\myspan}{span}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\bfbeta}{\boldsymbol{\beta}}
\DeclareMathOperator{\bfW}{\mathbf{W}}
\DeclareMathOperator{\bfSigma}{\boldsymbol{\Sigma}}
\DeclareMathOperator{\bfsigma}{\boldsymbol{\sigma}}
\DeclareMathOperator{\bftheta}{\boldsymbol{\theta}}
\DeclareMathOperator{\bfeps}{\boldsymbol{\varepsilon}}
\DeclareMathOperator{\eps}{\varepsilon}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\AVar}{AVar}
\DeclareMathOperator{\myb}{\mathbf{b}}
\DeclareMathOperator{\X}{\mathbf{X}}
\DeclareMathOperator{\Z}{\mathbf{Z}}
\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\toprob}{\overset{p}{\longrightarrow}}
\DeclareMathOperator{\toas}{\overset{a.s.}{\longrightarrow}}
\DeclareMathOperator{\toms}{\overset{m.s.}{\longrightarrow}}
\DeclareMathOperator{\todistr}{\overset{d}{\longrightarrow}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{comment}
% For the lecture (without proofs, without exercises, without solutions):
\includecomment{ShortVersion}
\excludecomment{LongVersion}
%%%%%%%%%%%%%
% After the lecture (incl. Proofs, without solutions)   
% \excludecomment{ShortVersion}
% \includecomment{LongVersion}
%%%%%%%%%%%%%
% Include all:
% \includecomment{ShortVersion}
% \includecomment{LongVersion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\setcounter{chapter}{3}

\chapter{An Introduction to the Bootstrap } % und Bootstrap}
\section{The empirical distribution function}


The distribution of a real-valued random variable $X$ can be completely described by
its distribution function
$$F(x)=P(X\leq x)\quad \text{for all } x\in\R.$$


\noindent {\bf Data:} i.i.d. random sample $X_1,\dots,X_n$

For given data, the sample analogue of $F$ is the so-called empirical distribution function, which is an important tool of statistical inference.
 Let $I(\cdot)$ denote the indicator function, i.e.,
$I(x\leq t)=1$ if $x\leq t$, and $I(x\leq t)=0$ if $x> t$.
\begin{center}
\begin{tabular}{|l|} \hline
{\large\bf Empirical distribution function:} \\
$F_n(x)=\frac{1}{n} \sum_{i=1}^n I(X_i\leq x)$,\\
i.e $F_n(x)$ is the proportion of observations with
$X_i\le x$
 \\ \hline
\end{tabular}
\end{center}

\newpage

Properties:
\begin{itemize}
\item $0\le F_n(x)\le 1$ \item $F_n(x)=0$, if $x<X_{(1)}$, where $X_{(1)}$ - smallest observation \item $F(x)=1$, if $x\ge X_{(n)}$, where $X_{(n)}$ - largest observation
 \item $F_n$ monotonically increasing step function
 \item Structurally, $F_n$ itself is a distribution function; it is equivalent to the distribution function of a discrete random variable $X^*$ with possible
 values $X_1,\dots,X_n$ and $P(X^*=X_i)=\frac{1}{n}$.
\end{itemize}


{\bf Example}
\begin{center}
\begin{tabular}{|cccccccc|} \hline
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$& $x_6$ &$x_7$ & $x_8$ \\
5,20 & 4,80& 5,40& 4,60 & 6,10 & 5,40 & 5,80 &5,50
 \\ \hline
\end{tabular}
\end{center}
 Corresponding empirical distribution function:\\
%\epsfig{file=Pics/kap2/empdistr1,width=9cm,height=7cm}
<<echo=FALSE, fig.height=5,out.height==5>>=
plot(ecdf(c(5.20, 4.80, 5.40, 4.60, 6.10, 5.40, 5.80, 5.50)),main="")
@




$F_n(x)$ depends on the observed sample and thus is random. We obtain
\begin{itemize}
\item For every $x\in\R$
$$nF_n(x)\sim B(n,F(x)),$$
i.e., $nF_n(x)$ has a binomial distribution  with parameters  $n$ and
$F(x)$.
\item  $E(F_n(x))=F(x)$
\item $Var(F_n(x))=\frac{F(x)(1-F(x))}{n}$
\item {\bf Theorem of Glivenko-Cantelli:}\\
$$P\left(\lim_{n\rightarrow\infty} \sup_{x\in\R} |F_n(x)-F(x)|=0\right)=1$$
\end{itemize}




\section{The bootstrap: Basic idea}
The bootstrap is an important tool of modern statistical analysis. It establishes
a general framework for simulation-based statistical inference. In simple situations the uncertainty
of an estimate may be gauged by analytical calculations leading, for example, to the construction of
confidence intervals based on an assumed probability model for the available data. The bootstrap replaces
complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by
computer simulations.

\newpage

{\bf The idea of the bootstrap:}
\begin{itemize}
\item  The random sample $X_1,\dots,X_n$ is generated by drawing observations independently and with replacement
from the underlying population (with distribution function $F$) For
each interval $[a,b]$ the probability of drawing an observation in
$[a,b]$ is given by $P(X\in [a,b])=F(b)-F(a)$.
\item $n$ large: The empirical distribution of the sample values is ``close'' to the
distribution of $X$ in the underlying population. The relative
frequency $F_n(b)-F_n(a)$ of observations in $[a,b]$ converges to
$P(X\in [a,b])=F(b)-F(a)$ as $n\rightarrow\infty$.
\item The idea of the bootstrap consists in mimicking the data generating process.
Random sampling from the true population is replaced by random
sampling from the {\bf observed data}. This is justified by the
insight that the empirical distribution of the observed data is
``similar'' to the true  distribution
 ($F_n\ra F$ for $n\ra\infty$).
\end{itemize}

{\small Some literature:
\begin{itemize}
\item Hall, P. (1992): The Bootstrap and Edgeworth Expansion; Springer Verlag
\item  Shao, J. and Tu, D. (1995): The Jacknife and Bootstrap; Springer Verlag
\item Horowitz, J.L. (2001): The Bootstrap. In: Handbook of Econometrics, Volume 5;
Elsevier Science B.V.
\item  Davison, A.C. and Hinkley, D.V. (2005): Bootstrap Methods and their
Applications; Cambridge University Press
\end{itemize}}

\newpage

\begin{center}
{\bf Example 1: Estimating a population mean }
\end{center}


\begin{itemize}
\item {\bf Population Model: } Continuous random variable $X$ with unknown mean $\mu$
\item Data: i.i.d. sample $X_1,\dots,X_n$ from $X$
\item Problem: Distribution of $\bar X -\mu$?
\end{itemize}
\bigbreak

Now assume that $n=8$ and that the {\bf observed sample} is
\begin{center}
\begin{tabular}{|cccccccc|}\hline
$X_1$&  $X_2$& $X_3$& $X_4$&  $X_5$&  $X_6$&  $X_7$&   $X_8$\\
-0.6& 1.0& 1.4&  -0.8& 1.6& 1.9& -0.1& 0.7
\\
\hline
\end{tabular}
\end{center}
$\Rightarrow \bar X= 0.6375$

{\bf Bootstrap approach: }
\begin{itemize}
\item The observed sample ${\cal S}_n=\{X_1,\dots,X_n\}$ is taken as underlying empirical ``population'' in
    order to generate ``Bootstrap data''
\item i.i.d. samples $X_1^*,\dots,X_n^*$ are generated by
by drawing   observations
independently and with replacement from ${\cal S}_n=\{X_1,\dots,X_n\}$.
\item The distribution of $\bar X -\mu$ is approximated by the conditional distribution of $\bar X^* -\bar X$
\end{itemize}

For the given data with $n=8$ observations there are $8^8$ (generally $n^n$) possible bootstrap samples
which are all equally probable. Conditional distributions can be approximated by Monte-Carlo simulations.
For example, $m=5$ simulation runs may lead to the following results:
{\small
\begin{center}
\begin{tabular}{|c|cccccccc|c|}\hline
Simul. & $X_1^*$&  $X_2^*$& $X_3^*$& $X_4^*$&  $X_5^*$&  $X_6^*$&  $X_7^*$&   $X_8^*$ & $\bar X^*_k$\\ \hline
1 & 1.9& -0.8& 1.9&  -0.6& 1.4& -0.1& -0.8& 1.0 & 0.4875 \\
2 & 0.7& -0.8& -0.8& 1.0 & 1.6& 1.0& -0.1& -0.8 & 0.225  \\
3 & -0.1& 1.9& 0.7&  1.0& -0.1& 1.6& 1.0& -0.6 &  0.675 \\
4 & 1.4& 1.0& 1.4&  -0.1& 1.9& -0.8& 1.9& 1.0 & 0.9625 \\
5 & 1.0& 0.7& -0.1&  0.7& 1.4& -0.8& 1.0& 1.6 &  0.6875\\
\hline
\end{tabular}
\end{center}}

Of course, $m=5$ simulations will not be enough. But using $m=1000$ or $m=2000$ then for $\delta>0$
$$P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right) \approx \frac{\sum_{k=1}^m
 I( \bar X^*_k-\bar X\leq \delta)}{m}$$
 will provide a fairly accurate approximation.

 Bootstrap focuses on the {\bf conditional} distribution of $X_1^*,\dots,X_n^*$ given
  the observed sample ${\cal S}_n=\{X_1,\dots,X_n\}$ and the resulting conditional distribution of
 $\bar X^* -\bar X$. Often these conditional distributions are called ``bootstrap distributions''.
 In the bootstrap literature one also frequently finds the notation $E^*(\cdot)$, $Var^*(\cdot)$, or
 $P^*(\cdot)$ to denote {\bf conditional} expectations, variances, or probabilities given the
 sample ${\cal S}_n$

 In order to analyze the bootstrap distribution of $\bar X^* -\bar X$, let us first study properties of
the random variables $X_i^*$.
  Some basic properties of the bootstrap distribution of  $X_i^*$ are easily
 verified.
 \begin{itemize}
\item For each $i=1,\dots,n$ the possible values of $X_i^*$ are $X_1,\dots,X_n$, and
$$\frac{1}{n}= P(X_i^*=X_1|{\cal S}_n)= P(X_i^*=X_2|{\cal S}_n)=\dots=P(X_i^*=X_n|{\cal S}_n)$$
\item The conditional mean of $X_i^*$ is
$$E^*(X_i^*):=E(X_i^*|{\cal S}_n)=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_2=\bar X$$
\item The conditional variance of $X_i^*$ is
$$Var^*(X_i^*):=Var(X_i^*|{\cal S}_n)=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2:=\hat\sigma^2$$
\end{itemize}


 Now consider the bootstrap distribution of $\bar X^* -\bar X$.

 Conditional on ${\cal S}_n$, $X_1^*,\dots,X_n^*$ are i.i.d. random variables with mean $E^*(X_i)=\bar X$ and
 variance $\hat\sigma^2$. This obviously implies that
 $$E^*(\bar X^*)=\bar X\quad \text{and } Var^*( \bar X^*)=\frac{\hat\sigma^2}{n}$$

 As $n\rightarrow\infty$ the central limit theorem implies that
 \begin{align*}
 \text{The law of } (\frac{\sqrt{n}(\bar X^* -\bar X)}{ \hat\sigma}|{\cal S}_n) & \text{ converges stochastically} \\
& \qquad \text{ to a }  N(0,1)\text{-distribution}\end{align*}



 Moreover, $\hat\sigma^2$ is a
consistent estimator of $\sigma^2$, and thus asymptotically $\hat\sigma^2$ may be replaced by $\sigma$. Therefore,

\begin{align*}
 \text{The law of } (\frac{\sqrt{n}(\bar X^* -\bar X)}{ \sigma}|{\cal S}_n) & \text{ converges stochastically} \\
& \qquad \text{ to a }  N(0,1)\text{-distribution}\end{align*}

But generally (central limit theorem) we have
\begin{align*}
 \text{The law of } (\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}) & \text{ converges in distribution} \\
& \qquad \text{ to a }  N(0,1)\text{-distribution}\end{align*}

This means that the bootstrap is {\bf consistent}: The bootstrap distribution
of $\sqrt{n}(\bar X^* -\bar X)$ asymptotically coincides with the distribution of $\sqrt{n}(\bar X-\mu)$ as $n\rightarrow
\infty$. In other words,  for large $n$,
$$\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu)$$
\vskip 1cm


\newpage

\begin{center}
{\bf Example 2:  Estimating a proportion }
\end{center}

\begin{itemize}
\item Data: i.i.d. random sample $X_1,\dots,X_n$; $X_i\in\{0,1\}$ is dichotomous, $P(X_i=1)=p$,
$P(X_i=0)=1-p$. \\ The problem is to estimate $p$.
\item Let $S$ denote the number of $X_i$ which are equal to 1. The maximum likelihood estimate of
$p$ is $\hat p=S/n$.
\item Recall: $n\hat p=S\sim B(n,p)$
\item As $n\rightarrow\infty$ the central limit theorem implies that
$$\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_L N(0,1)$$
\item $n$ large: the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by
$N(0,p(1-p))$ and $N(0,p(1-p)/n)$, respectively.
\end{itemize}

Bootstrap:
\begin{itemize}
\item Random sample
$X_1^*,\dots,X_n^*$  generated by drawing  observations
independently and with replacement from
${\cal S}_n:=\{X_1,\dots,X_n\}$. Let $S^*$ denote the number of $X_i^*$ which are equal to 1.
\item Bootstrap estimate of $p$: $\hat p^*=S^*/n$
\end{itemize}

The distribution of $\hat p^*$ depends on the observed sample ${\cal S}_n:=\{X_1,\dots,X_n\}$.
A different sample will lead to a different distribution. The bootstrap now tries to approximate the
true distribution of $\hat p - p$ by the {\bf conditional} distribution of $\hat p^*-\hat p$ given
the observed sample ${\cal S}_n$. The bootstrap is called {\bf consistent} if asymptotically ($n\rightarrow \infty$)
the conditional distribution of $\hat p^*-\hat p$  coincides with the true distribution of
$\hat p - p$ (note: a proper scaling is required!)
\begin{itemize}
\item We obtain
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|\ {\cal S}_n)=\hat p, \\  & P^*(X_i^*=0)=P(X_i^*=0|\ {\cal S}_n)=1-\hat p
\end{align*}
and
\begin{align*}
&E^*(\hat p^*)= E(\hat p^*|\ {\cal S}_n)=\hat p, \\ & Var^*(\hat p^*)=E[(\hat p^*-\hat p)^2|\ {\cal S}_n]=\frac{\hat p(1-\hat p)}{n}
\end{align*}
\item The bootstrap distribution of $n\hat p^*=S^*$ given ${\cal S}_n$ is equal to $B(n,\hat p)$.  For large $n$ this means that the bootstrap distribution of $\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}$ is approximately standard normal. In other words,
    \begin{align*}
 \text{The law of } (\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}|{\cal S}_n) & \text{ converges stochastically} \\
& \qquad \text{ to a }  N(0,1)\text{-distribution}\end{align*}
\item  Moreover, $\hat p$ is a
consistent estimator of $p$ and therefore $\hat p(1-\hat p)\rightarrow_P p(1-p)$ as $n\rightarrow\infty$. This implies
that asymptotically $\hat p(1-\hat p)$ may be replaced by $p(1-p)$, and
\begin{align*}
 \text{The law of } (\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}|{\cal S}_n) & \text{ converges stochastically} \\
& \qquad \text{ to a }  N(0,1)\text{-distribution}\end{align*}
More precisely, as $n\rightarrow\infty$
$$\sup_\delta |P\left(\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta|{\cal S}_n\right)-\Phi(\delta)|\rightarrow_P 0,$$
where $\Phi$ denotes the distribution function of the standard normal distribution.

\item This means that the bootstrap is consistent: For large $n$
{\small
$$\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx \text{distribution}(\sqrt{n}(\hat p -p))\approx N(0,p(1-p))$$}
as well as
$$\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx \text{distribution}(\hat p -p)\approx N(0,p(1-p)/n)$$
\end{itemize}



\section{General setup: The nonparametric (standard) bootstrap}

\bigbreak

\noindent {\bf Setup:}
\begin{itemize}
\item  Original data: i.i.d. random sample $X_1,\dots,X_n$; the
distribution of $X_i$ depends on an unknown parameter (vector) $\theta$
\item The data $X_1,\dots,X_n$ is used to estimate $\theta$ $\Rightarrow$
estimator $\hat\theta\equiv \hat\theta(X_1,\dots,X_n)$
\item We are interested in evaluating the distribution of  $\hat\theta-\theta$
in order to provide standard errors, to construct confidence intervals, or to perform tests of hypothesis.
\end{itemize}

%Deine IKEA Ordernummer: 800387422
\noindent
{\bf The bootstrap approach:}
\begin{itemize}
\item[1)] {\em Bootstrap samples}: Random samples
$X_1^*,\dots,X_n^*$ are generated by drawing   observations
independently and with replacement from the available sample
$X_1,\dots,X_n$.
\item[2)] {\em Bootstrap estimates}:  $\hat\theta^*\equiv \hat\theta(X_1^*,\dots,X_n^*)$
\item[3)] {\em In practice:} Steps 1) and 2) are repeated $m$ times (e.g. $m=2000$) $\Rightarrow$
$m$ values $\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*$
\item[4)] The bootstrap
  distribution of $\hat\theta^*-\hat\theta$ is used to approximate the distribution of $\hat\theta-\theta$.
\end{itemize}


The theoretical justification of the bootstrap is based on asymptotic arguments. In many important
applications the bootstrap is able to provide   confidence intervals or tests which are
 more accurate than those based on standard asymptotic approximations.

It must, however, be emphasized that the bootstrap does not always work.
A necessary condition for the use of the bootstrap is consistency of the bootstrap approximation.


{\bf Consistent Bootstrap:} For large $n$, the bootstrap distribution of $\thth^* -\thth$ is
a good approximation of the underlying distribution of $\thth-\tht$,
$$\text{distribution}(\thth^* -\thth\ |{\cal S}_n)\approx \text{distribution}(\thth-\tht).$$
More precisely, if for some $\alpha>0$ (usually: $\alpha=1/2$) we have $n^\alpha(\thth-\tht)\rightarrow_D Z$ for
some r.v. $Z$ with a non-degenerate distribution, then the bootstrap is {\bf consistent} if and only if
$$\sup_\delta \bigl|P\left(n^\alpha(\hat\theta^*-\hat\theta)\le \delta|{\cal S}_n\right)-P\left(n^\alpha (\hat\theta-\theta)\le \delta\right)\bigr|\rightarrow_P 0,$$
\bigbreak

The standard bootstrap ''works'' for a large number of statistical and econometrical problems. However,
 there are some crucial requirements:
\begin{itemize}
\item[1)]  Generation of the bootstrap sample ``reflects'' appropriately the way in which the original sample
has been generated (i.i.d. sampling!).
\item[2)]
The distribution of the estimator $\hat\theta$ is asymptotically normal
\end{itemize}
 The standard  {\bf bootstrap will usually fail} if one of the above conditions
1) or 2) is violated. Examples are
\begin{itemize}
\item The  bootstrap will not work if the i.i.d re-sample $X_1^*,\dots,X_n^*$ from $X_1,\dots,X_n$  does not
properly reflect the way how the $X_1,\dots,X_n$ are generated from the underlying population (e.g. dependent data; $X_1,\dots,
X_n$ {\bf not} i.i.d.).
\item The distribution of the estimator $\hat\theta$ is not asymptotically normal (e.g. extreme value problems)
\end{itemize}
Note: In order to deal with more complex situations alternative
 bootstrap procedures have been proposed in the literature.


\section{Basic bootstrap confidence intervals}
\begin{itemize}
\item Standard approaches to construct confidence intervals and tests are usually based on asymptotic normal approximations.
For example, if $\tht\in\R$ and $\sqrt{n}(\hat\theta-\tht)\rightarrow N(0,v^2)$ one usually tries to determine an approximation
$\hat v$ of $v$ from the data. An approximate $1-\alpha$ confidence interval is then given by
$$[\thth-z_{1-\alphad}\frac{\hat v}{\sqrt{n}},
\thth+z_{1-\alphad}\frac{\hat v}{\sqrt{n}}]$$
\item In some cases it is very difficult to obtain approximations $\hat v$ of $v$. Statistical inference is then
usually based on the bootstrap
\item In contemporary statistical analysis the bootstrap is frequently used even for
 standard problems, where estimates $\hat v$ of $v$ are easily constructed. The reason is
that in many situations  it can be shown that bootstrap confidence intervals or tests are {\bf
 more precise}
 than those determined analytically based on asymptotic formulas (this particularly applies to the
 bootstrap t-method discussed in the next section).
\end{itemize}
\bigbreak
{\bf General approach: Basic bootstrap $1-\alpha$ confidence interval}

Random sample ${\cal S}_n:=\{X_1,\dots,X_n\}$;  unknown parameter (vector) $\theta$

We will assume that the bootstrap is consistent:
$$\text{distribution}(\sqrt{n}(\thth^* -\thth)|{\cal S}_n)\approx \text{distribution}(\sqrt{n}(\thth-\tht))$$
  if $n$ is sufficiently large.

\begin{itemize}
\item Determine $\alphad$ and $1-\alphad$ quantiles $\hat t_\alphad$ and $\hat t_{1-\alphad}$  of the conditional distribution
of  $\thth^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}$ (the ``bootstrap distribution''). This implies
\begin{align*}
&P^*\left(\hat t_\alphad \leq \thth^* \leq \hat t_{1-\alphad}\right)
\approx 1-\alpha\\
\Rightarrow & P^*\left(\hat t_\alphad-\thth\leq\thth^*-\thth \leq \hat t_{1-\alphad}-\thth\right)
\approx 1-\alpha\\
\Rightarrow & P^*\left(
\sqrt{n}(\hat t_\alphad-\thth)\leq\sqrt{n}(\thth^*-\thth) \leq \sqrt{n}(\hat t_{1-\alphad}-\thth)\right)
\approx 1-\alpha\\
\end{align*}
Here, $P^*$  denotes probabilities with respect to the   conditional distribution of  $\thth^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}$.
\item Consistency of the bootstrap therefore  implies that for large $n$
\begin{align*}
&P\left(
\sqrt{n}(\hat t_\alphad-\thth)\leq\sqrt{n}(\thth-\tht) \leq \sqrt{n}(\hat t_{1-\alphad}-\thth)\right)\approx 1-\alpha\\
\Rightarrow &P\left(\hat t_\alphad-\thth\leq\thth-\tht \leq \hat t_{1-\alphad}-\thth\right)
\approx 1-\alpha\\
\Rightarrow &P\left(\thth-(\hat t_{1-\alphad}-\thth)\le \tht\le \thth-
 (\hat t_\alphad-\thth)\right)\approx 1-\alpha.
\end{align*}
\item $\Rightarrow$ Approximate $1-\alpha$ (symmetric) confidence interval:
$$[2\thth-\hat t_{1-\alphad}, 2\thth-\hat t_\alphad]$$
\end{itemize}


\bigskip

{\bf Example:
Estimating a population mean}

\begin{itemize}
\item  Let $X_1,\dots,X_n$ denote an i.i.d. random sample with mean $\mu$ and variance $\sigma^2$.
In the following $F$ will denote the corresponding distribution function.
\item $\bar X=\frac{1}{n} \sum_{i=1}^n X_i$ is an unbiased estimator of $\mu$
\item {\bf Problem: } Construct a confidence interval
\end{itemize}
\bigbreak

Traditional approach for constructing a $1-\alpha$ confidence interval:
\begin{itemize}
\item  $\bar X\sim N(\mu,\frac{\sigma^2}{n})$
\item Estimation of $\sigma^2$: $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$
\item This implies: $\sqrt{n}\frac{\bar X -\mu}{S}\sim t_{n-1}$, and hence
$$P(-t_{n-1,1-\alphad}\frac{S}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\alphad}\frac{S}{\sqrt{n}})=1-\alpha$$
\item 95\% confidence interval: $[\bar X-t_{n-1,1-\alphad}\frac{S}{\sqrt{n}},
\bar X+t_{n-1,1-\alphad}\frac{S}{\sqrt{n}}]$
\end{itemize}
{\bf Remark:} The construction relies on the assumption that $\bar X\sim N(\mu,\frac{\sigma^2}{n})$. This
is necessarily true if $X$ is normally distributed.
If the underlying distribution is {\em not} normal, then this condition is
{\em approximately} fulfilled if the sample size $n$ is sufficiently large (central limit
theorem). In this case the constructed confidence interval must also be seen as an
{\em approximation}
\bigbreak\noindent
The bootstrap offers an alternative method for constructing such confidence intervals. We already know
that the bootstrap is consistent in this situation.

Construction of a bootstrap confidence interval:
\begin{itemize}
\item Draw $m$ bootstrap samples (e.g. $m=2000$) and calculate the corresponding estimates $\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m$.
\item Order the resulting estimates $\Rightarrow$ $\bar X^*_{(1)}\le \bar X^*_{(2)}\le \dots \le \bar X^*_{(m)}$.
\item Set $\hat t_\alpha:=\bar X^*_{([(m+1)\alphad]}$ and $\hat t_{1-\alpha}:=\bar X^*_{([(m+1)(1-\alphad)])}$.
\end{itemize}
\begin{itemize}
\item $\Rightarrow$ Approximate $1-\alpha$ (symmetric) confidence interval:
$$[2\bar X-\hat t_{1-\alphad}, 2\bar X-\hat t_\alphad]$$
\end{itemize}

\section{ Pivot statistics and the ''bootstrap-t method''}
\par\noindent
In many situations it is possible to get more accurate  bootstrap confidence intervals
by using the bootstrap-t method (one also speaks of ``studentized bootstrap confidence intervals'').
The construction relies
 on so-called pivotal statistics.
\par\noindent
Let $X_1,\dots,X_n$ be an i.i.d. random sample and assume that the distribution of $X$
depends on an unknown parameter (or parameter vector) $\tht$.
\begin{itemize}
\item A statistics $T_n\equiv T(X_1,\dots,X_n)$ is called ''pivotal'', if the distribution
of $T_n$ does {\bf not depend on any unknown population parameter}.
\item A statistics $T_n\equiv T(X_1,\dots,X_n)$ is called ''asymptotically pivotal'', if its
asymptotic distribution does not depend on any unknown population parameter.
\end{itemize}

Exact pivotal statistics are rare and not available in most econometric applications. It is, however, often possible to construct an asymptotically pivotal statistic. Assume that an estimator
$\thth$ satisfies
$$\sqrt{n}(\thth-\tht)\rightarrow_D N(0,v^2),$$
where $v^2$ denotes the asymptotic variance. Additionally assume that it is possible to
 use the data in order to construct  a consistent estimator\\
$\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2$ of $v$:
$$\hat v_n^2\rightarrow_p v^2$$
Then of course also $\hat v_n\rightarrow_P v$, and
$$T_n:= \sqrt{n}\frac{(\thth-\tht)}{\hat v_n}\rightarrow_D N(0,1).$$
This means that $T_n= \sqrt{n}\frac{(\thth-\tht)}{\hat v_n}$ is asymptotically pivotal.
\bigbreak

\newpage

{\bf Example:  Population mean:}
 $X_1,\dots,X_n$ with mean
 $\mu$, variance $\sigma^2>0$, and
 $E|X|^3=\beta<\infty$. If $X$ is normally distributed we obtain
 $$T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\sim t_{n-1}$$
 with $S^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2$, where  $t_{n-1}$ denotes
 Student's $t$-distribution with $n-1$ degrees of freedom. We can conclude that $T_n$ is pivotal.
 \par\noindent
 If $X$ is {\em not} normally distributed, the central limit theorem implies that
 $$T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\ra_D N(0,1)$$
 In this case $T_n$ is an asymptotically pivotal statistics.
 \bigbreak

The general idea of
bootstrap-t confidence intervals (as well as many bootstrap based tests) now relies on approximating
the distribution of $T_n= \sqrt{n}\frac{(\thth-\tht)}{\hat v_n}$ by the conditional distribution of
$T_n^*=\sqrt{n}\frac{(\thth^*-\thth)}{\hat v_n^*}$, where $\hat v_n^*=v_n(X_1^*,\dots,X_n^*)$.
If the bootstrap is consistent, i.e. if the conditional distribution of $\sqrt{n}(\thth^*-\thth)$ yields
a consistent estimate of $N(0,v^2)$, then also the conditional distribution of $T_n^*$ provides
a consistent estimate of the asymptotic distribution of $T_n$: $T_n\rightarrow_D N(0,1)$ and
$$\sup_\delta |P\left(\sqrt{n} \frac{(\thth^*-\thth)}{\hat v_n^*}\le \delta|{\cal S}_n\right)-\Phi(\delta)|\rightarrow_P 0.$$
Usually, there is even a gain in accuracy.
As is easily seen when considering the above examples and expansions, the approximation of the law of
$T_n$ by the bootstrap law of $T_n^*$ is even more direct (and hence more accurate) than the approximation
of the law of $\sqrt{n}(\thth-\tht)$ by the bootstrap law of  $\sqrt{n}(\thth^*-\thth)$.

\bigbreak
{\bf General construction of a $1-\alpha$ bootstrap-t confidence interval }:

Random sample ${\cal S}_n:=\{X_1,\dots,X_n\}$;  unknown parameter (vector) $\theta$. Assume that
bootstrap is consistent and that
the estimator $\thth$ of $\tht$ is asymptotically normal. Furthermore, suppose
 that a {\bf consistent} estimator $\hat v\equiv \hat v(X_1,\dots,X_n)$ of the
asymptotic standard deviation $v$ is available.

\begin{itemize}
\item  Based on an i.i.d. re-sample
 $X_1^*,\dots,X_n^*$ from
 $\{X_1,\dots,X_n\}$, calculate bootstrap estimates $\thth^*$ and $\hat v^*$.
 \item Determine $\alphad$ and $1-\alphad$ quantiles $\hat \tau_\alphad$ and $\hat \tau_{1-\alphad}$  of the bootstrap distribution
of  $\frac{\thth^*-\thth}{\hat v^*}$.
\item Then
\begin{align*}
&P^*\left(\hat \tau_\alphad\leq \frac{\thth^*-\thth}{\hat v^*} \leq \hat \tau_{1-\alphad}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(\hat \tau_\alphad\leq \frac{\thth-\tht}{\hat v} \leq \hat \tau_{1-\alphad}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(-\hat v \hat \tau_{1-\alphad}\leq \tht-\thth \leq -\hat v\hat\tau_\alphad\right)
\approx 1-\alpha\\
\Rightarrow & P\left(\thth-\hat v \hat \tau_{1-\alphad}\leq \tht \leq \thth -\hat v\hat\tau_\alphad\right)
\approx 1-\alpha
\end{align*}
\item Bootstrap-t interval
$$[\thth-\hat \tau_{1-\alphad}\hat v,
\thth-\hat \tau_{\alphad}\hat v]$$
\end{itemize}


\newpage

{\bf Example:  Population mean}

Construction of a bootstrap-t interval:

\begin{itemize}
\item Draw i.i.d. random samples $X_1^*,\dots,X_n^*$ from ${\cal S}_n$ and calculate
$\bar X^*$ as well as $S^{*2}=\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2$.
\item Determine $\alphad$ and $1-\alphad$ quantiles $\hat \tau_\alphad$ and $\hat \tau_{1-\alphad}$  of the bootstrap distribution
of  $\frac{\bar X^*-\bar X}{S^*}$
\item This yields the $1-\alpha$ confidence interval
$$[\bar X-\hat \tau_{1-\alphad}S,
\bar X-\hat \tau_{\alphad}S]$$
\end{itemize}

\bigbreak

The use of pivotal statistics and the corresponding construction of bootstrap-t intervals
 is motivated by theoretical results which show that under mild conditions it is second order accurate.

Consider generally $1-\alpha$ confidence intervals of the form $[t_{low},t_{up}]$ of $\theta$.
Upper and lower bounds of such intervals are determined from the data, $t_{low}\equiv t_{low}(X_1,\dots,X_n)$,
$t_{up}\equiv t_{up}(X_1,\dots,X_n)$, and their accuracy depends on the particular procedure applied.

\begin{itemize}
\item (Symmetric) confidence intervals are said to be {\bf first-order accurate} if there exist some constants $d_1,d_2<\infty$ such that
for sufficiently large $n$
$$|P(\tht<t_{low})-\alphad|\le \frac{d_1}{\sqrt{n}}, \quad |P(\tht>t_{up})-\alphad|\le \frac{d_2}{\sqrt{n}}.$$
 \item (Symmetric) confidence intervals are said to be {\bf second-order accurate} if there exist some constants $d_3,d_4<\infty$ such that
for sufficiently large $n$
$$|P(\tht<t_{low})-\alphad|\le \frac{d_3}{n}, \quad |P(\tht>t_{up})-\alphad|\le \frac{d_4}{n}.$$
\end{itemize}

If the distribution of $\hat\theta$ is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that
\begin{itemize}
\item Standard confidence intervals based on asymptotic approximations are first-order accurate. The same holds for
the basic bootstrap intervals $[2\thth-\hat t_{1-\alphad}, 2\thth-\hat t_\alphad]$.
 \item Bootstrap-t intervals  are second-order accurate.
\end{itemize}
The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order
accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals
as well as tests should thus be based on pivotal statistics.


\section{Regression Analysis: Bootstrapping pairs}

{\bf Problem:} Analyze the influence of a vector    $X:=(X_1,X_2,\ldots,X_p)^T\in\R^p$ of explanatory
variables on a response
variable  (or ``dependent'' variable) $Y$.

Bootstrapping regression estimates is straightforward under {\em random design}.

\begin{itemize}
    \item {\bf Random design:}\\ Data consists of an i.i.d sample
        $(Y_1,X_1),\ldots,(Y_n,X_n)$
   \item  {\bf Model}
        \begin{align*}
           Y_i=X_i^T\beta+ \ep_i, i=1,\dots,n,
        \end{align*}
        where $E(\epsilon_i|X_i)=0$ and either
        \begin{itemize}
        \item[a)] $E(\epsilon_i^2|X_i)=\sigma^2$, $i=1,\dots,n$, for a fixed
        $\sigma^2<\infty$ (homoscedastic errors), or
        \item[b)] $E(\epsilon_i^2|X_i)=\sigma^2(X_i)<\infty$, $i=1,\dots,n$ (heteroscedastic errors).
        \end{itemize}
        \item Additional assumptions required for asymptotic theory
        \begin{itemize}
        \item[i)] There exists a positive definite matrix $M$ such that
        $E(X_iX_i^T)=M$.
        \item[ii)] There exists a positive semi-definite matrix $Q$ such that
        $$E(\epsilon_i^2X_iX_i^T)=E(\sigma^2(X_i)X_iX_i^T)=Q$$
        {\em Note: For homoscedastic errors we have $Q=\sigma^2 M$.}
        \end{itemize}
\end{itemize}

The least squares estimator $\hat\beta$ is given by
\begin{align*}
\hat\beta&=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
=\beta+(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\ep_i\\
&=\beta+M^{-1}\frac{1}{n}\sum_{i=1}^n X_i\ep_i+o_p(n^{-1/2}),
\end{align*}
and the central limit theorem implies that
$$\sqrt{n}(\hat\beta-\beta)\rightarrow_D N(0,M^{-1}QM^{-1})$$

Since $(Y_i,X_i)$ are i.i.d. one may apply the nonparametric bootstrap in order to approximate
the distribution of $\hat\beta-\beta$. In the literature this procedure is usually called ``bootstrapping
pairs''.


\begin{itemize}
\item Original data: i.i.d. sample $(Y_1,X_1),\dots,(Y_n,X_n)$
\item  Random samples
$(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)$ are generated by drawing  observations independently and with replacement
from the available sample ${\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}$.
\item $(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)$ $\Rightarrow$ least squares estimator
 $$\hat\beta^*=(\frac{1}{n}\sum_{i=1}^n X_i^*X_i^{*T})^{-1}\frac{1}{n}\sum_{i=1}^n X_i^*Y_i^*$$
    \end{itemize}

It can easily be shown that the {\bf bootstrap is consistent} and for large $n$
$$\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx N(0,M^{-1}QM^{-1})$$

This allows to construct basic bootstrap confidence intervals for the regression coefficients
$\beta_j$, $j=1,\dots,p$:

\begin{itemize}
\item Determine $\alphad$ and $1-\alphad$ quantiles $\hat t_{\alphad,j}$ and $\hat t_{1-\alphad,j}$  of the conditional distribution
of  $\hat\beta_j^*$ given ${\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}$.
\item $\Rightarrow$ Approximate $1-\alpha$ (symmetric) confidence interval:
$$[2\hat\beta_j-\hat t_{1-\alphad,j}, 2\hat\beta_j-\hat t_{\alphad,j}]$$
\end{itemize}

{\bf Remark:}
The basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval,
even if the errors are heteroscedastic (unequal variances)! This is not true for the standard $t$-intervals provided by standard software packages.


\section{Regression Analysis: Residual bootstrap}

Bootstrapping pairs is not necessarily applicable for fixed design or if the $X_i$-variables are correlated. In this
case $(Y_1,X_1),\dots,(Y_n,X_n)$ is {\bf not} an i.i.d. sample, and the procedure proposed above will generally not be consistent. However, if error terms are homoscedastic, then it is possible to rely on
the residual bootstrap.

In the following we will formally assume a regression model
\begin{align*}
           Y_i=X_i^T\beta+ \ep_i, \quad i=1,\dots,n,
        \end{align*}
under fixed design,         where $\ep_1,\dots,\ep_n$ are i.i.d. zero mean error terms with
$E(\epsilon_i^2)=\sigma^2$ (homoscedastic errors).

{\em Remark: Though we will formally rely on a fixed design assumption, the residual bootstrap is also
applicable for random design or stochastic, but correlated $X$-variables. In these cases all arguments are meant conditionally on the given $X_i$. The  above assumptions on the error terms then of course have to
be satisfied conditionally on $X_i$.}


The idea of the residual bootstrap is very simple: The model implies that $\epsilon_1,\dots,\ep_n$ are i.i.d which suggests a bootstrap based on resampling the error terms. These errors are unobserved, but they can be
approximated by the  corresponding residuals
$$\hat \ep_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,$$
where again $\hat\beta=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i$ denotes
the least squares estimator. It is well known that
$\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\ep_i^2\rightarrow_P \sigma^2$ provides an unbiased, consistent estimator of the error variance $\sigma^2$:
$$E(\hat\sigma^2)=\sigma^2 \quad \text{and } \hat\sigma^2\rightarrow_P \sigma^2$$



{\bf  Residual bootstrap}

\begin{itemize}
\item Based on the original data and a least squares estimate $\hat\beta$
 calculate the residuals $\hat \ep_1,\dots,\hat \ep_n$.
\item Generate random samples $\hat\ep_1^*,\dots,\hat\ep_n^*$ of residuals
 by drawing  observations independently and with replacement
from ${\cal S}_n:=\{\hat \ep_1,\dots,\hat \ep_n\}$.
\item Calculate
$$Y_i^*=X_i^T\hat\beta+\hat\ep_i^*,\quad i=1,\dots,n$$
\item Bootstrap estimators $\hat\beta^*$ are determined by least squares estimation from the data
$(Y_1^*,X_1),\dots,(Y_n^*,X_n)$:
$$\hat\beta^*=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*$$
\end{itemize}


It is not difficult to understand why the residual bootstrap generally works (for homoscedastic errors !).
We have
$$\hat\beta-\beta=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\epsilon_i$$
and for large $n$ the distribution of $\sqrt{n}(\hat\beta-\beta)$ is approximately normal with mean 0 and
covariance matrix $\sigma^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$.

By construction
$$\hat\beta^*-\hat\beta=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\epsilon_i^*$$
But conditional on ${\cal S}_n$ the bootstrap error terms are i.i.d with
$$E(\hat\epsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \epsilon_i =0$$
and
$$Var(\hat\epsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \epsilon_i^2$$
By central limit arguments limit arguments one will thus obtain that for large $n$ the bootstrap
distribution of $\sqrt{n}(\hat\beta^*-\hat\beta)$ is approximately normal with mean zero and
covariance matrix  $\frac{1}{n} \sum_{i=1}^n \hat \epsilon_i^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$.
Since  $\frac{1}{n} \sum_{i=1}^n \hat \epsilon_i^2 \rightarrow_P \sigma^2$ as $n\rightarrow\infty$,
the bootstrap is consistent. For large $n$ we have approximately
\begin{align*}
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)&\approx
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta))\\
&\approx  N(0,\sigma^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1})
\end{align*}


Basic bootstrap confidence intervals for the regression coefficients
$\beta_j$, $j=1,\dots,p$:
\begin{itemize}
\item Determine $\alphad$ and $1-\alphad$ quantiles $\hat t_{\alphad,j}$ and $\hat t_{1-\alphad,j}$  of the conditional distribution
of  $\hat\beta_j^*$.
\item $\Rightarrow$ Approximate $1-\alpha$ (symmetric) confidence interval:
$$[2\hat\beta_j-\hat t_{1-\alphad,j}, 2\hat\beta_j-\hat t_{\alphad,j}]$$
\end{itemize}

In the given situation one will of course prefer to use bootstrap-t intervals. If $\gamma_{jj}$ denotes
the $j$-th diagonal element of the matrix $(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$, then
$\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}$ is a pivotal statistics, since
$$\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\rightarrow_D N(0,1).$$

A bootstrap-t interval for $\beta_j$, $j=1,\dots,p$, can thus be constructed as follows:

\begin{itemize}
\item Additionally compute $\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\ep_i^{*2}$, and
 determine $\alphad$ and $1-\alphad$ quantiles $\hat \tau_{\alphad,j}$ and $\hat \tau_{1-\alphad,j}$  of the bootstrap distribution
of
$$\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}$$
\item This yields the $1-\alpha$ confidence interval
$$[\hat\beta_j-\hat \tau_{1-\alphad,j}\hat\sigma \sqrt{\gamma_{jj}},
\hat\beta_j-\hat \tau_{\alphad,j}\hat\sigma \sqrt{\gamma_{jj}}]$$
\end{itemize}




\end{document}
