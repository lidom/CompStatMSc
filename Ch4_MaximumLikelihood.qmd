# Maximum Likelihood

## Likelihood Principle

The basic idea behind maximum likelihood estimation is very simple:  Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g. the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed. 

In (classical) maximum likelihood estimation we must be rather specific about the process that generated the data.  This is a trade off -- by imposing a fair amount of structure on the data, we get in return a very desirable estimator.  The question remains, however, whether we have made the right decision about the specific distribution/density function.

### Properties of Maximum Likelihood Estimators {-}

Why do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator $\hat\theta$ of some parameter $\theta\in\mathbb{R}$ is

* **Consistent:**  $\hat\theta_n\rightarrow_p\theta$ as $n\to\infty$
* **Asymptotically normal:** $\sqrt{n}(\hat\theta_n-\theta) \stackrel{a}{\sim} \mathcal{N}(0, \sigma^2)$
* **Asymptotically efficient:** This means that no consistent estimator has lower asymptotic mean squared error than the maximum likelihood estimator. 

Thus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.


::: {.callout-important}
## ML-estimation requires to fix the family of distributions $f(\cdot|\theta)$

Classic ML-estimation requires us to fix the general family of density functions $f$ of the i.i.d. sample variables $X_i\sim f$, $i=1,\dots,n,$ such that $f$ is known up to the parameter (vector) $\theta.$  

Examples: 

* $f$ being the probability mass function of $\mathcal{Bern}(\theta)$ with $f(x_i|\theta)=\theta$ if $x_i=1$ and $f(x_i|\theta)=1-\theta$ if $x_i=0,$ but unknown propability parameter $\theta.$
* $f$ is the normal density $f(x_i|\theta)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)\right)$ with unknown parameter vector $\theta=(\mu,\sigma^2)^T.$

This requirement can be overly restrictive. In many applications we typically do not know the general distribution family of $f.$ To address this issue, the **quasi maximum likelihood method** generalizes classic ML estimation to cases where $f$ is misspecified (see @White1982).   
:::

### Example: Coin Flipping (Bernoulli Trial) 

To introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair $\text{Coin}$ can take the value $H$ (Head) or $T$ (Tail),
$$
\text{Coin}\in\{H,T\}.
$$ 
Let $\theta$ denote the probability that we get a head $H$ 
$$
\theta=P(\text{Coin}=H)
$$ 
which implies that the probability that we get a tail $T$ is 
$$
1-\theta=P(\text{Coin}=T).
$$ 

We don't know the probability $\theta$ and our goal is to estimate $\theta$ using an i.i.d. sample of size $n$
$$
\{X_1,\dots,X_n\}%\in\{0,1\}^n
$$ 
with 
$$
X_i=\left\{
    \begin{matrix}
    0 & \text{if $i$th Coinflip}=T\\[2ex]
    1 & \text{if $i$th Coinflip}=H 
    \end{matrix}
    \right.
$$ 
such that 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}\mathcal{Bern}(\theta),\quad i=1,\dots,n,
$$
where $\mathcal{Bern}(\theta)$ denotes the Bernoulli distribution with unknown probability of success parameter $\theta.$ 

A given realization of the random sample 
$$
\{X_1,\dots,X_n\}=\{x_1,\dots,x_n\}
$$ 
consists of 
$$
h=\sum_{i=1}^n1_{(x_i=1)}
$$ 
many heads and $n-h$ many tails, where  
$$
0\leq h\leq n\quad\text{and}\quad 0\leq n-h\leq n.
$$ 


#### The (Log-)Likelihood Function {-}

How do we combine the information from the $n$ observations 
$$
\{X_1,\dots,X_n\}=\{x_1,\dots,x_n\}
$$ 
to estimate $\theta$?

If the observations are realizations of an i.i.d. sample, then the joint probability of observing $h$ heads $H$ and $n-h$ tails $T$ in  $n$ coin flips is:
$$
\begin{align*}
\mathcal{L}(\theta)
&= \left(P(\text{Coin}=H)\right)^h\left(P(\text{Coin}=T)\right)^{n-h}\\[2ex]
&= \theta^h(1-\theta)^{n-h}  \\[2ex]
&= \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} 
\end{align*} 
$$
where 
$$
x_i=\left\{
    \begin{matrix}
    0 & \text{if $i$th Coinflip}=T\\[2ex]
    1 & \text{if $i$th Coinflip}=H 
    \end{matrix}
    \right.
$$ 
is the observed realization of $X_i.$ 


The function $\mathcal{L}$ is called the **likelihood function**. 

In general, when the observations $\{x_1,\dots,n\}$ are a realization of an i.i.d. sample $\{X_1,\dots,X_n\}$ with $X_i\sim f$ for all $i=1,\dots,n$, we have that
$$
\mathcal{L}(\theta)=\prod_{i=1}^n f(x_i|\theta),
$$
where $f(x_i | \theta)$ is the density function of the random variable $X_i$ evaluated at the realization $X_i=x_i$, and where $\theta$ denotes the (unknown) parameter (vector) of the density function. 


#### Estimation Idea {-}

We estimate the unknown $\theta$ by maximizing the likelihood of the observed data $\{x_1,\dots,x_n\}.$ The value $\hat\theta$ at which the likelihood function $\mathcal{L}(\cdot)$ is maximized is called the **maximum likelihood (ML) estimator**
$$
\begin{align*}
\hat\theta
&=\arg\max_\theta \mathcal{L}(\theta)\\[2ex]
&=\arg\max_\theta \prod_{i=1}^n f(x_i|\theta)
\end{align*}
$$


In our coin flip example this means to estimate the unknown $\theta$ by the value $\hat\theta$ at which the likelihood of the observed $0$ and $1$ outcomes $\{x_1,\dots,x_n\}$ is maximal 
$$
\hat\theta_{ML} = \arg\max_\theta \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}.
$$


Usually it's easier to work with sums rather than products, so we can apply a monotonic transformation by taking the logarithm of the likelihood which leads to the **log-likelihood function**:
$$
\ell(\theta)=\ln\mathcal{L}(\theta)=\sum_{i=1}^n \ln f(x_i|\theta).
$$
Since this is only a monotonic transformation we have that
$$
\begin{align*}
\hat\theta_{ML}
&=\arg\max_\theta \mathcal{L}(\theta)\\[2ex]
&=\arg\max_\theta \ell(\theta),
\end{align*}
$$
but $\ell(\theta)$ gives a more simple structure simplifying the maximization problem. 

In our coin flipping example:


$$
\begin{align*}
\mathcal{L}(\theta) &= \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} \\[2ex]
\Rightarrow\quad \ell(\theta)&=\sum_{i=1}^n\left( x_i \ln(\theta) + (1-x_i)\ln(1-\theta)\right)
\end{align*}
$$

The coin flip example is actually so simple that we can maximize $\ell(\theta)$ analytically:
$$
\begin{align*}
\ell'(\theta)&=\sum_{i=1}^n \left(x_i\dfrac{1}{\theta} - (1-x_i)\dfrac{1}{1-\theta}\right)\\[2ex]
&=\dfrac{h}{\theta} - \dfrac{n-h}{1-\theta} 
\end{align*}
$$
Setting the above expression to zero and solving gives us our maximum likelihood estimator (MLE):
$$
\begin{array}{rrcl}
&\ell'(\hat\theta_{ML})&\overset{!}{=}&0\\[2ex]
\Leftrightarrow&\dfrac{h}{\hat\theta_{ML}} &=& \dfrac{n-h}{1-\hat\theta_{ML}} \\[2ex]
\Leftrightarrow&h-h\hat\theta_{ML}  &=& n\hat\theta_{ML}-h\hat\theta_{ML}\\[2ex]
\Leftrightarrow&\hat\theta_{ML}&=&\dfrac{h}{n}
\end{array}
$$

Usually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE.  


## Numeric Optimization

Usually we are not so fortunate as to have a closed-form analytical solution for the MLE and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function. 

<!-- [numerically with the help of the computer](https://jaimemosg.github.io/EstimationTools/index.html) -->

**General idea:** 

1. Start at some value for parameters in the parameter space (i.e., in the space of all possible parameter-values)
2. Search across that space until values of parameters are found that yield a derivative of the log likelihood that is zero (or effectively zero, smaller than some convergence criterion.).


### Newton-Raphson Optimization

One of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function. 

::: {.callout-note}

* By strange convention Newton usually shares credit for this algorithm when
it is applied to root-finding, but not when it is used for optimization. However, root-finding can be used for finding the root of the first-derivative function and thus can be used for optimization. 

* Note that minimization and maximization are essentially the same problems since minimizing a function $f(x)$ with respect to $x$ is equivalent to maximizing $-f(x)$ with respect to $x.$
:::


Let $f$ be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of $f$ around the point $\theta$ are:
$$
\begin{align*}
\text{First-order:}\quad &f(\theta+h)\approx \overbrace{f(\theta)+f'(\theta)h}^{\text{Taylor Polynomial (Order 1)}} \\
\text{Second-order:}\quad& f(\theta+h)\approx \underbrace{f(\theta)+f'(\theta)h + \frac{1}{2} f''(\theta)h^2}_{\text{Taylor Polynomial (Order 2)}},
\end{align*}
$$
Locally, i.e. for $h\approx 0,$ the Taylor polynomials are very good approximations of $f(\theta + h);$ see @fig-taylorApprox.

```{r, echo = FALSE}
#| label: fig-taylorApprox
#| fig-cap: First- and second-order Taylor approximations of a function $f$ around $\theta=1.$

## #######################
## Taylor Approximation
## #######################
# install.packages("pracma")
library("pracma")

myFun <- function(x){
  1*exp((-1/2)*(x-1.25)^2)/10 + 
  1*exp((-1/3)*(x-3)^2)/5
}

## Taylor approximation of myFun around x0
x0              <- 1
taylor_poly_1   <- taylor(f = myFun, x0 = x0, n = 1)
taylor_poly_2   <- taylor(f = myFun, x0 = x0, n = 2)

## Plot
#x_seq           <- seq(from = x0 -2, to = x0 + 5, length.out=100)
x_seq           <- seq(from = -.5, to = 4, length.out=100)
y_myFun         <- myFun(x_seq)
y_taylor_poly_1 <- polyval(taylor_poly_1, x_seq)
y_taylor_poly_2 <- polyval(taylor_poly_2, x_seq)


plot(x  = x_seq, y = y_myFun,  type = "l", col = "black", lwd = 1, ylab="", xlab="", main = "Taylor Approximation", ylim = range(y_myFun,y_taylor_poly_1,y_taylor_poly_2))
axis(1, at=1, line=2, labels = expression(theta), tick=FALSE)
#axis(1, at=x_seq[which.max(y_taylor_poly_2)], line=2, labels = expression(theta[1]), tick=FALSE)
lines(x = x_seq, y = y_taylor_poly_1, col = "red", lty=1)
lines(x = x_seq, y = y_taylor_poly_2, col = "blue", lty=1)
points(x=1, y=myFun(1), col="black", pch=19)
# points(x=x_seq[which.max(y_taylor_poly_2)], y=max(y_taylor_poly_2), col="blue", pch=19)
lines(x=rep(x0, 2), y=c(par("usr")[3], myFun(1)), lty=2)
# lines(x=rep(x_seq[which.max(y_taylor_poly_2)],2), 
#       y=c(0,max(y_taylor_poly_2)), lty=2)
legend(x="topleft", legend=c("Function f", "Taylor Polynomial (Order 1)", "Taylor Polynomial (Order 2)"), col=c("black", "red", "blue"), lty=c(1,1,1), box.lwd=0)
box()
```

<!-- ::: {.callout-important}
The first-order and the second-order Taylor polynomial can both be used to approximate $f.$

The second-order Taylor polynomial can be used to apprixmate $f'.$
::: -->


#### Optimization Idea {-}

Let $\ell$ be a log-likelihood function with continuous first, $\ell',$ and second, $\ell'',$ derivative. 

To optimize the log-likelihood function $\ell,$ we try to find the root of $\ell',$ i.e. the value of $\theta$ such that 
$$
\ell'(\theta)=0.
$$ 
That is, we try to find the value of $\theta$ that fulfills the first order condition of the optimization problem. We do so using a step-wise ($h$ steps) optimization approach. 

**Initialization:** Let $\theta_0$ be our first guess of the root $\theta.$ 

**$h$-Steps:** Typically, $\theta_0\neq\theta$ and thus $\ell'(\theta_0)\neq 0.$ Therefore, we want to move from $\theta_0$ to a new root-candidate $\theta_1$ by doing an $h$-step update 
$$
\theta_1 = \theta_0 + h.
$$

<!-- 1. We select some starting value $\theta_0.$ 
2. Optimize the second-order Taylor polynomial of $f$ around $\theta_0$ with respect to $h.$ 
3. In each of the following steps, we optimize new second-order Taylor polynomials of $f$ at those values $\theta,$ for which the previous second-order Taylor polynomial was maximal.  -->

<!-- **Implementation-Idea:** The second-order Taylor-series approximation gives -->
The first-order Taylor-series approximation of $\ell'$ around our first guess $\theta_0$ gives 
$$
\begin{align*}
\ell'(\theta_0 + h) & \approx \ell'(\theta_0) + \ell''(\theta_0)h 
\end{align*}
$$
Thus, to find the $h$-step that brings us closer to the root of $\ell',$ we can (approximatively) use the $h$-step that brings us to the root of its first-order approximation, i.e.
$$
\begin{align*}
\ell'(\theta_0) + \ell''(\theta_0)h = 0\\[2ex]
\Rightarrow h = \frac{\ell'(\theta_0)}{\ell''(\theta_0)}.
\end{align*}
$$
Based on this $h$-step, the new root-candidate is 
$$
\theta_1 = \theta_0 - \frac{\ell'(\theta_0)}{\ell''(\theta_0)}.
$$
Likewise, the $n$th root-candidate is 
$$
\theta_n = \theta_{n-1} + \frac{\ell'(\theta_{n-1})}{\ell''(\theta_{n-1})};
$$
see also @fig-NR.


```{r, echo = FALSE}
#| label: fig-NR
#| fig-cap:  A step in the Newton-Raphson root-finding method.

## #######################
## Taylor Approximation
## #######################
# install.packages("pracma")
library("pracma")

my_ell <- function(x){
  1*exp((-1/2)*(x-1.25)^2)/10 + 
  1*exp((-1/3)*(x-3)^2)/5
}
my_ell_expr <- expression(
  1*exp((-1/2)*(x-1.25)^2)/10 + 
  1*exp((-1/3)*(x-3)^2)/5
)

my_ell_prime <- function(x){}
body(my_ell_prime) <- D(my_ell_expr, "x") 

## Taylor approximation of my_ell around theta_0
theta_0         <- 1.5
taylor_poly_1   <- taylor(f = my_ell_prime, x0 = theta_0, n = 1)

## Plot
theta_seq       <- seq(from       = .75, 
                       to         = 4, 
                       length.out = 100)
y_my_ell_prime  <- my_ell_prime(theta_seq)
y_taylor_poly_1 <- polyval(taylor_poly_1, theta_seq)


root_taylor_poly_1 <- -1 * taylor_poly_1[2] / taylor_poly_1[1]

plot(x = theta_seq, 
     y = y_my_ell_prime,  
     type = "l", col = "black", lwd = 1, ylab="", xlab="", main = "", ylim = range(y_my_ell_prime,y_taylor_poly_1))
abline(h=0)
points(x=theta_0, y=my_ell_prime(theta_0), col="black", pch=19)
lines(x = theta_seq, y = y_taylor_poly_1, col = "red", lty=1)
lines(x=rep(theta_0, 2), y=c(par("usr")[3], my_ell_prime(theta_0)), lty=2)
axis(1, at=theta_0, line=2, labels = expression(theta['n-1']), tick=FALSE)
points(x=root_taylor_poly_1, y= 0, col="red", pch=19)
lines(x=rep(root_taylor_poly_1, 2), y=c(par("usr")[3], 0), lty=2)
axis(1, at=root_taylor_poly_1, line=2, labels = expression(theta['n']), tick=FALSE)
legend(x="topright", legend=c("1st Deriv. of Log-Likelihoodfunc.", expression("Taylor Polynomial (Order 1) around"~theta[n-1])), 
col=c("black", "red"), lty=c(1,1), box.lwd=0)
box()
```



One can shown that if $\ell'$ is "well behaved" at its root $\theta$ (i.e. if $\ell''(\theta)\neq 0$ and if $\ell'''(\theta)$ is finite and continuous at $\theta$) and you start with $\theta_0$ "close enough" to the root $\theta,$ then $\theta_n$ will fastly converge to $\theta.$ Unfortunately, we don't know if $\ell'$ is well behaved at $\theta$ until we know $\theta,$ and we don't know beforehand how close is "close enough". So, we cannot guarantee convergence of the Newton-Raphson algorithm. 

<!-- However, if $\theta_n\to\theta$ then, since $\ell'$ and $\ell''$ are continuous, we have
$$
\begin{align*}
\theta = \lim_{n\to\infty}\theta_{n+1} 
&=\lim_{n\to\infty}\left(\theta_{n} + \frac{\ell'(\theta_{n})}{\ell''(\theta_{n})}\right)\\[2ex]
&=\lim_{n\to\infty}\theta_{n} + \frac{\ell'(\lim_{n\to\infty}\theta_{n})}{\ell''(\lim_{n\to\infty}\theta_{n})}\\[2ex]
&=\theta + \frac{\ell'(\theta)}{\ell''(\theta)}.
\end{align*}
$$  
Therefore, provided that $\ell''(\theta)\neq \pm\infty,$ we must have that $\ell'(\theta) = 0.$ -->

**Checking convergence:** Since we are expecting that $\ell'(\theta_n)\to 0,$ a good stopping condition for the Newton-Raphson algorithm is 
$$
|\ell'(\theta_n)|\leq \varepsilon
$$ 
for some (small) tolerance $\varepsilon>0.$ 

::: {.callout-note}
# Pseudo-Code: Newton-Raphson Algorithm
$$
\begin{array}{ll}
\texttt{\textbf{let }} \theta_0=s  &  \\
\texttt{\textbf{let }} n=0                &  \\
\texttt{\textbf{while }}  | \ell'(\theta_i) | >\varepsilon & \texttt{\textbf{do}}\\
&\left[
                                    \begin{array}{l}\texttt{\textbf{let }} n = n+1 \\
                                    \texttt{\textbf{let }} \theta_n = \theta_{n-1} - \frac{\ell'(\theta_{i-1})}{\ell''(\theta_{n-1})} \\
                                    \end{array} \right.\\
\texttt{\textbf{let }}\hat\theta=\theta_n & \\
\texttt{\textbf{return }} \hat\theta &  \\
\end{array}
$$
::: 

::: {.callout-tip}
* For problems that are globally concave, the starting value $s$ doesn't matter.  For more complex problems, however, the  Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to **try multiple starting values**.

* In actual practice, implementation of the Newton-Raphson algorithm can be tricky.
We may have $\ell''(\theta_n)=0,$ in which case the function looks locally like a straight line, with no solution to the Taylor series approximation 
$$
\begin{align*}
\ell'(\theta_n + h) & \approx \ell'(\theta_n) + \ell''(\theta_n)h. 
\end{align*}
$$
In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on $\ell'(\theta_n).$


* In other cases where $\theta_n$ is too far from the true maximizer $\theta$, the Taylor approximation may be so inaccurate that $\ell(\theta_{n+1})$ is actually smaller than $\ell(\theta_{n}).$ When this happens one may replace $\theta_{n+1}$ with $(\theta_{n+1}+\theta_{n})/2$ (or some other value between $\theta_{n}$ and $\theta_{n+1}$) in the hope that a smaller step will produce better results.
:::

#### Newton-Raphson Algorithm: Coin-Flipping Example {-} 

Let's return to our earlier coin-flipping example. 

If we observe, for instance, only one head $h=1$ for a sample size of $n=5,$ we already know that $\hat\theta_{ML}=\frac{h}{n}=\frac{1}{5}=0.2,$ but let's apply the Newton-Raphson Algorithm.  

Recall that
$$
\begin{align*}
\ell'(\theta)&=\dfrac{h}{\theta} - \dfrac{n-h}{1-\theta} \\[2ex]
\ell''(\theta) &= -\dfrac{h}{\theta^2} + \dfrac{n}{(1-\theta)^2}(-1)-\dfrac{h}{(1-\theta)^2}(-1)\\[2ex]
&= -\dfrac{h}{\theta^2} - \dfrac{n-h}{(1-\theta)^2}
\end{align*}
$$

Let's consider a sample size of $n=5,$ where one coin-flip resulted in H, i.e. $h=1,$ and four coin-flips resulted in T, i.e. $n-h=4.$ Setting the tolerance level $\varepsilon=10^{-10}$ as our convergence criterion and $\theta_0=0.4$ as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table @tbl-NR. The numeric optimization solution is $\hat\theta_{ML} = 0.2$ which equals the analytic solution. 


|$n$ 	|	$\hat\theta_n$	|	$\ell'(\hat\theta_n)$	 |  $\ell'(\hat\theta_n)/\ell''(\hat\theta_n)$ |
|-------------------|-------------------|----------------------------|---------------------------------------------|
|$0$| $0.40$| $-4.16$                        |$\phantom{-}2.40\cdot 10^{-1}$|
|$1$| $0.16$| $\phantom{-}1.48$              |$-3.32\cdot 10^{-2}$|
|$2$| $0.19$| $\phantom{-}2.15\cdot 10^{-1}$ |$-6.55\cdot 10^{-3}$|
|$3$| $0.19$| $\phantom{-}5.43\cdot 10^{-3}$ |$-1.73\cdot 10^{-4}$|
|$4$| $0.19$| $\phantom{-}3.53\cdot 10^{-6}$ |$-1.13\cdot 10^{-7}$|
|$5$| $0.20$| $\phantom{-}1.50\cdot 10^{-12}$|$-4.81\cdot 10^{-14}$|

: Result of applying the Newton Raphson optimaization algorithm to our coin flipping example for given data $h=1$ with sample size $n=5$. {#tbl-NR}





## OLS-Estimation as ML-Estimation

Now let's return to the linear regression model 
$$
Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
$${#eq-LinMod}
where $Y_i\in\mathbb{R}$ denotes the response (or "dependent") variable, 
$$
\beta=(\beta_1,\dots,\beta_p)^T\in\mathbb{R}^p
$$ 
denotes the vector of unknown parameter values, and 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
$$
denotes the vector of predictor variables, where the i.i.d. sample 
$$
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
$$
follows a **random design** (@def-RandomFixedDesign). 



For the following, it is convenient to write @eq-LinMod using matrix notation
$$
\begin{eqnarray*}
  \underset{(n\times 1)}{Y}&=&\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}
$$
where 
$$
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&\dots&X_{1K}\\\vdots&\ddots&\vdots\\ X_{n1}&\dots&X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}
$$



To apply classical ML-estimation, we must make a distributional assumption about $\varepsilon_i$ such as, for instance,
$$
\begin{equation*}
\varepsilon \sim \mathcal{N}\left(0, \sigma^2I_n\right).
\end{equation*}
$$
We could also choose another distributional assumption for $\varepsilon,$ but the classical ML estimation theory requires us to assumed the correct error distribution.  This requirement is much more restrictive than requirements for analyzing the OLS estimator under standard large sample inference. However, taking into account specific distributional assumptions allows us to consider also more complicated non-linear regression models such as, for instance, logistic regression. 


<!-- \begin{itemize} -->
<!-- \item The $\varepsilon$'s are jointly normally distributed. -->
<!-- \item The $\varepsilon$'s are independent of one another. -->
<!-- \item The $\varepsilon$'s are identically distributed, i.e. homoskedastic. -->
<!-- \end{itemize} -->


The multivariate density for $\varepsilon=(\varepsilon_1,\dots,\varepsilon_n)'$ is then
$$
\begin{equation*}
f(\varepsilon)=\dfrac{1}{(2\pi \sigma^2)^{n/2}} e^{-\left(\frac{\varepsilon'\varepsilon}{2\sigma^2}\right)}.
\end{equation*}
$$
Noting that $\varepsilon=Y-X\beta$, we get the log likelihood
$$
\begin{align*}
\ell(\beta,\sigma^2)& =-\dfrac{n}{2} \ln(2\pi) - \dfrac{n}{2}\ln(\sigma^2) - \dfrac{1}{2 \sigma^2}(Y-X\beta)'(Y-X\beta)
\end{align*}
$$
with $K$ unknown parameters $\beta=(\beta_1,\dots,\beta_K)'\in\mathbb{R}^K$ and $\sigma^2\in\mathbb{R}_{>0}.$

Taking derivatives gives 
$$
\begin{align*}
\dfrac{\partial \ell}{\partial \beta}(\beta,\sigma^2)    &= \underset{(K\times 1)}{- \dfrac{1}{\sigma^2}(-X'Y + X'X\beta)} \\[2ex]
\dfrac{\partial \ell}{\partial \sigma^2}(\beta,\sigma^2) 
%&= -\dfrac{n}{2\sigma^2}+ \dfrac{1}{2\sigma^4}(Y-X\beta)'(Y-X\beta)
&=-\frac{n}{2 \sigma^{2}}+\left[\frac{1}{2}(Y-X\beta)'(Y-X\beta)\right]\frac{1}{\left(\sigma^{2}\right)^{2}} \\
%&=\frac{1}{2 \sigma^{2}}\left[\frac{1}{\sigma^{2}} (Y-X\beta)'(Y-X\beta)-n\right]
\end{align*}
$$

::: {.callout-tip}

# Score Function

More generally, let $\ell(\theta)$ denote the log-likelihood function of a $p$-dimensional parameter vector $\theta=(\theta_1,\dots,\theta_p).$ 

Then the gradient vector 
$$\left(
  \dfrac{\partial \ell}{\partial \theta_1}(\theta), \dots, 
  \dfrac{\partial \ell}{\partial \theta_p}(\theta)
  \right),
$$ 
is called the **score-function**.  

The score function is random, since it depend on the random sample. The score function satisfies 
$$
\mathbb{E}\left(\dfrac{\partial \ell}{\partial \theta_j}(\theta)\right)=0
$$ 
for all $j=1,\dots,p.$
:::

So, we have a system of $K+1$ equations and $K+1$ unknowns. Setting equal to zero and solving gives:
$$
\begin{align*}
& - \dfrac{1}{\sigma^2}(-X'Y + X'X\hat\beta_{ML})  \overset{!}{=}0\\[2ex]
\Rightarrow\quad & \hat\beta_{ML}=(X'X)^{-1}X'Y\\[2ex]
\end{align*}
$$
and 
$$
\begin{align*}
&-\frac{n}{2 s_{ML}^2}+\left[\frac{1}{2}(Y-X\beta)'(Y-X\beta)\right]\frac{1}{\left(s_{ML}^2\right)^{2}}  \overset{!}{=}0\ \\[2ex]
\Rightarrow\quad  &
s_{ML}^2 =\dfrac{1}{n}(Y-X\hat\beta_{ML})'(Y-X\hat\beta_{ML})\\[2ex]
& =\dfrac{1}{n}\sum_i^n \hat\varepsilon_i^2.
\end{align*}
$$
Thus, the MLE of the linear model, $\hat\beta_{ML}$, is the same as the OLS estimator, $\hat\beta$. Moreover, since the ML estimator $\hat\beta_{ML}$ is here equivalent to the OLS estimator (same formula, same mean, same variance) we can use the classic inference machinery ($t$-test, $F$-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class). 


Since needed for the next chapter, we also give here the second derivatives of the log-likelihood function $\ell$ as well as the expressions of minus one times the mean of the second derivatives of the log-likelihood function $\ell$:

1. First and second derivative with respect to $\beta:$
$$
\begin{align*}
\dfrac{\partial^2 \ell}{\partial \beta\partial \beta}(\beta,\sigma^2)
&= - \dfrac{1}{\sigma^2}(X'X)
\end{align*}
$$
$$
\begin{align*}
\Rightarrow\quad 
&(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \beta\partial \beta}(\beta,\sigma^2)\right)\\[2ex]
&=   \dfrac{1}{\sigma^2} \mathbb{E}(X'X)\\[2ex]
&=  \dfrac{n}{\sigma^2} \Sigma_{X'X},
\end{align*}
$$
since  
$$
\mathbb{E}\left(X'X\right)
=\mathbb{E}\left(\sum_{i=1}^nX_iX_i'\right)
=n\underbrace{\mathbb{E}\left(X_iX_i'\right)}_{=:\Sigma_{X'X}} = n\Sigma_{X'X}.
$$ 

2. First and second derivative with respect to $\sigma^2:$
$$
\begin{align*}
\dfrac{\partial^2 \ell}{\partial \sigma^2\partial \sigma^2}(\beta,\sigma^2) 
&=\frac{n}{2 \left(\sigma^{2}\right)^2}-\dfrac{(Y-X\beta)'(Y-X\beta)}{\left(\sigma^{2}\right)^{3}} \\[2ex]
&=\frac{n}{2\sigma^{4}}-\frac{\sum_{i=1}^n\varepsilon_i^2}{\sigma^{6}} \\[2ex]
\end{align*}
$$
$$
\begin{align*}
\Rightarrow\quad 
&(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \sigma^2\partial \sigma^2}(\beta,\sigma^2)\right)\\[2ex]
&=\left(-\frac{n}{2\sigma^{4}}+\frac{\mathbb{E}\left(\sum_{i=1}^n\varepsilon_i^2\right)}{\sigma^{6}} \right)\\[2ex]
&=\left(-\frac{n}{2\sigma^{4}}+\frac{n\sigma^2}{\sigma^{6}}\right)\\[2ex]
&=\frac{n}{2\sigma^{4}}\\[2ex]
\end{align*}
$$
3. First derivative with respect to $\beta,$ second derivative with respect to $\sigma^2:$
$$
\begin{align*}
\dfrac{\partial^2 \ell}{\partial \beta \partial \sigma^2}(\beta,\sigma^2)
=   \dfrac{\partial^2 \ell}{\partial \sigma^2 \partial \beta}(\beta,\sigma^2)
&= -\frac{X'(Y-X\beta)}{\sigma^4}\\[2ex]
& = \frac{X'\varepsilon}{\sigma^4}\\
\end{align*}
$$
$$
\begin{align*}
\Rightarrow\quad 
&(-1)\cdot  \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \sigma^2 \partial \beta}(\beta,\sigma^2)\right)\\[2ex]
&(-1)\cdot  \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \beta\partial \sigma^2}(\beta,\sigma^2)\right)\\[2ex]
&=\frac{\mathbb{E}(X'\varepsilon)}{\sigma^4}\\[2ex]
&=\frac{\mathbb{E}(\mathbb{E}(X'\varepsilon|X))}{\sigma^4}\\[2ex]
&=\frac{\mathbb{E}(X'\mathbb{E}(\varepsilon|X))}{\sigma^4}\\[2ex]
&=0,
\end{align*}
$$
since $\mathbb{E}(\varepsilon_i|X_i)=0.$

### Variance of ML-Estimators $\hat\beta_{ML}$ and $s^2_{ML}$ {#sec-varMLE}

The variance of an MLE is given by the inverse of the **Fisher information matrix**. 

The Fisher information matrix is defined as minus one times the expected value matrix of second derivatives of the log-likelihood function. 

For the case of the multiple linear regression model (@eq-LinMod), the Fisher information matrix is
$$
\begin{align*}
&\mathcal{I}\left(\beta, \sigma^2\right)\\[2ex]
&=
\left[\begin{array}{cc}
(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \beta\partial \beta}(\beta,\sigma^2)\right) & 
(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 L}{\partial \sigma^2 \partial \beta}(\beta,\sigma^2)\right)\\
(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 L}{\partial \sigma^2 \partial \beta}(\beta,\sigma^2)\right) & 
(-1)\cdot \mathbb{E}\left(\dfrac{\partial^2 \ell}{\partial \sigma^2\partial \sigma^2}(\beta,\sigma^2) \right)
\end{array}\right]\\[2ex]
% &=
% \left[\begin{array}{cc}
% \frac{1}{\sigma^2}E(X'X) & 0 \\
% 0 & \frac{n}{2\sigma^4}
% \end{array}\right]\\[2ex]
&=
\left[\begin{array}{cc}
\dfrac{n}{\sigma^2}\Sigma_{X'X} & 0 \\[2ex]
0 & \ \dfrac{n}{2\sigma^4}
\end{array}\right],
\end{align*}
$$

<!-- While the upper left element of the Fisher information matrix is easily seen, the derivation of the lower right element is rather tedious and thus omitted.
[^1]: See [https://www.statlect.com/fundamentals-of-statistics/linear-regression-maximum-likelihood](https://www.statlect.com/fundamentals-of-statistics/linear-regression-maximum-likelihood) for more details. -->

Taking the inverse of the Fisher information matrix gives the variance-covariance matrix of the vector of estimators $(\hat\beta_{ML}, s_{ML}^2)$
$$
\begin{align*}
Var\left(\begin{array}{c}\hat\beta_{ML} \\ s_{ML}^2\end{array}\right) 
& = \left(\mathcal{I}\left(\beta, \sigma^2\right)\right)^{-1}\\[2ex]
& =
\left[\begin{array}{cc}
\dfrac{\sigma^2}{n}\Sigma_{X'X}^{-1} & 0 \\
0 & \ \dfrac{2\sigma^4}{n}
\end{array}\right].
\end{align*}
$$
That is, 
$$
Var\left(\hat\beta_{ML}\right) = \dfrac{\sigma^2}{n}\Sigma_{X'X}^{-1}
$$
and
$$
Var\left(s_{ML}^2\right) = \dfrac{2\sigma^4}{n}.
$$
Given this result, it is easy to see that 
$$
Var(\hat\beta_{ML}) \to 0\quad\text{and}\quad Var(s_{ML}^2) \to 0
$$ 
as $n\to\infty$.


### Consistency of $\hat\beta_{ML}$ and $s_{ML}^2$

If $E[\varepsilon|X]=0$ (strict exogeneity, follows from the random design (@def-RandomFixedDesign) assumption), then the bias of $\hat\beta$ is zero since $E[\hat\beta_{ML}]=\beta$
$$
\begin{align*}
E[\hat\beta_{ML}]&=E[(X'X)^{-1}X'(X\beta + \varepsilon)] \\
                 &=E[E[(X'X)^{-1}X'(X\beta + \varepsilon)|X]] \\
                 &=E[E[(X'X)^{-1}X'X\beta|X]] + E[E[(X'X)^{-1}X'\varepsilon|X]] \\
                 &=E[E[\beta|X]] + E[(X'X)^{-1}X'E[\varepsilon|X]] \\
                 &=        \beta + E[(X'X)^{-1}X'E[\varepsilon|X]] \\
                 &=        \beta  \\
\Leftrightarrow E[\hat\beta_{ML}]-\beta&=\operatorname{Bias}(\hat\beta_{ML})=0
\end{align*}
$$
Of course, from this it also follows that the squared bias is equal to zero 
$$
\text{Bias}^2(\hat\beta_{ML})=0.
$$  
This implies that the mean square error (MSE) of the ML estimator $\hat\beta_{ML}$ equals the variance of the ML estimator $\hat\beta_{ML}$: 
$$
\operatorname{MSE}(\hat\beta_{ML})=\underbrace{E[(\hat\beta_{ML}-\beta)^2]=Var(\hat\beta_{ML})}_{\text{MSE}(\hat\beta_{ML})=Var(\hat\beta_{ML})\text{ since }\hat\beta_{ML}\text{ is unbiased.}}\to 0\quad\text{as}\quad n\to\infty.
$$
Since convergence in mean square implies convergence in probability, we have established that the ML-estimator $\hat\beta_{ML}$ is a (weakly) consistent estimator of $\beta$
$$
\hat\beta_{ML}\to_p \beta\quad\text{as}\quad n\to\infty.
$$

Moreover, one can also show that $s_{ML}^2$ is a biased but *asymptotically unbiased* estimator, that is 
$$
\left(\operatorname{Bias}(s^2_{ML})\right)^2\to 0
$$ 
as $n\to\infty$. Together with the result that $Var(s^2_{ML})\to 0$ as $n\to\infty$ we have that
$$
\begin{align*}
\operatorname{MSE}(s^2_{ML})&=E[(s^2_{ML}-\sigma^2)^2]\\
&=\operatorname{Bias}^2(s^2_{ML})+Var(s^2_{ML})\to 0\quad\text{as}\quad n\to\infty.
\end{align*}
$$
Again, since convergence in mean square implies convergence in probability, we have established that the ML-estimator $s^2_{ML}$ is a (weakly) consistent estimator of $\sigma^2$
$$
s^2_{ML}\to_p \sigma^2\quad\text{as}\quad n\to\infty.
$$


<!-- In practice, however, one usually works with the unbiased (and consistent) alternative $s_{UB}^2=\dfrac{1}{n-K}\sum_{i=1}^n \hat{\varepsilon}_i^2$ even though one can show that $\operatorname{MSE}(s^2_{ML})<\operatorname{MSE}(\hat\sigma^2_{UB})$ for sufficiently large $n$. -->



## Asymptotic Theory of Maximum-Likelihood Estimators 

In the following, we consider the asymptotic distribution of ML-estimators. 


We only consider the simplest situation: Assume a random sample  
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X,
$$ 
where $X\in\mathbb{R}$ is a univariate random variable with density function 
$$f(x|\theta),
$$ 
where the true (unknown, univariate) parameter $\theta\in\mathbb{R}$ is an interior point of a compact parameter interval 
$$\Theta=[\theta_l,\theta_u]\subset\mathbb{R}.
$$ 
**Note:** $\theta$ is an "interior point" of $\Theta$ if $\theta_l<\theta<\theta_u.$

Moreover let

* Likelihood function:
$$
\mathcal{L}_n(\theta)=\prod_{i=1}^n f(X_i|\theta)
$$
* Log-likelihood function:
$$
\ell_n(\theta)=\ln\mathcal{L}(\theta)=\sum_{i=1}^n \ln f(X_i|\theta)
$$
* The maximum-likelihood estimator $\hat{\theta}_n$ maximizes $\ell_n(\theta)$ uniquely such that 
$$
\left.\ell_n'(\theta)\right|_{\theta=\hat\theta_n}=0\quad\text{and}\quad\left.\ell_n''(\theta)\right|_{\theta=\hat\theta_n}<0
$$
* It is assumed that the partial derivatives 
$$
\frac{\partial}{\partial\theta}f(x|\theta)\quad\text{and}\quad \frac{\partial^2}{\partial\theta^2}f(x|\theta)
$$
exist and that these partial derivatives can be passed under the integral such that
$$
\begin{align*}
\frac{\partial}{\partial\theta}\int f(x|\theta)dx 
&=\int\frac{\partial}{\partial\theta} f(x|\theta)dx\\
\frac{\partial^2}{\partial\theta^2}\int f(x|\theta)dx 
&=\int\frac{\partial^2}{\partial\theta^2} f(x|\theta)dx
\end{align*}
$$


::: {.callout-note}

A possible **example** that fits into the above setup is the density of the exponential distribution 
$$
f(x|\theta)=\left\{
    \begin{matrix}
    \theta\exp(- \theta x)& \text{for }x\geq 0\\
    0                     & \text{for }x < 0\\
    \end{matrix}\right.
$$
with unknown "rate" parameter $\theta>0.$ 


Or, more generally, the densities of the one-parameter, $\theta\in\Theta\subset\mathbb{R},$ exponential family  
$$
f(x|\theta)=h(x)\exp(\eta(\theta) T(x) - B(\theta))
$$
where $h:$ $\mathbb{R}\to\mathbb{R},$ $T:$ $\mathbb{R}\to\mathbb{R},$ $\eta:$ $\Theta\to\mathbb{R},$ and $B:$ $\Theta\to\mathbb{R}.$
:::



The derivation of the asymptotic distribution of the ML estimator, $\hat\theta_n,$ relies on a Taylor expansion of the derivative of the log-likelihood function, 
$$
\ell_n'(\cdot),
$$ 
around $\theta$ (see @eq-MVT). To derive this expression, we use the mean value theorem (@thm-MVT).


::: {.callout-note icon=false} 
## 
::: {#thm-MVT}

# Mean Value Theorem 

*Let $f$ be continuous over the closed interval $[a,b]$ and differentiable over the open interval $(a,b).$ Then, there exists at least one point $c\in(a,b)$ such that*
$$
f'(c) = \frac{f(b)-f(a)}{b-a}
$$
*or equivalently*
$$
f(b)=f(a) + f'(c)(b-a). 
$$
:::
:::


By the Mean Value Theorem (@thm-MVT), we know that 
$$
\ell_n'(\hat{\theta}_n)=\ell_n'(\theta)+\ell_n''(\psi_n)(\hat{\theta}_n-\theta)
$${#eq-MVT}
for some $\psi_n\in(\theta,\hat{\theta}_n).$


Since $\hat{\theta}_n$ maximizes the log-Likelihood function it follows that 
$$
\ell_n'(\hat{\theta}_n)=0.
$$ 
Together with @eq-MVT, this implies that 
$$
\overbrace{\ell_n'(\hat{\theta}_n)}^{=0}=\ell_n'(\theta)+\ell_n''(\psi_n)(\hat{\theta}_n-\theta)
$$
$$
\Rightarrow\quad \ell_n'(\theta)=-\ell_n''(\psi_n)(\hat{\theta}_n-\theta).
$${#eq-ml2}
Now, note that necessarily
$$
\int_{-\infty}^{\infty} f(x|\theta)dx=1
$$ 
for *all possible values* of $\theta,$ since $f$ is a density function. 

Therefore,
$$
\begin{align*}
\frac{\partial}{\partial \theta}\int_{-\infty}^{\infty} f(x|\theta)dx&=\frac{\partial}{\partial \theta}1 = 0.
\end{align*}
$$
Using that we can here pass the partial derivative under the integral sign, we have thus
$$
\int_{-\infty}^{\infty} \frac{\partial}{\partial \theta}f(x|\theta)dx=0.
$${#eq-zero1} 
And similarly, 
$$
\begin{align*}
\frac{\partial^2}{\partial \theta^2}\int_{-\infty}^{\infty} f(x|\theta)dx&=\frac{\partial^2}{\partial \theta^2}1 = 0.
\end{align*}
$$ 
Using again that we can here pass the partial derivative under the integral sign, we have thus
$$
\int_{-\infty}^{\infty} \frac{\partial^2}{\partial \theta^2}f(x|\theta)dx=0.
$${#eq-zero2} 


Using @eq-zero1 and @eq-zero2, we can now show that the average 
$$
\frac{1}{n}\ell_n'(\theta)=\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)
$$ 
is asymptotically normal. 


Firstly, for the mean one gets:
$$
\begin{align*}
\mathbb{E}\left(\frac{1}{n}\ell_n'(\theta)\right)
&=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)\\[2ex]
&=\frac{n}{n}\mathbb{E}\left(\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)\quad[\text{i.i.d.}]\\[2ex]
&=\mathbb{E}\left(\frac{\frac{\partial}{\partial \theta}f(X_i|\theta)}{f(X_i|\theta)}\right)\quad[\text{chain rule}]\\[2ex]
&=\int_{-\infty}^{\infty} \frac{\frac{\partial}{\partial \theta}  f(x|\theta)}
{f(x|\theta)}f(x|\theta)dx\\[2ex] 
&=\int_{-\infty}^{\infty} \frac{\partial}{\partial \theta}  f(x|\theta)dx\\[2ex] 
&=0,
\end{align*}
$$
where the last step follows from @eq-zero1. 

Secondly, for the variance one gets:
$$
\begin{align*}
Var\left(\frac{1}{n}\ell_n'(\theta)\right)
&=Var\left(\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)\\
&=\frac{1}{n}Var\left(\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)\quad[\text{i.i.d.}]\\
&=\frac{1}{n}Var\left(\frac{\frac{\partial}{\partial \theta} f(X_i|\theta)}{f(X_i|\theta)}\right)\quad[\text{chain rule}]\\
&=\frac{1}{n}\mathbb{E}\left(\left(\frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}{f(X_i|\theta)}\right)^2\right)\quad[\text{above mean zero result}]\\
&=:\frac{1}{n}\mathcal{J}(\theta)
\end{align*}
$$

Moreover, the average 
$$
\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)
$$ 
is taken over i.i.d. random variables 
$$
\frac{\partial}{\partial \theta} \ln f(X_i|\theta),\quad i=1,\dots,n.
$$ 
Thus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that
$$
\frac{\frac{1}{n}\ell_n'(\hat{\theta}_n)-\mathbb{E}(\frac{\partial}{\partial \theta} \ln f(X_i|\theta))}{\sqrt{\frac{1}{n}\mathcal{J}(\theta)} }=\frac{\ell_n'(\hat{\theta}_n)}{\sqrt{n\mathcal{J}(\theta)} } \to_d \mathcal{N}(0,1)
$$
Thus using our mean value expression (@eq-ml2), we also have
$$
\frac{-\ell_n''(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}\left(\hat{\theta}_n-\theta\right) \to_d \mathcal{N}(0,1),
$$
which is equivalent to 
$$
\left(\frac{-\frac{1}{n}\ell_n''(\psi_n)}{\sqrt{\mathcal{J}(\theta)}}\right)\;\sqrt{n}\left(\hat{\theta}_n-\theta\right) \to_d \mathcal{N}(0,1).
$${#eq-MLNorm}

Further analysis requires us to study the statistic 
$$
\frac{1}{n}\ell_n''(\psi_n).
$$ 

::: {.callout-important}
Before we consider $\frac{1}{n}\ell_n''(\psi_n),$ with $\psi_n\in(\theta,\hat\theta_n),$ we begin this with studying the mean and the variance of the simpler statistic 
$$
\frac{1}{n}\ell_n''(\theta).
$$
::: 

First, the mean of $\frac{1}{n}\ell_n''(\theta):$
$$
\begin{align*}
\frac{1}{n}\ell_n''(\theta)
&=\frac{1}{n}\sum_{i=1}^n\frac{\partial^2}{\partial \theta\partial \theta}\ln f(X_i|\theta)\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta}\left(\frac{\partial}{\partial\theta}\ln f(X_i|\theta)\right)\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta}\left(\frac{\frac{\partial}{\partial \theta}f(X_i|\theta)}{f(X_i|\theta)}\right)\quad[\text{chain rule}]
\end{align*}
$$
Applying the quotient rule yields
$$
\begin{align*}
\frac{1}{n}\ell_n''(\theta)
&=\frac{1}{n}\sum_{i=1}^n
\left(
\frac{\left(\frac{\partial^2}{\partial \theta\partial \theta}f(X_i|\theta)\right) f(X_i|\theta)-\frac{\partial}{\partial\theta}f(X_i|\theta)\frac{\partial}{\partial\theta} f(X_i|\theta)}{\left(f(X_i|\theta)\right)^2}\right).
\end{align*}
$$
Taking the mean of $\frac{1}{n}\ell_n''(\theta)$ yields that 
$$
\begin{align*}
\mathbb{E}\left(\frac{1}{n}\ell_n''(\theta)\right)
&=\frac{n}{n}\mathbb{E}\left( \frac{\frac{\partial^2}{\partial \theta^2}  f(X_i|\theta)}
{f(X_i|\theta)}-\left( \frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}
{f(X_i|\theta)}\right)^2\right)\quad[\text{i.i.d.}]\\[2ex]
&=0 - \mathbb{E}\left(\left( \frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}
{f(X_i|\theta)}\right)^2\right)\\[2ex]
&=-\mathcal{J}(\theta),
\end{align*}
$${#eq-AsymFI}
which implies that $\frac{1}{n}\ell_n''(\theta)$ is an **unbiased estimator** of $-\mathcal{J}(\theta)$, i.e.
$$
\begin{align*}
\operatorname{Bias}\left(\frac{1}{n}\ell_n''(\theta)\right)
&=\mathbb{E}\left(\frac{1}{n}\ell_n''(\theta)\right)-\mathcal{J}(\theta)\\[2ex]
&=0. 
\end{align*}
$$

::: {.callout-tip}
We have now gathered different equivalent expressions for $\mathcal{J}(\theta).$ The last one (@eq-AsymFI) shows that $\mathcal{J}(\theta)$ is nothing but the  **Fischer information** scaled by $\frac{1}{n}$: 
$$
\begin{align*}
\mathcal{J}(\theta)
%& = Var\left(\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)\\[2ex]
%& = Var\left(\frac{\frac{\partial}{\partial \theta} f(X_i|\theta)}{f(X_i|\theta)}\right)\\[2ex]
%&=\mathbb{E}\left(\left(\frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}{f(X_i|\theta)}\right)^2\right)\\[2ex]
&=-E\left(\frac{1}{n}\ell_n''(\theta)\right)\\[2ex]
&=\frac{1}{n} (-1)\cdot \mathbb{E}\left(\ell_n''(\theta)\right)\\[2ex]
&=\frac{1}{n}\mathcal{I}(\theta)
\end{align*}
$$

**Note:** For multivariate ($p$-dimensional) parameters $\theta,$ the Fisher information $(-1)\cdot \mathbb{E}\left(\ell_n''(\theta)\right)$  becomes the ($p\times p$) Fisher information matrix (see @sec-varMLE). 
:::

Second, the variance of variance of $\frac{1}{n}\ell_n''(\theta):$
$$
\begin{align*}
Var\left(\frac{1}{n}\ell_n''(\theta)\right)
&=Var\left(\frac{1}{n}\sum_{i=1}^n\frac{\partial^2}{\partial \theta\partial \theta}\ln f(X_i|\theta)\right)\\[2ex]
&=\frac{n}{n^2}
\underbrace{Var\left(\frac{\partial^2}{\partial \theta \partial \theta}  \ln f(X_i|\theta)\right)}_{=\text{some fixed, deterministic figure}}\\[2ex]
&=\frac{1}{n}\texttt{constant},
\end{align*}
$$
which implies that
$$
Var\left(\frac{1}{n}\ell_n''(\theta)\right)\to 0\quad\text{as}\quad n\to\infty.
$$

With these mean and variance results we can write down the Mean Squared Error (MSE) of the estimator $\frac{1}{n}\ell_n''(\theta)$ of $-\mathcal{J}(\theta):$
$$
\begin{align*}
&\operatorname{MSE}\left(\frac{1}{n}\ell_n''(\theta)\right)\\[2ex]
&=
\mathbb{E}\left(\left(\frac{1}{n}\ell_n''(\theta) -\left(-\mathcal{J}(\theta)\right)\right)^2\right)\\[2ex]
&=\left(\operatorname{Bias}\left(\frac{1}{n}\ell_n''(\theta)\right)\right)^2+Var\left(\frac{1}{n}\ell_n''(\theta)\right)\\[2ex]
&=Var\left(\frac{1}{n}\ell_n''(\theta)\right)\to 0\quad\text{as}\quad n\to\infty.
\end{align*}
$$

That is, the estimator $\frac{1}{n}\ell_n''(\theta)$ is a **mean square consistent** estimator, i.e.
$$
\frac{1}{n}\ell_n''(\theta)\to_{m.s.} -\mathcal{J}(\theta)\quad \hbox{as}\quad n\to\infty,
$$
which implies that $\frac{1}{n}\ell_n''(\theta)$ is also a **(weakly) consistent** estimator, i.e. 
$$
\frac{1}{n}\ell_n''(\theta)\to_p -\mathcal{J}(\theta)\quad \hbox{as}\quad n\to\infty, 
$$
since mean square convergence implies convergence in probability.


::: {.callout-important}
🤔 Remember, we wanted to study $\frac{1}{n}\ell_n''(\psi_n)$ in @eq-MLNorm **not** $\frac{1}{n}\ell_n''(\theta).$

Luckily, we are actually close now. 
:::

We know that the ML estimator $\hat\theta_n$ is (weakly) consistent, i.e., 
$$
\hat\theta_n\to_p\theta\quad\text{as}\quad n\to\infty.
$$


Since $\psi_n\in(\theta,\hat{\theta}_n)$ is a value between $\theta$ and $\hat{\theta}_n$ (@eq-MVT), the consistency of $\hat{\theta}_n$ implies that also
$$
\psi_n\to_p\theta\quad\text{as}\quad n\to\infty.
$$

Therefore, we have (by the continuos mapping theorem) that also
$$
\begin{align}
\frac{1}{n}\ell_n''(\psi_n)&\to_p -\mathcal{J}(\theta)\quad \hbox{ as }\quad n\to\infty\\
\Leftrightarrow\quad -\frac{1}{n}\ell_n''(\psi_n)&\to_p \phantom{-}\mathcal{J}(\theta)\quad \hbox{ as }\quad n\to\infty.
\end{align}
$$
<!-- Multiplying by $(-\sqrt{ \mathcal{J}(\theta)})^{-1}$ yields
$$
\frac{-\frac{1}{n}\ell_n''(\psi_n)}{\sqrt{ \mathcal{J}(\theta)}}
%=n^{-1/2}\frac{-\ell_n''(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}
\to_p \frac{-\mathcal{J}}{-\sqrt{ \mathcal{J}(\theta)}} 
=\sqrt{ \mathcal{J}(\theta)}
$$ -->

Now, using Slutsky's theorem, we can connect the above consistency result with the asymptotic normality result in @eq-MLNorm such that 
$$
\begin{align*}
\underbrace{\left(\frac{-\frac{1}{n}\ell_n''(\psi_n)}{\sqrt{\mathcal{J}(\theta)}}\right)}_{\to_p \sqrt{\mathcal{J}(\theta)} }\sqrt{n}\left(\hat{\theta}_n-\theta\right)\to_d\mathcal{N}(0,1)
\end{align*}
$$
or equivalently
$$
\begin{align*}
\sqrt{n}\left(\hat{\theta}_n-\theta\right)\to_d N\left(0,\frac{1}{\mathcal{J}(\theta)}\right),
\end{align*}
$${#eq-AsymNormMLE}
where 
$$
\mathcal{J}(\theta)=\frac{1}{n}(-1)\cdot E(\ell_n''(\theta))=\frac{1}{n}\mathcal{I}(\theta).
$$ 
@eq-AsymNormMLE is the asymptotic normality result we aimed for. 

::: {.callout-note} 
The above arguments can easily be generalized to multivariate ($p$-dimensional) parameter vectors $\theta\in\mathbb{R}^p$. In this case, $\mathcal{J}(\theta)$ becomes a $p\times p$ matrix, and
$$
\hat{\theta}_n-\theta\to_d \mathcal{N}_p\left(0,\frac{1}{n} \mathcal{J}(\theta)^{-1}\right),
$$
where $n\mathcal{J}(\theta)=-E(\ell_n''(\theta))=\mathcal{I}(\theta),$ with $\mathcal{I}(\theta)$ being the $(p\times p)$ Fisher information matrix. 
:::




::: {.callout-tip}

# Machine learning

The Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (@Kirkpatrick_2017).

Fisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (@Martens_2020).
:::

## Exercises {-}

## References {-}



















































<!-- **Example::** Assume an i.i.d. sample $X_1,\dots,X_n$ from an exponential distribution, i.e. the underlying density of $X_i$ is given by $f(x|\theta)=\theta\exp(-\theta x)$. We then have $\mu:=E(X_i)=\frac{1}{\theta}$ as well as $\sigma^2_X:=\textrm{var}(X_i)=\frac{1}{\theta^2}$. The -->
<!-- log-likelihood functions is given by  -->
<!-- $$l(\theta)=\sum_{i=1}^n \ln (\theta\exp(-\theta X_i)))=n \ln \theta -\sum_{i=1}^n \theta X_i$$ -->
<!-- $$\Rightarrow \quad \ell_n'(\theta)=n\frac{1}{\theta} + \sum_{i=1}^n X_i.$$ -->
<!-- As already mentioned above, the maximum-likelihood estimator of $\theta$ then is $\hat\theta_n=\frac{1}{\bar X}$. -->
<!-- Inference may then be based on likelihood-theory. We have -->
<!-- $$\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))=\frac{1}{\theta^2},$$ -->
<!-- and by the above theorem -->
<!-- $$\frac{1}{\bar X}-\theta\sim AN(0,\frac{1}{n \mathcal{J}(\theta)})\overset{a}{\sim}AN(0,\frac{\theta^2}{n}).$$ -->
<!-- This obviously coincides with the result obtained by the delta-method. -->





<!-- ## Discussion of Assumptions and Results {-} -->
<!-- \begin{itemize} -->
<!-- \item **Strict exogeneity**:  Needed to assume $\E[\varepsilon | X]=0$ to show consistency of $\hat\beta_{ML}$.  -->
<!-- \item **Homoskedasticity and non-autocorrelation**:  We used the assumption that $\E[\varepsilon eps']\sim(0, \sigma^2 I)$ to derive estimator of $\sigma^2$.   -->
<!-- \item **Normality**:  The normality assumption is used **only** to derive small-sample properties of the estimators. By using asymptotic arguments one can show that both $\hat\beta_{ML}$ and $s_{ML}^2$ will be distributed -->
<!-- asymptotically normally also without the normality assumption. -->
<!-- \end{itemize} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Best Linear Unbiased Estimator} -->
<!-- Given our assumptions, then by the Gauss-Markov theorem, it is possible to show that  -->
<!-- \begin{itemize}  -->
<!-- \item<1->$\hat\beta$ is the Best Linear Unbiased (BLUE) estimator of $\beta$ -->
<!-- \item<2-> The best linear unbiased estimator of any linear combination of the $\beta$'s is the same linear combination -->
<!-- of the $\hat\beta$'s. -->
<!-- \item<3-> The Best Linear Unbiased Predictor (BLUP) of $Y$ based on the vector $X_s$ is $\hat y_s=X'_s\hat\beta$ -->
<!-- \end{itemize} -->

<!-- \end{frame} -->


<!-- ## Hypothesis Testing -->
<!-- ### Testing Hypotheses about One Parameter -->

<!-- \noindent**Definition of the Score** -->

<!-- Define the **score of the log likelihood** (also known as the **gradient vector** -->
<!-- for observation $i$ -->
<!-- \begin{equation*} -->
<!-- s_i(\beta)\equiv \left(\dfrac{\partial L_i}{\partial \beta_0}(\beta), \dfrac{\partial L_i}{\partial \beta_1}(\beta), \dots, \dfrac{\partial L_i}{\partial \beta_k}(\beta)\right)' -->
<!-- \end{equation*} -->



<!-- %In the logit and probit cases, this can be shown to be -->
<!-- %\begin{equation*} -->
<!-- %s_i(\beta)\equiv\dfrac{g(x_i\beta)[y_i-G(x_i\beta)]} -->
<!-- %{G(x_i\beta)[1-G(x_i\beta)]}x_i' -->
<!-- %\end{equation*} -->
<!-- %Since $x_i$ is $1 \times (k+1)$, the score is a $(k+1) \times 1$ vector.  Recalling that in the probit %case -->
<!-- %\begin{center} -->
<!-- %$g(z)=\phi(z)$ and $G(z)=\Phi(z)$ -->
<!-- %\end{center} -->
<!-- %while with logit -->
<!-- %\begin{center} -->
<!-- %$g(z)=\exp(z)/[1+\exp(z)]^2$ and $G(z)=\exp(z)/[1+\exp(z)]$. -->
<!-- %\end{center} -->


<!-- #### Variance-Covariance Matrix {-} -->

<!-- Using the standard maximum likelihood theory it can be -->
<!-- show that the asymptotic-variance covariance matrix of the MLE $\hat\beta_{ML}$ is given by -->
<!-- \begin{equation*} -->
<!-- \text{Asy.~Var}(\hat\beta_{ML})=\left[\sum_{i=1}^N s_i(\hat\beta)s_i(\hat\beta)'\right]^{-1} -->
<!-- \end{equation*} -->
<!-- %and therefore in our case we have -->
<!-- %\begin{equation*} -->
<!-- %\text{Asy. Var-Cov}(\hat\beta)=\left[\sum_{i=1}^N\dfrac{[g(x_i\hat\beta)]^2 x_i' x_i}{G(x_i\hat\beta) -->
<!-- %[1-G(x_i\hat\beta)]}\right]^{-1} -->
<!-- %\end{equation*} -->
<!-- %with $g(\cdot)$ and $G(\cdot)$ defined as above. -->
<!-- %\vskip .1in -->
<!-- The square roots of the diagonals of this matrix will give us the -->
<!-- **standard errors** of the estimates. -->

<!-- \frametitle{Cramer-Rao Lower Bound} -->

<!-- Fisher, Cramer, and Rao showed that for any unbiased estimator $\hat\theta$, its variance-covariance -->
<!-- matrix cannot be smaller than $I^{-1}(\theta)$ where $I(\theta)$ is the **information matrix** -->
<!-- of the estimator, given by  -->
<!-- $$I(\theta) \equiv E[s(y,\theta)s(y,\theta)']$$ -->
<!-- where $s(\cdot)$ is the gradient or score.  Thus, the MLE attains the Cramer-Rao lower bound and will therefore be asymptotically efficient. -->
<!-- \end{frame} -->





<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Asymptotic Distribution} -->

<!-- Now, by the usual asymptotic theory, we have -->
<!-- \begin{equation*} -->
<!-- \dfrac{\hat\beta_j - \beta_j^0}{\text{std. err.}(\hat\beta_j)}\stackrel{a}{\sim} \mathcal{N}(0,1) -->
<!-- \end{equation*} -->
<!-- where $\beta_j^0$ is the value of the parameter under the null hypothesis. -->
<!-- So, we can do our usual "$t$-tests" although because we rely on asymptotics, -->
<!-- they should probably be more properly called $z$-tests. -->

<!-- \end{frame} -->


<!-- \subsection{Testing Hypotheses about Multiple Parameters} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Testing Joint Hypotheses} -->

<!-- We may also want to test hypotheses about multiple parameters.  Here it will -->
<!-- be useful to think about the regressions implied by imposing the restrictions. -->
<!-- So, for example,  -->
<!-- \begin{equation*} -->
<!-- \begin{array}{ll} -->
<!-- H_0: & R\beta - r = 0\\ -->
<!-- H_A: & H_0 \text{ is not true} \\ -->
<!-- \end{array} -->
<!-- \end{equation*} -->
<!-- where $R$ is a $q \times (k+1)$ matrix that defines the $q$ restrictions placed on the parameters -->
<!-- under the null hypothesis and $r$ is a $q \times 1$ vector of constants. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Restricted and Unrestricted Regressions} -->

<!-- We will define the **restricted regression** as one in which we force -->
<!-- the $R\hat\beta$ to be equal to  -->
<!-- $r$ (i.e. under the null hypothesis), and the -->
<!-- **unrestricted regression** to be one in which we allow the data to tell -->
<!-- us what the values of $\beta$ should be. -->
<!-- \vskip .2in -->
<!-- Define $L_r$ as the log-likelihood corresponding to the restricted regeression -->
<!-- and $L_u$ as the log-likelihood corresponding to the unrestricted regression. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Three Asymptotically Equivalent Tests} -->

<!-- We will discuss three asymptotically equivalent tests: -->
<!-- \begin{itemize} -->
<!-- \item **Wald test**: based on the unrestricted regression -->
<!-- \item **Likelihood ratio test**: based on both the restricted and unrestrcited regressions -->
<!-- \item **Lagrange multiplier test**: based on the restricted regression. -->
<!-- \end{itemize} -->

<!-- All three tests will give us the same answer asymptotically, but will differ -->
<!-- in their values in finite samples. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Wald Test (1)} -->

<!-- From maximum likelihood theory, we know that  -->
<!-- \begin{equation*} -->
<!-- \hat\beta \adist \mathcal{N}(\beta,V) -->
<!-- \end{equation*} -->
<!-- and therefore that $R\hat\beta$ also has an asymptotically normal distribution -->
<!-- (since it is just a linear combination of asymptotically normal variables): -->
<!-- \begin{equation*} -->
<!-- (R\hat\beta - R\beta) \adist \mathcal{N}(0, RVR') -->
<!-- \end{equation*} -->
<!-- This suggests a quadratic form which we can use to test hypotheses -->
<!-- \begin{equation*} -->
<!-- W\equiv(R\hat\beta - r)'[R \hat V_u R']^{-1}(R\hat\beta - r) \adist \chi_q^2 -->
<!-- \end{equation*} -->
<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Wald Test (2)} -->

<!-- Thus, with the **Wald test**, we need only estimate the *unrestricted* regression. -->

<!-- \vskip .25in -->

<!-- It measures how far apart the estimated parameters are from the values of  -->
<!-- the parameters under the null hypothesis. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Likelihood Ratio Test} -->

<!-- More conceptually simple, perhaps, is the **Likelihood Ratio Test**. -->
<!-- \vskip .15in -->
<!-- If the null hypothesis holds, imposing restrictions on the data should lead -->
<!-- to values of $L_r$ and $L_u$ that are ``close''.  The question then, is what -->
<!-- metric to use to judget how ``close '' they are. -->
<!-- \vskip .15in -->
<!-- It can be shown that -->
<!-- \begin{equation*} -->
<!-- LR\equiv -2 [L_r - L_u] \adist \chi^2_q -->
<!-- \end{equation*} -->
<!-- Therefore the $\chi^2_q$ distribution is the proper metric for judging how close -->
<!-- the likelihoods are. -->
<!-- \vskip .15in -->
<!-- We must fit both models to calculate the differences between the restricted -->
<!-- and restricted likelihoods. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Motivation} -->

<!-- The **Lagrange Multiplier Test** (also called the **Score Test**) is based -->
<!-- on the score, or gradient, vector (as defined earlier).  The idea is to measure -->
<!-- how far away from the peak of the *unrestricted* likelihood imposing the -->
<!-- restrctions forces us, which is some akin to the notion of the likelihood ratio -->
<!-- test.  -->
<!-- \vskip.15in -->
<!-- At the peak of the unrestricted log likelihood, the score would be a vector of -->
<!-- zeros.  Intuitively, then, the Lagrante Multiplier Test will measure how ``close'' -->
<!-- the score vector when we estimate the *restricted* regression is to  -->
<!-- the vector of zeroes. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Derivation (1)} -->

<!-- We can think about finding the maximum of the log likelihood subject to -->
<!-- the constraints imposed by the null hypothesis.  To simplify things, suppose we have only two -->
<!-- parameters, $\beta_1$ and $\beta_2$ with $H_0: \beta_2=c$. -->
<!-- Then: -->
<!-- \begin{equation*} -->
<!-- H(\beta, \lambda)=\sum_{i=1}^N  L_i(\beta) - \lambda'(\beta_2-c) -->
<!-- \end{equation*} -->
<!-- where $\lambda$ is the Lagrange multiplier.  Then the first order conditions -->
<!-- are -->
<!-- \begin{align*} -->
<!-- \sum_{i=1}^N  \dfrac{\partial L_i (\beta)}{\partial \beta_1} \big\vert_{\tilde\beta} -->
<!-- &=\sum_{i=1}^N s_{i1}(\tilde\beta)=0\\ -->
<!-- \tilde\lambda=\sum_{i=1}^N \dfrac{\partial L_i (\beta)}{\partial \beta_1} \big\vert_{\tilde\beta} &=\sum_{i=1}^N s_{i2}(\tilde\beta)\\ -->
<!-- \end{align*} -->
<!-- \end{frame} -->


<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Derivation (2)} -->

<!-- Define $s_{i1}$ and $s_{i2}$ are the subvectors of $s_i(\beta)$ corresponding to  -->
<!-- $\beta_1$ and $\beta_2$, respectively. -->

<!-- \vskip .15in -->

<!-- So we are in some sense testing whether $\tilde\lambda$ is ``close'' to zero or -->
<!-- not, evaluated at the restricted values of the parameters. -->

<!-- \vskip .15in -->

<!-- It's possible to show, then, that -->

<!-- \begin{equation*} -->
<!-- LM\equiv  s'(\tilde\beta) \tilde V_r^{-1} s(\tilde\beta) \adist \chi^2_q -->
<!-- \end{equation*} -->
<!-- where $s(\tilde\beta)$ is the score evaluated at the *restricted* estimates of -->
<!-- the parameters, and $\tilde V_r$ is the estimated variance-covariance matrix from the *restricted* regression. -->
<!-- \end{frame} -->


<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Relationshiop between W, LR, and LM tests} -->

<!-- \includegraphics[angle=90, scale=.60]{wald-lm-lr.ps} -->
<!-- \end{frame} -->


<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Relationship between W, LR, and LM} -->

<!-- While all three tests are asymptotically equivalent, it can be shown that in finite -->
<!-- samples -->
<!-- \begin{center} -->
<!-- $LM < LR < W$ -->
<!-- \end{center} -->
<!-- meaning that LM tests will favor not rejecting the null and W tests will favor rejecting -->
<!-- the null. -->

<!-- \end{frame} -->


<!-- \end{document} -->

<!-- \section{Goodness of Fit Measures} -->
<!-- \subsection{Goodness of Fit Measures} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Goodness of Fit in Probit and Logit} -->

<!-- As in the linear regression model, we would like to have some measure -->
<!-- of how well our model fits the data.  Unlike linear models, however, where -->
<!-- $R^2$ serves as the primary goodness-of-fit measure, there is no -->
<!-- standard metric that is used. -->
<!-- \vskip .15in -->
<!-- Now, define $L_0$ as the log likelihood of a model in which we constrain -->
<!-- all of the coefficients (except the constant) to be equal to zero. -->

<!-- \end{frame} -->




<!-- %------------------------------------------------- -->
<!-- %------------------------------------------------- -->
<!-- %\begin{frame} -->
<!-- %\frametitle{A Note on $L_0$} -->
<!-- %Note that we do not actually need to run a  regression to estimate $L_0$. -->
<!-- %\vskip .15in -->
<!-- %With just a constant term in the model, the likelihood function is given by -->
<!-- %\begin{align*} -->
<!-- %L_0&=\sum y_i \ln(N_1/N) + \sum (1-y_i) \ln(1-N_1/N)\\ -->
<!--  %    &=N_1 \ln(N_1/N) + N_0\ln(N_0/N)\\ -->
<!-- %\end{align*} -->
<!-- %where $N_1$ indicates the number of success and $N_0$ is the number of failures. -->
<!-- %\end{frame} -->


<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Pseudo-$R^2$} -->

<!-- The first goodness-of-fit measure is meant as an analog to the $R^2$ from -->
<!-- linear regression, called the pseudo-$R^2$.  It is defined as -->
<!-- \begin{equation*} -->
<!-- \text{pseudo}-R^2=1-\dfrac{1}{1+2(L_u - L_0)/N} -->
<!-- \end{equation*} -->
<!-- Intuitively, the greater the distance between the restricted and -->
<!-- unrestricted log likelihoods, the more the model explains the variation -->
<!-- in $y$, and the greater the pseudo-$R^2$ will be. -->

<!-- \end{frame} -->

<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{McFadden's $R^2$} -->

<!-- McFadden suggested an alternative goodness of fit-measures: -->

<!-- \begin{equation*} -->
<!-- \text{McFadden}-R^2= 1- L_u/L_0 -->
<!-- \end{equation*} -->
<!-- since the log likelihood is just the sum of log probabilities, it must be that -->
<!-- $L_0 < L_u < 0$. -->

<!-- \end{frame} -->


<!-- %------------------------------------------------- -->
<!-- %\begin{frame} -->
<!-- %\frametitle{Proportion of Correct Predictions} -->

<!-- %An additional measure of the fit of the model is the number of observations for -->
<!-- %which the model correctly predicts the outcome. -->

<!-- %\end{frame} -->