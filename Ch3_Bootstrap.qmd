# The Bootstrap 


<!-- TO-DO: 
1. Rework this chapter using the overview article of Horowitz
BOOTSTRAP METHODS IN ECONOMETRICS 
2. Remove the fraction estimator parts 
-->

The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.


Some literature:

* @Hall_1992: The Bootstrap and Edgeworth Expansion
* @Efron_Tibshirani_1994: An Introduction to the Bootstrap
* @Shao_Tu_1996: The Jackknife and Bootstrap
* @Horowitz_2001: The Bootstrap. In: Handbook of Econometrics
* @Davison_Hinkley_2013: Bootstrap Methods and their Applications


## The Empirical Distribution Function


The distribution of a real-valued random variable $X$ can be completely described by its distribution function
$$
F(x)=P(X\leq x)\quad \text{for all } x\in\mathbb{R}.
$$




For given data, the sample analogue of $F$ is the so-called **empirical distribution function**, which is an important tool of statistical inference.

**Data:** i.i.d. random sample $X_1,\dots,X_n$ from $X\sim F$


Let $1_{(\cdot)}$ denote the indicator function, i.e., $1_{(x\leq t)}=1$ if $x\leq t$, and $1_{(x\leq t)}=0$ if $x>t.$


::: {#def-ecdf}

# Empirical distribution function

$$
F_n(x)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}
$$
I.e $F_n(x)$ is the proportion of observations with $X_i\le x,$ $i=1,\dots,n.$
:::


**Properties:**

* $0\le F_n(x)\le 1$ 
* $F_n(x)=0,$ if $x<X_{(1)}$, where $X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}$ denotes the **order-statistic**. 
* $F_n(x)=1,$ if $x\ge X_{(n)}$, where $X_{(n)}$ is largest observation
* $F_n$ is a monotonically increasing step function


::: {.callout-note}
Structurally, $F_n$ itself is a distribution function. $F_n$ is the distribution function of a **discrete random variable** $X^*$ 

* with possible values $X^*\in\{X_1,\dots,X_n\}$ and 
* with $P(X^*=X_i)=\frac{1}{n}$ for each $i=1,\dots,n.$
:::


::: {#exm-ecdfexample}

# Empirical distribution function

<br>

Some data:

| $i$  | $X_i$| 
|------|------|
| 1    | 5.20 |
| 2    | 4.80 |
| 3    | 5.40 |
| 4    | 4.60 |
| 5    | 6.10 |
| 6    | 5.40 |
| 7    | 5.80 |
| 8    | 5.50 |


Corresponding empirical distribution function using `R`:

```{r, ecdfPlot}

observedSample <- c(5.20, 4.80, 5.30, 4.60, 
                    6.10, 5.40, 5.80, 5.50)

myecdf_fun     <- ecdf(observedSample)

plot(myecdf_fun, main="")
```
:::


$F_n(x)$ depends on the i.i.d. random sample $X_1,\dots,X_n$ and thus is itself a **random function**. 

We obtain

* For every $x\in\mathbb{R}$
 $$
 nF_n(x)\sim B(n, p=F(x))
 $$
 I.e., $nF_n(x)$ has a binomial distribution with parameters $n$ ("number of trials") and $p=F(x)$ ("probability of success on a single trial").
* $\mathbb{E}(F_n(x))=F(x)$
* $\mathbb{V}(F_n(x))=\frac{F(x)(1-F(x))}{n}$


::: {.callout-note icon=false} 
## 
::: {#thm-Clivenko-Cantelli}

# Theorem of Glivenko-Cantelli
$$
P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1
$$
:::
:::


## Basic Idea of the Bootstrap

The basic idea of the bootstrap is to replace random sampling from the true (unknown) population $F$ (infeasible Monte Carlo simulation) by random sampling from the empirical distribution $F_n$ (feasible Monte Carlo simulation). 


**Population distribution $F$:** The random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}F$ is generated by drawing observations independently and with replacement from the unknown population distribution function $F$. That is, for each interval $[a,b]$ the probability of drawing an observation in $[a,b]$ is given by 
$$
P(X\in [a,b])=F(b)-F(a).
$$


**Empirical distribution $F_n$:** For large $n,$ the empirical distribution $F_n$ of the sample values is "close" to the unknown distribution $F$ (Glivenko-Cantelli @thm-Clivenko-Cantelli). That is, for $n\rightarrow\infty$ the relative frequency of observations $X_i$ in $[a,b]$ converges to $P(X\in [a,b])$   
$$
  \begin{align*}
  \underbrace{\frac{1}{n}\sum_{i=1}^n1_{(X_i\in[a,b])}}_{=F_n(b)-F_n(a)}&\to_p \underbrace{P(X\in [a,b])}_{=F(b)-F(a)}
  \end{align*}
$$


**The idea of the bootstrap** consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution $F,$ the bootstrap uses random sampling from the known $F_n.$ This is justified by the insight that the empirical distribution $F_n$ of the observed data is "similar" to the true distribution $F$ (Glivenko-Cantelli @thm-Clivenko-Cantelli).


::: {.callout-tip}

## Bradley Efron

The bootstrap method is attributed to [Bradley Efron](https://statistics.stanford.edu/people/bradley-efron), who received the *[International Prize in Statistics](https://statsandstories.net/methods/2018/9/28/bootstrapping-an-international-prize)* (the Nobel price of statistics) for his seminal works on the bootstrap method. 
:::

## The Nonparametric (Standard) Bootstrap


**Setup:**

* i.i.d. sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ from $X\sim F$. 
* The distribution $X_i\sim F,$ $i=1,\dots,n,$ depends on an unknown parameter (vector) $\theta.$
* The data $X_1,\dots,X_n$ is used to estimate an element $\theta$ of the parameter vector $\theta.$  
* Thus, the estimator is a function of the random sample 
$$
  \hat\theta\equiv \hat\theta(X_1,\dots,X_n).
$$

**Inference:** We are interested in evaluating the distribution of 
$$
\hat\theta-\theta
$$ 
in order to provide standard errors, construct confidence intervals, and to  perform tests of hypothesis.


### The Bootstrap Algorithm {-}

1. **Draw a bootstrap sample:** Generate a new random sample $X_1^*,\dots,X_n^*$ by drawing observations independently and with replacement from the available sample $X_1,\dots,X_n.$
2. **Compute bootstrap estimate:** Compute the estimate 
$$
\hat\theta^*\equiv \hat\theta(X_1^*,\dots,X_n^*)
$$
3. **Bootstrap replications:** Repeat Steps 1 and 2 $m$ times (e.g. $m=2000$) leading to $m$ bootstrap estimates 
$$
\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*
$$

For large $m$, the estimates $\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*$ allow to approximate the **bootstrap distribution** of $\hat\theta^*-\hat\theta$ arbitrarily well. 

::: {.callout-tip}
The bootstrap distribution of 
$$
\hat\theta^*-\hat\theta
$$ 
is used to approximate the unknown distribution of 
$$
\hat\theta-\theta.
$$

Note: For the bootstrap distribution $\hat\theta$ is a "population parameter". 
:::

The theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are **more accurate** than those based on standard asymptotic approximations.


### Bootstrap Consistency {-}

::: {.callout-warning}

## Caution: The bootstrap does *not* always work

The bootstrap does **not always work**. A necessary condition for the use of the bootstrap is the **consistency of the bootstrap approximation**.
:::



The bootstrap is called **consistent** if, for large $n$, the bootstrap distribution of $\hat{\theta}^* -\hat{\theta}$ is a good approximation of the underlying distribution of $\hat{\theta}-\theta$, i.e.
$$
\text{distribution}(\hat{\theta}^* -\hat{\theta}\ |{\cal S}_n)\approx 
\text{distribution}(\hat{\theta}-\theta).
$$
The following definition states this more precisely. 

::: {#def-BootstrapConsistency}

## Bootstrap consistency

*If for some $\gamma>0$ (usually: $\gamma=1/2$) we have $n^\gamma(\hat{\theta}-\theta)\rightarrow_d Z$ for some random variable $Z$ with a non-degenerate distribution, then the bootstrap is **consistent** if and only if
$$
\sup_\delta \left|
   P\left(n^\gamma(\hat\theta^*-\hat\theta)\le \delta|{\cal S}_n\right) 
  -P\left(n^\gamma(\hat\theta      -\theta)\le \delta\right)
  \right|\rightarrow_p 0
$$
as $n\to\infty.$*

:::



Luckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are **some crucial requirements**:

1. Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).
2. The distribution of the estimator $\hat\theta$ needs to be asymptotically normal. 


The standard bootstrap **will usually fail** if one of the above conditions
1 or 2 is violated. For instance, 

* The  bootstrap will not work if the i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $X_1,\dots,X_n$ does not properly reflect the way how the $X_1,\dots,X_n$ are generated when $X_1,\dots,X_n$ is a time-series with auto-correlated data. 
* The distribution of the estimator $\hat\theta$ is not asymptotically normal. (For instance, in case of extreme value problems.)

**Note:** In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).



### Example: Inference About the Population Mean 


**Setup:**

* i.i.d. sample $X_1,\dots,X_n$ from $X\sim F.$
* Continuous random variable $X\sim F$ with unknown $F$ and thus unknown mean $$
\mu = \int x f(x) dx = \int x d F(x),$$ 
where $f=F'.$
* Estimator: empirical mean
$$
\begin{align*}
\bar{X}
&\equiv \bar{X}(X_1,\dots,X_n) \\[2ex]
&=\frac{1}{n}\sum_{i=1}^n X_i \\[2ex]
&=\int x d F_n(x)
\end{align*}
$$



**Inference Problem:** What is the distribution of 
$$
\bar{X} -\mu?
$$

Now assume that $n=8$ and that the **observed sample** is

| $i$  | $X_i$| 
|------|------|
| 1    | -0.6 |
| 2    |  1.0 |
| 3    |  1.4 |
| 4    | -0.8 |
| 5    |  1.6 |
| 6    |  1.9 |
| 7    | -0.1 |
| 8    |  0.7 |

```{r}
observedSample <- c(-0.6, 1.0,  1.4, -0.8, 
                     1.6, 1.9, -0.1,  0.7)
```

So the sample mean is 
<center>
$\bar X =$ `mean(observedSample)` $=$ `r mean(observedSample)`
</center>

<br>

**Bootstrap:**

The observed sample 
$$
{\cal S}_n=\{X_1,\dots,X_n\}
$$ is taken as underlying empirical "population" in order to generate the **bootstrap sample** $X_1^*,\dots,X_n^*$:

* i.i.d. samples $X_1^*,\dots,X_n^*$ are generated by drawing   observations independently and with replacement from ${\cal S}_n=\{X_1,\dots,X_n\}$.

```{r}
## generating a bootstrap sample
bootSample <- sample(x       = observedSample, 
                     size    = length(observedSample), 
                     replace = TRUE)
```
* The distribution of 
$$
\bar X -\mu
$$ is approximated by the conditional distribution of 
$$
\bar X^* -\bar X,
$$ 
given the original sample ${\cal S}_n,$ i.e. more formally
$$
\underbrace{P\left(\bar{X}^*-\bar{X}<\delta|\mathcal{S}_n\right)}_{\text{approximable}} 
\approx 
\underbrace{P\left(\bar{X}-\mu<\delta\right)}_{\text{unknown}}.
$$


For the given data with $n=8$ observations, there are 
$$
n^n=8^8=16,777,216
$$ 
possible bootstrap samples which are all equally probable. 

The conditional distribution function of $\bar{X}^*-\bar{X}$ given $\mathcal{S}_n$ 
$$
P\left(\bar{X}^*-\bar{X}<\delta|\mathcal{S}_n\right)
$$
can be approximated using a Monte-Carlo simulation. For this, we draw new data $X_1^*,\dots,X_n^*$ from $F_n,$ i.e., we sample with replacement data points from the observed sample $\mathcal{S}_n=\{X_1,\dots,X_n\}.$

Using a large number $m$ (e.g. $m=10000$) of simulation runs allows us to generate bootstrap estimates
$$
\bar{X}^*_1,\bar{X}^*_2,\dots,\bar{X}^*_m
$$ 


<!-- Simul. | $X_1^*$|  $X_2^*$| $X_3^*$| $X_4^*$|  $X_5^*$|  $X_6^*$|  $X_7^*$|   $X_8^*$ | $\bar X^*-\bar{X}$
----|----:|----:|----:|----:|----:|----:|----:|----:|:----:
1 | 1.9| -0.8| 1.9|  -0.6| 1.4| -0.1| -0.8| 1.0 | `r 0.4875 -mean(observedSample)`
2 | 0.7| -0.8| -0.8| 1.0 | 1.6| 1.0| -0.1| -0.8 | `r 0.225-mean(observedSample)`  
3 | -0.1| 1.9| 0.7|  1.0| -0.1| 1.6| 1.0| -0.6 |  `r 0.675-mean(observedSample)` 
4 | 1.4| 1.0| 1.4|  -0.1| 1.9| -0.8| 1.9| 1.0 | `r 0.9625-mean(observedSample)` 
5 | 1.0| 0.7| -0.1|  0.7| 1.4| -0.8| 1.0| 1.6 |  `r 0.6875-mean(observedSample)`
... ||||||||| -->


These bootstrap estimates are then used to approximate the bootstrap distribution
$$
%\overbrace{
  \underbrace{P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right)}_{\text{bootstrap distribution}}%}^{=P^*\left(\bar X^*-\bar X\leq \delta\right)}
\approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
$$
where this approximation will be **arbitrarily precise** as $m\to\infty.$ (So, we can effectively ignore this type of approximation error.)


```{r}
n                <- length(observedSample)
Xbar             <- mean(observedSample)
m                <- 10000 # number of bootstrap samples 
Xbar_boot        <- vector(mode = "double", length = m)

for(k in seq_len(m)){
 bootSample          <- sample(x       = observedSample, 
                               size    = n, 
                               replace = TRUE)
 Xbar_boot[k]        <- mean(bootSample)
}

plot(ecdf( Xbar_boot - Xbar ), 
     main="Approximate Bootstrap Distribution")
```

To approximate, for instance, the standard error of $\bar{X},$ we can now simply use the **empirical standard deviation** of $\bar{X}^*_k,$ $k=1,\dots,m.$

```{r}
round(sd(Xbar_boot), 2)
```

<!-- to test the null-hypothesis 
<center>
H$_0:$ $\mu = \mu_0$
</center>
against the alternative hypothesis that 
<center>
H$_1:$ $\mu \neq \mu_0$
</center>
we can use the simulated distribution of $\bar{X}^*_k,$ $k=1,\dots,m.$ 

Under H$_0,$ we know that the true (unknown) mean equals the hypothetical mean $\mu = \mu_0,$ and we know that $\bar{X}\approx \mu$ if $n$ is large.   -->




#### Theory: The Bootstrap Distribution of $\bar X^*- \bar{X}$ {-}


::: {.callout-tip}
## Notation $\mathbb{E}^*(\cdot),$ $\mathbb{V}^*(\cdot),$ and $P^*(\cdot)$
In the bootstrap literature one frequently finds the notation 
$$
\mathbb{E}^*(\cdot),\;\mathbb{V}^*(\cdot),\;\text{and}\;P^*(\cdot)
$$ 
to denote the **conditional** expectation
$$
\mathbb{E}^*(\cdot)=\mathbb{E}(\cdot|\mathcal{S}_n),
$$
the **conditional** variance 
$$
\mathbb{V}^*(\cdot)=\mathbb{V}(\cdot|\mathcal{S}_n),
$$ 
and the **conditional** probability 
$$
P^*(\cdot)=P(\cdot|\mathcal{S}_n),
$$ 
given the sample ${\cal S}_n.$
:::


The bootstrap focuses on the **conditional** distribution of $X_1^*,\dots,X_n^*$ given the observed sample ${\cal S}_n=\{X_1,\dots,X_n\}$ and the resulting conditional distribution of 
$$
(\bar X^* -\bar X)|\mathcal{S}_n.
$$ 
These conditional distributions are usually called **bootstrap distributions**.


::: {.callout-tip}
## We know the distribution of $X_i^*|\mathcal{S}_n$

We can analyze the bootstrap distribution of $\bar X^* -\bar X$, since **we *know* 🤟 the discrete distribution** of the conditional random variables 
$$
X_i^*|\mathcal{S}_n,\;i=1,\dots,n,
$$ 
even though, we do **not know** the distribution of $X_i\sim F,$ $i=1,\dots,n.$

<!-- The discrete distribution of the conditional random variables 
$X_i^*|\mathcal{S}_n,$ is 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\}
$$
with  -->
:::


For each $i=1,\dots,n$, the possible values of the discrete random variable $X_i^*|\mathcal{S}_n$ are 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
$$ 
and each of these values is equally probable 
$$
\begin{align*}
P^*(X_i^*=X_1)&= P(X_i^*=X_1|{\cal S}_n) = \frac{1}{n} \\[2ex]
P^*(X_i^*=X_2)&= P(X_i^*=X_2|{\cal S}_n) = \frac{1}{n} \\[2ex]
&\vdots\\[2ex] 
P^*(X_i^*=X_n)&= P(X_i^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
$$

Thus, **we know the whole distribution** of the (conditional) discrete random variable $X_i^*|\mathcal{S}_n$ and, therefore, can compute, for instance, easily its conditional mean and its variance.

* The conditional mean of $X_i^*$ is
$$
\begin{align*}
\mathbb{E}^*(X_i^*)
&=\mathbb{E}(X_i^*|{\cal S}_n)\\[2ex]
&=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_n\\[2ex]
&=\bar X
\end{align*}
$$

* The conditional variance of $X_i^*$ is
$$
\begin{align*}
\mathbb{V}^*(X_i^*)
&=\mathbb{V}(X_i^*|{\cal S}_n)\\[2ex] 
&=\mathbb{E}((X_i^* - \mathbb{E}(X_i^*|{\cal S}_n))^2|{\cal S}_n)\\[2ex] 
&=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\\[2ex] 
&=\hat\sigma^2
\end{align*}
$$

That is, in the bootstrap sample $X_1^*,\dots,X_n^*$ the "population" mean and the "population" variance are equal to the empirical mean, $\bar{X},$ and the empirical variance, $\hat{\sigma}^2,$ of the original sample $X_1,\dots,X_n.$ 

::: {.callout-tip}
## General case: Conditional moments of transformed $g(X_i^*)$

For any (measurable) function $g$ we have 
$$
\mathbb{E}^*(g(X_i^*))=\mathbb{E}(g(X_i^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
$$
For instance, $g(X_i)=1_{(X_i\leq \delta)}.$
:::

::: {.callout-important}

## Caution: Conditioning on $\mathcal{S}_n$ in important!

Conditioning on the observed sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ is very important. 

The unconditional distribution of $X_i^*$ is equal to the **unknown distribution** $F$ of $X_i.$ This can be seen from the following derivation: 
$$
\begin{align*}
P(X_i^*\leq \delta) 
&= P(1_{(X_i^*\leq \delta)}=1) \\[2ex]
&= P(1_{(X_i^*\leq \delta)}=1) \cdot 1 + P(1_{(X_i^*\leq \delta)}=0) \cdot 0\\[2ex]
&= E\left(1_{\left(X_i^*\leq \delta\right)}\right)\\[2ex]
&= E\left[E\left(1_{\left(X_i^*\leq \delta\right)}|\mathcal{S}_n\right)\right]\\[2ex]
&= E\left[\frac{1}{n}\sum_{i=1}^n 1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&= \frac{n}{n}E\left[1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&= P\left(X_i\leq \delta\right)=F(\delta)
\end{align*}
$$
:::


**Now consider the bootstrap distribution of $\bar X^*$**

<!-- We know the distribution of the i.i.d. random variables 
$$
X_i^*|\mathcal{S}_n, \quad i=1,\dots,n,
$$
it is straight forward to derive the asymptotic distribution of 
$$
(\bar X^*-\bar{X})|\mathcal{S}_n
$$ 
using the central limit theorem.  -->

Firstly, let us derive the conditional mean and variance of 
$$
\bar X^* = \frac{1}{n}\sum_{i=1}^nX_i^*.
$$

* The conditional mean of $\bar X^*$ is
$$
\begin{align*}
\mathbb{E}^*(\bar X^*)
&=\mathbb{E}(\bar X^*|{\cal S}_n)\\
&=\frac{1}{n}\sum_{i=1}^n\mathbb{E}(X_i^*|{\cal S}_n)\\
&=\frac{1}{n}\sum_{i=1}^n \bar X\\
&=\frac{n}{n}\bar X \\
&=\bar X
\end{align*}
$$

* The conditional variance of $\bar X^*$ is
$$
\begin{align*}
\mathbb{V}^*(\bar X^*)
&=\mathbb{V}(\bar X^*|{\cal S}_n)\\
&=\frac{1}{n^2}\sum_{i=1}^n \mathbb{V}(X_i^*|{\cal S}_n)\\
&=\frac{1}{n^2}\sum_{i=1}^n \hat\sigma^2\\
&=\frac{n}{n^2}\hat\sigma^2\\
&=\frac{1}{n}\hat\sigma^2,
\end{align*}
$$
where $\hat{\sigma}=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(X_i - \bar{X}\right)^2}.$

<!-- Next, we can check whether we can apply the classical Lindeberg-Lévy CLT. 

::: {.callout-tip}

## Good news: We can apply the CLT to $\bar X^*|\mathcal{S}_n$

Conditionally on ${\cal S}_n=\{X_1,\dots,X_n\}$,  

* the random variables $X_1^*,\dots,X_n^*$ are i.i.d.
* with mean $\mathbb{E}^*(X_i^*)=\bar X$ 
* and variance $\mathbb{V}^*(X^*)=\hat\sigma^2$

Thus, we can apply the [central limit theorem (Lindeberg-Lévy)](https://www.statlect.com/asymptotic-theory/central-limit-theorem) to the appropriately standardized bootstrap sample mean $\hat{X}^*$ conditionally on ${\cal S}_n$
$$
\left(\left.\frac{\sqrt{n}(\bar X^* - \bar X)}{\hat\sigma}\right|{\cal S}_n\right)
$$
::: -->

An appropriate central limit theorem argument implies that
$$
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$


Moreover, $\hat\sigma^2$ is a consistent estimator of $\sigma^2,$ and thus asymptotically $\hat\sigma^2$ may be replaced by $\sigma$. Therefore, 
$$
\begin{align*}
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X^* -\bar X)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$



On the other hand, by the CLT, we also have that 
$$
\begin{align*}
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right) 
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X - \mu)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$

This means that the bootstrap is **consistent**, since the bootstrap distribution of 
$$
\sqrt{n}(\bar X^* -\bar X)|{\cal S}_n
$$ 
asymptotically $(n\rightarrow\infty)$ coincides with the distribution of 
$$
\sqrt{n}(\bar X-\mu).
$$
In other words, for large $n$,
$$
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
$$

This bootstrap consistency result justifies using the bootstrap distribution 
$$
P(\bar{X}^*-\bar{X}\leq \delta|\mathcal{S}_n) \approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
$$ 
which we can approximate (arbitrary precise as $m\to\infty$) using the bootstrap realizations 
$$
\bar{X}^*_1,\;\bar{X}^*_2, \dots, \bar{X}^*_m.
$$

### Example: Inference about a Population Proportion

**Setup:** 

* **Data:** i.i.d. random sample $X_1,\dots,X_n,$ where $X_i\in\{0,1\}$ is dichotomous and $P(X_i=1)=p$, $P(X_i=0)=1-p$. 
* **Estimator:** Let 
$$
S=\sum_{i=1}^n 1_{(X_i = 1)}
$$ 
denote the number of $X_i$ which are equal to $1.$ Then, the  maximum likelihood estimate of $p$ is 
$$
\hat p=\frac{1}{n}S.
$$
* **Inference Problem:** What is the distribution of 
$$
(\hat{p} - p)?
$$



::: {.callout-note}

## Recall Asymptotics:

* $n\hat p=S\sim \mathcal{Binom}(n,p)$
* As $n\rightarrow\infty,$ the central limit theorem implies that
$$
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
$$
Thus for $n$ large, the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by $\mathcal{N}(0,p(1-p))$ and $\mathcal{N}(0,p(1-p)/n)$, respectively.

::: 

**Bootstrap Approach:**

* Random sample $X_1^*,\dots,X_n^*$  generated by drawing observations
independently and with replacement from
$$
{\cal S}_n:=\{X_1,\dots,X_n\}.
$$ 
* Let 
$$
S^*=\sum_{i=1}^n 1_{(X_i^* = 1)}
$$  
denote the number of $X_i^*$ which are equal to $1.$
* Bootstrap estimate of $p$: 
$$
\hat p^*=\frac{1}{n}S^*
$$


<!-- The distribution of $\hat p^*$ depends on the observed sample ${\cal S}_n:=\{X_1,\dots,X_n\}$. That is, a different sample ${\cal S}_n$ will lead to a different distribution.  -->


The bootstrap now tries to approximate the true distribution of $\hat p - p$ by the **conditional** distribution of $(\hat p^*-\hat p)|\mathcal{S}_n$ given the observed sample ${\cal S}_n,$ where the latter can be approximated arbitrarily well $(m\to\infty)$ using the bootstrap estimators 
$$
p^*_1,p^*_2,\dots,p^*_m;
$$
namely by
$$
P\left(\hat{p}^* - \hat{p} \leq \delta|\mathcal{S}_n\right)\approx \frac{1}{m}\sum_{k=1}^m 1_{(\hat{p}^*_k - \hat{p} \leq\delta )}. 
$$

The bootstrap is called **consistent** if asymptotically $(n\rightarrow \infty)$ the conditional distribution of $(\hat p^*-\hat p)|{\cal S}_n$  coincides with the true distribution of $\hat p - p.$ (Note: a proper scaling is required!)

**The distribution of $X_i^*|\mathcal{S}_n$**

The conditional random variable $X_i^*|\mathcal{S}_n$ is a binary random variable 
$$
X_i^*|\mathcal{S}_n\in\{0,1\}.
$$
Since $X_i^*$ is drawn independently and with replacement from $\mathcal{S}_n,$ we obtain for each $i=1,\dots,n,$
$$
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|{\cal S}_n)=\hat p, \\[2ex]  
& P^*(X_i^*=0)=P(X_i^*=0|{\cal S}_n)=1-\hat p.
\end{align*}
$$
Thus, $X_i^*|{\cal S}_n$ is a Bernoulli distributed random variable with parameter $p=\hat{p}$
$$
X_i^*|{\cal S}_n \sim\mathcal{Bern}(p=\hat p), \quad i=1,\dots,n.\\[5ex]
$$


**The distribution of $\hat{p}^*|\mathcal{S}_n$**

The above implies that $n \hat{p}^*|{\cal S}_n$ has a Binomial distribution with parameters $n$ and $p=\hat{p},$  
$$
\underbrace{n \hat{p}_i^*}_{=S^*}|{\cal S}_n \sim\mathcal{Binom}(n, p=\hat p), \quad i=1,\dots,n.
$$

Therefore,
$$
\begin{align*}
\mathbb{E}^*(n \hat p^*)
&=\mathbb{E}(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p}\\[2ex]
\Rightarrow \mathbb{E}^*(\hat p^*) & = \hat{p}
\end{align*}
$$
and 
$$
\begin{align*}
\mathbb{V}^*(n \hat p^*)
&=\mathbb{V}(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p} (1- \hat{p})\\[2ex]
\Rightarrow \mathbb{V}^*(\hat p^*) & = \frac{\hat{p}(1-\hat{p})}{n}
\end{align*}
$$

An appropriate central limit theorem argument implies that 
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$

Moreover, $\hat p$ is a consistent estimator of $p,$ and thus 
$$
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
$$ 
Therefore, $\hat p(1-\hat p)$ can be replaced asymptotically by $p(1-p)$, and
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
So, we can conclude that, 
$$
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0
$$
as $n\rightarrow\infty,$ where $\Phi$ denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large $n$
$$
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx 
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
$$
and therefore also
$$
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx 
\text{distribution}(\hat p -p).%\approx N(0,p(1-p)/n)
$$






### Confidence Intervals


::: {.callout-note}

## Recall: The traditional (non-bootstrap) approach {-}

Traditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if 

* $\theta\in\mathbb{R}$ and 
* $\sqrt{n}(\hat\theta-\theta)\rightarrow_d\mathcal{N}(0,v^2)$ as $n\to\infty,$ 

then one traditionally tries to determine an approximation $\hat v$ of $v$ (i.e. the standard error of $\hat\theta$) from the data. An approximate $(1-\alpha)\times 100\%$ confidence interval is then given by
$$
\left[
 \hat{\theta}-z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}},
 \hat{\theta}+z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}}
\right]
$$

In some cases it is, however, very difficult to obtain approximations $\hat v$ of $v$. Statistical inference is then usually based on the bootstrap.
::: 

In contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates $\hat v$ of $v$ are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed **more precise** than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)


#### The Bootstrap Approach {-}

<!-- General approach: Basic bootstrap $(1-\alpha)\times 100\%$ confidence interval -->

**Setup:**

* **Data:** i.i.d. random sample ${\cal S}_n:=\{X_1,\dots,X_n\}$ with $X_i\sim F$ for all $i=1,\dots,n$, where the distribution $F$ depends on the unknown parameter (vector) $\theta.$ 
* **Problem:** Construct a confidence interval for $\theta.$

::: {.callout-warning}

## Assumption: Bootstrap is consistent

In the following, we will assume that the bootstrap is consistent; i.e. that
$$
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^* -\hat{\theta})|{\cal S}_n)
&\approx 
\text{distribution}(\sqrt{n}(\hat{\theta}-\theta))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*-\hat{\theta})|{\cal S}_n
&\overset{d}{\approx} \sqrt{n}(\hat{\theta}-\theta)
\end{align*}
$$
if $n$ is sufficiently large. 

Caution: This is not always the case and in cases of doubt one needs to show this property. 
:::


**Derivation of the nonparametric bootstrap confidence intervals:**

* We can generate $m$ bootstrap estimates 
$$
\hat\theta_k^*\equiv\hat\theta(X_{1k}^*,\dots,X_{nk}^*),\quad k=1,\dots,m,
$$
by drawing bootstrap samples $X_{1k}^*,\dots,X_{nk}^*$ independently and with replacement from $\mathcal{S}_n=\{X_1,\dots,X_n\}.$


* The $m$ bootstrap estimates allow us to approximate the $\frac{\alpha}{2}$ quantile $\hat t_\frac{\alpha}{2}$ and the $1-\frac{\alpha}{2}$ quantile $\hat t_{1-\frac{\alpha}{2}}$ of the conditional distribution of $\hat{\theta}^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}.$ This can be done with negligible approximation error (for $m$ large) using the empirical quantiles 
$$
\hat t_{p}=\left\{
  \begin{array}{ll}
  \hat\theta^*_{(\lfloor mp\rfloor+1)},         & mp \text{ not a whole number}\\
  (\hat\theta^*_{(mp)}+\hat\theta^*_{(mp+1)})/2,& mp \text{ a whole number}
\end{array}\right.
$${#eq-empiricalQuantile}
for $p=\frac{\alpha}{2}$ or $p=1-\frac{\alpha}{2},$ where $\hat\theta_{(i)}^*$ denotes the order statistic 
$$
\hat\theta_{(1)}^* \leq \hat\theta_{(2)}^*\leq \dots\leq \hat\theta_{(m)}^*,
$$
and $\lfloor mp\rfloor$ denotes the greatest whole number less than or equal to $mp$ (e.g. $\lfloor 4.9\rfloor = 4$).

Then 
$$
\begin{align*}
&P^*\left(\hat t_\frac{\alpha}{2} \leq \hat{\theta}^* \leq \hat t_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P^*\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}^*-\hat{\theta} \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P^*\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)
\approx 1-\alpha,
\end{align*}
$$
where the approximation becomes arbitrarily precise for $m\to\infty.$ Here, $P^*$  denotes probabilities with respect to the   conditional distribution of  $\hat{\theta}^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}$.


Due to the assumed consistency of the bootstrap, we have that for large $n$
$$
{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}-\theta)}.
$$ 
Therefore, for large $n,$
$$
\begin{align*}
&P\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{blue}\sqrt{n}(\hat{\theta}-\theta)}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}-\theta \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(\hat{\theta}-(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\le \theta\le \hat{\theta}-
 (\hat t_\frac{\alpha}{2}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}\le \theta\le 2\hat{\theta}-
 \hat t_\frac{\alpha}{2}\right)\approx 1-\alpha.
\end{align*}
$$

Thus, the approximate $(1-\alpha)\times 100\%$ (symmetric) **bootstrap confidence interval** is given by 
$$
\left[2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}, 2\hat{\theta}-\hat t_\frac{\alpha}{2}\right],
$${#eq-NPBootCI}
where $\hat t_\frac{\alpha}{2}$ and $\hat t_{1-\frac{\alpha}{2}}$ are the  $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles of the bootstrap distribution approximated by the empirical quantiles of the $m$ bootstrap realizations $\hat{\theta}^*_1, \hat{\theta}^*_2,\dots, \hat{\theta}^*_m.$


#### Example: Confidence Intervals for the Population Mean {-}

**Setup:**

* **Data:** Let $X_1,\dots,X_n$ denote an i.i.d. random sample from $X\sim F$ with mean $\mu$ and variance $\sigma^2.$
  <!-- * In the following $F$ will denote the corresponding distribution function; i.e., $X_i\sim F$ for all $i=1,\dots,n.$ -->
* **Estimator:** $\bar X=\frac{1}{n} \sum_{i=1}^n X_i$ is an unbiased estimator of $\mu$
* **Inference Problem:** Construct a confidence interval for $\mu.$


::: {.callout-note}

## Recall: The traditional (non-bootstrap) approach {-}

Traditional, non-bootstrap approach for constructing a $(1-\alpha)\times 100\%$ confidence interval:

* By the CLT: $\bar X\overset{a}{\sim} \mathcal{N}(\mu,\frac{\sigma^2}{n})$ for large $n$
* Estimation of $\sigma^2$: $s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$
* This implies: $\sqrt{n}((\bar X -\mu)/s)\overset{a}{\sim} t_{n-1}$, and hence
$$
\begin{align*}
&P\left(-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right)\approx 1-\alpha\\
\Rightarrow 
&P\left(\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \mu\le 
        \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}
  \right)\approx 1-\alpha
\end{align*}
$$
* $(1-\alpha)\times 100\%$ confidence interval: 
$$
\left[\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}},
      \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right],
$$
where $t_{n-1,1-\frac{\alpha}{2}}$ denotes the $1-\frac{\alpha}{2}$-quantile of the $t$-distribution with $n-1$ degrees of freedom. 


::: {.callout-tip}

# Remark
This traditional construction relies on the assumption that $\bar X$ is exactly normal distributed, also for small $n,$ which requires that the random sample $X_1,\dots,X_n$ is i.i.d. *normally* distributed. 

If the underlying distribution is **not normal**, then this normal distribution holds *approximately* if the sample size $n$ is sufficiently large (central limit theorem), i.e.,
$$
\bar X \overset{a}{\sim}\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right).
$$ 
In this case the constructed confidence interval is an *approximate* $(1-\alpha)\times 100\%$ confidence interval. 
:::
:::

#### The Nonparametric (Standard) Bootstrap Approach {-}

The bootstrap offers an alternative method for constructing approximate $(1-\alpha)\times 100\%$ confidence intervals. We already know
that the bootstrap is consistent in this situation.

**Construction of the nonparametric (standard) bootstrap confidence interval:**

* Draw $m$ bootstrap samples (e.g. $m=10,000$) and calculate the corresponding estimates $\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m$.
* Compute the empirical quantiles $\hat t_{\frac{\alpha}{2}}$ and $\hat t_{1-\frac{\alpha}{2}}$ from $\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m$ 
* Compute the approximate $(1-\alpha)\times 100\%$ (symmetric) nonparametric bootstrap confidence interval as in @eq-NPBootCI:
$$
\left[2\bar X-\hat t_{1-\frac{\alpha}{2}}, 
      2\bar X-\hat t_\frac{\alpha}{2}\right]
$$




## Pivot Statistics and the Bootstrap-$t$ Method


In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-$t$ method (one also speaks of the "studentized bootstrap"). The construction relies on so-called pivotal statistics.

Let $X_1,\dots,X_n$ be an i.i.d. random sample and assume that the distribution of $X$ depends on an unknown parameter (or parameter vector) $\theta$.

::: {.callout-note icon=false} 
## 
::: {#def-pivotal}

## Asymptotically pivotal statistics
*A statistic 
$$
T_n\equiv T(X_1,\dots,X_n)
$$ 
is called (exact) **pivotal**, if the distribution of $T_n$ does not depend on any unknown population parameter. A statistic $T_n$ is called **asymptotically pivotal**, if the asymptotic distribution of $T_n$ does not depend on any unknown population parameter.*
:::
::: 


*Exact* pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an *asymptotically pivotal* statistic. Assume that an estimator $\hat{\theta}$ satisfies
$$
\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d\mathcal{N}(0,v^2),
$$
where $v^2$ denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator
$$
\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2
$$ 
of $v$ such that
$$
\hat v_n^2 \rightarrow_p v^2.
$$
Then, of course, also $\hat v_n\rightarrow_p v$, and
$$
T_n:= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
This means that 
$$
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
$$ 
is **asymptotically pivotal**.



#### Example: $\bar{X}$ is Asymptotically Pivotal {-}

Let $\mathcal{S}_n=\{X_1,\dots,X_n\}$ be a i.i.d. random sample with $X_i\sim X$ for all $i=1,\dots,n,$ with mean $\mathbb{E}(X)=\mu$, variance $\mathbb{V}(X)=\sigma^2>0$, and $\mathbb{E}(|X|^4)=\beta<\infty$. 
 
If $X$ is normally distributed, we obtain
$$
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\sim t_{n-1}
$$
with $s^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2$, where $t_{n-1}$ denotes the $t$-distribution with $n-1$ degrees of freedom. We can conclude that $T_n$ is pivotal.

If $X$ is *not* normally distributed, the central limit theorem implies that
$$
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
$$
In this case $T_n$ is an asymptotically pivotal statistics.


#### Bootstrap-$t$ Consistency {-}

The general idea of the bootstrap-$t$ method relies on approximating the unknown distribution of 
$$
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
$$ 
by the approximable (via bootstrap resampling) conditional distribution of
$$
T_n^*=\sqrt{n}\frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*},
$$ 
given $\mathcal{S}_n=\{X_1,\dots,X_n\},$ where the variance estimate $v_n^*$ is computed from the bootstrap sample $X_1^*,\dots,X_n^*,$ i.e.
$$
\hat v_n^*=v_n(X_1^*,\dots,X_n^*).
$$
 


::: {.callout-tip}
## Good news: Bootstrap-$t$ consistency follows if the standard nonparametric bootstrap is consistent 

If the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of $\sqrt{n}(\hat{\theta}^*-\hat{\theta})|\mathcal{S}_n$, given $\mathcal{S}_n$, yields a consistent estimate of $\mathcal{N}(0,v^2)$, then also the bootstrap-$t$ method is consistent. That is, then the conditional distribution of $T_n^*|\mathcal{S}_n$, given $\mathcal{S}_n$, provides a consistent estimate of the asymptotic distribution of $T_n\rightarrow_d \mathcal{N}(0,1)$ such that
$$
\sup_\delta \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
$$
where $\Phi$ denotes the distribution function of the standard normal distribution. 
:::


### Bootstrap-t Confidence Interval 

Let ${\cal S}_n:=\{X_1,\dots,X_n\}$ be an i.i.d. random sample from $X\sim F$ with unknown parameter (vector) $\theta.$ Assume that the bootstrap is consistent and that the estimator $\hat{\theta}$ of $\theta$ is asymptotically normal. Furthermore, suppose that a **consistent** estimator 
$$
\hat v\equiv \hat v(X_1,\dots,X_n)
$$ 
of the asymptotic standard error,  $v,$ of $\hat{\theta}$ is available.


**Derivation of the bootstrap-$t$ confidence interval:**

* Based on an i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $\mathcal{S}_n=\{X_1,\dots,X_n\},$ calculate the bootstrap estimates 
$$
\hat{\theta}^*\equiv \hat{\theta}^*(X_1^*,\dots,X_n^*)
$$ 
and
$$
v^*\equiv v^*(X_1^*,\dots,X_n^*)
$$ 
and the bootstrap statistic 
$$
\begin{align*}
T^*&=T^*(X_1^*,\dots,X_n^*)\\
   &=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}.
\end{align*}
$$ 
Repeating this yields $m$ (e.g. $m=100,000$) many bootstrap statistics 
$$
T_1^*,T_2^*, \dots, T_m^*
$$
which allow us to approximate the bootstrap distribution 
of $T^*=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*},$ conditionally on $\mathcal{S}_n,$ arbitrarily precise as $m\to\infty.$ 

* Approximate the $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_\frac{\alpha}{2}$ and $\hat \tau_{1-\frac{\alpha}{2}}$ of the bootstrap distribution of 
$$
\left.\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}\right|\mathcal{S}_n
$$ 
using the empirical quantiles based on $T_1^*,T_2^*, \dots, T_m^*$ (see @eq-empiricalQuantile).

This implies, for large $m,$
$$
\begin{align*}
&P^*\left(\hat \tau_\frac{\alpha}{2}\leq {\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha
\end{align*}
$$
Due to the assumed consistency of the bootstrap, we have that for large $n$ that 
$$
\left.{\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}}\right|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\frac{\hat{\theta}-\theta}{\hat v}}.
$$
Therefore, for lage $n,$ 
$$
\begin{align*}
& P\left(\hat \tau_\frac{\alpha}{2}\leq {\color{blue}\frac{\hat{\theta}-\theta}{\hat v}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta-\hat{\theta} \leq -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(\hat{\theta}-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta \leq \hat{\theta} -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha
\end{align*}
$$
Thus, the approximate $(1-\alpha)\times 100\%$ (symmetric) bootstrap-$t$ confidence interval is given by 
$$
\left[\hat{\theta}-\hat \tau_{1-\frac{\alpha}{2}}\hat v,\;
      \hat{\theta}-\hat \tau_{  \frac{\alpha}{2}}\hat v\right]
$${#eq-Boot_tCI}


#### Example: Bootstrap-$t$ Confidence Interval for the Mean {-}


**Algorithm:**

* Draw i.i.d. random samples $X_1^*,\dots,X_n^*$ from ${\cal S}_n$ and calculate
$\bar X^*$ as well as $s^*=\sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2}$ to generate $m$ (e.g. $m=100,000$) bootstrap realizations
$$
\frac{\bar X^*_1-\bar X}{s^*_1},\dots,\frac{\bar X^*_m-\bar X}{s^*_m}
$$ 
* Determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_\frac{\alpha}{2}$ and $\hat \tau_{1-\frac{\alpha}{2}}$ from $\frac{\bar X^*_1-\bar X}{s^*_1},\dots,\frac{\bar X^*_m-\bar X}{s^*_m}$ using @eq-empiricalQuantile.
* This yields the $1-\alpha$ confidence interval (using @eq-Boot_tCI):
$$
\left[\bar X-\hat \tau_{1-\frac{\alpha}{2}}s,
      \bar X-\hat \tau_{\frac{\alpha}{2}}s\right],
$$
where $s$ is computed from the original sample, i.e., 
$$
s=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2}. 
$$




### Accuracy of the Bootstrap-$t$ method 

Usually, the bootstrap-$t$ provides a **gain in accuracy** over the standard nonparametric bootstrap. The reason is that the approximation of the law of $T_n$ by the bootstrap law of 
$$
\left.\frac{\sqrt{n}(\hat{\theta}^*-\hat{\theta})}{v^*_n}\right|\mathcal{S}_n
$$ 
is more direct and hence more accurate ($v^*_n$ depends on the bootstrap sample --- not the original sample) than by the bootstrap law of 
$$
\left.\sqrt{n}(\hat{\theta}^*-\hat{\theta})\right|\mathcal{S}_n.
$$


The use of pivotal statistics and the corresponding construction of bootstrap-$t$ confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-$t$ methods are  **second order accurate**.

Consider generally $(1-\alpha)\times 100\%$ confidence intervals of the form $[L_n,U_n]$ of $\theta$. The lower, $L_n$, and upper bounds, $U_n$, of such intervals are determined from the data and are thus random, 
$$
L_n\equiv L(X_1,\dots,X_n)
$$
$$
U_n\equiv U(X_1,\dots,X_n)
$$ 
and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-$t$).


* (Symmetric) confidence intervals are said to be **first-order accurate** if there exist some constants $c_1,c_2<\infty$ such that for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta<L_n)-\frac{\alpha}{2}\right|\le \frac{c_1}{\sqrt{n}}\\
\left|P(\theta>U_n)-\frac{\alpha}{2}\right|\le \frac{c_2}{\sqrt{n}}
\end{align*}
$$
* (Symmetric) confidence intervals are said to be **second-order accurate** if there exist some constants $c_3,c_4<\infty$ such that
for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta<L_n)-\frac{\alpha}{2}\right|\le \frac{c_3}{n}\\
\left|P(\theta>U_n)-\frac{\alpha}{2}\right|\le \frac{c_4}{n}
\end{align*}
$$


If the distribution of $\hat\theta$ is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that

* Standard confidence intervals based on asymptotic approximations are **first-order** accurate. 
* Nonparametric (standard) boostrap confidence intervals are **first-order** accurate.
* Bootstrap-$t$ confidence intervals are **second-order** accurate.

The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to *much* better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.

::: {.callout-note}
Proofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field. 
:::

## Regression Analysis: Bootstrapping Pairs

Consider the linear regression model 
$$
Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
$$
where $Y_i\in\mathbb{R}$ denotes the response (or "dependent") variable and 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
$$
denotes the vector of predictor variables. In the following, we differentiate between a **random design** and a **fixed design**. 

::: {.callout-note icon=false} 
## 
::: {#def-RandomFixedDesign}

## Random and fixed design 
<br>

***Random Design:**
$$
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
$$
are i.i.d. random variables and $\mathbb{E}(\varepsilon_i|X_i)=0$ with either*

* ***homoscedastic** errors: $\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2$, $i=1,\dots,n$, for a constant $\sigma^2<\infty$ or* 
* ***heteroscedastic** errors: $\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2(X_i)<\infty$, $i=1,\dots,n.$*

***Fixed Design:** 
$$
X_1, X_2, \dots, X_n
$$
are deterministic vectors in $\mathbb{R}^p$ and $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d. random variables with zero mean $\mathbb{E}(\varepsilon_i)=0$ and **homoscedastic errors** $\mathbb{E}(\varepsilon_i^2)=\sigma^2$ for all $i=1,\dots,n.$*
:::
:::




The least squares estimator $\hat\beta\in\mathbb{R}^p$ is given by
$$
\begin{align*}
\hat\beta 
&=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\\
&=\beta+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
$$




### Bootstrapping Pairs: Bootstrap under Random Design


Under the random design, we additionally assume that there exists a positive definite (thus invertible) matrix $M$ 
$$
M=\mathbb{E}(X_iX_i^T)
$$
and a positive semi-definite matrix $Q$ such that
$$
Q=\mathbb{E}(\varepsilon_i^2X_iX_i^T)=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)
$$

::: {.callout-tip}
For homoscedastic errors we have 
$$
\begin{align*}
Q
&=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)\\
&=\sigma^2\mathbb{E}(X_iX_i^T)\, =\sigma^2 M.
\end{align*}
$$
:::

The law of large numbers, the continuous mapping theorem, Slutsky's theorem, and the central limit theorem (see your econometrics lecture) implies that
$$
\sqrt{n}(\hat\beta-\beta)\rightarrow_d\mathcal{N}(0,M^{-1}QM^{-1}),\quad n\to\infty.
$$

Bootstrapping regression estimates $\hat\beta$ is straightforward under a **random design** (@def-RandomFixedDesign). 


Under a random design, $(Y_i,X_i)$ are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of the estimation errors
$$
\hat\beta-\beta.
$$ 
In the literature this procedure is usually called **bootstrapping pairs**, namely, $(Y_i, X_i)$-pairs.

**Algorithm:**

* Original data: i.i.d. sample ${\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}$
* Generate bootstrap samples
  $$
  (Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
  $$ 
  by drawing observations independently and with replacement from ${\cal S}_n.$
* Each bootstrap sample $(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)$ leads to a bootstrap realization of the least squares estimator
$$
 \hat\beta^*=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
$$
 

It can be shown that bootstrapping pairs is **consistent**; i.e. that for large $n$
$$
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx\mathcal{N}(0,M^{-1}QM^{-1})
$$

#### Confidence Intervals {-}

This allows to construct basic bootstrap confidence intervals for the $j$th regression coefficient $\beta_j$, $j=1,\dots,p$:

* Generate $m$ (e.g. $m=100,000$) bootstrap realizations
$$
\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*
$$

* Determine the $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles 
$\hat t_{\frac{\alpha}{2},j}$ 
and 
$\hat t_{1-\frac{\alpha}{2},j}$  
from the bootstrap realizations $\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*$ using @eq-empiricalQuantile.

* Compute the approximate $(1-\alpha)\times 100\%$ (symmetric) confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j}, 
      2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}\right]
$$


::: {.callout-note}

## Remark 

This basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are **heteroscedastic**. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages.
::: 

## Regression Analysis: Residual bootstrap

If the sample 
$$
(Y_1,X_1),\dots,(Y_n,X_n)
$$ 
is **not** an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for **fixed designs** and also generally not in time-series regression contexts. However, if error terms are **homoscedastic**, then it is possible to rely on the **residual bootstrap**.


In the following we will formally assume a regression model
$$
Y_i=X_i^T\beta+ \varepsilon_i, \quad i=1,\dots,n,
$$
with 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
$$
under **fixed design** (@def-RandomFixedDesign), i.e., where $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d. with zero mean $\mathbb{E}(\varepsilon_i)=0$ and **homoscedastic** errors
$$
\mathbb{E}(\varepsilon_i^2)=\sigma^2.
$$ 


::: {.callout-tip}

## Applicability of the Residual Bootstrap 

Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated $X$-variables (time-series). In these cases all arguments are meant conditionally on the given $X_1,\dots,X_n$. The above assumptions on the error terms then of course have to be satisfied conditionally on $X_1,\dots,X_n$.
:::

The idea of the residual bootstrap is very simple: The model implies that the error terms 
$$
\varepsilon_1,\dots,\varepsilon_n
$$ 
are i.i.d which suggests a bootstrap based on resampling the error terms. 

These errors are, of course, unobserved, but they can be approximated by the corresponding residuals
$$
\hat \varepsilon_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,
$$
where again 
$$
\hat\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
$$ 
denotes the least squares estimator. 

It is well known that
$$
\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2
$$ 
provides an unbiased, consistent estimator of the error variance $\sigma^2$. That is,
$$
\mathbb{E}(\hat\sigma^2)=\sigma^2 \quad \text{and}\quad \hat\sigma^2\rightarrow_p \sigma^2.
$$


#### The Residual Bootstrap Algorithm {-}

Based on the original data $(Y_i,X_i)$, $i=1,\dots,n$, and the least squares estimate $\hat\beta$, calculate the residuals $\hat\varepsilon_1,\dots,\hat \varepsilon_n$.

1. Generate random bootstrap samples $\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*$ of residuals by drawing observations independently and with replacement
from 
$$
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
$$
2. Calculate new depend variables 
$$
Y_i^*=X_i^T\hat\beta+\hat\varepsilon_i^*,\quad i=1,\dots,n
$$
3. Bootstrap estimators $\hat\beta^*$ are determined by least squares estimation from the data $(Y_1^*,X_1),\dots,(Y_n^*,X_n)$:
$$
\hat\beta^*=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
$$

Repeating Steps 1-3 $m$ many times yields $m$ (e.g. $m=100,000$) bootstrap estimators 
$$
\hat\beta^*_1,\hat\beta^*_2,\dots,\hat\beta^*_m
$$
which allow us to approximate the bootstrap distribution $\hat\beta^*-\hat\beta|\mathcal{S}_n$ arbitrarily well as $m\to\infty.$


#### Motivating the Residual Bootstrap {-}

It is not difficult to understand why the residual bootstrap generally works for *homoscedastic* (!) errors. We have
$$
\hat\beta-\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
$$
and for large $n$ the distribution of $\sqrt{n}(\hat\beta-\beta)$ is approximately normal with mean 0 and covariance matrix $\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}$
$$
\sqrt{n}(\hat\beta-\beta)\to_d\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)
$$

On the other hand (the bootstrap world), we have construction
$$
\hat\beta^*-\hat\beta
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*
$$
Conditional on ${\cal S}_n,$ the bootstrap error terms are i.i.d with
$$
\mathbb{E}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
$$
and
$$
\mathbb{V}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2.
$$
An appropriate central limit theorem argument implies that 
$$
\left.\sqrt{n}(\hat\beta^*-\hat\beta)\right|\mathcal{S}_n\to_d\mathcal{N}\left(0,\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right),
$$
for $n\to\infty.$

Since  
$$
\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2\rightarrow_p \sigma^2
$$ 
as $n\rightarrow\infty$, the bootstrap is consistent. That is, for large $n$, we have approximately
$$
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta-\beta))}_{\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)}
$$


### Bootstrap confidence intervals for the regression coefficients


#### Nonparametric bootstrap confidence intervals {-}

Basic **nonparametric bootstrap** confidence intervals for the regression coefficients $\beta_j$, $j=1,\dots,p,$ can be constructed as following: 

Approximate the $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat t_{\frac{\alpha}{2},j}$ and $\hat t_{1-\frac{\alpha}{2},j}$ of the bootstrap distribution of  $\hat\beta_j^*$ using the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles (see @eq-empiricalQuantile) based on the $m$ bootstrap estimates 
$$
\hat\beta_{j1}^*,\hat\beta_{j2}^*, \dots, \hat\beta_{jm}^*.
$$
<!-- This approximation step is with arbitrary accuracy as $m\to\infty.$ -->

Compute the approximate $(1-\alpha)\times 100\%$ (symmetric) nonparametric bootstrap confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j}, 
      2\hat\beta_j-\hat t_{ \frac{\alpha}{2},j }\right]
$$


#### Bootstrap-$t$ confidence intervals {-}

Bootstrap-$t$ confidence intervals for the regression coefficients $\beta_j$, $j=1,\dots,p,$ can be constructed as following: 

Let $\gamma_{jj}$ denote the $j$-th diagonal element of the matrix $(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$, i.e.,
$$
\gamma_{jj}:=\left[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right]_{jj}.
$$
Then
$$
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}
$$ 
with 
$$
\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}
$$
is an asymptotically pivotal statistic, 
$$
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
$$

A bootstrap-$t$ interval for $\beta_j$, $j=1,\dots,p$, can thus be constructed as follows:

Approximate the $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_{\frac{\alpha}{2},j}$ and $\hat \tau_{1-\frac{\alpha}{2},j}$  of the bootstrap distribution of
$$
T^*=\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}
$$
with 
$$
\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^{*2},
$$ 
using the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles  (see @eq-empiricalQuantile) based on the $m$ bootstrap realizations
$$
T^*_1,T_2^*,\dots, T_m^*.
$$

Compute the $(1-\alpha)\times 100\%$ bootstrap-$t$ confidence interval as in @eq-Boot_tCI:
$$
\left[
  \hat\beta_j-\hat \tau_{1-\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}},\; 
  \hat\beta_j-\hat \tau_{\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}}
\right],
$$
where $\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}.$

::: {.callout-tip}
There are many more bootstrap procedures. In case of heteroscedastic errors, for instance, there's also the "Wild Bootstrap." 

For high-dimensional problems ($p$ as large as $n$ or larger), one can use (under certain regularity assumptions) the "Multiplier Bootstrap".
:::

## Exercises {-}


#### Exercise 1. {-} 

Consider the empirical distribution function 
$$
F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
$$
for a random sample 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} F.
$$

(a) Derive the exact distribution of $nF_n(x)$ for a given $x\in\mathbb{R}.$ 

(b) Derive the asymptotic distribution of $F_n(x)$ for a given $x\in\mathbb{R}.$  

(c) Show that $F_n(x)$ is a point-wise (weakly) consistent estimator of $F(x)$ for each given $x\in\mathbb{R}$.


#### Exercise 2. {-} 

::: {.callout-tip}
Exercise 1 shows that the empirical distribution function is a **point-wise** consistent estimator for each given $x\in\mathbb{R}.$ However, point-wise consistency generally does not imply **uniformly** consistency for all $x\in\mathbb{R},$ and therefore the Clivenko-Cantelli (@thm-Clivenko-Cantelli) is so famous.  

This exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.
:::  

Point-wise convergence of a function $g_n(x),$ i.e.,
$$
|g_n(x) - g(x)|\to 0
$$ 
for each $x\in\mathcal{X}\subset\mathbb{R}$ as $n\to\infty$ generally does not imply uniform convergence, i.e., 
$$
\sup_{x\in\mathcal{X}}|g_n(x) - g(x)|\to 0
$$ 
as $n\to\infty.$ 

Show this by providing an example for $g_n$ which converges point-wise, but not uniformly for $x\in\mathcal{X}$. 

<!-- 
http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln03.pdf 
-->


#### Exercise 3. {-} 

Consider the following setup:

*  iid data $X_1,\dots,X_n$ with $X_i\sim F$
*  $\mathbb{E}(X_i)=\mu$
*  $\mathbb{V}(X_i)=\sigma^2<\infty$
*  Estimator: $\bar{X}_n=n^{-1}\sum_{i=1}^nX_i$

(a) Derive the classic confidence interval for $\mu$ using the asymptotic normality of the estimator $\bar{X}.$ Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of $n=20$ and, 

* Part 1: For $F$ being the normal distribution with $\mu=1$ and standard deviation $\sigma=2$, and 
* Part 2: For $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. 

(b) Reconsider the case of $n=20$ and $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.  

(c) Reconsider the case of $n=20$ and $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-$t$ confidence interval.


#### Exercise 4. {-}

<!-- Computational Statistics, James E. Gentle,  Exercise 13.1. -->

Let $\mathcal{S}_n = \{Y_1 , \dots, Y_n\}$ be a random sample from a population with mean $\mu$, variance $\sigma^2,$ and distribution function $F.$ Let $F_n$ be the empirical distribution function. Let $\bar{Y}$ be the sample mean for $\mathcal{S}_n.$ Let $\mathcal{S}^*_n = \{Y_1^∗,\dots, Y_n^∗\}$ be a random sample taken independently and with replacement from $\mathcal{S}_n.$ Let $\bar{Y}^*$ be the sample mean for $\mathcal{S}^*_n.$


(a) Show that 
$$
\mathbb{E}^*(\bar{Y}^*) = \bar{Y}
$$

(b) Show that 
$$
\mathbb{E}(\bar{Y}^*) = \mu
$$


<!-- 
#### Exercise 5. {-}
Computational Statistics, James E. Gentle,  Exercise 13.6. 
-->

<!-- {{< include Ch3_Solutions.qmd >}} -->


## Solutions {-}

#### Solutions of Exercise 1. {-} 

##### (a) {-}

The exact point-wise distribution of $nF_n(x)$ for a given $x\in\mathbb{R}.$  
$$
\begin{align*}
F_n(x) 
& = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\
\Rightarrow nF_n(x) 
& = \sum_{i=1}^n 1_{(X_i\leq x)} \sim \mathcal{Binom}\left(n,p=F(x)\right),
\end{align*}
$$
since $1_{(X_i\leq x)}$ is a Bernoulli random variable with parameter 
$$
\begin{align*}
p 
& = P(1_{(X_i\leq x)} = 1) 
& = P(X_i \leq x) 
& = F(x).
\end{align*}
$$

##### (b) {-}

<!-- The asymptotic  point-wise distribution of $F_n(x)$ for a given $x\in\mathbb{R}.$   -->
From (a), we have that 
$$
\begin{align*}
\mathbb{E}(nF_n(x)) &= nF(x)\\ 
\Leftrightarrow\quad  \mathbb{E}(F_n(x)) &= F(x)
\end{align*}
$$
and that 
$$
\begin{align*}
\mathbb{V}(nF_n(x)) &= nF(x)(1-F(x))\\
\Leftrightarrow \quad \mathbb{V}(F_n(x)) &= \frac{F(x)(1-F(x))}{n}.
\end{align*}
$$

Moreover, since $F_n(x)  = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}$ is an average over i.i.d. random variables $1_{(X_1\leq x)},\dots,1_{(X_n\leq x)},$ the standard CLT implies 
$$
\frac{F_n(x)-F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}}\to_d\mathcal{N}(0,1).
$$
Or with a slight abuse of notation: 
$$
F_n(x)\overset{a}{\sim}\mathcal{N}\left(F(x),\frac{F(x)(1-F(x))}{n}\right).
$$


##### (c) {-}


The mean squared error between $F_n(x)$ and $F(x)$ is given by
$$
\begin{align*}
\operatorname{MSE}(F_n(x)) 
&= \mathbb{E}\left((F_n(x)-F(x))^2\right)\\[2ex]
&= \mathbb{V}(F_n(x)) + \left(\mathbb{E}(F_n(x))-F(x)\right)^2.
\end{align*}
$$
It follows from our previous results that for each $x\in\mathbb{R}$
$$
\mathbb{V}(F_n(x)) = \frac{F(x)(1-F(x))}{n} \to 0 
$$
as $n\to\infty,$ and that 
$$
\mathbb{E}(F_n(x)) -F(x) = 0 
$$
for all $n.$ Therefore, 
$$
\operatorname{MSE}(F_n(x)) = \mathbb{V}(F_n(x)) \to 0
$$
as $n\to\infty.$ Thus we can conclude that $F_n(x)$ converges in the mean-square sense to $F(x)$ for each $x\in\mathbb{R},$ 
$$
F_n(x)\to_{ms} F(x)
$$
as $n\to\infty.$ 

Since convergence in the mean square sense implies convergence in probability, we also have that for each $x\in\mathbb{R}$
$$
F_n(x)\to_{p} F(x)
$$
as $n\to\infty$ which shows that $F_n(x)$ is weakly consistent for $F(x)$ for each $x\in\mathbb{R}.$



#### Solutions of Exercise 2. {-} 

::: {.callout-tip}
Another, equivalent way to define uniform convergence: 

$g_n(\cdot)$ converges **uniformly** to $g(\cdot)$ if for every $\varepsilon>0,$ there exists an $N$ such that 
$$
|g_n(x) - g(x)| < \varepsilon 
$$ 
for all $n\geq N$ and **for all** $x\in\mathcal{X}.$ 


I.e., $g_n(\cdot)$ converges **uniformly** to $g(\cdot)$ if it is possible to draw an $\varepsilon$-band around the graph of $g(x)$ that contains **all of the graphs** of $g_n(x)$ for large enough $n.$
::: 

**Example 1:** $\mathcal{X}=\mathbb{R}$<br>
The function 
$$
g_n(x) = x\left(1+\frac{1}{n}\right)
$$ 
converges point-wise to 
$$
g(x)=x,
$$ 
since 
$$
|g_n(x)-g(x)|=\frac{|x|}{n}%\to 0\quad \text{as}\quad n\to\infty.
$$
converges to zero as $n\to\infty$ for each given $x\in\mathcal{X}.$

However, $g_n$ does not converge uniformly to $g$ since
$$
\sup_{x\in\mathbb{R}}|g_n(x)-g(x)|=\sup_{x\in\mathbb{R}}\frac{|x|}{n}=\infty\neq 0
$$
for each $n.$ 

Note that for a small $\varepsilon> 0,$ an $\varepsilon$-band around $g(x) = x$ fails to capture the graphs of $g_n(x)=x(1+1/n).$ 


**Example 2:** $\mathcal{X}=(0,1)$<br>
The function 
$$
g_n(x) = x^n
$$ 
converges point-wise to 
$$
g(x)=0,
$$ 
since 
$$
|g_n(x)-g(x)|=x^n
$$
converges to zero as $n\to\infty$ for each given $x\in(0,1).$

However, $g_n$ does not converge uniformly to $g$ since
$$
\sup_{x\in(0,1)}|g_n(x)-g(x)|=\sup_{x\in(0,1)}x^n=1\neq 0
$$
for each $n.$ 

Note that for a small $\varepsilon> 0,$ an $\varepsilon$-band around $g(x) = 0$ fails to capture the graphs of $g_n(x)=x^n.$ 



#### Solutions of Exercise 3. {-}

Link to the video: [HERE](https://www.dropbox.com/s/w6pdr98s04hyk4t/Ch3_Ex3.mp4?dl=0)

##### (a) Part 1: {-}

Setup:

*  iid data $X_1,\dots,X_n$ with $X_i\sim F$
*  $\mathbb{E}(X_i)=\mu$
*  $\mathbb{V}(X_i)=\sigma^2<\infty$
*  Estimator: $\bar{X}_n=n^{-1}\sum_{i=1}^nX_i$

If $F$ is a normal distribution:


$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\sim \mathcal{N}(0,1)\quad\text{for all}\;n.
\end{array}
$$

For non-normal distributions $F$ we have by the classic CLT:
$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
$$

Usually, we do not know $\sigma$ and have to estimate this parameter using a consistent estimator such as $s^2=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2$, where $s\to_p\sigma$ as $n\to\infty$.


Then by Slusky's Theorem (allows to combine "$\to_d$" and "$\to_p$" statements) we have that: 
$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{s}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
$$


The **classic confidence interval** is then based on the above (asymptotic) normality result:
$$
\operatorname{CI}_{\operatorname{classic},n}=\left[\bar{X}_n\,-\,z_{1-\alpha/2}\frac{s}{\sqrt{n}},\bar{X}_n\,+\,z_{1-\alpha/2}\frac{s}{\sqrt{n}}\right],
$$
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$-quantile of the standard normal distribution. Alternatively, one can apply a "small-sample correction" by using the $(1-\alpha/2)$-quantile $t_{n-1, 1-\alpha/2}$ of the $t$-distribution with $n-1$ degrees of freedom.  


From the above arguments it follows that:
$$
P\left(\mu\in \operatorname{CI}_{\operatorname{classic},n}\right)\to 1-\alpha\quad\text{as}\quad n\to\infty.
$$

Let us consider the finite-$n$ (with $n=20$) performance of the classic confidence interval for the case where $F$ is a **normal distribution** with mean $\mu=1$ and standard deviation $\sigma=2$:
```{r}
##  Setup:
n     <-   20 # Sample Size
mean  <-    1 # Mean
sdev  <-    2 # Standard Deviation
alpha <- 0.05 # Level

set.seed(123)
B          <- 1500 # MC repetitions
CI.lo.vec  <- rep(NA, B)
CI.up.vec  <- rep(NA, B)
  
## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  X.sample     <- rnorm(n=n, mean = mean, sd = sdev) 
  ## Estimates:
  X.bar.MC     <- mean(X.sample)
  sd.hat.MC    <- sd(X.sample)
  ## Classic CIs:
  
  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))

  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Classic 95% Confidence Intervals\n(Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==TRUE], 
       x1=CI.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==FALSE], 
       x1=CI.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (a) Part 2: Classic Confidence Interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


Now, we consider the  finite-$n$ performance of the classic confidence interval under the same setup as above, but for the case where $F$ is a **non-normal distribution**, namely, a $\chi^2_1$-distribution with $1$ degree of freedom:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
alpha <- 0.05 # Level

set.seed(123)
B          <- 1500 # MC repetitions
CI.lo.vec  <- rep(NA, B)
CI.up.vec  <- rep(NA, B)
  
## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  X.sample     <- rchisq(n, df=df)
  ## Estimates:
  X.bar.MC     <- mean(X.sample)
  sd.hat.MC    <- sd(X.sample)
  ## Classic CIs:
  
  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  
  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Classic 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==TRUE], 
       x1=CI.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==FALSE], 
       x1=CI.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (b) Standard bootstrap confidence interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


Let's generate an iid random sample $S_n$ with $X_i\sim\chi^2_1$ and the corresponding estimate $\bar X_n$:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)

## IID random sample:
set.seed(123)
S_n  <- rchisq(n, df=df)

## Empirical mean:
(X.bar <- mean(S_n))
```



The **standard bootstrap confidence interval** is given by (see lecture script):
$$
\left[2\bar{X}_n - \hat{t}_{1-\alpha/2}, 2\bar{X}_n - \hat{t}_{\alpha/2}\right],
$$
where $\hat{t}_{\alpha/2}$ and $\hat{t}_{1-\alpha/2}$ denote the $(\alpha/2)$ and $(1-\alpha/2)$-quantiles of the conditional distribution of $\bar{X}_n^\ast$ given $\mathcal{S}_n=\left\{X_1,\dots,X_n\right\}$, i.e., of the **bootrap distribution** of $\bar{X}_n^\ast$. 

In the following we approximate the bootstrap distribution of $\bar{X}_n^\ast$ using $m=1500$ boostrap resamplings, compute the quantiles $\hat{t}_{\alpha/2}$ and $\hat{t}_{1-\alpha/2}$, and plot all of this:

```{r, fig.margin = TRUE,fig.width=4.5, fig.height=3.5}
## Bootstr-Setup:
alpha            <- 0.05
n.Bootsrap.draws <- 1500

## Generate bootstap samples:
Bootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)

for(j in 1:n.Bootsrap.draws){
  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)
}
## Boostrap draws of \bar{X}_n^*:
X.bar.bootstr.vec <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)

## Quantile of the bootstr.-distribution of \bar{X}_n^*:
t.1 <- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)
t.2 <- quantile(X.bar.bootstr.vec, probs = alpha/2)
## plot
plot(ecdf(X.bar.bootstr.vec), xlab="", ylab="",
     main=expression(paste("Bootstr.-Distr. of ",bar(X)[n]^{" *"})))
abline(v=c(t.1,t.2),col="red")
```


Using our preparatory work above, the standard bootstrap confidence interval can be computed as following:
```{r}
## Basic Bootstrap Confidence Interval:
CI.Basic.Bootstr.lo <- 2*X.bar - t.1
CI.Basic.Bootstr.up <- 2*X.bar - t.2

## Re-labeling of otherwise false names:
attr(CI.Basic.Bootstr.lo, "names") <- c("2.5%")
attr(CI.Basic.Bootstr.up, "names") <- c("97.5%")
##
c(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)
```


Now, we can investigate the finite-$n$ performance of the standard bootstrap confidence interval:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
mean  <-   df
alpha <- 0.05 # Level
n.Bootsrap.draws <- 1500

## MC-Setup:
set.seed(123)
B          <- 1500 # MC repetitions
CI.Basic.Bstr.lo.vec <- rep(NA, B)
CI.Basic.Bstr.up.vec <- rep(NA, B)

## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  S_n.MC        <- rchisq(n, df=df)
  ## Estimate:
  X.bar.MC      <- mean(S_n.MC)
  ## 
  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)
  for(j in 1:n.Bootsrap.draws){
    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)
  }
  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)
  ## (1-alpha/2)-quantile:
  t.1.MC <- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)
  t.2.MC <- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)
  ## Basic Bootstrap CIs:
  CI.Basic.Bstr.lo.vec[b] <- 2*X.bar.MC - t.1.MC
  CI.Basic.Bstr.up.vec[b] <- 2*X.bar.MC - t.2.MC
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.Basic.Bstr.lo.vec<=mean & mean<=CI.Basic.Bstr.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), 
     ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Basic Bootrap 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], 
       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], 
       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (c) Bootstrap-$t$ confidence interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


The bootstrap-t confidence interval is given by (see lecture script):
$$
\left[\bar{X}_n-\hat{\tau}_{1-\alpha/2}\hat\sigma,  \bar{X}_n-\hat{\tau}_{\alpha/2}\hat\sigma\right],
$$
where $\hat\sigma=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2$, and where $\hat{\tau}_{\alpha/2}$ and $\hat{\tau}_{1-\alpha/2}$ denote the $(\alpha/2)$ and the $(1-\alpha/2)$-quantiles of the bootstrap distribution of: 
$$
\frac{\bar{X}_n^\ast-\bar{X}_n}{\hat\sigma^\ast}.
$$


In the following we approximate the bootstrap distribution of $(\bar{X}_n^\ast-\bar{X}_n)/\hat\sigma^\ast$, compute the quantiles $\hat{\tau}_{\alpha/2}$ and $\hat{\tau}_{1-\alpha/2}$, and plot all of this:

```{r, fig.margin = TRUE, fig.width=4.5, fig.height=3.5}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)

## IID random sample:
set.seed(123)
S_n  <- rchisq(n, df=df)

## Empirical mean and sd:
X.bar   <- mean(S_n)
sd.hat  <- sd(S_n)

## Bootstr-Setup:
alpha            <- 0.05
n.Bootsrap.draws <- 1500

## Generate bootstap samples:
Bootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)

for(j in 1:n.Bootsrap.draws){
  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)
}
## Compute boostrap draws of (\bar{X}_n^*-\bar{X}_n)/\hat{\sigma}^\ast:
X.bar.bootstr.vec    <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)
sd.bootstr.vec       <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)
##
Bootstr.t.sample.vec <- (X.bar.bootstr.vec - X.bar)/sd.bootstr.vec
## Quantile of the bootstr.-distribution of \bar{X}_n^*:
tau.1 <- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)
tau.2 <- quantile(Bootstr.t.sample.vec, probs = alpha/2)
## plot
plot(ecdf(Bootstr.t.sample.vec), xlab="", ylab="",
     main=expression(paste("Bootstr.-t-Distr. of ",
          (bar(X)[n]^{" *"}-bar(X)[n])/hat(sigma)^{"*"})))
abline(v=c(tau.1,tau.2),col="red")
```



Using our preparatory work above, the basic bootstrap confidence interval can be computed as following:
```{r}
## Basic Bootstrap Confidence Interval:
CI.Bstr.t.lo <- X.bar - tau.1 * sd.hat
CI.Bstr.t.up <- X.bar - tau.2 * sd.hat

## Re-labeling of otherwise false names:
attr(CI.Bstr.t.lo, "names") <- c("2.5%")
attr(CI.Bstr.t.up, "names") <- c("97.5%")
##
c(CI.Bstr.t.lo, CI.Bstr.t.up)
```



Let us investigate the finite-$n$ performance of the bootstrap-t confidence interval:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
mean  <-   df
alpha <- 0.05 # Level
n.Bootsrap.draws <- 1500

## MC-Setup:
set.seed(123)
B          <- 1500 # MC repetitions
CI.Bstr.t.lo.vec <- rep(NA, B)
CI.Bstr.t.up.vec <- rep(NA, B)

## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  S_n.MC        <- rchisq(n, df=df)
  ## Estimates:
  X.bar.MC      <- mean(S_n.MC)
  sd.MC         <- sd(S_n.MC)
  ## 
  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)
  for(j in 1:n.Bootsrap.draws){
    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)
  }
  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)
  sd.bootstr.MC.vec    <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)
  ## Make it a "Bootstrap-t" sample:
  Bootstr.t.MC.vec <- (X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec
  ## (1-alpha/2)-quantile:
  tau.1.MC <- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)
  tau.2.MC <- quantile(Bootstr.t.MC.vec, probs = alpha/2)
  ## Basic Bootstrap CIs:
  CI.Bstr.t.lo.vec[b] <- X.bar.MC - tau.1.MC * sd.MC
  CI.Bstr.t.up.vec[b] <- X.bar.MC - tau.2.MC * sd.MC
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.Bstr.t.lo.vec<=mean & mean<=CI.Bstr.t.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), 
     ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Bootrap-t 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], 
       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], 
       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```



#### Solutions of Exercise 4. {-}

Link to the video: [HERE](https://www.dropbox.com/s/upsl5hggr0jcrgb/Ch3_Ex4.mp4?dl=0)

##### (a)  {-}

$$
\begin{align*}
\mathbb{E}^*(\bar{Y}^*) 
& = \mathbb{E}\left(\left.\bar{Y}^*\right|\mathcal{S}_n\right)\\[2ex]
& = \mathbb{E}\left(\left.\frac{1}{n}\sum_{i=1}^n Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \sum_{i=1}^n \frac{1}{n} Y_i
 = \bar{Y}
\end{align*}
$$
since $(Y_i^*|\mathcal{S}_n)\in\{Y_1,\dots,Y_n\}$ and $P(Y_j^*=Y_i|\mathcal{S}_n)=\frac{1}{n}$ for each $i,j\in 1,\dots,n.$


##### (b)  {-}


$$
\begin{align*}
\mathbb{E}(\bar{Y}^*) 
& = \mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n Y_i^*\right)\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(Y_i^*\right)\\[2ex]
& = \mathbb{E}\left(Y_i^*\right)\\[2ex]
& = \mu
\end{align*}
$$
since $Y_i^*\sim Y_i\sim F.$ 


## References {-}